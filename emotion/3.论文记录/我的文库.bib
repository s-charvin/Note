@inproceedings{aftabLIGHTSERNETLightweightFully2022,
  title = {{{LIGHT-SERNET}}: {{A Lightweight Fully Convolutional Neural Network}} for {{Speech Emotion Recognition}}},
  shorttitle = {{{LIGHT-SERNET}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Aftab, Arya and Morsali, Alireza and Ghaemmaghami, Shahrokh and Champagne, Benoit},
  date = {2022},
  pages = {6912--6916},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746679},
  abstract = {Detecting emotions directly from a speech signal plays an important role in effective human-computer interactions. Existing speech emotion recognition models require massive computational and storage resources, making them hard to implement concurrently with other machine-interactive tasks in embedded systems. In this paper, we propose an efficient and lightweight fully convolutional neural network for speech emotion recognition in systems with limited hard-ware resources. In the proposed FCNN model, various feature maps are extracted via three parallel paths with different filter sizes. This helps deep convolution blocks to extract high-level features, while ensuring sufficient separability. The extracted features are used to classify the emotion of the input speech segment. While our model has a smaller size than that of the state-of-the-art models, it achieves a higher performance on the IEMOCAP and EMO-DB datasets. The source code is available https://github.com/AryaAftab/LIGHT-SERNET},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,Computational modeling,Convolution,convolutional neural network,Convolutional neural networks,Emotion recognition,Feature extraction,lightweight model,Mel frequency Cep-strum coefficient (MFCC),ObsCite,Speech coding,Speech emotion recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Aftab et al_2022_LIGHT-SERNET.pdf}
}

@article{ahnCrossCorpusSpeechEmotion2021,
  title = {Cross-{{Corpus Speech Emotion Recognition Based}} on {{Few-Shot Learning}} and {{Domain Adaptation}}},
  author = {Ahn, Youngdo and Lee, Sung Joo and Shin, Jong Won},
  date = {2021},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {28},
  pages = {1190--1194},
  issn = {1558-2361},
  doi = {10.1109/LSP.2021.3086395},
  abstract = {Within a single speech emotion corpus, deep neural networks have shown decent performance in speech emotion recognition. However, the performance of the emotion recognition based on data-driven learning methods degrades significantly for the cross-corpus scenario. To relieve this issue without any labeled samples from the target domain, we propose a cross-corpus speech emotion recognition based on few-shot learning and unsupervised domain adaptation, which is trained to learn the class (emotion) similarity from the source domain samples adapted to the target domain. In addition, we utilize multiple corpora in training to enhance the robustness of the emotion recognition to the unseen samples. Experiments on emotional speech corpora with three different languages showed that the proposed method outperformed other approaches.},
  eventtitle = {{{IEEE Signal Processing Letters}}},
  language = {en},
  keywords = {Cross-corpus,cross-lingual,Databases,Emotion recognition,Feature extraction,few-shot learning,Measurement,Neural networks,speech emotion recognition,Speech recognition,Training,unsupervised domain adaptation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ahn et al_2021_Cross-Corpus Speech Emotion Recognition Based on Few-Shot Learning and Domain.pdf}
}

@inproceedings{ahnMultiCorpusSpeechEmotion2022,
  title = {Multi-{{Corpus Speech Emotion Recognition}} for {{Unseen Corpus Using Corpus-Wise Weights}} in {{Classification Loss}}},
  booktitle = {Interspeech 2022},
  author = {Ahn, Youngdo and Lee, Sung Joo and Shin, Jong Won},
  date = {2022-09-18},
  pages = {131--135},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-111},
  url = {https://www.isca-speech.org/archive/interspeech_2022/ahn22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ahn et al_2022_Multi-Corpus Speech Emotion Recognition for Unseen Corpus Using Corpus-Wise.pdf}
}

@inproceedings{ahnRecurrentMultiheadAttention2022,
  title = {Recurrent Multi-Head Attention Fusion Network for Combining Audio and Text for Speech Emotion Recognition},
  booktitle = {Interspeech 2022},
  author = {Ahn, Chung-Soo and Kasun, Chamara and Sivadas, Sunil and Rajapakse, Jagath},
  date = {2022-09-18},
  pages = {744--748},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-888},
  url = {https://www.isca-speech.org/archive/interspeech_2022/ahn22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ahn et al_2022_Recurrent multi-head attention fusion network for combining audio and text for.pdf}
}

@article{almekhlafiClassificationBenchmarkArabic2022,
  title = {A Classification Benchmark for {{Arabic}} Alphabet Phonemes with Diacritics in Deep Neural Networks},
  author = {Almekhlafi, Eiad and AL-Makhlafi, Moeen and Zhang, Erlei and Wang, Jun and Peng, Jinye},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101274},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101274},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000772},
  urldate = {2021-09-28},
  abstract = {Although the Arabic language is the fourth most popular language in the world, it has not received sufficient attention in artificial intelligence research, especially in automatic speech recognition (ASR). The key feature of the Arabic language is that its words are pronounced exactly as they are written. Above all, taking into account the diacritics,22Throughout this paper, the alphabet is considered with diacritics.~there are no words with similar pronunciation and writing. This motivates us to think of building an Arabic ASR system by recognizing its alphabet phonetics. Therefore, the Arabic alphabet phonemes classification must be studied, this is what the paper aims to achieve. In this paper, we create a new dataset, called Arabic alphabet phonetics dataset (AAPD). AAPD was collected by taking sound recordings of 1420 persons. We build several Arabic alphabet phonemes classification systems using three feature extraction techniques and four deep neural networks. Based on AAPD, we designed numerous experiments to compare the performance of feature extraction and classification methods, which can be used as a benchmark. Experimental results showed that Mel-frequency Cepstral Coefficient (MFCC) is considered most effective to feature extraction due to its highest accuracy, particularly when using 20 for Mel-bands number the training time is the least. Additionally, the appropriate model that achieved the highest accuracy with the least computational load is the proposed model VGG\textendash based, where acquired an accuracy of 95.68\%.},
  language = {en},
  keywords = {Arabic alphabet phonemes,Auto speech recognition,Deep neural networks,Mel-frequency Cepstral Coefficient,Mel-spectrogram},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Almekhlafi et al_2022_A classification benchmark for Arabic alphabet phonemes with diacritics in deep.pdf}
}

@article{alomariDeepReinforcementTransfer2022,
  title = {Deep Reinforcement and Transfer Learning for Abstractive Text Summarization: {{A}} Review},
  shorttitle = {Deep Reinforcement and Transfer Learning for Abstractive Text Summarization},
  author = {Alomari, Ayham and Idris, Norisma and Sabri, Aznul Qalid Md and Alsmadi, Izzat},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101276},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101276},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000796},
  urldate = {2021-09-28},
  abstract = {Automatic Text Summarization (ATS) is an important area in Natural Language Processing (NLP) with the goal of shortening a long text into a more compact version by conveying the most important points in a readable form. ATS applications continue to evolve and utilize effective approaches that are being evaluated and implemented by researchers. State-of-the-Art (SotA) technologies that demonstrate cutting-edge performance and accuracy in abstractive ATS are deep neural sequence-to-sequence models, Reinforcement Learning (RL) approaches, and Transfer Learning (TL) approaches, including Pre-Trained Language Models (PTLMs). The graph-based Transformer architecture and PTLMs have influenced tremendous advances in NLP applications. Additionally, the incorporation of recent mechanisms, such as the knowledge-enhanced mechanism, significantly enhanced the results. This study provides a comprehensive review of recent research advances in the area of abstractive text summarization for works spanning the past six years. Past and present problems are described, as well as their proposed solutions. In addition, abstractive ATS datasets and evaluation measurements are also highlighted. The paper concludes by comparing the best models and discussing future research directions.},
  language = {en},
  keywords = {Abstractive summarization,Pre-trained models,Reinforcement learning,Sequence-to-sequence},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Alomari et al_2022_Deep reinforcement and transfer learning for abstractive text summarization.pdf}
}

@inproceedings{andoSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition Based}} on {{Listener Adaptive Models}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ando, Atsushi and Masumura, Ryo and Sato, Hiroshi and Moriya, Takafumi and Ashihara, Takanori and Ijima, Yusuke and Toda, Tomoki},
  date = {2021-06},
  pages = {6274--6278},
  issn = {2379-190X},
  doi = {10/gmr2j8},
  abstract = {This paper presents a novel speech emotion recognition scheme that can deal with the individuality of emotion perception. Most conventional methods directly model the majority decision of multiple listener's perceived emotions. However, emotion perception varies with the listener, which means the conventional methods can mismatch the recognition results to human perception. In order to mitigate this problem, we propose a Listener Adaptive (LA) model that reflects emotion recognition criteria of each listener. One-hot listener codes with several adaptation layers are employed in the LA model. The LA model yields the posterior probabilities of the listener-specific perceived emotions. Majority-voted emotion can be also estimated by averaging, in the LA model, the posterior probabilities for all listeners. Experiments on two emotional speech datasets demonstrate that the proposed approach offers improved listener-wise perceived emotion recognition performance in natural speech.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Acoustics,Adaptation models,Conferences,Emotion recognition,Listener Adaptation,Natural languages,Perceived Emotion,Signal processing,Speech Emotion Recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ando et al_2021_Speech Emotion Recognition Based on Listener Adaptive Models.pdf}
}

@article{anidjarHybridSpeechText2021,
  title = {Hybrid {{Speech}} and {{Text Analysis Methods}} for {{Speaker Change Detection}}},
  author = {Anidjar, Or Haim and Lapidot, Itshak and Hajaj, Chen and Dvir, Amit and Gilad, Issachar},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2324--2338},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3093817},
  abstract = {Speaker Change Detection (SCD) is the task of segmenting an input audio-recording according to speaker interchanges. Nowadays, many applications, such as Speaker Diarization (SD) or automatic vocal transcription, depend on this segmentation task. In this paper, we focus on the essential task of the SD problem, the audio segmenting process, and suggest a solution for the SCD problem, as well as the assignment of clustered speaker labels for the extracted segments, and applying the solution over two datasets: a commercial dataset in Hebrew and the ICSI Meeting Corpus. As such, we propose a hybrid framework for the SCD problem that is learned by textual information and speech signals and the meta-data features that can be extracted from them. Moreover, we demonstrate the negative correlation between an increase in the number of speakers in the training dataset and the influence on the overall diarization system's performance, which is improved using our efficient SCD component. Finally, we show how our proposed hybrid framework remains robust compared to the ICSI Meeting Corpus, as the experimental evaluation's training and testing is based on two languages.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {\#nosource,Clustering algorithms,D-vectors,Encoding,Feature extraction,Mel frequency cepstral coefficient,Speaker change detection,speaker diarization,speaker verification,speech analysis,Speech processing,Task analysis,Training}
}

@inproceedings{atmajaMultitaskLearningMultistage2020,
  title = {Multitask {{Learning}} and {{Multistage Fusion}} for {{Dimensional Audiovisual Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Atmaja, Bagus Tris and Akagi, Masato},
  date = {2020-05},
  pages = {4482--4486},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9052916},
  abstract = {Due to its ability to accurately predict emotional state using multimodal features, audiovisual emotion recognition has recently gained more interest from researchers. This paper proposes two methods to predict emotional attributes from audio and visual data using a multitask learning and a fusion strategy. First, multitask learning is employed by adjusting three parameters for each attribute to improve the recognition rate. Second, a multistage fusion is proposed to combine results from various modalities' final prediction. Our approach used multitask learning, employed at unimodal and early fusion methods, shows improvement over single-task learning with an average CCC score of 0.431 compared to 0.297. A multistage method, employed at the late fusion approach, significantly improved the agreement score between true and predicted values on the development set of data (from [0.537, 0.565, 0.083] to [0.68, 0.656, 0.443]) for arousal, valence, and liking.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,Acoustics,audiovisual emotion recognition,Conferences,dimensional emotion,Emotion recognition,multistage fusion,multitask learning,Signal processing,Speech processing,Speech recognition,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Atmaja_Akagi_2020_Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion.pdf}
}

@article{azadiRobustVoiceFeature2021,
  title = {Robust {{Voice Feature Selection Using Interval Type-2 Fuzzy AHP}} for {{Automated Diagnosis}} of {{Parkinson}}'s {{Disease}}},
  author = {Azadi, Hamid and Akbarzadeh-T, Mohammad-R. and Kobravi, Hamid-R. and Shoeibi, Ali},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2792--2802},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3097215},
  abstract = {Goal: Human voice is a promising noninvasive indicator for diagnosing Parkinson's Disease (PD). It is also unique since it can be collected remotely, increasing accessibility to a wide range of underprivileged patients. However, recognizing PD's signature in the human voice is nontrivial since the available features are many, and the signal may be noisy. Methods: A new mechanism based on Interval Type-2 Fuzzy Analytical Hierarchy Process is proposed here for choosing a reduced feature set from 339 dysphonia speech features, based on five criteria of 1) Robustness, 2) Relief, 3) Minimum Redundancy and Maximum Relevance, 4) Gaussian mixture model separation, and 5) Classifier separation ability. A Least Squares Support Vector Machine then categorizes the samples as belonging to either a healthy subject or a patient with PD. The database of 47 subjects with an average age of 67 is obtained from the elderly in nursing homes and Parkinson's specialized clinics. By reducing signal quality similar to a standard phone line, we study the teleoperation prospect of the proposed technique. Results: Ten-fold cross-validation shows an overall accuracy of 95.32\%(93.11\%) for noiseless(noisy) conditions, with separate analysis for male, female, and both genders populations. Furthermore, Leave-One-Speaker-Out analysis yields an overall accuracy of 93.11\%(84.61\%) for noiseless(noisy) conditions. Conclusion: The proposed strategy offers viable remote PD diagnosis with higher accuracy for the male population. Significance: The proposed method suggests reduced feature sets that meet differing objectives of simplicity, performance, and robustness. Results could be particularly significant in PD diagnosis in remote areas.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {\#nosource,Analytical hierarchy process,Feature extraction,feature selection,Fuzzy sets,Human voice,interval type-2 fuzzy sets,Parkinson's disease,Sociology,speech signal processing,Statistics,Uncertainty}
}

@unpublished{baevskiWav2vecFrameworkSelfSupervised2020,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020-10-22},
  eprint = {2006.11477},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2006.11477},
  urldate = {2021-10-24},
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {_Waiting for read,⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Baevski et al_2020_wav2vec 2.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\CECDZXNI\\2006.html}
}

@article{baiFastEndtoEndSpeech2021,
  title = {Fast {{End-to-End Speech Recognition Via Non-Autoregressive Models}} and {{Cross-Modal Knowledge Transferring From BERT}}},
  author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Wen, Zhengqi and Zhang, Shuai},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1897--1911},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3082299},
  abstract = {Attention-based encoder-decoder (AED) models have achieved promising performance in speech recognition. However, because the decoder predicts text tokens (such as characters or words) in an autoregressive manner, it is difficult for an AED model to predict all tokens in parallel. This makes the inference speed relatively slow. In contrast, we propose an end-to-end non-autoregressive speech recognition model called LASO (Listen Attentively, and Spell Once). The model aggregates encoded speech features into the hidden representations corresponding to each token with attention mechanisms. Thus, the model can capture the token relations by self-attention on the aggregated hidden representations from the whole speech signal rather than autoregressive modeling on tokens. Without explicitly autoregressive language modeling, this model predicts all tokens in the sequence in parallel so that the inference is efficient. Moreover, we propose a cross-modal transfer learning method to use a text-modal language model to improve the performance of speech-modal LASO by aligning token semantics. We conduct experiments on two scales of public Chinese speech datasets AISHELL-1 and AISHELL-2. Experimental results show that our proposed model achieves a speedup of about \$50\textbackslash times\$ and competitive performance, compared with the autoregressive transformer models. And the cross-modal knowledge transferring from the text-modal model can improve the performance of the speech-modal model.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,attention,BERT,Bit error rate,cross-modal,Decoding,end-to-end,fast,Hidden Markov models,non-autoregressive,Predictive models,Semantics,Speech recognition,transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Bai et al_2021_Fast End-to-End Speech Recognition Via Non-Autoregressive Models and.pdf}
}

@article{baiIntegratingKnowledgeEndtoEnd2021,
  title = {Integrating {{Knowledge Into End-to-End Speech Recognition From External Text-Only Data}}},
  author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Wen, Zhengqi and Tian, Zhengkun and Zhang, Shuai},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1340--1351},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3066274},
  abstract = {Attention-based encoder-decoder (AED) models have achieved promising performance in speech recognition. However, because of the end-to-end training, an AED model is usually trained with speech-text paired data. It is challenging to incorporate external text-only data into AED models. Another issue of the AED model is that it does not use the right context of a text token while predicting the token. To alleviate the above two issues, we propose a unified method called LST (Learn Spelling from Teachers) to integrate knowledge into an AED model from the external text-only data and leverage the whole context in a sentence. The method is divided into two stages. First, in the representation stage, a language model is trained on the text. It can be seen as that the knowledge in the text is compressed into the LM. Then, at the transferring stage, the knowledge is transferred to the AED model via teacher-student learning. To further use the whole context of the text sentence, we propose an LM called causal cloze completer (COR), which estimates the probability of a token, given both the left context and the right context of it. Therefore, with LST training, the AED model can leverage the whole context in the sentence. Different from fusion based methods, which use LM during decoding, the proposed method does not increase any extra complexity at the inference stage. We conduct experiments on two scales of public Chinese datasets AISHELL-1 and AISHELL-2. The experimental results demonstrate the effectiveness of leveraging external text-only data and the whole context in a sentence with our proposed method, compared with baseline hybrid systems and AED model based systems.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,Context modeling,Data models,Decoding,End-to-End,Hidden Markov models,language modeling,Neural networks,speech recognition,teacher-student learning,Training,transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Bai et al_2021_Integrating Knowledge Into End-to-End Speech Recognition From External.pdf}
}

@article{BaiQingGanYuYinHeChengJiShuHuoDuiShengWenJianDingZhunQueXingChanShengYingXiang2018,
  title = {情感语音合成技术或对声纹鉴定准确性产生影响},
  author = {白, 海莉},
  date = {2018-01-01},
  journaltitle = {科技创新与应用},
  number = {36},
  pages = {24,26},
  issn = {2095-2945},
  doi = {10.3969/j.issn.2095-2945.2018.36.009},
  url = {https://d.wanfangdata.com.cn/periodical/qgsj201836009},
  urldate = {2021-09-10},
  abstract = {声纹鉴定运用语言学、计算机科学等知识,对涉案语音和样本语音进行听觉辨识,同时运用频谱图进行综合分析,判断二者是否由同一音源发出,或者判断声音性质.声纹鉴定结果可以为侦查和诉讼活动提供线索或证据.深度学习可以促进情感语音合成技术的发展,而情感语音合成技术的发展水平越高,合成语音与人声的差异就越小,声纹鉴定的难度就越大.文章旨在研究深度学习以及情感语音合成技术发展的发展状况,以此分析其可能对声纹鉴定准确性产生影响的因素.},
  issue = {36},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,声纹鉴定,情感语音合成,深度学习,语音,语音合成},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\白_2018_情感语音合成技术或对声纹鉴定准确性产生影响.pdf}
}

@inproceedings{bairdPrototypicalNetworkApproach2021,
  title = {A {{Prototypical Network Approach}} for {{Evaluating Generated Emotional Speech}}},
  booktitle = {Interspeech 2021},
  author = {Baird, Alice and Mertes, Silvan and Milling, Manuel and Stappen, Lukas and Wiest, Thomas and Andr\'e, Elisabeth and Schuller, Bj\"orn W.},
  date = {2021-08-30},
  pages = {3161--3165},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1123},
  url = {https://www.isca-speech.org/archive/interspeech_2021/baird21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Baird et al_2021_A Prototypical Network Approach for Evaluating Generated Emotional Speech.pdf}
}

@article{baljekarSpeechSynthesisFound,
  title = {Speech {{Synthesis}} from {{Found Data}}},
  author = {Baljekar, Pallavi},
  pages = {143},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Baljekar_Speech Synthesis from Found Data.pdf}
}

@article{barra-chicoteAnalysisStatisticalParametric2010,
  title = {Analysis of Statistical Parametric and Unit Selection Speech Synthesis Systems Applied to Emotional Speech},
  author = {Barra-Chicote, Roberto and Yamagishi, Junichi and King, Simon and Montero, Juan Manuel and Macias-Guarasa, Javier},
  date = {2010-05},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {52},
  number = {5},
  pages = {394--404},
  issn = {01676393},
  doi = {10.1016/j.specom.2009.12.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639309001824},
  urldate = {2021-09-10},
  abstract = {We have applied two state-of-the-art speech synthesis techniques (unit selection and HMM-based synthesis) to the synthesis of emotional speech. A series of carefully designed perceptual tests to evaluate speech quality, emotion identification rates and emotional strength were used for the six emotions which we recorded \textendash{} happiness, sadness, anger, surprise, fear, disgust. For the HMM-based method, we evaluated spectral and source components separately and identified which components contribute to which emotion.},
  issue = {5},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Barra-Chicote et al_2010_Analysis of statistical parametric and unit selection speech synthesis systems.pdf}
}

@inproceedings{baruahSpeechEmotionRecognition2022,
  title = {Speech {{Emotion Recognition}} via {{Generation}} Using an {{Attention-based Variational Recurrent Neural Network}}},
  booktitle = {Interspeech 2022},
  author = {Baruah, Murchana and Banerjee, Bonny},
  date = {2022-09-18},
  pages = {4710--4714},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-753},
  url = {https://www.isca-speech.org/archive/interspeech_2022/baruah22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Baruah_Banerjee_2022_Speech Emotion Recognition via Generation using an Attention-based Variational.pdf}
}

@unpublished{battenbergExploringNeuralTransducers2017,
  title = {Exploring {{Neural Transducers}} for {{End-to-End Speech Recognition}}},
  author = {Battenberg, Eric and Chen, Jitong and Child, Rewon and Coates, Adam and Gaur, Yashesh and Li, Yi and Liu, Hairong and Satheesh, Sanjeev and Seetapun, David and Sriram, Anuroop and Zhu, Zhenyao},
  date = {2017-07-24},
  eprint = {1707.07413},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.07413},
  urldate = {2021-09-14},
  abstract = {In this work, we perform an empirical comparison among the CTC, RNN-Transducer, and attention-based Seq2Seq models for end-to-end speech recognition. We show that, without any language model, Seq2Seq and RNN-Transducer models both outperform the best reported CTC models with a language model, on the popular Hub5'00 benchmark. On our internal diverse dataset, these trends continue - RNNTransducer models rescored with a language model after beam search outperform our best CTC models. These results simplify the speech recognition pipeline so that decoding can now be expressed purely as neural network operations. We also study how the choice of encoder architecture affects the performance of the three models - when all encoder layers are forward only, and when encoders downsample the input representation aggressively.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Battenberg et al_2017_Exploring Neural Transducers for End-to-End Speech Recognition.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\CGNBZ52S\\1707.html}
}

@article{begusLocalNonlocalDependency2022,
  title = {Local and Non-Local Dependency Learning and Emergence of Rule-like Representations in Speech Data by Deep Convolutional Generative Adversarial Networks},
  author = {Begu\v{s}, Ga\v{s}per},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101244},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101244},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000516},
  urldate = {2021-09-28},
  abstract = {This paper argues that training Generative Adversarial Networks (GANs) on local and non-local dependencies in speech data offers insights into how deep neural networks discretize continuous data and how symbolic-like rule-based morphophonological processes emerge in a deep convolutional architecture. Acquisition of speech has recently been modeled as a dependency between latent space and data generated by GANs in Begu\v{s} (2020b), who models learning of a simple local allophonic distribution. We extend this approach to test learning of local and non-local phonological processes that include approximations of morphological processes. We further parallel outputs of the model to results of a behavioral experiment where human subjects are trained on the data used for training the GAN network. Four main conclusions emerge: (i) the networks provide useful information for computational models of speech acquisition even if trained on a comparatively small dataset of an artificial grammar learning experiment; (ii) local processes are easier to learn than non-local processes, which matches both behavioral data in human subjects and typology in the world's languages. This paper also proposes (iii) how we can actively observe the network's progress in learning and explore the effect of training steps on learning representations by keeping latent space constant across different training steps. Finally, this paper shows that (iv) the network learns to encode the presence of a prefix with a single latent variable; by interpolating this variable, we can actively observe the operation of a non-local phonological process. The proposed technique for retrieving learning representations has general implications for our understanding of how GANs discretize continuous speech data and suggests that rule-like generalizations in the training data are represented as an interaction between variables in the network's latent space.},
  language = {en},
  keywords = {_Waiting for read,Behavioral experiments,Learning biases,Machine learning,Morphology,Neural networks,Speech},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Beguš_2022_Local and non-local dependency learning and emergence of rule-like.pdf}
}

@article{ben-youssefEarlyDetectionUser2021,
  title = {Early {{Detection}} of {{User Engagement Breakdown}} in {{Spontaneous Human-Humanoid Interaction}}},
  author = {Ben-Youssef, Atef and Clavel, Chlo\'e and Essid, Slim},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {776--787},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2898399},
  abstract = {This paper presents a supervised classification system for forecasting a potential user engagement breakdown in human-robot interaction. We define engagement breakdown as a failure to successfully complete a predefined interaction scenario, where the user leaves before the expected end. The goal is thus to detect as early as possible such a potential engagement breakdown during the interaction between a human and a humanoid robot. To this end, we exploit a dataset that we have collected in real-world conditions where a set of participants were left to spontaneously engage in an interaction with the robot. The dataset is labeled according to the presence/absence of engagement breakdown. This study investigates the use of a multimodal approach to this problem, where a set of non-verbal features is considered to characterize the users' behavior. The use of combined multimodal features is found to effectively improve the performance of the system. The optimal set of data streams useful for this task is the combination of the distance to the robot, gaze and head motion, as well as facial expressions and speech. We study the time extent over which a user's departure can be anticipated. We find that this ability to anticipate the departure depends on the window during which we observe the user behavior.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {Analytical models,Computational modeling,Electric breakdown,Feature extraction,HRI,prediction of engagement breakdown,Predictive models,real-time prediction,Robots,spontaneous interaction,Task analysis,User engagement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ben-Youssef et al_2021_Early Detection of User Engagement Breakdown in Spontaneous Human-Humanoid.pdf}
}

@article{bertiniAutomaticAlzheimerDisease2022,
  title = {An Automatic {{Alzheimer}}'s Disease Classifier Based on Spontaneous Spoken {{English}}},
  author = {Bertini, Flavio and Allevi, Davide and Lutero, Gianluca and Calz\`a, Laura and Montesi, Danilo},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101298},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101298},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000991},
  urldate = {2021-09-28},
  abstract = {According to the World Health Organization, the number of people suffering from dementia worldwide will grow to 150 million by mid-century, and Alzheimer's disease is the most common form of dementia contributing to 60\%\textendash 70\% of cases. The problem is compounded by the fact that current pharmacologic treatments are only symptomatic, and therapies are ineffective in slow down or cure the degenerative process. An automatic and standardize classifier for Alzheimer's disease is thereby extremely important to rapidly respond and deliver as preventive as possible interventions. Speech alterations might be one of the earliest signs of cognitive defect and, recently, the researchers showed that they can be observable well in advance other cognitive deficits become manifest. In this paper, we propose a full automated method able to classify the spontaneous spoken production of the subjects. In particular, we trained an artificial neural network using the spectrogram of the audio signal, which is the visual representation of the speech of the subject. Moreover, to overcome the problem of the large amount of annotated data usually required for training deep learning models, we used a specific data augmentation approach that avoids distorting the original samples. We evaluated the proposed method using the English Pitt Corpus from DementiaBank. The used dataset consists of 180 subjects: 43 healthy controls and 137 Alzheimer's disease patients. The proposed method outperformed the other approaches in the literature based on manual and semi-automatic transcription and annotation of speech, improving the classification capability by 5.93\%, and obtained good classification results compared to the state-of-the-art neuropsychological screening tests (i.e., the Mini-Mental State Examination and the Activities of Daily Living portion of the Blessed Dementia Rating Scale) exhibiting an accuracy of 93.30\% and an F1 score of 88.50\%.},
  language = {en},
  keywords = {\#nosource,Alzheimer’s disease,Autoencoder neural networks,Data augmentation,Speech analysis,Speech classification}
}

@inproceedings{bharadwajAnalysisProsodicFeatures2020,
  title = {Analysis of {{Prosodic}} Features for the Degree of Emotions of an {{Assamese Emotional Speech}}},
  booktitle = {2020 4th {{International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  author = {Bharadwaj, Sippee and Acharjee, Purnendu Bikash},
  date = {2020-11-05},
  pages = {1441--1452},
  publisher = {{IEEE}},
  location = {{Coimbatore, India}},
  doi = {10.1109/ICECA49313.2020.9297453},
  url = {https://ieeexplore.ieee.org/document/9297453/},
  urldate = {2021-09-10},
  abstract = {This research work has attempted to study the s peech prosody for designing a speech synthesizer. Speech prosody carries all relevant informati on about the utter ances. Here, it has dealt wi th the suprasegmental features of the speech utterance. The propsoed research work is experi mentally studying the i nteraction and behavi or of segmental and suprasegmental features by concerning the North-East Indi an Language Assamese. For efficientl y designing an emoti onal speech recogniti on system or a s peech synthesizer, a proper cl arity on understandi ng the tone of a voice, boundaries of the prosodic, prominent porti on of utterance concerning specific words, sentences are required. This paper also discusses the variati ons of prosodic features for `anger' `normal' `sad' `surprise' `disgust' for different accent types. The degree of emoti on considered here are head-high, mi d-high, and flat with the hel p of some selected speech items for female and male taken from our Assamese speech database. This research work has analyzed and discussed the prosodic behavi or of utterances from the mean speech rate and fundamental frequency based on the graphs and tables deri ved from the proposed experiment.},
  eventtitle = {2020 4th {{International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  isbn = {978-1-72816-387-1},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Bharadwaj_Acharjee_2020_Analysis of Prosodic features for the degree of emotions of an Assamese.pdf}
}

@unpublished{bianMultireferenceTacotronIntercross2019,
  title = {Multi-Reference {{Tacotron}} by {{Intercross Training}} for {{Style Disentangling}},{{Transfer}} and {{Control}} in {{Speech Synthesis}}},
  author = {Bian, Yanyao and Chen, Changbin and Kang, Yongguo and Pan, Zhenglin},
  date = {2019-04-04},
  eprint = {1904.02373},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.02373},
  urldate = {2021-09-10},
  abstract = {Speech style control and transfer techniques aim to enrich the diversity and expressiveness of synthesized speech. Existing approaches model all speech styles into one representation, lacking the ability to control a specific speech feature independently. To address this issue, we introduce a novel multi-reference structure to Tacotron and propose intercross training approach, which together ensure that each sub-encoder of the multi-reference encoder independently disentangles and controls a specific style. Experimental results show that our model is able to control and transfer desired speech styles individually.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Bian et al_2019_Multi-reference Tacotron by Intercross Training for Style.pdf}
}

@article{biswasCodeswitchedAutomaticSpeech2022,
  title = {Code-Switched Automatic Speech Recognition in Five {{South African}} Languages},
  author = {Biswas, Astik and Y\i lmaz, Emre and van der Westhuizen, Ewald and de Wet, Febe and Niesler, Thomas},
  options = {useprefix=true},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101262},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101262},
  url = {https://www.sciencedirect.com/science/article/pii/S088523082100067X},
  urldate = {2021-09-28},
  abstract = {Most automatic speech recognition (ASR) systems are optimised for one specific language and their performance consequently deteriorates drastically when confronted with multilingual or code-switched speech. We describe our efforts to improve an ASR system that can process code-switched South African speech that contains English and four indigenous languages: isiZulu, isiXhosa, Sesotho and Setswana. We begin using a newly developed language-balanced corpus of code-switched speech compiled from South African soap operas, which are rich in spontaneous code-switching. The small size of the corpus makes this scenario under-resourced, and hence we explore several ways of addressing this sparsity of data. We consider augmenting the acoustic training sets with in-domain data at the expense of making it unbalanced and dominated by English. We further explore the inclusion of monolingual out-of-domain data in the constituent languages. For language modelling, we investigate the inclusion of out-of-domain text data sources and also the inclusion of synthetically-generated code-switch bigrams. In our experiments, we consider two system architectures. The first considers four bilingual speech recognisers, each allowing code-switching between English and one of the indigenous languages. The second considers a single pentalingual speech recogniser able to process switching between all five languages. We find that the additional inclusion of each acoustic and text data source leads to some improvements. While in-domain data is substantially more effective, performance gains were also achieved using out-of-domain data, which is often much easier to obtain. We also find that improvements are achieved in all five languages, even when the training set becomes unbalanced and heavily skewed in favour of English. Finally, we find the use of TDNN-F architectures for the acoustic model to consistently outperform TDNN\textendash BLSTM models in our data-sparse scenario.},
  language = {en},
  keywords = {African languages,Bantu languages,Code-switching,Speech recognition,TDNN-F,TDNN–BLSTM,Under-resourced languages},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Biswas et al_2022_Code-switched automatic speech recognition in five South African languages.pdf}
}

@article{bjornsdottirPerceivingAcculturationNeutral2021,
  title = {Perceiving Acculturation from Neutral and Emotional Faces},
  author = {Bjornsdottir, R. Thora and Rule, Nicholas O.},
  date = {2021},
  journaltitle = {Emotion},
  volume = {21},
  number = {4},
  pages = {720--729},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1931-1516},
  doi = {10.1037/emo0000735},
  abstract = {Facial expressions of emotion convey more than just emotional experience. Indeed, they can signal a person's social group memberships. For instance, extant research shows that nonverbal accents in emotion expression can reveal one's cultural affiliation (Marsh, Elfenbein, \& Ambady, 2003). That work tested distinctions only between people belonging to one of two cultural categories, however (Japanese vs. Japanese Americans). What of people who identify with more than one culture? Here we tested whether nonverbal accents might signal not only cultural identification but also the degree of cultural identification (i.e., acculturation). Using neutral, happy, and angry photos of East Asian individuals varying in acculturation to Canada, we found that both Canadian and East Asian perceivers could accurately detect the targets' level of acculturation. Although perceivers used hairstyle cues when available, once we removed hair, accuracy was greatest for happy expressions\textemdash supporting the idea that nonverbal accents convey cultural identification. Finally, the intensity of targets' happiness related to both their self-reported and perceived acculturation, helping to explain perceivers' accuracy and aligning with research on cultural display rules and ideal affect. Thus, nonverbal accents appear to communicate cultural identification not only categorically, as previous work has shown, but also continuously. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  issue = {4},
  language = {en},
  keywords = {\#nosource,Acculturation,Emotion Recognition,Emotionality (Personality),Experiences (Events),Face Perception,Facial Expressions,Linguistics,Membership,Social Groups}
}

@article{bohlenderExploitingTemporalContext2021,
  title = {Exploiting {{Temporal Context}} in {{CNN Based Multisource DOA Estimation}}},
  author = {Bohlender, Alexander and Spriet, Ann and Tirry, Wouter and Madhu, Nilesh},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1594--1608},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3067113},
  abstract = {Supervised learning methods are a powerful tool for direction of arrival (DOA) estimation because they can cope with adverse conditions where simplified models fail. In this work, we consider a previously proposed convolutional neural network (CNN) approach that estimates the DOAs for multiple sources from the phase spectra of the microphones. For speech, specifically, the approach was shown to work well even when trained entirely on synthetically generated data. However, as each frame is processed separately, temporal context cannot be taken into account. This prevents the exploitation of interframe signal correlations, and the fact that DOAs do not change arbitrarily over time. We therefore consider two different extensions of the CNN: the integration of a long short-term memory (LSTM) layer, or of a temporal convolutional network (TCN). In order to accommodate the incorporation of temporal context, the training data generation framework needs to be adjusted. To obtain an easily parameterizable model, we propose to employ Markov chains to realize a gradual evolution of the source activity at different times, frequencies, and directions, throughout a training sequence. A thorough evaluation demonstrates that the proposed configuration for generating training data is suitable for the tasks of single-, and multi-talker localization. In particular, we note that with temporal context, it is important to use speech, or realistic signals in general, for the sources. Experiments with recorded impulse responses and noise reveal that the CNN with the LSTM extension outperforms all other considered approaches, including the plain CNN, and the TCN extension.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Convolutional neural networks,direction-of-arrival,Direction-of-arrival estimation,Estimation,Feature extraction,Microphone arrays,temporal context,Time-frequency analysis,Training,Training data,training data generation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Bohlender et al_2021_Exploiting Temporal Context in CNN Based Multisource DOA Estimation.pdf}
}

@article{borgstromSpeechEnhancementAttention2021,
  title = {Speech {{Enhancement}} via {{Attention Masking Network}} ({{SEAMNET}}): {{An End-to-End System}} for {{Joint Suppression}} of {{Noise}} and {{Reverberation}}},
  shorttitle = {Speech {{Enhancement}} via {{Attention Masking Network}} ({{SEAMNET}})},
  author = {Borgstr\"om, Bengt J. and Brandstein, Michael S.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {515--526},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3043655},
  abstract = {This paper proposes the Speech Enhancement via Attention Masking Network (SEAMNET), a neural network-based end-to-end single-channel speech enhancement system designed for joint suppression of noise and reverberation. It formalizes an end-to-end network architecture, referred to as b-Net, which accomplishes noise suppression through attention masking in a learned embedding space. A key contribution of SEAMNET is that the b-Net architecture contains both an enhancement and an autoencoder path. This paper proposes a novel loss function which simultaneously trains both the enhancement and the autoencoder paths, so that disabling the masking mechanism during inference causes SEAMNET to reconstruct the input speech signal. This allows dynamic control of the level of suppression applied by SEAMNET via a minimum gain level, which is not possible in other state-of-the-art approaches to end-to-end speech enhancement. This paper also proposes a perceptually-motivated waveform distance measure. In addition to the b-Net architecture, this paper proposes a novel method for designing target waveforms for network training, so that joint suppression of additive noise and reverberation can be performed by an end-to-end enhancement system, which has not been previously possible. Experimental results show the SEAMNET system to outperform a variety of state-of-the-art baselines systems, both in terms of objective speech quality measures and subjective listening tests. Finally, this paper draws parallels between SEAMNET and conventional statistical model-based enhancement approaches, offering interpretability of many network components.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Convolutional neural network,end-to-end neural network,Estimation,Noise measurement,Noise reduction,noise suppression,Reverberation,reverberation suppression,speech enhancement,Speech enhancement,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Borgström_Brandstein_2021_Speech Enhancement via Attention Masking Network (SEAMNET).pdf}
}

@inproceedings{boseParametricDistributionsModel2021,
  title = {Parametric {{Distributions}} to {{Model Numerical Emotion Labels}}},
  booktitle = {Interspeech 2021},
  author = {Bose, Deboshree and Sethu, Vidhyasaharan and Ambikairajah, Eliathamby},
  date = {2021-08-30},
  pages = {4498--4502},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1000},
  url = {https://www.isca-speech.org/archive/interspeech_2021/bose21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Bose et al_2021_Parametric Distributions to Model Numerical Emotion Labels.pdf}
}

@article{boularesUnsupervisedSignLanguage2022,
  title = {Unsupervised Sign Language Validation Process Based on Hand-Motion Parameter Clustering},
  author = {Boulares, Mehrez and Barnawi, Ahmed},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101256},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101256},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000632},
  urldate = {2021-09-28},
  abstract = {Automatic sign language translation process relies mainly on dictionaries of signs to interpret the right meaning of gestures. Due to the lack of large multi sign language dictionaries covering all the aspect of sign languages, the collaborative approach to create signs becomes essential. In fact, the collaborative sign creation process based on Kinect motion capture tool requires the collaboration of non expert users to make sign language dictionaries. However, due to the availability constraint of sign language experts to validate the created signs and the huge amount of signs to be validated manually, the automatic sign language validation process becomes the most suitable solution. In this paper, we present a new automatic and unsupervised sign validation process based on machine learning techniques applied on sign replicas. Given a set of replicas (records) of the same sign created by different non expert sign language user, our main goal is to select the adequate sign records to be used to generate the closest sign signature compared to the one created by sign language expert. For this aim, we present an automatic sign selection and validation solution based on unsupervised clustering of sign motion parameters related to the different sign replicas. We conducted an experimental study to validate 300 ASL signs based on four unsupervised clustering methods, namely, Kernel PCA Kmeans, GMM, Spectral clustering and kernel Kmeans. We concluded that the use our sign validation process using Spectral clustering method allows us to select the right sign replicas to be used to generate the user sign signature. The use of our unsupervised sign validation process onto 3000 ASL sign replicas (300 sign * 10 replicas) lead us to enhance the R2 score average from 0.4830 without sign validation to 0.9123 with sign validation compared to expert sign signature.},
  language = {en},
  keywords = {ASL,Automatic sign motion approximation,Clustering,Collaborative sign creation,Unsupervised sign validation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Boulares_Barnawi_2022_Unsupervised sign language validation process based on hand-motion parameter.pdf}
}

@article{bozkurtAffectiveSynthesisAnimation2020,
  title = {Affective Synthesis and Animation of Arm Gestures from Speech Prosody},
  author = {Bozkurt, Elif and Yemez, Y\"ucel and Erzin, Engin},
  date = {2020-05},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {119},
  pages = {1--11},
  issn = {01676393},
  doi = {10.1016/j.specom.2020.02.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639319301980},
  urldate = {2021-09-10},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Bozkurt et al_2020_Affective synthesis and animation of arm gestures from speech prosody.pdf}
}

@inproceedings{brueggemanSpeakerTraitEnhancement2022,
  title = {Speaker {{Trait Enhancement}} for {{Cochlear Implant Users}}: {{A Case Study}} for {{Speaker Emotion Perception}}},
  shorttitle = {Speaker {{Trait Enhancement}} for {{Cochlear Implant Users}}},
  booktitle = {Interspeech 2022},
  author = {Brueggeman, Avamarie and Hansen, John H.L.},
  date = {2022-09-18},
  pages = {2268--2272},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10951},
  url = {https://www.isca-speech.org/archive/interspeech_2022/brueggeman22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Brueggeman_Hansen_2022_Speaker Trait Enhancement for Cochlear Implant Users.pdf}
}

@article{bruggemeierPerceptionsReactionsConversational2022,
  title = {Perceptions and Reactions to Conversational Privacy Initiated by a Conversational User Interface},
  author = {Br\"uggemeier, Birgit and Lalone, Philip},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101269},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101269},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000747},
  urldate = {2021-09-28},
  abstract = {In 2019, media reports raised awareness about privacy and security violations in Conversational User Interfaces (CUI) like Alexa, Siri and Google. Users report that they perceive CUI as creepy and that they are concerned about their privacy. The European General Data Protection Regulation (GDPR) and several other laws across the globe, give users the right to control processing of their data, for example by requesting deletion of their data and it gives them the right to obtain information about their data. Furthermore, GDPR and other laws advise for seamless communication of user rights, which, currently, is poorly implemented in CUI. We used a bespoke data collection interface to generate speaking chatbots and made them available as tasks on the crowd sourcing platform Mechanical Turk. With those chatbots we simulated how privacy can be communicated in a dialogue between user and machine. We find that conversational privacy can affect user perceptions of privacy and security positively. Moreover, user choices suggest that users are interested in obtaining information on their privacy and security in dialogue form. We discuss implications and limitations of this research.},
  language = {en},
  keywords = {Conversational privacy,Conversational user interface,Crowd sourcing,CUI,Mechanical turk,Privacy,Security,Speech interface,Usability},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Brüggemeier_Lalone_2022_Perceptions and reactions to conversational privacy initiated by a.pdf}
}

@inproceedings{bussoIterativeFeatureNormalization2011,
  title = {Iterative Feature Normalization for Emotional Speech Detection},
  booktitle = {2011 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Busso, Carlos and Metallinou, Angeliki and Narayanan, Shrikanth S.},
  date = {2011-05},
  pages = {5692--5695},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2011.5947652},
  abstract = {Contending with signal variability due to source and channel effects is a critical problem in automatic emotion recognition. Any approach in mitigating these effects however has to be done so as to not compromise emotion-relevant information in the signal. A promising approach to this problem has been through feature normalization using features drawn from non-emotional ("neutral") speech samples. This paper considers a scheme for minimizing the inter-speaker differences while still preserving the emotional discrimination of the acoustic features. This can be achieved by estimating the normalization parameters using only neutral speech, and then applying the coefficients to the entire corpus (including emotional set). Specifically, this paper introduces a feature normalization scheme that implements these ideas by iteratively detecting neutral speech and normalizing the features. As the approximation error of the normalization parameters is reduced, the accuracy of the emotion detection system increases. The accuracy of the proposed iterative approach, evaluated across three databases, is only 2.5\% lower than the one trained with optimal normalization parameters, and 9.7\% higher than the one trained without any normalization scheme.},
  eventtitle = {2011 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Accuracy,Acoustics,Databases,emotion recognition,Emotion recognition,emotions,Feature extraction,feature normalization,fundamental frequency,Hidden Markov models,Speech},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Busso et al_2011_Iterative feature normalization for emotional speech detection2.pdf}
}

@article{byunMonauralSpeechSeparation2021,
  title = {Monaural {{Speech Separation Using Speaker Embedding From Preliminary Separation}}},
  author = {Byun, Jaeuk and Shin, Jong Won},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2753--2763},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3101617},
  abstract = {In speech separation, the identities of the speakers may be an important cue to discriminate speeches in the mixture and separate them better. A few recent researches used the speaker embedding as an additional information, but they often require prior information about the target speaker or used noisy speaker embedding extracted from the mixture signal. In this article, we propose monaural speech separation that utilizes the speaker embedding in the later separator blocks, which is extracted from the intermediate separated results obtained by the early stages of the separator network. The later blocks in the separator networks consisting of repeated blocks such as the fully-convolutional time-domain audio separation network (Conv-TasNet) or the successive downsampling and resampling of multi-resolution features (SuDoRM-RF) are modified to take the speaker information as a form of affine transformation or addition to the original input tensor. The experimental results showed that the proposed methods significantly improved the performances of existing separation systems with a moderate number of additional parameters.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {\#nosource,Convolution,Data mining,Decoding,deep learning,Estimation,Monaural speech separation,Particle separators,speaker representation,Speech enhancement,time domain,Time-domain analysis}
}

@inproceedings{caiEmotionControllableSpeech2021,
  title = {Emotion {{Controllable Speech Synthesis Using Emotion-Unlabeled Dataset}} with the {{Assistance}} of {{Cross-Domain Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cai, Xiong and Dai, Dongyang and Wu, Zhiyong and Li, Xiang and Li, Jingbei and Meng, Helen},
  date = {2021-06},
  pages = {5734--5738},
  publisher = {{IEEE}},
  issn = {2379-190X},
  doi = {10/gmr2fw},
  abstract = {Neural text-to-speech (TTS) approaches generally require a huge number of high quality speech data, which makes it difficult to obtain such a dataset with extra emotion labels. In this paper, we propose a novel approach for emotional TTS synthesis on a TTS dataset without emotion labels. Specifically, our proposed method consists of a cross-domain speech emotion recognition (SER) model and an emotional TTS model. Firstly, we train the cross-domain SER model on both SER and TTS datasets. Then, we use emotion labels on the TTS dataset predicted by the trained SER model to build an auxiliary SER task and jointly train it with the TTS model. Experimental results show that our proposed method can generate speech with the specified emotional expressiveness and nearly no hindering on the speech quality.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustics,Conferences,Emotion,Emotion recognition,expressive,global style token,Predictive models,Signal processing,speech emotion recognition,Speech recognition,speech synthesis,Speech synthesis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cai et al_2021_Emotion Controllable Speech Synthesis Using Emotion-Unlabeled Dataset with the.pdf;C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cai et al_2021_Emotion Controllable Speech Synthesis Using Emotion-Unlabeled Dataset with the2.pdf}
}

@inproceedings{caiSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition}} with {{Multi-Task Learning}}},
  author = {Cai, Xingyu and Yuan, Jiahong and Zheng, Renjie and Huang, Liang and Church, Kenneth},
  date = {2021-08-30},
  pages = {4508--4512},
  doi = {10.21437/Interspeech.2021-1852},
  eventtitle = {Interspeech},
  language = {en},
  keywords = {_Code,_readed},
  annotation = {ECC: 0000002},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cai et al_2021_Speech Emotion Recognition with Multi-Task Learning.pdf}
}

@inproceedings{caiSpeechEmotionRecognition2021a,
  title = {Speech {{Emotion Recognition}} with {{Multi-Task Learning}}},
  booktitle = {Interspeech 2021},
  author = {Cai, Xingyu and Yuan, Jiahong and Zheng, Renjie and Huang, Liang and Church, Kenneth},
  date = {2021-08-30},
  pages = {4508--4512},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1852},
  url = {https://www.isca-speech.org/archive/interspeech_2021/cai21b_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cai et al_2021_Speech Emotion Recognition with Multi-Task Learning2.pdf}
}

@inproceedings{caoHierarchicalNetworkBased2021,
  title = {Hierarchical {{Network Based}} on the {{Fusion}} of {{Static}} and {{Dynamic Features}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cao, Qi and Hou, Mixiao and Chen, Bingzhi and Zhang, Zheng and Lu, Guangming},
  date = {2021-06},
  pages = {6334--6338},
  issn = {2379-190X},
  doi = {10/gmr2jz},
  abstract = {Many studies on automatic speech emotion recognition (SER) have been devoted to extracting meaningful emotional features for generating emotion-relevant representations. However, they generally ignore the complementary learning of static and dynamic features, leading to limited performances. In this paper, we propose a novel hierarchical network called HNSD that can efficiently integrate the static and dynamic features for SER. Specifically, the proposed HNSD framework consists of three different modules. To capture the discriminative features, an effective encoding module is firstly designed to simultaneously encode both static and dynamic features. By taking the obtained features as inputs, the Gated Multi-features Unit (GMU) is conducted to explicitly determine the emotional intermediate representations for frame-level features fusion, instead of directly fusing these acoustic features. In this way, the learned static and dynamic features can jointly and comprehensively generate the unified feature representations. Benefiting from a well-designed attention mechanism, the last classification module is applied to predict the emotional states at the utterance level. Extensive experiments on the IEMOCAP benchmark dataset demonstrate the superiority of our method in comparison with state-of-the-art baselines.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Conferences,Dynamic Features,Emotion recognition,Feature extraction,Hierarchical Network,Interference,Logic gates,Signal processing,Speech Emotion Recognition,Speech recognition,Static Features},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cao et al_2021_Hierarchical Network Based on the Fusion of Static and Dynamic Features for.pdf}
}

@thesis{CaoJiYuYunLuCanShuYouHuaDeQingGanYuYinHeCheng2020,
  type = {硕士},
  title = {基于韵律参数优化的情感语音合成},
  author = {曹, 欣怡},
  date = {2020},
  institution = {{南京师范大学}},
  doi = {10.27245/d.cnki.gnjsu.2020.001227},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1021516016.nh&uniplatform=NZKPT&v=ec04Yeswxq4w2tBWFJhSRGXcZFP%25mmd2FMwjdTlZF1vnorB3vJhfu0dRkLHINJgqqgCml},
  urldate = {2021-09-10},
  abstract = {人类的语音中包含了丰富的情感信息,若缺少了情感则无法准确传达语句的含义。随着计算机技术和自然语言处理技术的发展,人们对合成语音的质量提出了更高的要求,情感语音合成作为提高合成语音质量的重要途径,具有了越来越重要的价值。在计算机辅助教学领域,情感语音合成能应用于语言教学中,有效的促进语音的学习,但是目前合成的情感语音仍存在自然度欠缺或情感表达度不准确的问题。所以本文提出了将Tacotron模型与韵律特征优化相结合的情感语音合成方法,在端到端模型合成的情感语音的基础上进一步优化语音的韵律参数,得到了较自然流畅、情感色彩明显的语音,以期为后续将情感语音合成应用于双语教学提供条件。论文的主要工作及创新如下:1.建立了端到端的情感语音合成模型。在原有Tacotron模型的基础上进行微调,经过训练得到了中性、愤怒、开心、厌恶、困倦5种不同的情感语音合成模型,合成出了自然流畅的情感语音。2.分析了不同情感的韵律特征。建立韵律特征和情感状态之间的映射关系,确定了不同情感语音的韵律参数。3.调整了情感语音韵律特征。对合成的情感语音的韵律参数进行优化,从而提升情感语音的情感表现力。实验结果表明,该方法在一定程度上提升了合成情感语音的情感表达度,虽然相比于未进行韵律特征修改的情感语音在自然度上略微有所下降,但仍有较为自然的表现。},
  editora = {王, 蔚},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_readed,_中文,_情感语音合成,Emotional speech synthesis,End-to-End,Feature modification,Prosodic features,特征修改,端到端,韵律特征},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\曹_2020_基于韵律参数优化的情感语音合成.pdf}
}

@article{chaiCrossEntropyGuidedMeasureCEGM2021,
  title = {A {{Cross-Entropy-Guided Measure}} ({{CEGM}}) for {{Assessing Speech Recognition Performance}} and {{Optimizing DNN-Based Speech Enhancement}}},
  author = {Chai, Li and Du, Jun and Liu, Qing-Feng and Lee, Chin-Hui},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {106--117},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3036783},
  abstract = {A new cross-entropy-guided measure (CEGM) is proposed to indirectly assess accuracies of automatic speech recognition (ASR) of degraded speech with a speech enhancement front-end and without directly performing ASR experiments. The proposed CEGM is calculated in three steps, namely: (1) a low-level representations via feature extraction, (2) a high-level nonlinear mapping using an acoustic model, and (3) a final CEGM calculation between the high-level representations of clean and enhanced speech. Specifically, state posterior probabilities from outputs of conventional hybrid acoustic model of the target ASR system are adopted as the high-level representations and a cross-entropy criterion is used to calculate the CEGM. Due to CEGM's differentiability, it can also be used to replace the conventional minimum mean squared error (MMSE) criterion as an objective function for deep neural network (DNN)-based speech enhancement. Therefore, the front-end enhancement model can be optimized towards improving the accuracies of the back-end ASR system. Experiments on single-channel CHiME-4 Challenge show that CEGM yields consistently the highest correlations with word error rate (WER) which is often costly to calculate, and achieves the most accurate assessment of ASR performance when compared to the perceptual evaluation metrics commonly used for assessing speech enhancement performance. Furthermore, CEGM-optimized speech enhancement could effectively reduce the WER on the CHiME-4 real test set when compared to unprocessed noisy speech and enhanced speech obtained with MMSE-optimized enhancement for ASR systems with fixed multi-condition acoustic models in various deep architectures.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustic measurements,Acoustic model,Acoustics,cross entropy,deep neural network (DNN),Neural networks,Noise robustness,Optimization,robust automatic speech recognition,speech enhancement,Speech enhancement,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chai et al_2021_A Cross-Entropy-Guided Measure (CEGM) for Assessing Speech Recognition.pdf}
}

@article{changEmotioninfusedDeepNeural2021,
  title = {Emotion-Infused Deep Neural Network for Emotionally Resonant Conversation},
  author = {Chang, Yung-Chun and Hsing, Yan-Chun},
  date = {2021-12-01},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {113},
  pages = {107861},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2021.107861},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494621007833},
  urldate = {2021-09-28},
  abstract = {The widespread development of conversational agents (chatbots) has enabled us to communicate and collaborate with different forms and functions of robots using natural language, thus facilitating a closer relationship between humans and technology. Given that chatbot services infused with domain knowledge are of great interest to not only global businesses but also academics, chatbots have in recent years become a popular research topic in the field of natural language processing. We therefore aim at improving current chatbots with the addition of natural emotions. In contrast to previous work, we intend to distinguish fine-grained emotion differences between words in order to better understand emotion expressions in sentences. Our approach infuses fine-grained emotion content into the response generation process to make the dialog more emotionally resonant. The experimental results demonstrate that this method can classify emotions more effectively. In addition, the proposed hybrid model, which consists of recurrent and convolutional neural networks with additional emotion-specific valence-arousal features, can correctly identify five emotions with a 67.89\% overall F1-score. We further evaluate the subjective quality of the responses and discover that the infusion of fine-grained emotion information substantially improves the quality and fluency of automatically generated empathetic conversation. We conclude that the proposed model can greatly improve the efficiency and usability of a conversational chatbot system.},
  language = {en},
  keywords = {_Waiting for read,Chinese emotional conversation,Dialog emotion recognition,Dialog system,Emotional dimensions,Natural language processing,Sentiment analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chang_Hsing_2021_Emotion-infused deep neural network for emotionally resonant conversation.pdf}
}

@article{chenCorrelatingSubwordArticulation2021,
  title = {Correlating Subword Articulation with Lip Shapes for Embedding Aware Audio-Visual Speech Enhancement},
  author = {Chen, Hang and Du, Jun and Hu, Yu and Dai, Li-Rong and Yin, Bao-Cai and Lee, Chin-Hui},
  date = {2021-11-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {143},
  pages = {171--182},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.06.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021002355},
  urldate = {2021-09-28},
  abstract = {In this paper, we propose a visual embedding approach to improve embedding aware speech enhancement (EASE) by synchronizing visual lip frames at the phone and place of articulation levels. We first extract visual embedding from lip frames using a pre-trained phone or articulation place recognizer for visual-only EASE (VEASE). Next, we extract audio-visual embedding from noisy speech and lip frames in an information intersection manner, utilizing a complementarity of audio and visual features for multi-modal EASE (MEASE). Experiments on the TCD-TIMIT corpus corrupted by simulated additive noises show that our proposed subword based VEASE approach is more effective than conventional embedding at the word level. Moreover, visual embedding at the articulation place level, leveraging upon a high correlation between place of articulation and lip shapes, demonstrates an even better performance than that at the phone level. Finally the experiments establish that the proposed MEASE framework, incorporating both audio and visual embeddings, yields significantly better speech quality and intelligibility than those obtained with the best visual-only and audio-only EASE systems.},
  language = {en},
  keywords = {Audio-visual,Deep learning,Representation learning,Speech enhancement,Universal attribute recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen et al_2021_Correlating subword articulation with lip shapes for embedding aware.pdf}
}

@inproceedings{chenCTARNNChannelTemporalwise2022,
  title = {{{CTA-RNN}}: {{Channel}} and {{Temporal-wise Attention RNN}} Leveraging {{Pre-trained ASR Embeddings}} for {{Speech Emotion Recognition}}},
  shorttitle = {{{CTA-RNN}}},
  booktitle = {Interspeech 2022},
  author = {Chen, Chengxin and Zhang, Pengyuan},
  date = {2022-09-18},
  pages = {4730--4734},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10403},
  url = {https://www.isca-speech.org/archive/interspeech_2022/chen22m_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen_Zhang_2022_CTA-RNN.pdf}
}

@inproceedings{chenEmotionShiftAwareCRF2022,
  title = {Emotion-{{Shift Aware CRF}} for {{Decoding Emotion Sequence}} in {{Conversation}}},
  booktitle = {Interspeech 2022},
  author = {Chen, Chun-Yu and Lin, Yun-Shao and Lee, Chi-Chun},
  date = {2022-09-18},
  pages = {1148--1152},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10438},
  url = {https://www.isca-speech.org/archive/interspeech_2022/chen22n_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen et al_2022_Emotion-Shift Aware CRF for Decoding Emotion Sequence in Conversation.pdf}
}

@article{chengDeepAdaptationNetwork2021,
  title = {A {{Deep Adaptation Network}} for {{Speech Enhancement}}: {{Combining}} a {{Relativistic Discriminator With Multi-Kernel Maximum Mean Discrepancy}}},
  shorttitle = {A {{Deep Adaptation Network}} for {{Speech Enhancement}}},
  author = {Cheng, Jiaming and Liang, Ruiyu and Liang, Zhenlin and Zhao, Li and Huang, Chengwei and Schuller, Bj\"orn},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {41--53},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3036611},
  abstract = {In deep-learning-based speech enhancement (SE) systems, trained models are often used to handle unseen noise types and language environments in real-life scenarios. However, since production environments differ from training conditions, mismatch problems arise that may cause a serious decrease in the performance of an SE system. In this study, a domain adaptive method combining two adaptation strategies is proposed to improve the generalization of unlabeled noisy speech. In the proposed encoder-decoder-based SE framework, a domain discriminator and a domain confusion adaptation layer are introduced to conduct adversarial training. The model has two main innovations. First, the algorithm optimizes adversarial training by introducing a relativistic discriminator that relies on relative values by applying the difference, thus avoiding possible bias and better reflecting domain differences. Second, the multi-kernel maximum mean discrepancy (MK-MMD) between domains is taken as the regularization term of the domain adversarial loss, thereby further decreasing the edge distribution distance between domains. The proposed model improves the adaptability to unseen noises by encouraging the feature encoder to generate domain-invariant features. The model was evaluated using cross-noise and cross-language-and-noise experiments, and the results show that the proposed method provides considerable improvements over the baseline without an adaptation in the perceptual evaluation of speech quality (PESQ), the short time objective intelligibility (STOI) and the frequency-weighted signal-to-noise ratio (FWSNR).},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Adaptation models,Deep neural network,domain adaptation,Feature extraction,maximum mean discrepancy,Noise measurement,relativistic discriminator,speech enhancement,Speech enhancement,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cheng et al_2021_A Deep Adaptation Network for Speech Enhancement.pdf}
}

@thesis{ChengJiYuTeZhengCengRongHeDeDuoMoTaiQingGanShiBieYanJiu2017,
  type = {硕士},
  title = {基于特征层融合的多模态情感识别研究},
  author = {程, 晓},
  date = {2017},
  institution = {{南京邮电大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1017859078.nh&uniplatform=NZKPT&v=i%25mmd2FC758VzSpGrvf4qKYis6ew%25mmd2BWqWPt6miPJvqTEVOGsNPGCGUpYhrqQEhsiHZphb3},
  urldate = {2021-09-10},
  abstract = {情感识别作为情感计算领域中的重要分支,在人工智能、人机交互等许多方面都具有广阔的应用前景。不同模态的情感识别可以优势互补,不断提高识别系统的精度,以促进产生更高质量、更加和谐的人机交互系统,使人类生活变得更加智能便捷。本文主要研究基于面部情感特征、语音情感特征、姿态情感特征以及生理信号相关情感特征的情感识别系统,具体包括:提取不同模态的情感特征、对提取的不同模态的情感特征进行融合以及最后的分类识别。论文的主要工作如下:（1）对数据库中的样本提取不同模态的特征,包括:使用二维Gabor滤波器提取面部表情样本的Gabor特征;使用OPENSMILE对包含情感的语音样本提取其综合的语音情感特征（包括:振幅、基频、共振峰等）;使用Harris角点检测姿态情感样本中的兴趣点进而提取其时空特征。（2）提出一种基于遗传算法的特征层融合方法,并将该方法与基于主成分分析的特征层融合方法、基于核典型相关分析的特征层融合方法、基于核矩阵融合的特征层融合方法进行比较,实验结果表明,论文提出的方法具有更好的融合效果。（3）将通过不同方法得到的融合特征输入支持向量机（SVM）进行分类识别,实现多模态情感识别,得到多模态的情感识别结果,将多模态的情感识别结果与单一模态的情感识别结果进行比较,实验结果表明,从识别结果的角度来看,使用适当融合方法的多模态情感识别结果要比单模态情感识别结果有明显提高。},
  editora = {卢, 官明},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Feature level fusion,Genetic algorithm,Multimodal emotion recognition,Support vector machine,多模态情感识别,支持向量机,特征层融合,遗传算法},
  annotation = {2 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\程_2017_基于特征层融合的多模态情感识别研究.pdf}
}

@article{chengUnifiedTargetOrientedSequencetoSequence2021,
  title = {A {{Unified Target-Oriented Sequence-to-Sequence Model}} for {{Emotion-Cause Pair Extraction}}},
  author = {Cheng, Zifeng and Jiang, Zhiwei and Yin, Yafeng and Li, Na and Gu, Qing},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2779--2791},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3102194},
  abstract = {Emotion-cause pair extraction is a recently proposed task that aims at extracting all potential clause-level pairs of emotion and cause in text. To solve this task, researchers first proposed a two-step pipeline method. This method extracts the emotions and causes individually in the first step, then pairs the extracted emotions and causes and filters the invalid emotion-cause pairs in the second step. Due to that the two-step method has the error accumulation problem and is hard to be optimized jointly, several one-step end-to-end models have been proposed. These models share a similar underlying idea, that is, reframing the emotion-cause pair extraction task as a classification problem of candidate clause pairs. Unlike these models, in this paper, we reframe the emotion-cause pair extraction task as a unified sequence labeling problem, which allows to extract emotion-cause pairs through one pass of sequence labeling. This is realized by designing a special set of unified labels. In the unified label, we design a content part for emotion/cause identification and a pairing part for clause pairing. Then the emotion-cause pairs can be implicitly derived from the unified labels. To address this unified sequence labeling problem, we propose a unified target-oriented sequence-to-sequence model, which comprehensively utilizes the information of target clause, global context, and former decoded label, to perform end-to-end unified sequence labeling. The experimental results demonstrate the effectiveness of both our proposed unified sequence labeling scheme and unified target-oriented sequence-to-sequence model. All the code and data of this work can be obtained at https://github.com/zifengcheng/UTOS.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Decoding,Emotion-cause pair extraction,Labeling,Object recognition,Pipelines,sequence labeling,sequence-to-sequence learning,Software,Speech processing,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cheng et al_2021_A Unified Target-Oriented Sequence-to-Sequence Model for Emotion-Cause Pair.pdf}
}

@thesis{ChenJiYuDuoLiDuTeZhengRongHeDeWeiDuYuYinQingGanShiBieFangFaYanJiu2016,
  type = {硕士},
  title = {基于多粒度特征融合的维度语音情感识别方法研究},
  author = {陈, 肖},
  date = {2016},
  institution = {{哈尔滨工业大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201701&filename=1016774418.nh&uniplatform=NZKPT&v=62cn8M%25mmd2BICIHaaqvd8VvoOXQSlJsBEyh%25mmd2FkR0NpCLxW7rC3XH0Trh0q00owPFDv6S%25mmd2F},
  urldate = {2021-09-10},
  abstract = {源于人工智能领域语音处理技术的飞速发展,人们希望可以和机器进行更自然、贴切的交流。语音情感识别是继语音识别之后,人机交互中的又一热点问题,得到了研究者们广泛的关注。近年来,随着心理学、生理学、神经科学、认知科学和计算机科学的发展,探究符合人类情感表达相关联的新特征,是当前研究领域内十分重要的研究课题。但目前在维度语音情感识别方面的研究较少,而且没有公开认可的维度语音情感特征集和高效的分类方法。在维度语音情感特征集的构建方面,本文提取了维度语音情感识别常用的韵律学特征、音质特征和基于谱的特征,另外根据Teager等人实验发现的语音非线性产生模型,结合梅尔听觉心理认知规律,我们提取了非线性TeagerMel特征。这样提取的特征既可以考虑到语音产生的过程的非线性,同时也综合了人耳听觉的心理效应,分别在公开的情感语料库DISEC和VAM进行了实验,结果表面基于Teager非线性理论提取的特征识别效果要优于在语音处理中常用的梅尔倒谱系数。在上面提取的维度语音情感特征集的基础了,我们对此特征集做了一系列的后处理。传统的维度语音情感识别系统都是采用全局统计特征,即将提取的全句的帧特征进行统计,然而这种划分的方式有可能造成韵律学细节信息的丢失,所以本文研究了更加合适的情感识别单元,在语段粒度上进行各种统计量的计算。同时我们考虑到人脑对情感认知处理的三阶段过程,即酝酿阶段、情感充分表达阶段和情感收尾阶段。将这种认知过程的起伏变化用数学上的高斯函数进行模型化,这样我们得到了窗特征。为了能从人脑处理语音情感信号的角度出发,同时考虑到语音的时序信息,本文提出了一个基于认知机理的回馈神经网络（CMRNN）,并将CMRNN应用于维度语音情感识别。我们考虑到基于认知机理的反馈神经网络既可以综合短时帧上的情感特征,又可以融合长粒度的段统计特征和窗特征。与传统的语音情感识别系统相比,我们不仅探究了合适的情感表达时长,而且将短时帧特征和较长时的段特征和窗特特征融合体现在分类器的处理过程中,实现了时序信息对情感识别的补声道充作用。最后,我们用基于认知机理的回馈神经网络进行维度语音情感识别,在VAM维度语料库上进行了测试,在情感的三个维度,平均得到0.66相关性。同时,语段特征和情感认知窗特征在维度情感识别上有不同程度的提高,较之前的全局统计特征,在情感的效价维提高了16\%,证明了网络的有效性。},
  editora = {李, 海峰},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,CMRNN,cognitive mechanism,dimensional speech emotion recognition,extract multi granularity features,多粒度特征提取,维度语音情感识别,认知机理},
  annotation = {4 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陈_2016_基于多粒度特征融合的维度语音情感识别方法研究.pdf}
}

@thesis{ChenJiYuShenDuXueXiDeYuYinQingGanShiBie2019,
  type = {硕士},
  title = {基于深度学习的语音情感识别},
  author = {陈, 嘉},
  date = {2019},
  institution = {{南京邮电大学}},
  doi = {10.27251/d.cnki.gnjdc.2019.000586},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019691199.nh&uniplatform=NZKPT&v=GCl8fK%25mmd2FC%25mmd2FRULUwGEidsilLx8qmlxKXtnulAiAQmjpzZ7TenNWcGuDYk%25mmd2FbFlYHZAW},
  urldate = {2021-09-10},
  abstract = {情感是人类的一种丰富的心理行为,一直是很多科研领域的研究热点。语音信号是人与人之间最自然的交流方式,它不仅包含要传递的内容,而且包含丰富的情感因素,并已应用于情感研究。语音情感识别是以语音作为情感的载体来研究语音中各种情感的形成与变化,让计算机可以通过语音来解析出说话人的具体情感状况,从而使得人机交互变得更加人性化。在语音情感识别领域中,情感特征参数的提取和分类模型的训练是目前重要的研究方向,它们的好坏会直接影响着整个系统的识别率。本文结合当前热门的深度学习,提出了基于卷积神经网络（CNN）深浅层特征融合的语音情感识别方法以及基于深度神经网络（DNN）瓶颈层特征融合的语音情感识别框架。具体的研究工作如下:（1）综述了大量语音情感识别领域的相关文献,并对文献中的一些理论和常用的语音情感识别方法进行了仿真实验。详细介绍了语音情感识别的相关技术以及常用的分类模型等,为后续深入的研究工作做好充足的准备。（2）常用于语音情感识别的声学特征包括谱相关特征,韵律特征,音质特征以及上述特征的融合特征。这些特征往往只关注时域或频域,但是,语音信号中频域和时域存在相关性,而这种相关性在语音情感识别中起到关键的作用。语谱图作为语音信号的视觉表示,不仅表现了语音的时频特征,而且还反映了说话者的语言特征。本文利用卷积神经网络和语谱图进行语音情感识别研究,提出一个新型卷积神经网络,该网络可以将深层特征和浅层特征融合在一起,得到区分性更大的情感特征,采用目前较为流行的迁移学习的方法进行网络的训练和测试。实验结果表明,与传统的卷积神经网络相比较,所提出的深浅层特征融合的卷积神经网络在语音情感识别率上有一定的提升。（3）在利用卷积神经网络和语谱图进行语音情感识别的过程中,卷积神经网络中的每个层中的很多参数设置对于最终的识别效果有着很大影响,而在实验中很难找出这些参数的最优值,导致识别率无法取得显著的提高。近几年来,DNN在语音识别领域中的应用越来越多,本文设计了一种含有瓶颈层的DNN用来提取语音信号的瓶颈特征。该DNN可以将语音中的情感信息集中在瓶颈层,通过提取瓶颈特征来获取语音中包含的情感信息。然后,通过设置瓶颈层的位置提取不同层的瓶颈特征,融合不同瓶颈层的特征,结合支持向量机实现各类情感分类。实验结果表明,所提出的识别方法可以一定程度上提升语音情感识别率。},
  editora = {孙, 林慧},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Bottleneck Features,Convolutional Neural Network,Deep Learning,Deep Neural Network,Speech Emotion Recognition,Support Vector Machine,卷积神经网络,支持向量机,深度学习,深度神经网络,瓶颈特征,语音情感识别},
  annotation = {5 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陈_2019_基于深度学习的语音情感识别.pdf}
}

@thesis{ChenJiYuShiXuShenDuXueXiMoXingDeYuYinQingGanShiBieFangFaYanJiu2018,
  type = {硕士},
  title = {基于时序深度学习模型的语音情感识别方法研究},
  author = {陈, 晓敏},
  date = {2018},
  institution = {{哈尔滨工业大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1018896203.nh&uniplatform=NZKPT&v=wWT5WwShQJf7KPoE4dHvhKCLANyg1q5C0mZRW3ZN804KsVX6uy6P%25mmd2B8uTaHeAp8Hg},
  urldate = {2021-09-10},
  abstract = {随着语音识别技术的成熟,人们对语音情感识别技术的呼声越来也高,因为语音情感识别技术的发展将使机器步入更加人性化的时代,同时在很多领域都有不可估量的作用,如汽车驾驶、医疗服务、远程教育、疾病诊断等。但目前语音情感识别技术还未达到实用的程度,一方面因为情感活动本身是一种复杂的生理过程,另一方面用于语音情感识别的数据库、模型等还需要进一步的开发。本文从语音情感识别模型出发,针对传统长短时记忆模型（Long-Short Term Memory,LSTM）对所有语音帧信息都要学习的问题,认为情感语音的帧序列中分为情感帧和非情感帧,并提出面向情感语音识别的LSTM-CTC时序深度学习模型,通过联结主义时间分类（Connectionist temporal classification,CTC）方法自动对齐能力将情感标签对齐到语音中的情感帧上。在IEMOCAP情感数据库使用4类情感进行话者独立实验（高兴、悲伤、中性、生气）取得了65.7\%（UAR）和64.2\%（WAR）的识别性能,比目前性能最好的LSTM-ELM模型提高了2.3\%（UAR）和1.8\%（WAR）。接着,针对LSTM-CTC模型中对语音情感帧一视同仁的问题,分析认为语音情感中每一个情感帧的情感信息含量不同,所以本文从注意力机制角度出发提出Att RNN-RNN时序深度学习模型,将语音情感识别过程看作是一个编解码问题,考虑到人类的注意力具有从整体到局部的变化特性,使用LSTM作为解码器在每个时间步中计算注意力进行情感识别推断,模拟人类的注意力转变过程。在IEMOCAP数据库的四类情感识别上,获得了67.6\%（UAR）和67.5\%（WAR）的性能,优于LSTM-CTC模型。但是考虑到CTC方法具有将情感标签与语音帧自动对齐的特性,为了充分利用这一优势,在Att RNN-RNN模型基础上引入CTC方法,提出Attention-CTC融合模型,通过共用一个情感语义编码器,将CTC方法和Attention机制联系起来,CTC负责对齐语音中的情感关键帧,Attention机制负责在不同情感帧中抽取不同程度的信息进行学习。此模型在IEMOCAP库上取得了70.3\%（UAR）和65.1\%（WAR）的识别性能。最后,本论文实现一个在线语音情感识别系统\textemdash\textemdash OESERS系统,将上述的研究成果转化为实际应用产品。系统采用Client/Server结构,具有良好的识别性能、友好的人机交互界面和大规模并发任务处理能力。该系统为三星Bixby语音助手提供语音情感识别支持。本文的研究工作为目前语音情感识别领域中存在的关键性问题提供了有效的改进方案,经过实验证明,本文所提出的时序深度学习模型对语音情感识别任务效果显著,同时也为深度学习技术在处理时序序列问题上提供了新的思路和方向。},
  editora = {李, 海峰},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Attention mechanism,Deep learning,Neural network,Speech emotion recognition,注意力机制,深度学习,神经网络,语音情感识别},
  annotation = {12 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陈_2018_基于时序深度学习模型的语音情感识别方法研究.pdf}
}

@inproceedings{chenKeySparseTransformerMultimodal2022,
  title = {Key-{{Sparse Transformer}} for {{Multimodal Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Weidong and Xing, Xiaofeng and Xu, Xiangmin and Yang, Jichen and Pang, Jianxin},
  date = {2022},
  pages = {6897--6901},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746598},
  abstract = {Speech emotion recognition is a challenging research topic that plays a critical role in human-computer interaction. Multimodal inputs further improve the performance as more emotional information is used. However, existing studies learn all the information in the sample while only a small portion of it is about emotion. The redundant information will become noises and limit the system performance. In this paper, a key-sparse Transformer is proposed for efficient emotion recognition by focusing more on emotion related information. The proposed method is evaluated on the IEMOCAP and LSSED. Experimental results show that the proposed method achieves better performance than the state-of-the-art approaches.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Conferences,Emotion recognition,Focusing,Fuses,Human computer interaction,modality interaction,sparse network,speech emotion recognition,Speech recognition,System performance},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen et al_2022_Key-Sparse Transformer for Multimodal Speech Emotion Recognition.pdf}
}

@article{chenPhonemeUnitSpecificTimeDelayNeural2021,
  title = {Phoneme-{{Unit-Specific Time-Delay Neural Network}} for {{Speaker Verification}}},
  author = {Chen, Xianhong and Bao, Changchun},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1243--1255},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3065202},
  abstract = {Variations of speech content increase the difficulty of speaker verification. In this paper, to alleviate the negative effect of the variations, phoneme-unit-specific time-delay neural network (PUSTDNN) is proposed and applied to the state-of-the-art x-vector system. It models each phoneme unit with an individual time-delay neural network (TDNN). That is to say, each TDNN mainly deals with a phoneme unit. Compared with handling all phoneme units together, when handling a phoneme unit, a TDNN can extract more discriminative speaker information, thus improving the system performance. Two realizations of the PUSTDNN are proposed. The first one can retain speech temporal information. The second one further combines all the TDNNs in a PUSTDNN into a larger TDNN to reduce computational complexity. To avoid model overfitting, the phoneme units are obtained by clustering phonemes based on the phonetic knowledge and phonetic sparsity degree. The PUSTDNN is also compared with two other techniques, i.e., phonetic vector and multitask. Experiments on the Fisher, NIST SRE10, and VoxCeleb datasets show that the phonetic vector technique is most robust to the phoneme unit recognition accuracy. When the accuracy is high enough, the multitask performs better than the phonetic vector, and the PUSTDNN performs best and can achieve over 10\% relative improvement compared with the x-vector baseline.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Neural networks,Phoneme recognition accuracy,phoneme-unit-specific time-delay neural network (PUSTDNN),Phonetics,speaker verification,Speech processing,Speech recognition,System performance,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen_Bao_2021_Phoneme-Unit-Specific Time-Delay Neural Network for Speaker Verification.pdf}
}

@unpublished{chenSemiSupervisedLearningMultiHead2021,
  title = {Semi-{{Supervised Learning}} with {{Multi-Head Co-Training}}},
  author = {Chen, Mingcai and Du, Yuntao and Zhang, Yi and Qian, Shuwei and Wang, Chongjun},
  date = {2021-09-12},
  number = {arXiv:2107.04795},
  eprint = {2107.04795},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.04795},
  url = {http://arxiv.org/abs/2107.04795},
  urldate = {2022-05-31},
  abstract = {Co-training, extended from self-training, is one of the frameworks for semi-supervised learning. Without natural split of features, single-view co-training works at the cost of training extra classifiers, where the algorithm should be delicately designed to prevent individual classifiers from collapsing into each other. To remove these obstacles which deter the adoption of single-view co-training, we present a simple and efficient algorithm Multi-Head Co-Training. By integrating base learners into a multi-head structure, the model is in a minimal amount of extra parameters. Every classification head in the unified model interacts with its peers through a "Weak and Strong Augmentation" strategy, in which the diversity is naturally brought by the strong data augmentation. Therefore, the proposed method facilitates single-view co-training by 1). promoting diversity implicitly and 2). only requiring a small extra computational overhead. The effectiveness of Multi-Head Co-Training is demonstrated in an empirical study on standard semi-supervised learning benchmarks.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen et al_2021_Semi-Supervised Learning with Multi-Head Co-Training.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\59CR72TA\\2107.html}
}

@article{chenSensorImperfectionTolerance2021,
  title = {Sensor {{Imperfection Tolerance Analysis}} of {{Robust Linear Differential Microphone Arrays}}},
  author = {Chen, Zuolong and Chen, Huawei and Tu, Quansheng},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2915--2929},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2021.3110136},
  abstract = {An \$M\$th-order linear differential microphone array (LDMA) is conventionally designed by using a linear array of \$M+1\$ closely-spaced microphones. It is known that the conventional LDMAs suffer from the white noise amplification problem which may cause significant performance degradation in the presence of sensor imperfections. In order to resolve this, a more advanced solution has been proposed to employ more than \$M+1\$ microphones in the design of the LDMAs, which is shown to be effective against the white noise amplification and, hence, the resultant LDMAs are known as the robust LDMAs. Previous studies have shown that sensor imperfections can lead to dramatic performance degradation or even failure of the LDMAs due to the presence of the mainlobe orientation reversal (MOR) phenomenon. Therefore, avoiding the occurrence of the detrimental MOR phenomenon can serve as a minimum condition for the design of robust LDMAs in the presence of sensor imperfections. However, it is not yet quite clear how the sensor imperfections, such as microphone gain and phase mismatches, affect the robust LDMAs. Particularly in practical design, it will be of interest to know what the requirement is on the microphone imperfection tolerance for a given number of microphones, or alternatively, how many microphones should be used with a given microphone imperfection tolerance to guarantee no occurrence of the MOR phenomenon, which remains to be addressed. Motivated by the above, in this paper we first give an in-depth analysis of the impact of microphone imperfections on the mainlobe orientation of the robust LDMAs, and reveal how the MOR phenomenon occurs with the robust LDMAs. Then based on our theoretical foundation, the microphone imperfection tolerance analysis is performed, which provides a useful guidance when incorporating the influence of sensor imperfections into the practical design of robust LDMAs. Extensive numerical results are also presented to validate the effectiveness of our analysis.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {\#nosource,Array signal processing,Degradation,Differential microphone array,Microphone arrays,Sensor arrays,sensor imperfections,Speech processing,tolerance analysis,Tolerance analysis,White noise}
}

@article{ChenShenRuXueXiGuanCheXiJinPingWaiJiaoSiXiangTuiDongXinShiDaiZhongGuoGongHuiWaiShiGongZuoChuangXinFaZhan2021,
  title = {深入学习贯彻习近平外交思想 推动新时代中国工会外事工作创新发展},
  author = {陈, 刚},
  date = {2021},
  journaltitle = {当代世界},
  number = {10},
  pages = {4--9},
  issn = {1006-4206},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDAUTO&filename=JSDD202110003&uniplatform=NZKPT&v=R5r4pkxln1%25mmd2Bk3%25mmd2FRG0PCWOQ0hEarh3xrb9dQYWTvGC5K5b4Xz8EggFm8oo3V7B0kC},
  urldate = {2021-11-21},
  abstract = {\&lt;正\&gt;党的十八大以来,面对国际形势风云变幻,以习近平同志为核心的党中央深刻把握新时代中国和世界发展大势,着眼实现中华民族伟大复兴中国梦的战略全局和世界百年未有之大变局,积极推进外交理论和实践创新,形成了习近平外交思想,实现了中国外交理论的重大飞跃。习近平外交思想是习近平新时代中国特色社会主义思想的重要组成部分,是以习近平同志为核心的党中央治国理政思想在外交领域的集中体现,是新时代我国对外工作的根本遵循和行动指南。中国工会是中国共产党领导的职工自愿结合的工人阶级群众组织。},
  issue = {10},
  language = {zh-CN},
  keywords = {⛔ No DOI found,中国特色社会主义工会发展道路,习近平外交思想,习近平新时代中国特色社会主义思想,国际劳工组织,外事工作,工会组织},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陈_2021_深入学习贯彻习近平外交思想 推动新时代中国工会外事工作创新发展.pdf}
}

@thesis{ChenShuangZiKongJianQianYiXueXiFangFaDeKuaKuYuYinQingGanShiBie2019,
  type = {硕士},
  title = {双子空间迁移学习方法的跨库语音情感识别},
  author = {陈, 颖},
  date = {2019},
  institution = {{苏州大学}},
  doi = {10.27351/d.cnki.gszhu.2019.000944},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019262253.nh&uniplatform=NZKPT&v=9nXJ9EPHLyFtdGYVRxGxn14RXeYj29r4ddzWvFoFjYQy91kQFTsJ%25mmd2FJV4CNR5cVhT},
  urldate = {2021-09-10},
  abstract = {随着人工智能的快速发展,语音作为人类传达情感的重要方式之一,占据着越来越重要的位置。传统的语音情感识别技术都基于一个共同的假设:训练数据和测试数据都来源于同一个数据库,即训练集和测试集具有同样的特征空间分布。然而,由于不同语料库的情感获取方法、情感种类以及录音环境有所不同,此时训练集和测试集存在分布差异,从而导致基于同分布假设的传统语音情感识别方法不能够很好地解决跨库识别问题。而迁移学习的引入己被证明可以显著减少不同域之间特征分布的差异性,因此,本文提出了双子空间迁移学习框架（Dual-Subspace Transfer Learning,DSTL）以提高跨语料库的情感识别性能。针对特征映射迁移学习方法忽略特有信息的缺陷,本文工作提出了融合共性与特性的双子空间迁移学习框架,对仅利用共性的特征映射迁移学习进行改进,以提高情感识别性能。本文具体研究内容如下:（1）为进行跨库语音情感识别的性能比较,本文工作建立了汉语情感语音数据库（Mandarin Emotional Speech Dataset Portrayed,MES-P）。该数据库是由说话人根据离散情感标签录制完成,随后由标注者通过听觉感知及主观判断将每个语音样本的情感定量转化到效价度/唤醒度（Valence/Arousal,VA）空间。因此,该数据库不仅为本文的跨库语音情感识别研究提供了重要的数据基础,还可用于离散情感到维度空间转换的后续研究。（2）研究了全局与局部分布差异约束作为正则项的特征映射迁移学习方法。本文工作将基于类间距离和类内距离特征分组的改进主成分分析方法作为基础方法,利用全局相关的最大均值差异和局部相关的图嵌入方法分别作为正则项对其进行分布差异约束,得到三种不同的特征映射迁移学习方法。实验结果表明,与传统机器学习方法相比,特征映射迁移学习方法的召回率提升了 8.11\%。并且全局与局部分布差异算法在平衡库与不平衡库方案下展现出不同的识别性能。（3）针对主流特征映射迁移学习方法仅利用共性,而忽略特性的缺陷,提出融合共性与特性的双子空间迁移学习框架,双子空间指的是:a)公共子空间:利用特征映射迁移学习方法学习公共子空间,在该子空间中通过减少源域和目标域的分布差异以保留域之间的共有信息;b)特性子空间:针对特征映射迁移学习方法未利用特有信息的不足之处,提出了目标化源域特有信息（Source-specific Mapping to Target subspace,SMT）方法,能够在特性子空间中保留源域和目标域的特有信息。因此,该双子空间框架通过引入特有信息,能够对仅利用共性的特征映射迁移学习方法进行改进。结果表明,双子空间迁移学习方法的平均召回率较其基线方法得到3.05\%的提升,并且召回率高达61.67\%。},
  editora = {赵, 勋杰 and 肖, 仲喆},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Graph Embedding,Maximum Mean Discrepancy,Speech Emotion Recognition,Transfer Learning,最大均值差异,双子空间迁移学习,图嵌入,语音情感识别},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陈_2019_双子空间迁移学习方法的跨库语音情感识别.pdf}
}

@thesis{ChenShuoHuaRenShiBieQingGanHeChengWenTiDeGaiLuMoXingYanJiu2016,
  type = {硕士},
  title = {说话人识别情感合成问题的概率模型研究},
  author = {陈, 昊},
  date = {2016},
  institution = {{浙江大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201602&filename=1016063552.nh&uniplatform=NZKPT&v=FrgX7ai%25mmd2FzNEimcF2Ee3sFpvtOEgt5ALoIL%25mmd2FbyRRKDtBhKDFIpYfO65OMRrFUuXVg},
  urldate = {2021-09-10},
  abstract = {说话人识别技术对智能通信和信息处理具有重要的推进作用。同时,语音数据具有高维时间序列的典型特征,是信号处理和模式识别领域各种算法验证的珍贵数据资源。因此,这个领域的研究工作一直火热地进行着。语音识别系统的效率容易受到会话差异的影响。即测试语音和训练语音不匹配的情况下,算法准确率会相对下降。其中由于录音者情感变化导致的测试和训练语音不匹配被称为情感失配,这至今是一个开放问题,一直没有很好地被解决。本论文针对说话人识别情感失配问题,进行了原理的分析和现象的总结。证明说话人情感变换与常见的信道失配问题的不同,解释了现有算法难以解决这个问题的原因。并针对情感噪音的特质提出了基于概率模型的统计推断方法。完善了已有情感合成算法的理论基础,获得了在MASC数据库上个人可以重现的最佳效果。针对当前主流的因子分析模型过度拟合的问题,将原模型改进为更符合语音数据特征的完全贝叶斯模型和非参数IBP模型,并给出求解的迭代算法和MCMC采样算法。成功地在不降低推断效果的基础上,将因子分析的模型参数缩减到原问题的30\%。本论文的主要贡献如下：1.针对说话人识别情感失配问题,进行了原理的分析和现象的总结。针对不同的说话人以及不同的情感,目前仍然没有通用的模型可以描述情感变化的原因。针对不同的语音元素（音素）、说话人身份、情绪特性等等对说话人特征引起的变化具有很强的非线性特性。与一般的信道失配不同,情感失配没有办法用分隔开的空间分别表征说话人身份和信道的特征。由于数据量的不足,也没有办法通过LDA等技术进行无关信息的降维处理。值得注意的是,情感变化具有邻居相似的特性。即中性语音特征相似的说话人,一般在其他情感下也具有类似的特征。2.对说话人识别问题的概率推断模型进行抽象,将测试样本和模型参数数量引入分布提出了基于统计距离的分类模型。现阶段常用的通用背景模型系统经常依赖阶数很高的高斯混合模型训练。这类模型依赖相对庞大的背景语音数据库以及长度相对统一的训练和测试数据。基于贝叶斯统计的推断方法依指数分布族的共轭先验对进行生成模型的估计,不需要高斯混合模型的EM迭代训练,与之相比具有计算复杂性和算法效率上的优势。此外,通过基于AIC和BIC等模型选择理论的统计距离标准,可以有效地解决因语音长度变化等引起的信道失配问题。3.基于流型学习理论,对原有的近邻合成算法进行了推广,并在该框架下提出最优合成的求解方式,获得了模型合成最好的结果。由于情感变化具有邻居相似的特点,我们可以通过背景数据集中与训练模型相似的数据合成对应说话人其他情感的模型。本文提出了邻域最优线性合成算法,这种算法通过求解有约束的二阶优化问题,基于背景对中性语音模型进行最优重建。这个重建具有多种映射不变的特性,因此这个规律可以迁移到其他情感空间,采用对应的情感模型参数,对训练语音模型的情感参数进行估计。4.针对联合因子分析模型过度拟合的问题,提出了完整的概率方法,规范了模型的理论依据,并进行了非参数推广。联合因子分析模型在求解时,采用的是对特征变量进行估计,再对参数矩阵进行优化的EM迭代。由于对参数的稀疏性没有限制,并且参数矩阵的参数个数远大于特征向量,模型十分容易过度拟合,让说话人因子的范数无限接近O。通过对参数矩阵添加先验分布,我们可以用坐标下降的方法对模型进行迭代更新。实验证明这个算法可以有效地解决过度拟合的问题。在此基础上,本文又提出了基于IBP的非参数GMM超向量因子分析模型。可以自动适应说话人特征维度,将与说话人特征无关的高斯分量参数舍弃,提高模型的鲁棒性。实验证明可以以一般JFA模型30\%的参数达到与其相同的识别效果。},
  editora = {杨, 莹春},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Bayesian Statistics,Channel Mismatch,Emotional Speaker Recognition,Exponential Family,Joint Factor Analysis,Metropolis-Hasting Sampling,Metropolis-Hasting采样,Model Selection,Nonpara- metric Models,Novelty Detection,Statistical Distance,信道适配,情感说话人识别,指数分布组,新奇度检测,模型选择,统计距离,联合因子分析,贝叶斯统计,非参数模型},
  annotation = {1 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陈_2016_说话人识别情感合成问题的概率模型研究.pdf}
}

@article{chenSpeechSynthesisPresent,
  title = {Speech {{Synthesis}}: {{Past}}, {{Present}} and {{Future}}},
  author = {Chen, Yunlin},
  pages = {70},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen_Speech Synthesis.pdf}
}

@article{chenTaskLoadEstimation2021,
  title = {Task {{Load Estimation}} from {{Multimodal Head-Worn Sensors Using Event Sequence Features}}},
  author = {Chen, Siyuan and Epps, Julien},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {622--635},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2956135},
  abstract = {For longitudinal behavior analysis, task type is an inevitable and important variable. In this article, we propose an event-based behavior modeling approach and employ non-invasive wearable sensing modalities (eye activity, speech and head movement) to recognize task load level under four different task load types. The novelty lies in converting physiological and behavioral signals into meaningful events and utilizing their sequence across multiple modalities to distinguish load levels and types. We evaluated this approach on head-worn sensor data from 24 participants completing four different tasks for recognizing (i) low and high load level for a given task load type, (ii) low and high load level regardless of load type, and (iii) both load level and load type. Findings show that the recognition rate is reasonable in (i), close to chance level in (ii), and well above chance level in (iii) for 8 classes using participant-dependent and -independent schemes. Further, a fusion of the proposed event-based features and conventional continuous features achieved the best or similar performance in most cases. These results suggest that task type needs to be considered when using continuous features and that the proposed event-based modeling paradigm is promising for longitudinal behavior analysis.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {bag-of-words,Brain modeling,Computational modeling,Emotion recognition,Eye activity,head movement,Load modeling,physiological sensing,Sensors,speech,Speech recognition,Task analysis,task load,topic models},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen_Epps_2021_Task Load Estimation from Multimodal Head-Worn Sensors Using Event Sequence.pdf}
}

@thesis{ChenZhongWenYuYinQingGanWaJueDeYanJiuYuShiXian2018,
  type = {硕士},
  title = {中文语音情感挖掘的研究与实现},
  author = {陈, 琢},
  date = {2018},
  institution = {{电子科技大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201802&filename=1018991107.nh&uniplatform=NZKPT&v=lp8SKfzBubaai5ikJ9QDGAvJQcygyBeDVCvibgxYe3BpCeQH9ssj5RHoDic6lNbA},
  urldate = {2021-09-10},
  abstract = {语音情感挖掘（Speech Emotion Mining）是机器学习、模式识别等领域的核心应用之一,其主要研究目标是围绕语音的信号分析、特征提取、算法模型建立,对叙述人产生的连续语音信号进行情感分类。在当前信息社会进程中,语音情感挖掘的研究不仅具有重要的理论研究意义,同时具有十分重要的工程价值。当前关于语音情感挖掘问题的相关研究较多,但是由于这些工作主要围绕某种特定语言种类或某特定数据集进行,很难具有普适性应用环节,这导致当下一些基于非中文语音情感数据的研究工作不能很好适应中文背景的挖掘任务。此外,由于语音情感类数据在收集与标注上的困难,导致当下可用于研究的数据规模较小、种类较为贫乏。本文的研究主要集中在基于中文语音的情感挖掘模型设计上,同时针对当前中文情感语音数据集存在的不足,设计可对其进行数据增强的策略。本文的主要工作包含:1、提出了一种以基于多层、多通道特征图作为输入,配合卷积神经网络（CNN,Convolutional Neural Networks）过程与门控循环单元（GRU,Gated Recurrent Unit）循环过程的神经网络结构:多通道卷积循环网络（MSCGNN,Multi-Channel Spectrogram Conv-GRU Neural NetWork）结构作为识别模型。依托于中科院自动化所录制的CASIA中文语音情感数据集（CASIA,Institute of Automation,Chinese Academy of Science）进行模型训练,并与相关的语音情感挖掘模型进行对比实验,最后实验表明了MSCGNN在中文语音情感挖掘任务上表现较为出色。2、本文创新的提出了一种借助于变分自编码器进行语音语谱图生成,再通过计算该图与各情感类别稀疏编码器的重建误差,从而实现对其标注情感类别的数据增强策略。最后通过实验证明了该数据增强策略对深度学习模型学习起到较好的作用。3、根据语音情感挖掘任务的特殊性以及实际应用中的需求,本文在最后给出了一套基于在线学习的中文语音情感挖掘系统的架构体系,并配以原型系统中任务处理过程以及客户端界面的相关展示。},
  editora = {刘, 贵松},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,data augmentation,deep learning,online learning,speech emotion mining,在线学习,数据增强,深度学习,语音情感挖掘},
  annotation = {2 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陈_2018_中文语音情感挖掘的研究与实现.pdf}
}

@article{chieaOptimalEnvelopeBasedNoise2021,
  title = {An {{Optimal Envelope-Based Noise Reduction Method}} for {{Cochlear Implants}}: {{An Upper Bound Performance Investigation}}},
  shorttitle = {An {{Optimal Envelope-Based Noise Reduction Method}} for {{Cochlear Implants}}},
  author = {Chiea, Rafael A. and Costa, M\'arcio H. and Cordioli, J\'ulio A.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1729--1739},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3076363},
  abstract = {Cochlear implants (CI) are surgically implanted auditory prostheses for partially compensating severe to profound sensorineural hearing losses. Despite providing significant intelligibility levels under silent conditions, their performance is highly affected by additive noise. Time-frequency masks, such as the binary mask, the Wiener filter (WF) and their variations, have been widely employed for noise reduction. However, they were not originally designed for CI applications, which have very particular characteristics. In this work, we propose a new method for noise reduction, especially designed for CI. It is based on the minimization of the mean square error between the squared envelopes of the estimated and target speech. The theoretical derivation of the proposed time-frequency mask is presented and a closed-form for the one-coefficient optimal solution is obtained. Numerical simulations with objective criteria indicate that the proposed method results in higher intelligibility scores as compared to the time-domain implementation of the WF time-frequency mask. Psychoacoustic experiments with normal hearing volunteers and vocoded signals, as well as CI users, corroborate simulation results, showing significant increases in intelligibility, especially for SNR {$<$} -5 dB. The proposed method may result in an intelligibility increase of up to 70\%, for SNR = -25 dB, as compared to the WF. These findings were obtained considering an ideal signal-to-noise ratio estimator.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Auditory system,Cochlear implant,Cochlear implants,Noise measurement,Noise reduction,Signal to noise ratio,Temporal envelope,Time-domain analysis,Time-frequency analysis,Time-frequency mask,Wiener filters},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chiea et al_2021_An Optimal Envelope-Based Noise Reduction Method for Cochlear Implants.pdf}
}

@article{chienAcousticMeasureVocal2021,
  title = {Acoustic {{Measure}} of {{Vocal Strain Based}} on {{Glottal Airflow Periodicity}}},
  author = {Chien, Yu-Ren and Gu\dh nason, J\'on},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {563--574},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3044168},
  abstract = {In the clinical practice of dysphonia, the effects of treatment are traditionally monitored by a sequence of auditory-perceptual assessments aimed at measuring vocal quality for the patient. Alternatively, acoustic measurement of vocal quality promises to automate perceptual assessments while keeping the assessments accurate and non-invasive. However, acoustic measures of vocal quality need to be further developed in both functional and technical terms. On the one hand, many of them are susceptible to non-dysphonic perturbations from articulatory movements in continuous speech, while on the other, their accuracy in approximating the generally nonlinear mapping from observation to vocal quality is limited by their use of a linear model. This paper presents an acoustic measure of vocal strain, a specific vocal quality that typically co-occurs with the development of vocal-fold nodules in vocal hyper-function. Vocal strain merits acoustic measurement more than other vocal qualities because its perceptual assessment typically exhibits a lower intra- and inter-rater reliability than the assessment of other vocal qualities. Based on an assumed correlation between vocal strain and the degree of periodicity in vocal-fold vibrations, this paper presents an acoustic measure in which a nonlinear regression model is used to predict the strain from some periodicity features extracted from a glottal airflow estimate. When tested on a set of listener-rated utterances composed mostly of continuous speech, the proposed glottal measure outperformed a direct-analysis measure in producing strain assessments which are consistent with perceptual ratings.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustic measurements,Current measurement,Feature extraction,glottal airflow estimation,glottal inverse filtering,objective assessment,Pathology,Strain,Strain measurement,Time measurement,vocal strain,Vocal strain},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chien_Guðnason_2021_Acoustic Measure of Vocal Strain Based on Glottal Airflow Periodicity.pdf}
}

@unpublished{chienInvestigatingIncorporatingPretrained2021,
  title = {Investigating on {{Incorporating Pretrained}} and {{Learnable Speaker Representations}} for {{Multi-Speaker Multi-Style Text-to-Speech}}},
  author = {Chien, Chung-Ming and Lin, Jheng-Hao and Huang, Chien-yu and Hsu, Po-chun and Lee, Hung-yi},
  date = {2021-05-01},
  eprint = {2103.04088},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.04088},
  urldate = {2021-09-10},
  abstract = {The few-shot multi-speaker multi-style voice cloning task is to synthesize utterances with voice and speaking style similar to a reference speaker given only a few reference samples. In this work, we investigate different speaker representations and proposed to integrate pretrained and learnable speaker representations. Among different types of embeddings, the embedding pretrained by voice conversion achieves the best performance. The FastSpeech 2 model combined with both pretrained and learnable speaker representations shows great generalization ability on few-shot speakers and achieved 2nd place in the one-shot track of the ICASSP 2021 M2VoC challenge.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chien et al_2021_Investigating on Incorporating Pretrained and Learnable Speaker Representations.pdf}
}

@article{choConvolutionalMaximumLikelihoodDistortionless2021,
  title = {Convolutional {{Maximum-Likelihood Distortionless Response Beamforming With Steering Vector Estimation}} for {{Robust Speech Recognition}}},
  author = {Cho, Byung Joon and Park, Hyung-Min},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1352--1367},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3067202},
  abstract = {Beamforming has been one of the most successful approaches using multi-microphones for robust speech recognition. Although a beamforming method, called the ``maximum-likelihood distortionless response (MLDR)'' beamformer, was recently presented to achieve promising performance, it requires an accurate steering vector for a target speaker in advance like many kinds of beamformers. In this paper, we present a method for steering vector estimation (SVE) by replacing the noise spatial covariance matrix estimate with a normalized version of the variance-weighted spatial covariance matrix estimate for the observed noisy speech signal obtained by the iterative update rule in the MLDR beamforming framework. In addition, an MLDR beamforming method without a steering vector for a target speaker given in advance is presented where the SVE and the beamforming are alternately repeated. Furthermore, an online algorithm based on recursive least squares (RLS) is derived to cope with various practical applications including time-varying situations, and the power method is introduced for further efficient online processing. We also present batch and online convolutional MLDR beamforming with SVE for simultaneous beamforming and dereverberation where the weighted prediction error (WPE) dereverberation and the MLDR beamforming with the SVE were jointly optimized based on the maximum-likelihood estimation (MLE) for a zero-mean complex Gaussian signal with time-varying variances. Moreover, input signals masked by a neural network (NN) for estimating target speech or noise components can be used to further improve the presented beamformers. Experimental results on the CHiME-4 and REVERB challenge datasets demonstrate the effectiveness of the presented methods.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Array signal processing,Beamforming,Convolution,Covariance matrices,dereverberation,Maximum likelihood estimation,maximum-likelihood estimation,Noise measurement,robust speech recognition,Speech processing,Speech recognition,steering vector estimation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cho_Park_2021_Convolutional Maximum-Likelihood Distortionless Response Beamforming With.pdf}
}

@inproceedings{choiStarGANUnifiedGenerative2018,
  title = {{{StarGAN}}: {{Unified Generative Adversarial Networks}} for {{Multi-domain Image-to-Image Translation}}},
  shorttitle = {{{StarGAN}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  date = {2018-06},
  pages = {8789--8797},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00916},
  abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  language = {en},
  keywords = {Gallium nitride,Generative adversarial networks,Generators,Hair,Image reconstruction,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Choi et al_2018_StarGAN.pdf}
}

@unpublished{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  options = {useprefix=true},
  date = {2014-09-02},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1406.1078},
  urldate = {2021-10-20},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\FL2KY4EU\\1406.html}
}

@misc{choMultispeakerEmotionalTexttospeech2021,
  title = {Multi-Speaker {{Emotional Text-to-speech Synthesizer}}},
  author = {Cho, Sungjae and Lee, Soo-Young},
  date = {2021-12-07},
  number = {arXiv:2112.03557},
  eprint = {2112.03557},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.03557},
  url = {http://arxiv.org/abs/2112.03557},
  urldate = {2022-10-01},
  abstract = {We present a methodology to train our multi-speaker emotional text-to-speech synthesizer that can express speech for 10 speakers' 7 different emotions. All silences from audio samples are removed prior to learning. This results in fast learning by our model. Curriculum learning is applied to train our model efficiently. Our model is first trained with a large single-speaker neutral dataset, and then trained with neutral speech from all speakers. Finally, our model is trained using datasets of emotional speech from all speakers. In each stage, training samples of each speaker-emotion pair have equal probability to appear in mini-batches. Through this procedure, our model can synthesize speech for all targeted speakers and emotions. Our synthesized audio sets are available on our web page.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cho_Lee_2021_Multi-speaker Emotional Text-to-speech Synthesizer.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\92N2PZD2\\2112.html}
}

@inproceedings{chopraMetaLearningLowResourceSpeech2021,
  title = {Meta-{{Learning}} for {{Low-Resource Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chopra, Suransh and Mathur, Puneet and Sawhney, Ramit and Shah, Rajiv Ratn},
  date = {2021-06},
  pages = {6259--6263},
  issn = {2379-190X},
  doi = {10/gmr2j3},
  abstract = {While emotion recognition is a well-studied task, it remains unexplored to a large extent in cross-lingual settings. Speech Emotion Recognition (SER) in low-resource languages poses difficulties as existing approaches for knowledge transfer do not generalize seamlessly. Probing the learning process of generalized representations across languages, we propose a meta-learning approach for low-resource speech emotion recognition. The proposed approach achieves fast adaptation on a number of unseen target languages simultaneously. We evaluate the Model Agnostic Meta-Learning (MAML) algorithm on three low-resource target languages -Persian, Italian, and Urdu. We empirically demonstrate that our proposed method - MetaSER1, considerably outperforms multitask and transfer learning-based methods for speech emotion recognition task, and discuss the benefits, efficiency, and challenges of MetaSER on limited data settings.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_reading,Emotion recognition,Learning systems,low-resource language adaptation,meta-learning,Signal processing algorithms,speech emotion recognition,Speech enhancement,Speech recognition,Training,transfer learning,Transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chopra et al_2021_Meta-Learning for Low-Resource Speech Emotion Recognition.pdf}
}

@inproceedings{chouExploitingAnnotatorsTyped2022,
  title = {Exploiting {{Annotators}}' {{Typed Description}} of {{Emotion Perception}} to {{Maximize Utilization}} of {{Ratings}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chou, Huang-Cheng and Lin, Wei-Cheng and Lee, Chi-Chun and Busso, Carlos},
  date = {2022},
  pages = {7717--7721},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746990},
  abstract = {The decision of ground truth for speech emotion recognition (SER) is still a critical issue in affective computing tasks. Previous studies on emotion recognition often rely on consensus labels after aggregating the classes selected by multiple annotators. It is common for a perceptual evaluation conducted to annotate emotional corpora to include the class ``other,'' allowing the annotators the opportunity to describe the emotion with their own words. This practice provides valuable emotional information, which, however, is ignored in most emotion recognition studies. This paper utilizes easy-accessed natural language processing toolkits to mine the sentiment of these typed descriptions, enriching and maximizing the information obtained from the annotators. The polarity information is combined with primary and secondary annotations provided by individual evaluators under a label distribution framework, creating a complete representation of the emotional content of the spoken sentences. Finally, we train multitask learning SER models with existing learning methods (soft-label, multi-label, and distribution-label) to show the performance of the novel ground truth in the MSP-Podcast corpus.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Affective computing,Annotations,Conferences,Distribution-label learning,Emotion recognition,Learning systems,Multi-label learning,Signal processing,Soft-label learning,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chou et al_2022_Exploiting Annotators’ Typed Description of Emotion Perception to Maximize.pdf}
}

@inproceedings{chouExploitingCooccurrenceFrequency2022,
  title = {Exploiting {{Co-occurrence Frequency}} of {{Emotions}} in {{Perceptual Evaluations To Train A Speech Emotion Classifier}}},
  booktitle = {Interspeech 2022},
  author = {Chou, Huang-Cheng and Lee, Chi-Chun and Busso, Carlos},
  date = {2022-09-18},
  pages = {161--165},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11041},
  url = {https://www.isca-speech.org/archive/interspeech_2022/chou22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chou et al_2022_Exploiting Co-occurrence Frequency of Emotions in Perceptual Evaluations To.pdf}
}

@article{chuangImprovingAutomaticSpeech2021,
  title = {Improving {{Automatic Speech Recognition}} and {{Speech Translation}} via {{Word Embedding Prediction}}},
  author = {Chuang, Shun-Po and Liu, Alexander H. and Sung, Tzu-Wei and Lee, Hung-yi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {93--105},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3037543},
  abstract = {In this article, we target speech translation (ST). We propose lightweight approaches that generally improve either ASR or end-to-end ST models. We leverage continuous representations of words, known as word embeddings, to improve ASR in cascaded systems as well as end-to-end ST models. The benefit of using word embedding is that word embedding can be obtained easily by training on pure textual data, which alleviates data scarcity issue. Also, word embedding provides additional contextual information to speech models. We motivate to distill the knowledge from word embedding into speech models. In ASR, we use word embeddings as a regularizer to reduce the WER, and further propose a novel decoding method to fuse the semantic relations among words for further improvement. In the end-to-end ST model, we propose leveraging word embeddings as an intermediate representation to enhance translation performance. Our analysis shows that it is possible to map speech signals to semantic space, which motivates future work on applying the proposed methods in spoken language processing tasks.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Decoding,Multitasking,Predictive models,Semantics,Speech processing,Speech recognition,speech translation,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chuang et al_2021_Improving Automatic Speech Recognition and Speech Translation via Word.pdf}
}

@unpublished{chungSpeech2VecSequencetoSequenceFramework2018,
  title = {{{Speech2Vec}}: {{A Sequence-to-Sequence Framework}} for {{Learning Word Embeddings}} from {{Speech}}},
  shorttitle = {{{Speech2Vec}}},
  author = {Chung, Yu-An and Glass, James},
  date = {2018-06-09},
  eprint = {1803.08976},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.08976},
  urldate = {2021-10-20},
  abstract = {In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec. Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {_reading,⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chung_Glass_2018_Speech2Vec.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\SV72EPPN\\1803.html}
}

@unpublished{chungUnsupervisedCrossModalAlignment2018,
  title = {Unsupervised {{Cross-Modal Alignment}} of {{Speech}} and {{Text Embedding Spaces}}},
  author = {Chung, Yu-An and Weng, Wei-Hung and Tong, Schrasing and Glass, James},
  date = {2018-09-20},
  eprint = {1805.07467},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1805.07467},
  urldate = {2021-10-19},
  abstract = {Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {_readed,⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chung et al_2018_Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\KAGSM7ZD\\1805.html}
}

@inproceedings{contiDeepfakeSpeechDetection2022,
  title = {Deepfake {{Speech Detection Through Emotion Recognition}}: {{A Semantic Approach}}},
  shorttitle = {Deepfake {{Speech Detection Through Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Conti, Emanuele and Salvi, Davide and Borrelli, Clara and Hosler, Brian and Bestagini, Paolo and Antonacci, Fabio and Sarti, Augusto and Stamm, Matthew C. and Tubaro, Stefano},
  date = {2022},
  pages = {8962--8966},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747186},
  abstract = {In recent years, audio and video deepfake technology has advanced relentlessly, severely impacting people's reputation and reliability. Several factors have facilitated the growing deepfake threat. On the one hand, the hyper-connected society of social and mass media enables the spread of multimedia content worldwide in real-time, facilitating the dissemination of counterfeit material. On the other hand, neural network-based techniques have made deepfakes easier to produce and difficult to detect, showing that the analysis of low-level features is no longer sufficient for the task. This situation makes it crucial to design systems that allow detecting deepfakes at both video and audio levels. In this paper, we propose a new audio spoofing detection system leveraging emotional features. The rationale behind the proposed method is that audio deepfake techniques cannot correctly synthesize natural emotional behavior. Therefore, we feed our deepfake detector with high-level features obtained from a state-of-the-art Speech Emotion Recognition (SER) system. As the used descriptors capture semantic audio information, the proposed system proves robust in cross-dataset scenarios outperforming the considered baseline on multiple datasets.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {audio forensics,deep learning,deepfake,Emotion recognition,Semantics,Signal processing algorithms,Speech recognition,Streaming media,Transfer learning,Voice activity detection},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Conti et al_2022_Deepfake Speech Detection Through Emotion Recognition.pdf}
}

@inproceedings{cuiEMOVIEMandarinEmotion2021,
  title = {{{EMOVIE}}: {{A Mandarin Emotion Speech Dataset}} with a {{Simple Emotional Text-to-Speech Model}}},
  shorttitle = {{{EMOVIE}}},
  booktitle = {Interspeech 2021},
  author = {Cui, Chenye and Ren, Yi and Liu, Jinglin and Chen, Feiyang and Huang, Rongjie and Lei, Ming and Zhao, Zhou},
  date = {2021-08-30},
  pages = {2766--2770},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1148},
  url = {https://www.isca-speech.org/archive/interspeech_2021/cui21c_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cui et al_2021_EMOVIE.pdf}
}

@thesis{DaiJiYuBiaoQingHeYuYinShuangMoTaiDeErTongQingGanShiBieYanJiu2016,
  type = {硕士},
  title = {基于表情和语音双模态的儿童情感识别研究},
  author = {戴, 惟嘉},
  date = {2016},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201701&filename=1016323954.nh&uniplatform=NZKPT&v=oPSI81qF18BtIKwKayw%25mmd2FNlHgf0Do3M%25mmd2B5kWX01XaVQ8ACJe%25mmd2B5q2YrqmatMP8kR%25mmd2FF3},
  urldate = {2021-09-10},
  abstract = {近年来,基于生物信号（如人脸表情图像、语音信号、脑电信号等）的情感识别研究受到了国内外研究人员的广泛关注,已经成为情感计算、模式识别以及计算机视觉等领域的一个研究热点。表情、语音等生物信号是人类表达情感的重要媒介,研究人员通过设计一些特征以及算法对这些生物信号进行分析,可以使计算机具备识别人类情感状态的能力。尽管单模态情感识别研究已经取得了很大的进步,然而可以注意到,目前情感识别的研究很大一部分是针对如表情、语音或脑电等单一生物信号（模态）进行的。相比单模态,两种或多种模态拥有更多的情感信息,因此,深度挖掘和融合多种模态的生物信号,是进一步丰富情感识别的研究、提出更为有效的情感识别方法的一种有效途径。本文主要研究了基于语音信号和视频信号的双模态情感识别这一问题,在双模态数据采集与整理、双模态特征提取以及分类等方面做了一些工作,具体研究内容如下：（1）创建了一个儿童双模态情感数据库。现有的双模态情感数据库均以成人为采集对象,以儿童为对象的公共数据库很少。借助儿童发展与学习科学教育部重点实验室这个平台,设计了专业的针对儿童的情感诱发实验范式,共采集了26名儿童的语音和表情双模态情感数据,并通过投票方式,对数据进行情感标注。（2）全面综述了基于表情或语音的单模态情感识别研究中的特征提取技术,并详细介绍表情识别、语音情感识别中常用的特征,同时在多个人脸表情库、语音数据库上做了简单的单模态情感识别实验。此外,结合上述表情和语音特征,给出了儿童双模态情感数据库的基准识别性能。（3）提出了一种基于双稀疏线性判别分析（Bi-Sparse Linear Discriminant Analysis, BSLDA）的语音与表情特征融合算法。BSLDA算法首先利用语音和视频的情感标签信息构造出一个语义特征空间,然后将语音和视频特征同时投影到该语义特征空间中。在该语义特征空间中,投影后的语音和视频特征集具有尽可能相似的分布,从而能够有效地进行更进一步的融合,融合后的特征具有较强的情感判别能力。此外,在特征投影过程中,BSLDA还能够根据各模态的特征对于情感分类的贡献,分别选择出各模态的重要的特征。（4）提出一种融合全局与局部信息的双模态情感识别方法。现有的双模态情感识别研究大多采用的是全局特征,较少考虑各模态信号的时序和局部信息。针对语音和视频的时序信号特点,本方法考虑了两种模态的局部信息,提出一种多尺度特征提取方法提取两种模态的局部特征与全局特征,一起用于双模态情感识别。为了进一步地对全局与局部特征进行处理以及实现双模态情感识别,提出了一个联合稀疏减秩回归模型（Joint Sparse Reduced Rank Regression,JSRRR）。JSRRR模型能够根据全局特征和局部特征对于情感识别的贡献大小,学习出衡量它们贡献度的权值。同时,JSRRR还能够对加权后的全局与局部特征进行二次学习,选择出其中具有区分不同情感状态能力的特征。},
  editora = {郑, 文明},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,bimodal emotion database,bimodal emotion recognition,facial expression recognition,feature fusion,feature selection,speech emotion recognition,人脸表情识别,双模态情感数据库,双模态情感识别,特征融合,特征选择,语音情感识别},
  annotation = {8 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\戴_2016_基于表情和语音双模态的儿童情感识别研究.pdf}
}

@inproceedings{daiLearningDiscriminativeFeatures2019,
  title = {Learning {{Discriminative Features}} from {{Spectrograms Using Center Loss}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Dai, Dongyang and Wu, Zhiyong and Li, Runnan and Wu, Xixin and Jia, Jia and Meng, Helen},
  date = {2019},
  pages = {7405--7409},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8683765},
  abstract = {Identifying the emotional state from speech is essential for the natural interaction of the machine with the speaker. However, extracting effective features for emotion recognition is difficult, as emotions are ambiguous. We propose a novel approach to learn discriminative features from variable length spectrograms for emotion recognition by cooperating soft-max cross-entropy loss and center loss together. The soft-max cross-entropy loss enables features from different emotion categories separable, and center loss efficiently pulls the features belonging to the same emotion category to their center. By combining the two losses together, the discriminative power will be highly enhanced, which leads to network learning more effective features for emotion recognition. As demonstrated by the experimental results, after introducing center loss, both the unweighted accuracy and weighted accuracy are improved by over 3\% on Mel-spectrogram input, and more than 4\% on Short Time Fourier Transform spectrogram input.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Center loss,discriminative features,Emotion recognition,Feature extraction,Mathematical model,Spectrogram,speech emotion recognition,Speech recognition,Support vector machines,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dai et al_2019_Learning Discriminative Features from Spectrograms Using Center Loss for Speech.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\LE9ZFIRF\\Dai 等。 - 2019 - Learning Discriminative Features from Spectrograms.pdf}
}

@inproceedings{dasTransferableSpeechEmotion2022,
  title = {Towards {{Transferable Speech Emotion Representation}}: {{On Loss Functions}} for {{Cross-Lingual Latent Representations}}},
  shorttitle = {Towards {{Transferable Speech Emotion Representation}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Das, Sneha and Nadine L\o nfeldt, Nicole and Katrine Pagsberg, Anne and Clemmensen, Line H.},
  date = {2022},
  pages = {6452--6456},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746450},
  abstract = {In recent years, speech emotion recognition (SER) has been used in wide ranging applications, from healthcare to the commercial sector. In addition to signal processing approaches, methods for SER now also use deep learning techniques which provide transfer learning possibilities. However, generalizing over languages, corpora and recording conditions is still an open challenge. In this work we address this gap by exploring loss functions that aid in transferability, specifically to non-tonal languages. We propose a variational autoencoder (VAE) with KL annealing and a semi-supervised VAE to obtain more consistent latent embedding distributions across data sets. To ensure transferability, the distribution of the latent embedding should be similar across non-tonal languages (data sets). We start by presenting a low-complexity SER based on a denoising-autoencoder, which achieves an unweighted classification accuracy of over 52.09\% for four-class emotion classification. This performance is comparable to that of similar baseline methods. Following this, we employ a VAE, the semi-supervised VAE and the VAE with KL annealing to obtain a more regularized latent space. We show that while the DAE has the highest classification accuracy among the methods, the semi-supervised VAE has a comparable classification accuracy and a more consistent latent embedding distribution over data sets.\textsuperscript{1}},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Annealing,cross-lingual,Deep learning,Emotion recognition,latent representation,loss functions,Medical services,Signal processing,speech emotion recognition (SER),Speech recognition,transfer learning,Transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Das et al_2022_Towards Transferable Speech Emotion Representation.pdf}
}

@article{dawalatabadNovelArchitecturesUnsupervised2021,
  title = {Novel {{Architectures}} for {{Unsupervised Information Bottleneck Based Speaker Diarization}} of {{Meetings}}},
  author = {Dawalatabad, Nauman and Madikeri, Srikanth and Sekhar, C. Chandra and Murthy, Hema A.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {14--27},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3036231},
  abstract = {Speaker diarization is an important problem that is topical, and is especially useful as a preprocessor for conversational speech related applications. The objective of this article is two-fold: (i) segment initialization by uniformly distributing speaker information across the initial segments, and (ii) incorporating speaker discriminative features within the unsupervised diarization framework. In the first part of the work, a varying length segment initialization technique for Information Bottleneck (IB) based speaker diarization system using phoneme rate as the side information is proposed. This initialization distributes speaker information uniformly across the segments and provides a better starting point for IB based clustering. In the second part of the work, we present a Two-Pass Information Bottleneck (TPIB) based speaker diarization system that incorporates speaker discriminative features during the process of diarization. The TPIB based speaker diarization system has shown improvement over the baseline IB based system. During the first pass of the TPIB system, a coarse segmentation is performed using IB based clustering. The alignments obtained are used to generate speaker discriminative features using a shallow feed-forward neural network and linear discriminant analysis. The discriminative features obtained are used in the second pass to obtain the final speaker boundaries. In the final part of the paper, variable segment initialization is combined with the TPIB framework. This leverages the advantages of better segment initialization and speaker discriminative features that results in an additional improvement in performance. An evaluation on standard meeting datasets shows that a significant absolute improvement of 3.9\% and 4.7\% is obtained on the NIST and AMI datasets, respectively.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Feature extraction,Information bottleneck,Mel frequency cepstral coefficient,Mutual information,phoneme rate,speaker diarization,speaker discriminative features,Speech processing,Task analysis,two-pass system,varying length segment},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dawalatabad et al_2021_Novel Architectures for Unsupervised Information Bottleneck Based Speaker.pdf}
}

@article{debruyneJointEmotionLabel2022,
  title = {Joint Emotion Label Space Modeling for Affect Lexica},
  author = {De Bruyne, Luna and Atanasova, Pepa and Augenstein, Isabelle},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101257},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101257},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000644},
  urldate = {2021-09-28},
  abstract = {Emotion lexica are commonly used resources to combat data poverty in automatic emotion detection. However, vocabulary coverage issues, differences in construction method and discrepancies in emotion framework and representation result in a heterogeneous landscape of emotion detection resources, calling for a unified approach to utilizing them. To combat this, we present an extended emotion lexicon of 30,273 unique entries, which is a result of merging eight existing emotion lexica by means of a multi-view variational autoencoder (VAE). We showed that a VAE is a valid approach for combining lexica with different label spaces into a joint emotion label space with a chosen number of dimensions, and that these dimensions are still interpretable. We tested the utility of the unified VAE lexicon by employing the lexicon values as features in an emotion detection model. We found that the VAE lexicon outperformed individual lexica, but contrary to our expectations, it did not outperform a naive concatenation of lexica, although it did contribute to the naive concatenation when added as an extra lexicon. Furthermore, using lexicon information as additional features on top of state-of-the-art language models usually resulted in a better performance than when no lexicon information was used.},
  language = {en},
  keywords = {_reading,Emotion detection,Emotion lexica,NLP,VAE},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\De Bruyne et al_2022_Joint emotion label space modeling for affect lexica.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\CXCG7V3Z\\S0885230821000644.html}
}

@inproceedings{delvauxTellingSelfdefiningMemories2022,
  title = {Telling Self-Defining Memories: {{An}} Acoustic Study of Natural Emotional Speech Productions},
  shorttitle = {Telling Self-Defining Memories},
  booktitle = {Interspeech 2022},
  author = {Delvaux, Veronique and Lavall\'ee, Audrey and Degouis, Fanny and Saloppe, Xavier and Nandrino, Jean-Louis and Pham, Thierry},
  date = {2022-09-18},
  pages = {1337--1341},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10554},
  url = {https://www.isca-speech.org/archive/interspeech_2022/delvaux22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Delvaux et al_2022_Telling self-defining memories.pdf}
}

@thesis{DengJiYuShenDuXueXiDeYuYinQingGanShiBieYanJiu2020,
  type = {硕士},
  title = {基于深度学习的语音情感识别研究},
  author = {邓, 梦霞},
  date = {2020},
  institution = {{浙江大学}},
  doi = {10.27461/d.cnki.gzjdx.2020.003223},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020436659.nh&uniplatform=NZKPT&v=WBHSbZ%25mmd2B42A9OPGrjQxPcZn5MrPaE49zXfmTTzlOqtMxmcf4Zr8dRoWiapO9rBy5Y},
  urldate = {2021-09-10},
  abstract = {语音情感识别是指通过语音来识别说话人的情绪状态,是语音技术领域最具挑战的任务之一。而随着语音交互技术的广泛应用,能使机器更人性化的语音情感识别技术有着广阔的应用前景和商业价值。近年来随着深度学习技术的发展,语音情感识别领域也出现了许多成果。然而即便如此,现阶段的语音情感识别技术仍面临许多困难,例如高维度情感特征难以人工提取、情感语音数据量小且标注难度大、深度学习模型受语义语种信息的干扰等,导致模型识别准确率差且跨语种表现差。因此本文为了提高情感识别的准确率和跨语种跨数据集的表现力,开展了以下工作:首先,基于情感信息的高维特性,本文在语音声谱图的基础上提出一种符合高斯分布的情感特征\textemdash\textemdash 平滑实谱图,解决了传统声谱图中的零值问题,并去除数据中异常值和极端值的影响,提高语音情感识别性能。其次,为了提高模型从低维特征中提取高维情感信息的能力,本文提出了一种基于注意力机制的CNN-RNN模型（ACRNN）,结合CNN的特征提取优势、RNN的序列任务优势以及注意力机制的局部关注优势,有效提高识别准确率。再次,为了克服小数据量限制并去除语义等信息的干扰,以提高模型识别准确性和跨语种跨数据集有效性,本文提出了一种对抗语义擦除方法,在小数据量的限制下,利用现有大数据量的语音识别任务成果进行辅助,将语义信息从语音特征中进行擦除,提高模型识别效果和跨语种有效性。最后,本文提出一种基于多尺度卷积的多模态情感识别方法,结合语音和文本情感模态信息,提高情感识别准确率。另外针对多模态情感识别系统在实际使用场景中常遇到的模态缺失问题,本文提出了一种模态缺失自适应方法,提高了系统在实际应用场景中的鲁棒性和实用性。},
  editora = {赵, 航芳 and 陈, 长海},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_Waiting for read,_中文,Attention Mechanism,Modal Absence,Multi-modal,Semantic Anti-erasure,Speech Emotion Recognition,多模态,对抗语义擦除,模态缺失,注意力机制,语音情感识别},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\邓_2020_基于深度学习的语音情感识别研究.pdf}
}

@inproceedings{depintoEmotionsUnderstandingModel2020,
  title = {Emotions {{Understanding Model}} from {{Spoken Language}} Using {{Deep Neural Networks}} and {{Mel-Frequency Cepstral Coefficients}}},
  booktitle = {2020 {{IEEE Conference}} on {{Evolving}} and {{Adaptive Intelligent Systems}} ({{EAIS}})},
  author = {de Pinto, Marco Giuseppe and Polignano, Marco and Lops, Pasquale and Semeraro, Giovanni},
  options = {useprefix=true},
  date = {2020-05},
  pages = {1--5},
  issn = {2473-4691},
  doi = {10/gmw5p8},
  abstract = {The ability to understand people through spoken language is a skill that many human beings take for granted. On the contrary, the same task is not as easy for machines, as consequences of a large number of variables which vary the speaking sound wave while people are talking to each other. A sub-task of speeches understanding is about the detection of the emotions elicited by the speaker while talking, and this is the main focus of our contribution. In particular, we are presenting a classification model of emotions elicited by speeches based on deep neural networks (CNNs). For the purpose, we focused on the audio recordings available in the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset. The model has been trained to classify eight different emotions (neutral, calm, happy, sad, angry, fearful, disgust, surprise) which correspond to the ones proposed by Ekman plus the neutral and calm ones. We considered as evaluation metric the F1 score, obtaining a weighted average of 0.91 on the test set and the best performances on the "Angry" class with a score of 0.95. Our worst results have been observed for the sad class with a score of 0.87 that is nevertheless better than the state-of-the-art. In order to support future development and the replicability of results, the source code of the proposed model is available on the following GitHub repository: https://github.com/marcogdepinto/Emotion-Classification-Ravdess.},
  eventtitle = {2020 {{IEEE Conference}} on {{Evolving}} and {{Adaptive Intelligent Systems}} ({{EAIS}})},
  language = {en},
  keywords = {_Code,_readed,classification,cnn,deep learning,emotion detection,machine learning,mel-frequency cepstral coefficients,natural language understanding,ravdess,sentiment analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\de Pinto et al_2020_Emotions Understanding Model from Spoken Language using Deep Neural Networks.pdf}
}

@inproceedings{dhamyalPositionalEncodingCapturing2022,
  title = {Positional {{Encoding}} for {{Capturing Modality Specific Cadence}} for {{Emotion Detection}}},
  booktitle = {Interspeech 2022},
  author = {Dhamyal, Hira and Raj, Bhiksha and Singh, Rita},
  date = {2022-09-18},
  pages = {166--170},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11085},
  url = {https://www.isca-speech.org/archive/interspeech_2022/dhamyal22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dhamyal et al_2022_Positional Encoding for Capturing Modality Specific Cadence for Emotion.pdf}
}

@inproceedings{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  date = {2021},
  volume = {34},
  pages = {8780--8794},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc//paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
  urldate = {2022-08-20},
  language = {en},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dhariwal_Nichol_2021_Diffusion Models Beat GANs on Image Synthesis.pdf}
}

@article{dingGuestEditorialEvolutionary2021,
  title = {Guest {{Editorial Evolutionary Computation Meets Deep Learning}}},
  author = {Ding, Weiping and Pedrycz, Witold and Yen, Gary G. and Xue, Bing},
  date = {2021},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {25},
  number = {5},
  pages = {810--814},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2021.3096336},
  abstract = {Deep learning is a timely research direction in machine learning, where breakthrough progress has been made in both academe and industries, bringing promising results in speech recognition, computer vision, industrial control and automation, etc. The motivation of deep learning is primarily to establish a model to simulate the neural connection structure of the human brain. While dealing with complex tasks, deep learning adopts a number of transformation stages to deliver the in-depth description and interpretation of the data. Deep learning achieves exceptional power and flexibility by learning to represent the task through a nested hierarchy of layers, with more abstract representations formed successively in terms of less abstract ones. One of the key issues of existing deep learning approaches is that the meaningful representations can be learned only when their hyperparameter settings are properly specified beforehand, and general parameters are learned during the training process. Until now, not much research has been dedicated to automatically set the hyperparameters, and accurately find the globally optimal general parameters. However, this problem can be formulated as optimization problems, including discrete optimization, constrained optimization, large-scale global optimization, and multiobjective optimization, by engaging mechanisms of evolutionary computation.},
  eventtitle = {{{IEEE Transactions}} on {{Evolutionary Computation}}},
  issue = {5},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ding et al_2021_Guest Editorial Evolutionary Computation Meets Deep Learning.pdf}
}

@thesis{DingJiYuYuYinXinXiDeDuoTeZhengQingXuShiBieSuanFaYanJiu2015,
  type = {硕士},
  title = {基于语音信息的多特征情绪识别算法研究},
  author = {丁, 倩},
  date = {2015},
  institution = {{山东大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201601&filename=1015380141.nh&uniplatform=NZKPT&v=kVD6kQapztGQUJKxOlr0z038zXMQa64n%25mmd2Bqea5TxZT%25mmd2B60%25mmd2B1vDujRmox6Yzn1ayDBk},
  urldate = {2021-09-10},
  abstract = {情感计算是对与情感相关、来源于情感或者影响情感方面的计算,其目的是赋予智能机器感知,理解和表达各种情感状态的能力。情感识别是情感计算的关键研究内容,是实现和谐人机交互的重要基础。目前,情感计算主要以表情、姿态、语音、文本和生理信号为基础进行情感识别,其中语音信号,是情感信息表达的主要载体,且获取方便,越来越受到研究人员的重视。寻找能够准确表征语音情绪状态的特征参数和有效的情绪状态识别模型是语音情绪识别的主要困难,一直是研究的热点之一。本文主要以语音信息的情感特征参数和情感模型对四种情绪（happy, angry, sad, fear）识别的有效性为研究内容。首先,提取语音信号的样本熵作为一个表征语音情绪状态的特征参数,分析其在语音情绪识别中的区分性,并与传统的声学参数进行特征融合,用于情绪识别；然后,在传统BP网络基础上,结合基于贡献分析的PCA算法,建立了一种基于PCA贡献分析的神经网络语音情感识别模型。对实验结果的分析表明,对样本熵与传统的声学参数进行特征融合可以改进分类器的性能；基于PCA与神经网络建立的语音情感识别模型提高了识别效率。具体研究内容如下：（1）信号获取。根据语音情感信号的产生机理和国际上对情感类型的划分方法,综合比较国内外典型情感语音数据库,最终确定本文的情感类型划分方法和采用的情感语音库。（2）前期处理。对原始数据进行采样、量化、预加重、分帧、加窗等预处理后,针对端点效应问题,本文提出基于样本熵改进的两级判别端点检测算法。采用模糊C均值聚类算法和贝叶斯信息判决算法确定双门限阈值,将其作为第一级判别,初步判断信号的起止点,然后将修正的过零率作为第二级判别,最终确定信号的起止点。（3）特征提取与特征选择。本研究将样本熵及目前比较成熟的声学参数（包括语速、能量特征、基音频率、MFCC）及其统计参数,进行特征融合应用于语音情绪识别。采用PCA贡献分析对原始特征向量集合进行降维,得到最简约向量集,降低网络模型的复杂性,降低训练时间。（4）语音情绪识别模型。本文在传统BP网络基础上,结合基于贡献分析的PCA算法,建立一种基于PCA的神经网络语音信号情感识别模型。基于上述融合的特征,利用该情感模型对800个样本的四种情绪状态进行识别。实验结果分析表明：（a）样本熵是一个能够表征语音信号情感变化的有效参数。样本熵和传统声学特征参数的融合可以显著提高分类器的识别率,改善情感识别模型的性能。（b）基于PCA贡献分析的神经网络语音情感识别模型解决了传统BP算法计算量大,运算速度慢的问题,有效的提高了系统的识别效率。综合考虑以上分析,利用样本熵与传统声学参数进行特征融合,结合基于PCA的神经网络情感识别模型,可以建立有效的语音情感识别系统。},
  editora = {杨, 立才},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Contribution Analysis,Endpoint Detection,Sample Entropy,Speech Emotion Recognition,样本熵,端点检测,语音情绪识别,贡献分析},
  annotation = {7 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\丁_2015_基于语音信息的多特征情绪识别算法研究.pdf}
}

@inproceedings{dingScalingYourKernels2022,
  title = {Scaling {{Up Your Kernels}} to 31x31: {{Revisiting Large Kernel Design}} in {{CNNs}}},
  shorttitle = {Scaling {{Up Your Kernels}} to 31x31},
  author = {Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  date = {2022},
  pages = {11963--11975},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html},
  urldate = {2022-07-09},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  language = {en},
  keywords = {_Code,_readed},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ding et al_2022_Scaling Up Your Kernels to 31x31.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\3FNAJGAC\\Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html}
}

@article{dinkelVoiceActivityDetection2021,
  title = {Voice {{Activity Detection}} in the {{Wild}}: {{A Data-Driven Approach Using Teacher-Student Training}}},
  shorttitle = {Voice {{Activity Detection}} in the {{Wild}}},
  author = {Dinkel, Heinrich and Wang, Shuai and Xu, Xuenan and Wu, Mengyue and Yu, Kai},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1542--1555},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3073596},
  abstract = {Voice activity detection is an essential pre-processing component for speech-related tasks such as automatic speech recognition (ASR). Traditional supervised VAD systems obtain frame-level labels from an ASR pipeline by using, e.g., a Hidden Markov model. These ASR models are commonly trained on clean and fully transcribed data, limiting VAD systems to be trained on clean or synthetically noised datasets. Therefore, a major challenge for supervised VAD systems is their generalization towards noisy, real-world data. This work proposes a data-driven teacher-student approach for VAD, which utilizes vast and unconstrained audio data for training. Unlike previous approaches, only weak labels during teacher training are required, enabling the utilization of any real-world, potentially noisy dataset. Our approach firstly trains a teacher model on a source dataset (Audioset) using clip-level supervision. After training, the teacher provides frame-level guidance to a student model on an unlabeled, target dataset. A multitude of student models trained on mid- to large-sized datasets are investigated (Audioset, Voxceleb, NIST SRE). Our approach is then respectively evaluated on clean, artificially noised, and real-world data. We observe significant performance gains in artificially noised and real-world scenarios. Lastly, we compare our approach against other unsupervised and supervised VAD methods, demonstrating our method's superiority.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Convolutional neural networks,Data models,Hidden Markov models,Mathematical model,Speech activity detection. Weakly supervised learning,Speech enhancement,Speech recognition,Teacher-student learning,Training,Training data,Voice activity detection},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dinkel et al_2021_Voice Activity Detection in the Wild.pdf}
}

@inproceedings{dissanayakeSelfsupervisedRepresentationFusion2022,
  title = {Self-Supervised {{Representation Fusion}} for {{Speech}} and {{Wearable Based Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Dissanayake, Vipula and Seneviratne, Sachith and Suriyaarachchi, Hussel and Wen, Elliott and Nanayakkara, Suranga},
  date = {2022-09-18},
  pages = {3598--3602},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11258},
  url = {https://www.isca-speech.org/archive/interspeech_2022/dissanayake22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dissanayake et al_2022_Self-supervised Representation Fusion for Speech and Wearable Based Emotion.pdf}
}

@thesis{DongXiJinPingZhouBianWaiJiaoSiXiangJiQiShiDaiJieZhiYanJiu2020,
  type = {硕士},
  title = {习近平周边外交思想及其时代价值研究},
  author = {董, 丹凤},
  date = {2020},
  institution = {{重庆理工大学}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202002&filename=1020060561.nh&uniplatform=NZKPT&v=0BbE6OdrJtUA1eFs7W44oagcx4yh1aU6i3nMRFjWEvvfFztvedT7J%25mmd2FzT2W3FnGqe},
  urldate = {2021-11-21},
  abstract = {中国邻国众多,周边环境复杂,长期以来周边外交面临严峻的挑战,海域问题、地区热点冲突、大国的干扰、周边国家政局动荡、经济不振等问题,时刻影响着中国的稳定与发展,营造一个和平、稳定与安全的周边环境不仅关乎``中国梦''能否实现,也涉及中华民族能否真正的崛起。党的十八大以来,以习近平同志为核心的党中央高度重视中国周边外交,习总书记主持召开了一系列外事工作会议并发表了重要讲话,他在总结过往中国周边外交理论与实践经验的基础上,提出了一系列周边外交新理念和新观点,对中国周边外交进行了一系列理论和实践创新,形成了具有中国特色的周边外交思想。习近平周边外交思想是习近平新时代中国特色社会主义思想的重要组成部分。这一...},
  language = {zh-CN},
  keywords = {neighborhood diplomacy,thought,Time value,Xi Jinping,习近平,周边外交,思想,时代价值},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\董_2020_习近平周边外交思想及其时代价值研究.pdf}
}

@article{duangpummetBlindEstimationSpeech2022,
  title = {Blind Estimation of Speech Transmission Index and Room Acoustic Parameters Based on the Extended Model of Room Impulse Response},
  author = {Duangpummet, Suradej and Karnjana, Jessada and Kongprawechnon, Waree and Unoki, Masashi},
  date = {2022-01-01},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {185},
  pages = {108372},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2021.108372},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X21004667},
  urldate = {2021-09-28},
  abstract = {The speech transmission index (STI) and room-acoustic parameters, such as the reverberation time and clarity index, are essential to assess the quality of room acoustics. However, in everyday spaces occupied by people, it is difficult to obtain such parameters since the room impulse response (RIR) cannot be measured. Blind estimation of room acoustic parameters from observed signals without measuring RIR is therefore necessary. Although existing methods can estimate one of these parameters, a single parameter is inadequate to describe comprehensive subjective aspects. To this end, this paper proposes a method for estimating STI and five room-acoustic parameters simultaneously. The temporal amplitude envelope of a reverberant speech signal is mapped to the parameters of an RIR model for each sub-band. Instead of using Schroeder's RIR model, the extended RIR model is used to approximate an unknown RIR so that the STI and room-acoustic parameters can be derived. We performed simulations to evaluate the proposed method in unseen reverberant environments. The root-mean-square errors between the ground-truths and estimated parameters suggest that the accuracy of the proposed scheme outperformed that with Schroeder's RIR model and was close to the standard measurements.},
  language = {en},
  keywords = {Extended RIR model,Reverberation time,Room acoustics,Room impulse response,Speech transmission index,Temporal amplitude envelope},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Duangpummet et al_2022_Blind estimation of speech transmission index and room acoustic parameters.pdf}
}

@article{dubaguntaAdjustableDeterministicPseudonymization2022,
  title = {Adjustable Deterministic Pseudonymization of Speech},
  author = {Dubagunta, S. Pavankumar and van Son, Rob J. J. H. and Magimai.-Doss, Mathew},
  options = {useprefix=true},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101284},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101284},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000851},
  urldate = {2021-09-28},
  abstract = {While public speech resources become increasingly available, there is a growing interest to preserve the privacy of the speakers, through methods that anonymize the speaker information from speech while preserving the spoken linguistic content. In this paper, a method for pseudonymization (reversible anonymization) of speech is presented, that allows to obfuscate the speaker identity in untranscribed running speech. The approach manipulates the spectro-temporal structure of the speech to simulate a different length and structure of the vocal tract by modifying the formant locations, as well as by altering the pitch and speaking rate. The method is deterministic and partially reversible, and the changes are adjustable on a continuous scale. The method has been evaluated in terms of (i) ABX listening experiments, and (ii) automatic speaker verification and speech recognition. ABX experimental results indicate that the speaker identifiability among forced choice pairs reduced from over 90\% to less than 70\% through pseudonymization, and that de-pseudonymization was partially effective. An evaluation on the VoicePrivacy 2020 challenge data showed that the proposed approach performs better than the signal processing based baseline method that uses McAdams coefficient and performs slightly worse than the neural source filtering based baseline method. Further analysis showed that the proposed approach: (i) is comparable to the neural source filtering baseline based method in terms of phone posterior feature based objective intelligibility measure, (ii) preserves formant tracks better than the McAdams based method, and (iii) preserves paralinguistic aspects such as dysarthria in several speakers.},
  language = {en},
  keywords = {Speech features,Speech privacy,Speech pseudonymization,Speech signal processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dubagunta et al_2022_Adjustable deterministic pseudonymization of speech.pdf}
}

@inproceedings{duDisentanglementEmotionalStyle2022,
  title = {Disentanglement of {{Emotional Style}} and {{Speaker Identity}} for {{Expressive Voice Conversion}}},
  booktitle = {Interspeech 2022},
  author = {Du, Zongyang and Sisman, Berrak and Zhou, Kun and Li, Haizhou},
  date = {2022-09-18},
  pages = {2603--2607},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10249},
  url = {https://www.isca-speech.org/archive/interspeech_2022/du22c_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Du et al_2022_Disentanglement of Emotional Style and Speaker Identity for Expressive Voice.pdf}
}

@article{dufourUsingCircularModels2021,
  title = {Using {{Circular Models}} to {{Improve Music Emotion Recognition}}},
  author = {Dufour, Isabelle and Tzanetakis, George},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {666--681},
  issn = {1949-3045},
  doi = {10.1109/taffc.2018.2885744},
  abstract = {The two commonly accepted models of affect used in affective computing are categorical and two-dimensional. However, categorical models are limited to datasets that only contain music for which human annotators fully agree upon, while two-dimensional models use descriptors to which users may not relate to (e.g., Valence and Arousal). This paper explores the hypothesis that the music emotion problem is circular, and shows how circular models can be used for automatic music emotion recognition. This hypothesis is tested through experiments on the two commonly accepted models of affect, as well as on an original circular model proposed by the authors. First, an original dataset was assembled and annotated as a way to investigate agreement among annotators. Then, polygonal approximations of circular regression are proposed as a practical method to investigate whether the circularity of the annotations can be exploited. Experiments with different polygons demonstrate consistent improvements over the categorical model on a dataset containing musical extracts for which the human annotators did not fully agree upon. Finally, a proposed multi-tagging strategy based on the circular predictions is put forward as a pragmatic method to automatically annotate music based on the circular models.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_reading,circular annotations,Computational modeling,Emotion recognition,emotional model,Mood,Music,Music emotion recognition,Predictive models,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dufour_Tzanetakis_2021_Using Circular Models to Improve Music Emotion Recognition.pdf}
}

@thesis{DuJiYuTeZhengZiKongJianLiangHuaYuShenDuZhiXinWangLuoDeShuoHuaRenNianLingShiBie2017,
  type = {硕士},
  title = {基于特征子空间量化与深度置信网络的说话人年龄识别},
  author = {杜, 先娜},
  date = {2017},
  institution = {{苏州大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1018034374.nh&uniplatform=NZKPT&v=RiWxBsnDApxzw7bYQb0ICgU5lAWLWUwbn95UjxvLuxnXeQKplJhK1mAg48c0r%25mmd2Bso},
  urldate = {2021-09-10},
  abstract = {说话人年龄识别的主要目的是利用语音推断说话人的年龄范围。说话人年龄识别在语音处理领域占据关键位置,该系统在医疗咨询、多媒体系统应用、自适应人机语音交互、司法鉴定等诸多领域有着广泛的应用价值。现有的年龄识别中常用的特征参数,如梅尔倒谱系数等一般都是基于语音信号短时平稳特性,描述了说话者语音静态的特征而缺乏对动态特征的表述;较为常用的年龄识别方法是以高斯混合模型为建模方式进行年龄识别,该模型以各语音参数帧之间统计独立为前提,忽略了相邻帧之间连续性,另一方面,该方法需要对特征向量进行加权平均,会造成参数过于平滑。本课题提出特征子空间量化与深度置信网络相结合进行年龄识别以改善说话人年龄识别系统性能。输入语音,经过小波包等变换获取有效频带多分辨率特征参数,数据库根据说话人年龄分为四个阶段:儿童、青年、中年以及老年,对各年龄段的语音特征向量进行特征子空间量化,依照性别训练语音获得相应高斯混合模型,通过最大似然准则判定说话人性别;在此基础上经由拓展训练集语音形成超帧信息用作深度置信网络输入,说话人年龄类别是网络输出,经训练得到说话人年龄识别系统框架。实验结果表明,相对于GMM算法和传统DBN算法,本文提出的说话人年龄识别网络具有更好的识别性能。},
  editora = {俞, 一彪},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,age recognition,DBN,Feature Subspace Quantization,Wavelet Packet Mel-Frequency Cepstrum,有效频带多分辨率特征参数,深度置信网络,特征子空间量化,说话人年龄识别},
  annotation = {2 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\杜_2017_基于特征子空间量化与深度置信网络的说话人年龄识别.pdf}
}

@article{duranteCausalIndicatorsAssessing2022,
  title = {Causal Indicators for Assessing the Truthfulness of Child Speech in Forensic Interviews},
  author = {Durante, Zane and Ardulov, Victor and Kumar, Manoj and Gongola, Jennifer and Lyon, Thomas and Narayanan, Shrikanth},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101263},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101263},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000693},
  urldate = {2021-09-28},
  abstract = {When interviewing a child who may have witnessed a crime, the interviewer must ask carefully directed questions in order to elicit a truthful statement from the child. The presented work uses Granger causal analysis to examine and represent child\textendash interviewer interaction dynamics over such an interview. Our work demonstrates that Granger Causal analysis of psycholinguistic and acoustic signals from speech yields significant predictors of whether a child is telling the truth, as well as whether a child will disclose witnessing a transgression later in the interview. By incorporating cross-modal Granger causal features extracted from audio and transcripts of forensic interviews, we are able to substantially outperform conventional deception detection methods and a number of simulated baselines. Our results suggest that a child's use of concreteness and imageability in their language are strong psycholinguistic indicators of truth-telling and that the coordination of child and interviewer speech signals is much more informative than the specific language used throughout the interview.},
  language = {en},
  keywords = {Automated deception detection,Child forensic interviewing,Granger causal analysis,Narrative truth induction},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Durante et al_2022_Causal indicators for assessing the truthfulness of child speech in forensic.pdf}
}

@article{duSpatioTemporalEncoderDecoderFully2021,
  title = {Spatio-{{Temporal Encoder-Decoder Fully Convolutional Network}} for {{Video-Based Dimensional Emotion Recognition}}},
  author = {Du, Zhengyin and Wu, Suowei and Huang, Di and Li, Weixin and Wang, Yunhong},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {565--578},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2940224},
  abstract = {Video-based dimensional emotion recognition aims to map human affect into the dimensional emotion space based on visual signals, which is a fundamental challenge in affective computing and human-computer interaction. In this paper, we present a novel encoder-decoder framework to tackle this problem. It adopts a fully convolutional design with the cascaded 2D convolution based spatial encoder and 1D convolution based temporal encoder-decoder for joint spatio-temporal modeling. In particular, to address the key issue of capturing discriminative long-term dynamic dependency, our temporal model, referred to as Temporal Hourglass Convolutional Neural Network (TH-CNN), extracts contextual relationship through integrating both low-level encoded and high-level decoded clues. Temporal Intermediate Supervision (TIS) is then introduced to enhance affective representations generated by TH-CNN under a multi-resolution strategy, which guides TH-CNN to learn macroscopic long-term trend and refined short-term fluctuations progressively. Furthermore, thanks to TH-CNN and TIS, knowledge learnt from the intermediate layers also makes it possible to offer customized solutions to different applications by adjusting the decoder depth. Extensive experiments are conducted on three benchmark databases (RECOLA, SEWA and OMG) and superior results are shown compared to state-of-the-art methods, which indicates the effectiveness of the proposed approach.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_Waiting for read,Convolution,Decoding,Dimensional emotion recognition,Emotion recognition,Feature extraction,spatio-temporal fully convolutional network,Task analysis,temporal hourglass CNN,temporal intermediate supervision,Videos,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Du et al_2021_Spatio-Temporal Encoder-Decoder Fully Convolutional Network for Video-Based.pdf}
}

@inproceedings{duttaMultimodalTransformerLearnable2022,
  title = {Multimodal {{Transformer}} with {{Learnable Frontend}} and {{Self Attention}} for {{Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Dutta, Soumya and Ganapathy, Sriram},
  date = {2022},
  pages = {6917--6921},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747723},
  abstract = {In this work, we propose a novel approach for multi-modal emotion recognition from conversations using speech and text. The audio representations are learned jointly with a learnable audio front-end (LEAF) model feeding to a CNN based classifier. The text representations are derived from pre-trained bidirectional encoder representations from transformer (BERT) along with a gated recurrent network (GRU). Both the textual and audio representations are separately processed using a bidirectional GRU network with self-attention. Further, the multi-modal information extraction is achieved using a transformer that is input with the textual and audio embeddings at the utterance level. The experiments are performed on the IEMOCAP database, where we show that the proposed framework improves over the current state-of-the-art results under all the common test settings. This is primarily due to the improved emotion recognition performance achieved in the audio domain. Further, we also show that the model is more robust to textual errors caused by an automatic speech recognition (ASR) system.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Bit error rate,Conferences,Databases,Emotion recognition,Information retrieval,learnable front-end,Logic gates,Multi-modal emotion recognition,self-attention models,Transformer networks,Transformers},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dutta_Ganapathy_2022_Multimodal Transformer with Learnable Frontend and Self Attention for Emotion.pdf}
}

@article{EditorialBoard2022,
  title = {Editorial {{Board}}},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101293},
  issn = {0885-2308},
  doi = {10.1016/s0885-2308(21)00094-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000942},
  urldate = {2021-09-28},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\2022_Editorial Board.pdf}
}

@article{edrakiSpeechIntelligibilityPrediction2021,
  title = {Speech {{Intelligibility Prediction Using Spectro-Temporal Modulation Analysis}}},
  author = {Edraki, Amin and Chan, Wai-Yip and Jensen, Jesper and Fogerty, Daniel},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {210--225},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3039929},
  abstract = {Spectro-temporal modulations are believed to mediate the analysis of speech sounds in the human primary auditory cortex. Inspired by humans' robustness in comprehending speech in challenging acoustic environments, we propose an intrusive speech intelligibility prediction (SIP) algorithm, wSTMI, for normal-hearing listeners based on spectro-temporal modulation analysis (STMA) of the clean and degraded speech signals. In the STMA, each of 55 modulation frequency channels contributes an intermediate intelligibility measure. A sparse linear model with parameters optimized using Lasso regression results in combining the intermediate measures of 8 of the most salient channels for SIP. In comparison with a suite of 10 SIP algorithms, wSTMI performs consistently well across 13 datasets, which together cover degradation conditions including modulated noise, noise reduction processing, reverberation, near-end listening enhancement, and speech interruption. We show that the optimized parameters of wSTMI may be interpreted in terms of modulation transfer functions of the human auditory system. Thus, the proposed approach offers evidence affirming previous studies of the perceptual characteristics underlying speech signal intelligibility.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Degradation,Frequency modulation,Indexes,Modulation,Prediction algorithms,Spectro-temporal modulation,Spectrogram,speech intelligibility,Speech processing,speech quality model},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Edraki et al_2021_Speech Intelligibility Prediction Using Spectro-Temporal Modulation Analysis.pdf}
}

@inproceedings{elgaarMultiSpeakerMultiDomainEmotional2020,
  title = {Multi-{{Speaker}} and {{Multi-Domain Emotional Voice Conversion Using Factorized Hierarchical Variational Autoencoder}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Elgaar, Mohamed and Park, Jungbae and Lee, Sang Wan},
  date = {2020-05},
  pages = {7769--7773},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9054534},
  url = {https://ieeexplore.ieee.org/document/9054534/},
  urldate = {2021-09-10},
  abstract = {Due to the complexity of emotional features, there has been limited success in emotional voice conversion. One major challenge is that conversion between more than two kinds of emotions often accompanies distortion of voice signal.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5},
  language = {en},
  keywords = {_reading},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Elgaar et al_2020_Multi-Speaker and Multi-Domain Emotional Voice Conversion Using Factorized.pdf}
}

@inproceedings{esserTamingTransformersHighResolution2021,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bj\"orn},
  date = {2021-06},
  pages = {12868--12878},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01268},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  language = {en},
  keywords = {Computer architecture,Computer vision,Image segmentation,Image synthesis,Rendering (computer graphics),Transformers,Vocabulary},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf}
}

@article{fanGatedRecurrentFusion2021,
  title = {Gated {{Recurrent Fusion With Joint Training Framework}} for {{Robust End-to-End Speech Recognition}}},
  author = {Fan, Cunhang and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Liu, Bin and Wen, Zhengqi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {198--209},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3039600},
  abstract = {The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition (ASR). However, these methods only utilize the enhanced feature as the input of the speech recognition component, which are affected by the speech distortion problem. In order to address this problem, this paper proposes a gated recurrent fusion (GRF) method with joint training framework for robust end-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, the GRF is applied to address the speech distortion problem. Thirdly, to improve the performance of ASR, the state-of-the-art speech transformer algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate (CER) reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio (0 dB), our proposed method can achieves better performances with 12.67\% CER reduction, which suggests the potential of our proposed method.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustic distortion,Gated recurrent fusion,Logic gates,Noise measurement,robust end-to-end speech recognition,speech distortion,speech enhancement,Speech enhancement,Speech recognition,speech transformer,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fan et al_2021_Gated Recurrent Fusion With Joint Training Framework for Robust End-to-End.pdf}
}

@inproceedings{fanLSSEDLargeScaleDataset2021,
  title = {{{LSSED}}: {{A Large-Scale Dataset}} and {{Benchmark}} for {{Speech Emotion Recognition}}},
  shorttitle = {{{LSSED}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Fan, Weiquan and Xu, Xiangmin and Xing, Xiaofen and Chen, Weidong and Huang, Dongyan},
  date = {2021-06},
  pages = {641--645},
  issn = {2379-190X},
  doi = {10/gj25nn},
  abstract = {Speech emotion recognition is a vital contributor to the next generation of human-computer interaction (HCI). However, current existing small-scale databases have limited the development of related research. In this paper, we present LSSED, a challenging large-scale english speech emotion dataset, which has data collected from 820 subjects to simulate real- world distribution. In addition, we release some pre-trained models based on LSSED, which can not only promote the development of speech emotion recognition, but can also be transferred to related downstream tasks such as mental health analysis where data is extremely difficult to collect. Finally, our experiments show the necessity of large-scale datasets and the effectiveness of pre-trained models. The dateset will be released on https://github.com/tobefans/LSSED.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Analytical models,Databases,dataset,deep learning,Emotion recognition,Human computer interaction,Mental health,pretrained model,Signal processing algorithms,speech emotion recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fan et al_2021_LSSED.pdf}
}

@article{fanMultiTaskSequenceTagging2021,
  title = {Multi-{{Task Sequence Tagging}} for {{Emotion-Cause Pair Extraction Via Tag Distribution Refinement}}},
  author = {Fan, Chuang and Yuan, Chaofa and Gui, Lin and Zhang, Yue and Xu, Ruifeng},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2339--2350},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2021.3089837},
  abstract = {The task emotion-cause pair extraction deals with finding all emotions and the corresponding causes from emotion texts. Existing joint methods solve it as multi-task learning, which introduces two auxiliary tasks (i.e., emotion extraction and cause extraction) to make use of task correlations for their mutual benefits. However, these methods focus on capturing such correlations by sharing parameters in an implicit way, not only have a limitation of cannot explicitly model their information interaction, but also suffer from low interpretability. Towards these issues, we propose a multi-task sequence tagging framework, which can extract emotions with the associated causes simultaneously by encoding their distances into a novel tagging scheme. In addition, the output of both auxiliary tasks can be directly used as inductive bias, to refine the tag distribution for benefiting emotion-cause pair extraction, so that the information exchange between them can be more explicit and interpretable. Results show that our model achieves the best performance, outperforming a number of competitive baselines by at least 1.03\% (\$p{$<$} 0.01\$) in \$F\_1\$ score. The comprehensive analysis further confirms the superiority and robustness of our model.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Correlation,Data mining,Electronic mail,Emotion-cause pair extraction,Fans,Feature extraction,multi-task learning,sequence tagging,tag distribution refinement,Tagging,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fan et al_2021_Multi-Task Sequence Tagging for Emotion-Cause Pair Extraction Via Tag.pdf}
}

@article{fanOrderguidedDeepNeural2021,
  title = {Order-Guided Deep Neural Network for Emotion-Cause Pair Prediction},
  author = {Fan, Wei and Zhu, Yuexuan and Wei, Ziyun and Yang, Tianyu and Ip, W. H. and Zhang, Yuxiang},
  date = {2021-11-01},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {112},
  pages = {107818},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2021.107818},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494621007390},
  urldate = {2021-09-28},
  abstract = {Emotion-Cause Pair Extraction (ECPE) is a prediction task aiming to extract the emotions and their corresponding causes in a target document. The existing methods for this problem mainly focus on modeling the dependence between emotion clauses and related cause clauses and the interaction among emotion-cause pairs. However, these methods ignore the order information between emotion clauses and their cause clauses, which can be proved useful for the ECPE task. In this paper, we propose an order-guided deep predictive model, which integrates different orders between emotion clauses and their cause clauses into an end-to-end framework to tackle this task. Specifically, we build an order-guided clause encoder with a three-level long-short term memory (LSTM) network to learn the different orders from forward LSTM, backward LSTM and Bi-LSTM, respectively. In this way, the deep networks with different directions can effectively capture different orders, and therefore improve the performance of our model in this prediction task. Additionally, the previous methods use only a shared word encoder to capture word-level emotion and cause information, resulting in paying more attention to emotion information and lacking the ability to capture cause information. In order to overcome this deficiency, we design both an emotion-aware word encoder and a cause-aware word encoder to enhance the ability to capture the emotion and cause information. The experiment results illustrate that our method outperforms the other baselines on two real-world datasets, and demonstrate the effectiveness of the proposed method.},
  language = {en},
  keywords = {_Waiting for read,Cause clause extraction,Deep neural network,Emotion clause extraction,Emotion-cause pair extraction},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fan et al_2021_Order-guided deep neural network for emotion-cause pair prediction.pdf}
}

@inproceedings{fengEnhancingPrivacyDomain2022,
  title = {Enhancing {{Privacy Through Domain Adaptive Noise Injection For Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Feng, Tiantian and Hashemi, Hanieh and Annavaram, Murali and Narayanan, Shrikanth S.},
  date = {2022},
  pages = {7702--7706},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747265},
  abstract = {Speech Emotion Recognition (SER) techniques have gained considerable interest in many applications including smart virtual assistants and health state tracking. SER systems often acquire and transmit speech data collected at the client-side to remote cloud platforms for inference and decision making. However, speech data carries rich information not only about emotions conveyed in vocal expressions, but also other sensitive demographic traits, such as gender, age, and language background. It is desirable to select only features that are necessary for the emotion classification while protecting sensitive features. However, there are some features that are necessary for emotion classification. These features may also reveal other demographic traits. In this work, we propose a method to improve inference privacy for sensitive features by injecting noise into the input speech data, but without degrading the SER system performance. The approach combines a noise representation learning architecture, called Cloak [1], with adversarial training to keep relevant information inside the data for emotion classification while removing information that would enable inferring sensitive demographic attributes. Experimental results show that our method can effectively prevent inference of sensitive demographic information, and that the improved privacy comes at a cost of only a minor utility loss for the emotion classification.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {adversarial training,Decision making,Emotion recognition,fairness,machine learning,noise enjection,Privacy,Representation learning,speech emotion,statistical privacy,System performance,Training,Virtual assistants},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Feng et al_2022_Enhancing Privacy Through Domain Adaptive Noise Injection For Speech Emotion.pdf}
}

@inproceedings{fengSemiFedSERSemisupervisedLearning2022,
  title = {Semi-{{FedSER}}: {{Semi-supervised Learning}} for {{Speech Emotion Recognition On Federated Learning}} Using {{Multiview Pseudo-Labeling}}},
  shorttitle = {Semi-{{FedSER}}},
  booktitle = {Interspeech 2022},
  author = {Feng, Tiantian and Narayanan, Shrikanth},
  date = {2022-09-18},
  pages = {5050--5054},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-141},
  url = {https://www.isca-speech.org/archive/interspeech_2022/feng22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  keywords = {_Code},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Feng_Narayanan_2022_Semi-FedSER.pdf}
}

@inproceedings{fengUserLevelDifferentialPrivacy2022,
  title = {User-{{Level Differential Privacy}} against {{Attribute Inference Attack}} of {{Speech Emotion Recognition}} on {{Federated Learning}}},
  booktitle = {Interspeech 2022},
  author = {Feng, Tiantian and Peri, Raghuveer and Narayanan, Shrikanth},
  date = {2022-09-18},
  pages = {5055--5059},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10060},
  url = {https://www.isca-speech.org/archive/interspeech_2022/feng22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  keywords = {_Code},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Feng et al_2022_User-Level Differential Privacy against Attribute Inference Attack of Speech.pdf}
}

@inproceedings{fernauAutomatedDialogPersonalization2022,
  title = {Towards {{Automated Dialog Personalization}} Using {{MBTI Personality Indicators}}},
  booktitle = {Interspeech 2022},
  author = {Fernau, Daniel and Hillmann, Stefan and Feldhus, Nils and Polzehl, Tim},
  date = {2022-09-18},
  pages = {1968--1972},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-376},
  url = {https://www.isca-speech.org/archive/interspeech_2022/fernau22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fernau et al_2022_Towards Automated Dialog Personalization using MBTI Personality Indicators.pdf}
}

@unpublished{ferreiraLearningGenerateMusic2021,
  title = {Learning to {{Generate Music With Sentiment}}},
  author = {Ferreira, Lucas N. and Whitehead, Jim},
  date = {2021-03-08},
  eprint = {2103.06125},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.06125},
  urldate = {2021-09-10},
  abstract = {Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy. A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ferreira_Whitehead_2021_Learning to Generate Music With Sentiment.pdf}
}

@article{ferrerSpeakerVerificationBackend2022,
  title = {A Speaker Verification Backend with Robust Performance across Conditions},
  author = {Ferrer, Luciana and McLaren, Mitchell and Br\"ummer, Niko},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101258},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101258},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000656},
  urldate = {2021-09-28},
  abstract = {In this paper, we address the problem of speaker verification in conditions unseen or unknown during development. A standard method for speaker verification consists of extracting speaker embeddings with a deep neural network and processing them through a backend composed of probabilistic linear discriminant analysis (PLDA) and global logistic regression score calibration. This method is known to result in systems that work poorly on conditions different from those used to train the calibration model. We propose to modify the standard backend, introducing an adaptive calibrator that uses duration and other automatically extracted side-information to adapt to the conditions of the inputs. The backend is trained discriminatively to optimize binary cross-entropy. When trained on a number of diverse datasets that are labeled only with respect to speaker, the proposed backend consistently and, in some cases, dramatically improves calibration, compared to the standard PLDA approach, on a number of held-out datasets, some of which are markedly different from the training data. Discrimination performance is also consistently improved. We show that joint training of the PLDA and the adaptive calibrator is essential \textemdash{} the same benefits cannot be achieved when freezing PLDA and fine-tuning the calibrator. To our knowledge, the results in this paper are the first evidence in the literature that it is possible to develop a speaker verification system with robust out-of-the-box performance on a large variety of conditions.},
  language = {en},
  keywords = {Probabilistic linear discriminant analysis,Robust calibration,Speaker verification},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ferrer et al_2022_A speaker verification backend with robust performance across conditions.pdf}
}

@article{filntisisVideorealisticExpressiveAudiovisual2017,
  title = {Video-Realistic Expressive Audio-Visual Speech Synthesis for the {{Greek}} Language},
  author = {Filntisis, Panagiotis Paraskevas and Katsamanis, Athanasios and Tsiakoulis, Pirros and Maragos, Petros},
  date = {2017-12},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {95},
  pages = {137--152},
  issn = {01676393},
  doi = {10.1016/j.specom.2017.08.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639317300419},
  urldate = {2021-09-10},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Filntisis et al_2017_Video-realistic expressive audio-visual speech synthesis for the Greek language.pdf}
}

@article{fischerRobustConstrainedMFMVDR2021,
  title = {Robust {{Constrained MFMVDR Filters}} for {{Single-Channel Speech Enhancement Based}} on {{Spherical Uncertainty Set}}},
  author = {Fischer, D\"orte and Doclo, Simon},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {618--631},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3042013},
  abstract = {Aiming at exploiting speech correlation across consecutive time-frames in the short-time Fourier transform domain, the multi-frame minimum variance distortionless response (MFMVDR) filter for single-channel speech enhancement has been proposed. The MFMVDR filter requires an accurate estimate of the normalized speech correlation vector in order to avoid speech distortion and artifacts. In this paper we investigate the potential of using robust MVDR filtering techniques to estimate the normalized speech correlation vector as the vector maximizing the total signal output power within a spherical uncertainty set, which corresponds to imposing a quadratic inequality constraint. Whereas the singly-constrained (SC) MFMVDR filter only considers the quadratic inequality constraint to estimate the (non-normalized) speech correlation vector, the doubly-constrained (DC) MFMVDR filter integrates a linear normalization constraint into the optimization problem to directly estimate the normalized speech correlation vector. To set the upper bound of the quadratic inequality constraint for each time-frequency point, we propose to use a trained non-linear mapping function that depends on the a-priori signal-to-noise ratio (SNR). Experimental results for different speech signals, noise types and SNRs show that the proposed constrained approaches yield a more accurate estimate of the normalized speech correlation vector than a state-of-the-art maximum-likelihood (ML) estimator. An instrumental and a perceptual evaluation show that both constrained MFMVDR filters lead to less speech and noise distortion but a lower noise reduction than the ML-MFMVDR filter, where the DC-MFMVDR filter is preferred in terms of overall quality compared to the SC-MFMVDR and ML-MFMVDR filters.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Correlation,Estimation error,Multi-Frame MVDR Filter,Noise measurement,Power generation,single-microphone speech enhancement,Speech enhancement,speech interframe correlation,Time-frequency analysis,Uncertainty},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fischer_Doclo_2021_Robust Constrained MFMVDR Filters for Single-Channel Speech Enhancement Based.pdf}
}

@thesis{FuJiYuGATeZhengRongHeHeJueCeShuJieGouDeYuYinQingGanShiBieJiShuYanJiu2020,
  type = {硕士},
  title = {基于GA特征融合和决策树结构的语音情感识别技术研究},
  author = {傅, 升},
  date = {2020},
  institution = {{南京邮电大学}},
  doi = {10.27251/d.cnki.gnjdc.2020.000237},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020431381.nh&uniplatform=NZKPT&v=oIDb7CQEzuSaf8nI60PJm5iqE1TZnzd49jBHnOQRf0RXCW0xBi7mmp8o4iZTAJ%25mmd2FC},
  urldate = {2021-09-10},
  abstract = {随着智能人机交互需求的增长,语音情感识别技术吸引了众多学者进行研究。目前的研究方向主要包含语音信号处理、情感特征集提取、情感特征的选择和融合、分类器构建等。考虑到情感特征集和分类器对语音情感识别的最终效果起关键性作用,本文针对这两点进行重点研究,研究内容包括:如何融合不同特征以实现高质量情感识别以及如何构建有效的分类结构获取适应当前情感类别的特征集。本文开展的研究工作如下:（1）针对语音情感识别技术的现状和未来发展进行调研,完成了当前主流语音情感识别技术各个模块的原理分析,主要包括语音情感识别中所需的数据库、情感特征、特征选择和特征融合方法、分类器等模块,并通过实验仿真完成各模块的性能评估。在上述基础上,对语音情感识别技术中存在的问题进行探讨,找出潜在解决方法,为后续研究工作提供理论基础。（2）针对语音情感识别中单一特征不能全面表征情感信息的问题,本文提出一种基于遗传算法融合深度瓶颈特征和声学情感特征的语音情感识别方法。该方法一方面通过提取语音的梅尔频率倒谱系数（Mel-Frequency Cepstral Coefficients,MFCC）、基音频率、能量、过零率等声学特征,用于表征语音中不同情感的声学变化信息,另一方面利用深度神经网络（Deep Neural Network,DNN）提取语音的深度瓶颈特征,用于弥补声学情感特征中缺乏的与分类标签相关联的信息。最后引入GA（Genetic Algorithm,GA）用于搜索融合两类特征的贡献度权值,将搜索得到的结果用于实现两类特征的融合,并使用支持向量机（Support Vector Machines,SVM）实现训练和分类。实验结果表明,基于遗传算法融合能得到更具有情感区分性的特征集,该特征集比单一特征集具备更高的识别性能。（3）由于不同情感对应的最佳特征集有所不同,因此针对当前情感使用适应度更高的特征集可以进一步提升识别效果。在上述情感识别方法的基础上,本文提出树型结构和直分型结构的语音情感识别方法。基于树型结构的语音情感识别对具有相似情感特性的情感使用相同的寻优目标,得到更适应这些情感类别的特征集,并用于情感识别。基于直分型结构的语音情感识别对每一类情感使用不同的特征集,最后实现对每类情感的高质量分类。实验结果表明,两种结构的语音情感识别系统都可以提升语音情感识别效果。树型结构在时间复杂度上优于直分型结构,而直分型结构则可以达到更佳的识别效果。},
  editora = {孙, 林慧},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Acoustic Feature,Decision Tree,Deep Bottleneck Feature,Feature Selection,GA,Speech Emotion Recognition,决策树结构,声学特征,深度瓶颈特征,特征筛选,语音情感识别,遗传算法},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\傅_2020_基于GA特征融合和决策树结构的语音情感识别技术研究.pdf}
}

@inproceedings{fuMAECMultiInstanceLearning2021,
  title = {{{MAEC}}: {{Multi-Instance Learning}} with an {{Adversarial Auto-Encoder-Based Classifier}} for {{Speech Emotion Recognition}}},
  shorttitle = {{{MAEC}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Fu, Changzeng and Liu, Chaoran and Ishi, Carlos Toshinori and Ishiguro, Hiroshi},
  date = {2021-06},
  pages = {6299--6303},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413640},
  abstract = {In this paper, we propose an adversarial auto-encoder-based classifier, which can regularize the distribution of latent representation to smooth the boundaries among categories. Moreover, we adopt multi-instance learning by dividing speech into a bag of segments to capture the most salient moments for presenting an emotion. The proposed model was trained on the IEMOCAP dataset and evaluated on the in-corpus validation set (IEMOCAP) and the cross-corpus validation set (MELD). The experiment results show that our model outperforms the baseline on in-corpus validation and increases the scores on cross-corpus validation with regularization.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustics,adversarial auto-encoder,Conferences,Emotion recognition,multi-instance,Signal processing,speech emotion recognition,Speech processing,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fu et al_2021_MAEC.pdf}
}

@article{furnonDNNBasedMaskEstimation2021,
  title = {{{DNN-Based Mask Estimation}} for {{Distributed Speech Enhancement}} in {{Spatially Unconstrained Microphone Arrays}}},
  author = {Furnon, Nicolas and Serizel, Romain and Essid, Slim and Illina, Irina},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2310--2323},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3092838},
  abstract = {Deep neural network (DNN)-based speech enhancement algorithms in microphone arrays have now proven to be efficient solutions to speech understanding and speech recognition in noisy environments. However, in the context of ad-hoc microphone arrays, many challenges remain and raise the need for distributed processing. In this paper, we propose to extend a previously introduced distributed DNN-based time-frequency mask estimation scheme that can efficiently use spatial information in form of so-called compressed signals which are pre-filtered target estimations. We study the performance of this algorithm named Tango under realistic acoustic conditions and investigate practical aspects of its optimal application. We show that the nodes in the microphone array cooperate by taking profit of their spatial coverage in the room. We also propose to use the compressed signals not only to convey the target estimation but also the noise estimation in order to exploit the acoustic diversity recorded throughout the microphone array.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Distortion,Distributed algorithm,Estimation,microphone arrays,Microphone arrays,Noise measurement,Noise reduction,speech enhancement,Speech enhancement,Speech processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Furnon et al_2021_DNN-Based Mask Estimation for Distributed Speech Enhancement in Spatially.pdf}
}

@article{ganinDomainAdversarialTrainingNeural2016,
  title = {Domain-{{Adversarial Training}} of {{Neural Networks}}},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran\c{c}ois and March, Mario and Lempitsky, Victor},
  date = {2016},
  journaltitle = {Journal of Machine Learning Research},
  volume = {17},
  number = {59},
  pages = {1--35},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v17/15-239.html},
  urldate = {2021-10-20},
  abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
  issue = {59},
  language = {en},
  keywords = {_reading,⛔ No DOI found},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ganin et al_2016_Domain-Adversarial Training of Neural Networks.pdf}
}

@inproceedings{gaoDomainAdversarialAutoencoderAttention2021,
  title = {Domain-{{Adversarial Autoencoder}} with {{Attention Based Feature Level Fusion}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gao, Yuan and Liu, JiaXing and Wang, Longbiao and Dang, Jianwu},
  date = {2021-06},
  pages = {6314--6318},
  issn = {2379-190X},
  doi = {10/gmr2jt},
  abstract = {Over the past two decades, although speech emotion recognition (SER) has garnered considerable attention, the problem of insufficient training data has been unresolved. A potential solution for this problem is to pre-train a model and transfer knowledge from large amounts of audio data. However, the data used for pre-training and testing originate from different domains, resulting in the latent representations to contain non-affective information. In this paper, we propose a domain-adversarial autoencoder to extract discriminative representations for SER. Through domain-adversarial learning, we can reduce the mismatch between domains while retaining discriminative information for emotion recognition. We also introduce multi-head attention to capture emotion information from different subspaces of input utterances. Experiments on IEMOCAP show that the proposed model outperforms the state-of-the-art systems by improving the unweighted accuracy by 4.15\%, thereby demonstrating the effectiveness of the proposed model.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Conferences,Data models,domain adaptation,Emotion recognition,Feature extraction,multi-head attention,representation learning,Signal processing,speech emotion recognition,Speech recognition,Training data},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gao et al_2021_Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for.pdf}
}

@inproceedings{gaoDomainAdversarialAutoencoderAttention2021a,
  title = {Domain-{{Adversarial Autoencoder}} with {{Attention Based Feature Level Fusion}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gao, Yuan and Liu, JiaXing and Wang, Longbiao and Dang, Jianwu},
  date = {2021-06},
  pages = {6314--6318},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413710},
  abstract = {Over the past two decades, although speech emotion recognition (SER) has garnered considerable attention, the problem of insufficient training data has been unresolved. A potential solution for this problem is to pre-train a model and transfer knowledge from large amounts of audio data. However, the data used for pre-training and testing originate from different domains, resulting in the latent representations to contain non-affective information. In this paper, we propose a domain-adversarial autoencoder to extract discriminative representations for SER. Through domain-adversarial learning, we can reduce the mismatch between domains while retaining discriminative information for emotion recognition. We also introduce multi-head attention to capture emotion information from different subspaces of input utterances. Experiments on IEMOCAP show that the proposed model outperforms the state-of-the-art systems by improving the unweighted accuracy by 4.15\%, thereby demonstrating the effectiveness of the proposed model.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Conferences,Data models,domain adaptation,Emotion recognition,Feature extraction,multi-head attention,representation learning,Signal processing,speech emotion recognition,Speech recognition,Training data},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gao et al_2021_Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for2.pdf}
}

@inproceedings{gaoDomainInvariantFeatureLearning2022,
  title = {Domain-{{Invariant Feature Learning}} for {{Cross Corpus Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gao, Yuan and Okada, Shogo and Wang, Longbiao and Liu, Jiaxing and Dang, Jianwu},
  date = {2022},
  pages = {6427--6431},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747129},
  abstract = {To deal with speech emotion recognition (SER) in real-life applications, researchers have to focus on cross corpus SER, where the feature distribution of source and target datasets are different. In this paper, we propose an efficient domain adversarial training method to cope with the non-affective information during feature extraction. Through the proposed domain-adversarial learning, we can reduce the domain divergence between train and test data. Furthermore, we incorporate center loss with the emotion classifier to reduce the intra-class variation of features learned from the same emotion. We conduct experiments on four emotional benchmark datasets to verify the performance of the proposed method. The experimental results demonstrate that our proposed model outperform the baseline system in both cross-corpus and multi-corpus evaluation.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Adaptation models,center loss,Deep learning,domain adaptation,Emotion recognition,Representation learning,Signal processing,Speech emotion recognition,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gao et al_2022_Domain-Invariant Feature Learning for Cross Corpus Speech Emotion Recognition.pdf}
}

@thesis{GaoHanWeiQingGanYuYinYunLuTeZhengDuiBi2021,
  type = {硕士},
  title = {汉维情感语音韵律特征对比},
  author = {高, 政飞},
  date = {2021},
  institution = {{西北民族大学}},
  doi = {10.27408/d.cnki.gxmzc.2021.000045},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFDTEMP&filename=1021611583.nh&uniplatform=NZKPT&v=dnlw7AIuGT4nmEgtKLfXBF7g5TA3nJlJAXRyNhPB2qR%25mmd2Fuoi%25mmd2Fqh7fEg4C07KwYNgV},
  urldate = {2021-09-28},
  abstract = {本文旨在描述汉语以及维吾尔语中情感语音韵律特征的基本图式,并对比分析汉语和维吾尔语情感语音的韵律特征存在何种异同以及其成因。本文首先以话语的交际意图为核心来划分所要研究的情感种类,再关注以句子为单位的汉语和维吾尔语情感语音在基频、音长、音强这些物理特征方面的参数表现如何;本文提取这些物理特征中的有如平均值、峰值变化率、变化率范围等方面参数,之后统计概括汉语和维吾尔语不同性别、不同情感种类的语音韵律特征。最后选取普遍存在的语外、语内异同来进行对比分析总结,并最终主要取得如下结论:音高方面,女性平均基频要普遍高于男性水平,但基频变化率幅度都要远远高于女性。在维汉语中同一交际意图满足状态的情绪下不同生理唤醒度的三种情绪,它们的平均基频会展现出较强的正相关关系。音长和音强方面,汉维语承担异化情绪的韵律特征各有不同,维吾尔语情绪韵律异化主要依托音强变化实现,汉语情绪韵律异化主要依托语速变化实现。},
  editora = {沙, 文杰},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {Chinese,Emotion,Prosodic features,Uyghur language,voice,情感,汉语,维吾尔语,语音,韵律特征},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\高_2021_汉维情感语音韵律特征对比.pdf}
}

@inproceedings{gaoMetricLearningBased2021,
  title = {Metric {{Learning Based Feature Representation}} with {{Gated Fusion Model}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Gao, Yuan and Liu, Jiaxing and Wang, Longbiao and Dang, Jianwu},
  date = {2021-08-30},
  pages = {4503--4507},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1133},
  url = {https://www.isca-speech.org/archive/interspeech_2021/gao21e_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gao et al_2021_Metric Learning Based Feature Representation with Gated Fusion Model for Speech.pdf}
}

@article{GaoMianXiangQingGanYuYinHeChengDeYanYuQingGanMiaoShuYuYuCe2017,
  title = {面向情感语音合成的言语情感描述与预测},
  author = {高, 莹莹 and 朱, 维彬},
  date = {2017},
  journaltitle = {清华大学学报(自然科学版)},
  volume = {57},
  number = {02},
  pages = {202--207},
  issn = {1000-0054},
  doi = {10.16511/j.cnki.qhdxxb.2017.22.015},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2017&filename=QHXB201702015&uniplatform=NZKPT&v=5ZE1lb4q0Eqlsdk62s5xN%25mmd2BYSSMJgX%25mmd2B%25mmd2Fz31HmTKndVs3R8rJMD5w3m%25mmd2BeW2UG%25mmd2BpEKR},
  urldate = {2021-09-10},
  abstract = {针对情感语音合成系统中情感的细腻刻画与自动预测问题,提出多视角情感描述模型,从认知评价、心理感受、生理反应和发音方式4个方面刻画言语情感的产生过程和衍化机制;引入能够支持分布式特征且具有堆叠结构的多层神经网络\textemdash\textemdash 深层堆叠网络构建从文本到情感描述的预测模型。实验结果表明在预测模型中引入不同情感成分和上下文信息作为特征有助于提升预测效果,验证了采用深层堆叠网络进行情感预测的有效性与多视角情感描述模型的合理性。},
  issue = {02},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,deep neural network,emotion description,speech synthesis,text-based emotion prediction,情感描述,文本情感预测,深层神经网络},
  annotation = {4 citations(CNKI)[2021-9-10]{$<$}北大核心, EI, CSCD{$>$}},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\高_朱_2017_面向情感语音合成的言语情感描述与预测.pdf}
}

@thesis{GaoTongJiCanShuYuYinHeChengZhongDeJiPinJianMoYuShengChengFangFaYanJiu2015,
  type = {硕士},
  title = {统计参数语音合成中的基频建模与生成方法研究},
  author = {高, 丽},
  date = {2015},
  institution = {{中国科学技术大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201502&filename=1015615503.nh&uniplatform=NZKPT&v=quV1SYWXAaHEBudJ7KVmae%25mmd2BB6mgeO6mpCIDxpPX01K%25mmd2FHf5AZagnivL153TFdVF0g},
  urldate = {2021-09-10},
  abstract = {基于隐马尔科夫模型（Hidden Markov Model, HMM）的统计参数语音合成是当今主流的语音合成方法之一。该方法在训练阶段利用录制的语音数据库,建立描述不同上下文环境下频谱、基频等声学特征分布的统计声学模型；在合成阶段,该方法依据输入文本的上下文信息,从训练的统计声学模型中生成声学特征,最终通过参数合成器重构语音波形。相对单元挑选与波形拼接合成方法,基于HMM的参数合成方法具有系统构建自动化程度高、合成语音平滑流畅、系统尺寸小等优势,但是其合成语音的自然度仍有待提高。基频描述了浊音产生过程中声带震动的频率,是一种重要的语音声学特征。在基于HMM的参数语音合成中,基频特征的预测性能对于合成语音的自然度有着直接的影响。此外,基频特征的差异对于体现情感语音合成中的不同目标情感也起到重要作用。相对频谱特征,基频特征是一种超音段特征,长时的基频轨迹形状受到语调、短语边界、轻重读等韵律属性的影响。而传统基于HMM的参数语音合成使用和频谱类似的基频特征提取尺度和建模方法,忽略了基频的长时特性,影响了合成语音的自然度。本文围绕统计参数语音合成中的基频建模与生成方法开展研究工作,使用长度规整基频矢量（FO Vector, FV）、目标逼近（Target Approximation, TA）特征等音节层表征作为基频特征,实现了基于目标逼近特征的基频建模,提出了基于音节层特征的生成基频后处理方法,提高了合成语音的自然度。此外,本文还进一步将基于目标逼近特征和高斯双向联想贮存器（Gaussian Bidirectional Associative Memories, GBAM）的后处理方法应用于合成语音的情感转换,对于高兴和生气情感,取得了优于传统的模型自适应方法的转换后语音情感表现力。本文的具体内容组织如下：第一章是绪论,将简要回顾语音合成技术的发展史,并介绍现阶段语音合成的主流方法、情感语音合成、以及基频的相关背景知识。在第二章将介绍基于HMM的参数语音合成方法,包括方法概述、训练端与合成端的核心算法、存在问题分析等,最后阐述了本文研究内容的动机与出发点。第三章具体介绍基于目标逼近特征的基频建模方法。该方法在训练阶段利用目标逼近模型对音节层的基频轨迹进行参数化处理,然后构建聚类决策树来描述不同上下文环境下的目标逼近特征分布；在合成阶段,该方法从预测的目标逼近参数中恢复音节基频轨迹,并结合传统方法生成的频谱特征恢复语音波形。实验结果表明了该方法可以生成较为自然的合成语音,也指出了其存在对于基频轨迹细节丢失的问题。第四章具体介绍基于音节层特征的生成基频后处理方法。该方法在训练阶段首先提取传统HMM合成方法预测基频以及录音语料中的自然基频所对应的音节层基频特征,包括基频矢量特征和目标逼近特征等,然后构建从预测基频音节层特征向自然基频音节层的特征映射的后处理模型,包括全局方差均衡化、GBAM.残差补偿模型等；在合成阶段,该方法对传统HMM方法预测的基频轨迹进行后处理,以得到最终的基频生成结果。主观实验结果表明该方法可以有效提高合成语音的自然度。第五章具体介绍基于目标逼近特征的合成语音情感转换方法。该方法针对在目标情感数据量有限情况下的情感语音合成系统构建问题,通过建立中立合成语音的音节层目标逼近特征向目标情感语音相应特征的映射关系,实现中立合成语音基频特征向目标情感的转换。本章使用GBAM模型进行不同情感间目标逼近特征的转换。实验结果表明,该方法对于高兴、生气等高唤醒度情感可以取得比最大似然线性回归（Maximum Likelihood Linear Regression, MLLR）模型自适应方法更好的合成语音情感表现力。第六章对全文进行了总结归纳。},
  editora = {凌, 震华},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,emotional speech synthesis,FO generation,FO modeling,speech synthesis,基频建模,基频生成,语音合成},
  annotation = {6 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\高_2015_统计参数语音合成中的基频建模与生成方法研究.pdf}
}

@article{garcia-martinezReviewNonlinearMethods2021,
  title = {A {{Review}} on {{Nonlinear Methods Using Electroencephalographic Recordings}} for {{Emotion Recognition}}},
  author = {Garc\'ia-Mart\'inez, Beatriz and Mart\'inez-Rodrigo, Arturo and Alcaraz, Ra\'ul and Fern\'andez-Caballero, Antonio},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {801--820},
  issn = {1949-3045},
  doi = {10.1109/taffc.2018.2890636},
  abstract = {Electroencephalographic (EEG) recordings are receiving growing attention in the field of emotion recognition, since they monitor the brain's first response to an external stimulus. Traditionally, EEG signals have been studied from a linear viewpoint by means of statistical and frequency features. Nevertheless, given that the brain follows a completely nonlinear and nonstationary behavior, linear metrics present certain important limitations. In this sense, the use of nonlinear methods has recently revealed new information that may help to understand how the brain works under a series of emotional states. Hence, this paper summarizes the most recent works that have applied nonlinear methods in EEG signal analysis for emotion recognition. This paper also identifies some nonlinear indices that have not been employed yet in this research area.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {Complexity theory,Distance measurement,Electrodes,Electroencephalogram,Electroencephalography,emotion recognition,Emotion recognition,nonlinear metrics,survey,Time series analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\García-Martínez et al_2021_A Review on Nonlinear Methods Using Electroencephalographic Recordings for.pdf}
}

@inproceedings{gatSpeakerNormalizationSelfSupervised2022,
  title = {Speaker {{Normalization}} for {{Self-Supervised Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gat, Itai and Aronowitz, Hagai and Zhu, Weizhong and Morais, Edmilson and Hoory, Ron},
  date = {2022},
  pages = {7342--7346},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747460},
  abstract = {Large speech emotion recognition datasets are hard to obtain, and small datasets may contain biases. Deep-net-based classifiers, in turn, are prone to exploit those biases and find shortcuts such as speaker characteristics. These shortcuts usually harm a model's ability to generalize. To address this challenge, we propose a gradient-based adversary learning framework that learns a speech emotion recognition task while normalizing speaker characteristics from the feature representation. We demonstrate the efficacy of our method on both speaker-independent and speaker-dependent settings and obtain new state-of-the-art results on the challenging IEMOCAP dataset.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustics,Conferences,Emotion recognition,self-supervised learning,Signal processing,speaker normalization,Speech emotion recognition,Speech processing,Speech recognition,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gat et al_2022_Speaker Normalization for Self-Supervised Speech Emotion Recognition.pdf}
}

@article{gaultierSparsityBasedAudioDeclipping2021,
  title = {Sparsity-{{Based Audio Declipping Methods}}: {{Selected Overview}}, {{New Algorithms}}, and {{Large-Scale Evaluation}}},
  shorttitle = {Sparsity-{{Based Audio Declipping Methods}}},
  author = {Gaultier, Cl\'ement and Kiti\'c, Sr\dj an and Gribonval, R\'emi and Bertin, Nancy},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1174--1187},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3059264},
  abstract = {Recent advances in audio declipping have substantially improved the state of the art. Yet, practitioners need guidelines to choose a method, and while existing benchmarks have been instrumental in advancing the field, larger-scale experiments are needed to guide such choices. First, we show that the clipping levels in existing small-scale benchmarks are moderate and call for benchmarks with more perceptually significant clipping levels. We then propose a general algorithmic framework for declipping that covers existing and new combinations of variants of state-of-the-art techniques exploiting time-frequency sparsity: synthesis vs. analysis sparsity, with plain or structured sparsity. Finally, we systematically compare these combinations and a selection of state-of-the-art methods. Using a large-scale numerical benchmark and a smaller scale formal listening test, we provide guidelines for various clipping levels, both for speech and various musical genres. The code is made publicly available for the purpose of reproducible research and benchmarking.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Audio declipping,Benchmark testing,Degradation,Distortion,listening test,Power measurement,sparsity,Speech processing,structured sparsity,Systematics,time-frequency,Time-frequency analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gaultier et al_2021_Sparsity-Based Audio Declipping Methods.pdf}
}

@article{genestInfluenceLengthHuman1985,
  title = {The Influence of the Length of the Human {{Y}} Chromosome on Spontaneous Abortions. {{A}} Prospective Study in Family Lines with Inherited Polymorphic {{Y}} Chromosomes},
  author = {Genest, P. and Genest, F. B.},
  date = {1985-01-01},
  journaltitle = {Annales de genetique},
  shortjournal = {Ann Genet},
  volume = {28},
  number = {3},
  eprint = {3879146},
  eprinttype = {pmid},
  pages = {143--148},
  issn = {0003-3995},
  abstract = {Following reports indicating a close association between the presence of a long Y chromosome in males and the risk of spontaneous abortion in their female partners, the incidence of spontaneous fetal loss was investigated in four family lines whose patrilineary ancestors emigrated from France to Canada during the second half of the seventeenth century. In two of the lines the males were carriers of a Yq+, in the other two the males had a Yq- or a normal Y chromosome. Results showed that in one of family lines with a Yq+, 17/26 (65.4\%) wives had 33 (2.8\%) spontaneous abortions in 151 pregnancies, whereas in each of the three other family lines 7/30 (23.3\%) wives aborted 8 (4.9\%), 15 (7.5\%) and 10 (5.7\%) times in 165, 200 and 175 pregnancies respectively. The high incidence of fetal loss found in one of the family lines whose males have a long Y chromosome correlates with previous observations on the influence of Yq+ on spontaneous abortions, and draws attention to the inheritable nature of this peculiarity. However, the low incidence of miscarriages observed in the other family line carrying a Yq+ seems to indicate that long Y chromosomes are of various types and could be produced by several mechanisms. Yq- does not seem to represent an increased risk of pregnancy loss. Results also demonstrated that while a long Y chromosome may affect the viability of the zygote, it does not affect the fertility of its carrier.},
  language = {en},
  keywords = {\#nosource}
}

@thesis{GengYongHuYuYinShuJuQingGanFenXiYanJiu2021,
  type = {硕士},
  title = {用户语音数据情感分析研究},
  author = {耿, 佳宁},
  date = {2021},
  institution = {{中国科学技术大学}},
  doi = {10.27517/d.cnki.gzkju.2021.001365},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202102&filename=1021073408.nh&uniplatform=NZKPT&v=JQJ75UbYEsO8VzB3orD3I0EkedIPpra4hBgsU1r%25mmd2BMCsA%25mmd2BwKTbXNi8uhm20nMlzZj},
  urldate = {2021-09-10},
  abstract = {作为一种最常见的自然交互方式,语音在人机交互领域也是重要的方式,更是物联网发展过程中尤受重视的研究领域。研究人员对于语音的研究,多集中于语音识别、语音文字转换等方向,这些方向是对语音中语义的识别和处理,但语音是一种复杂的高级行为,不应仅仅包含文字部分,还蕴含着情感等复杂的组成信息。语音情感识别不关注语音的具体语义信息,而是通过语音的变化,识别出语音中蕴藏的情感。语音情感识别的挑战主要在于个人间的语音表述差异和语音内容的差异:对于同一种情感,不同的人会具有不同的表达和习惯,分类方法很难适应所有人的特质;语音情感种类是有限的,而语音内容的组合却是无限的,这种不对称性使得分类模型在提取情感特征上面临着较大的挑战。因此,语音特征的选择和提取,以及分类器的设计是影响语音情感分类结果的重要因素。在语音的特征选择方面,论文对主流的语音特征进行了介绍,并阐述了其具体的计算方法,最终选取了梅尔倒谱系数（Mel cepstrum coefficient,MCEP）和梅尔频率倒谱系数（Mel-frequency cepstrum coefficient,MFCC）,这两个系数能够整合语音数据在时域和频域的特征,是语音领域常用的特征。在分类器设计方面,得益于深度卷积神经网络（Deep Convolutional Neural Network,DCNN）在图像识别的优异表现,论文首先设计了由卷积神经网络组成的模型\textemdash CNNSpeech;接着考虑到语音的远程情境效应和情感标签表达的不确定性,借鉴了多注意力头机制（Transformer）的编码器部分,得到的RawSeeSpeech模型能够提取语音中的远程上下文依赖,获得了丰富的情感特征;最后为了进一步减小同种情感之间的距离,进一步引入了中心损失函数（Central Loss Function）,使用联合决策的方式,该模型被称为SeeSpeech。SeeSpeech模型不仅可以获得较高的分类准确度,且由于使用中心损失函数和 Softmax 交叉熵损失函数（Softmax Cross Entropy Loss Function）联合决策,减小了类内距离,增加了类间差距,使得模型与说话人无关的,具有较高的泛化性。在日常工作场景下,设计了真实环境中的实验,首先,通过对语音加噪的方式,增加模型对噪声的鲁棒性,接着使用带通滤波和小波滤波,对数据进行滤波,增加了噪声数据的分类准确度。论文在实验中通过对比发现,使用梅尔倒谱系数和SeeSpeech模型能够得到较优的分类结果,分类准确率达到了 94\%。交叉验证的结果也表明了模型结果是与说话人不相关的。最后,论文展示了在实际的物联网场景中,SeeSpeech模型在准确度测试一项,实现了了 82\%的准确率,在运行性能测试中,证明了我们的模型可以在物联网的瘦设备上使用,具有广泛的应用场景。},
  editora = {李, 向阳},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Convolutional Neural Network,Internet of Things,Mel cepstrum Coefficient,Multi-Attention Head Mechanism,Speech Emotion Recognition,卷积神经网络,多注意力头机制,梅尔倒谱系数,物联网,语音情感识别},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\耿_2021_用户语音数据情感分析研究.pdf}
}

@article{gerdesEmotionalSoundsGuide2021,
  title = {Emotional Sounds Guide Visual Attention to Emotional Pictures: {{An}} Eye-Tracking Study with Audio-Visual Stimuli},
  shorttitle = {Emotional Sounds Guide Visual Attention to Emotional Pictures},
  author = {Gerdes, Antje B. M. and Alpers, Georg W. and Braun, Hanna and K\"ohler, Sabrina and Nowak, Ulrike and Treiber, Lisa},
  date = {2021},
  journaltitle = {Emotion},
  volume = {21},
  number = {4},
  pages = {679--692},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1931-1516},
  doi = {10.1037/emo0000729},
  abstract = {For the realm of visual cues, it has been well documented that attention is preferentially oriented toward emotionally relevant cues. Preliminary evidence suggests that emotional cues from other sensory modalities may also steer visual attention toward emotional pictures. However, more research is needed to elucidate the mechanisms that are involved. Therefore, a novel design was used to investigate whether emotional sounds promote attentional orientation toward emotional pictures. To this end, 48 participants viewed pairs of pictures with either neutral or unpleasant content in a free-viewing paradigm. In addition, neutral or unpleasant sounds were presented either on the left-hand or on the right-hand side of the monitor. Eye movements were recorded as an index of visual spatial attention toward the pictures. Most interestingly, position and valence of the sounds independently modulated visual orienting towards unpleasant pictures. For initial capture and sustained attention, orienting towards unpleasant pictures was significantly enhanced when any sound was heard on the same side as the unpleasant picture. In addition, unpleasant sounds (irrespective of the side) boosted leftward bias of initial attention toward emotionally congruent pictures. Taken together, this study clearly shows that emotional auditory cues guide visual spatial allocation of attention specifically to emotionally congruent pictures. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  issue = {4},
  language = {en},
  keywords = {\#nosource,Attentional Bias,Auditory Stimulation,Cues,Eye Movements,Orienting Responses,Visual Attention,Visual Tracking,Visuospatial Ability}
}

@inproceedings{ghrissSentimentAwareAutomaticSpeech2022,
  title = {Sentiment-{{Aware Automatic Speech Recognition Pre-Training}} for {{Enhanced Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ghriss, Ayoub and Yang, Bo and Rozgic, Viktor and Shriberg, Elizabeth and Wang, Chao},
  date = {2022},
  pages = {7347--7351},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747637},
  abstract = {We propose a novel multi-task pre-training method for Speech Emotion Recognition (SER). We pre-train SER model simultaneously on Automatic Speech Recognition (ASR) and sentiment classification tasks to make the acoustic ASR model more "emotion aware". We generate targets for the sentiment classification using text-to-sentiment model trained on publicly available data. Finally, we fine-tune the acoustic ASR on emotion annotated speech data. We evaluated the proposed approach on MSP-Podcast dataset, where we achieved the best reported concordance correlation coefficient (CCC) of 0.41 for valence prediction.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_wreading,Acoustics,automatic speech recognition,Correlation coefficient,Emotion recognition,Multitasking,Phonetics,pre-training,sentiment analysis,Signal processing,Speech emotion recognition,Speech enhancement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ghriss et al_2022_Sentiment-Aware Automatic Speech Recognition Pre-Training for Enhanced Speech.pdf}
}

@unpublished{girdharAttentionalPoolingAction2017,
  title = {Attentional {{Pooling}} for {{Action Recognition}}},
  author = {Girdhar, Rohit and Ramanan, Deva},
  date = {2017-12-29},
  eprint = {1711.01467},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.01467},
  urldate = {2022-05-02},
  abstract = {We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII dataset with 12.5\% relative improvement. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {_readed,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Girdhar_Ramanan_2017_Attentional Pooling for Action Recognition.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\SSC5EGTH\\1711.html}
}

@inproceedings{girishInterpretabiltySpeechEmotion2022,
  title = {Interpretabilty of {{Speech Emotion Recognition}} Modelled Using {{Self-Supervised Speech}} and {{Text Pre-Trained Embeddings}}},
  booktitle = {Interspeech 2022},
  author = {Girish, K V Vijay and Konjeti, Srikanth and Vepa, Jithendra},
  date = {2022-09-18},
  pages = {4496--4500},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10685},
  url = {https://www.isca-speech.org/archive/interspeech_2022/girish22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Girish et al_2022_Interpretabilty of Speech Emotion Recognition modelled using Self-Supervised.pdf}
}

@article{goblRoleVoiceQuality2003,
  title = {The Role of Voice Quality in Communicating Emotion, Mood and Attitude},
  author = {Gobl, C},
  date = {2003-04},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {40},
  number = {1-2},
  pages = {189--212},
  issn = {01676393},
  doi = {10.1016/S0167-6393(02)00082-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639302000821},
  urldate = {2022-01-23},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gobl_2003_The role of voice quality in communicating emotion, mood and attitude.pdf}
}

@inproceedings{gomez-canonLanguageSensitiveMusicEmotion2021,
  title = {Language-{{Sensitive Music Emotion Recognition Models}}: Are {{We Really There Yet}}?},
  shorttitle = {Language-{{Sensitive Music Emotion Recognition Models}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {G\'omez-Ca\~n\'on, Juan Sebasti\'an and Cano, Estefan\'ia and Pandrea, Ana Gabriela and Herrera, Perfecto and G\'omez, Emilia},
  date = {2021-06},
  pages = {576--580},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413721},
  abstract = {Our previous research showed promising results when transferring features learned from speech to train emotion recognition models for music. In this context, we implemented a denoising autoencoder as a pretraining approach to extract features from speech in two languages (English and Mandarin). From that, we performed transfer and multi-task learning to predict classes from the arousal-valence space of music emotion. We tested and analyzed intra-linguistic and cross-linguistic settings, depending on the language of speech and lyrics of the music. This paper presents additional investigation on our approach, which reveals that: (1) performing pretraining with speech in a mixture of languages yields similar results than for specific languages - the pretraining phase appears not to exploit particular language features, (2) the music in Mandarin dataset consistently results in poor classification performance - we found low agreement in annotations, and (3) novel methodologies for representation learning (Contrastive Predictive Coding) may exploit features from both languages (i.e., pretraining on a mixture of languages) and improve classification of music emotions in both languages. From this study we conclude that more research is still needed to understand what is actually being transferred in these type of contexts.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Annotations,Contrastive predictive coding,Emotion recognition,multi-task learning,music emotion recognition,Noise reduction,Predictive coding,representation learning,Speech coding,speech emotion recognition,Speech recognition,transfer learning,Transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gómez-Cañón et al_2021_Language-Sensitive Music Emotion Recognition Models.pdf}
}

@inproceedings{goncalvesAuxFormerRobustApproach2022,
  title = {{{AuxFormer}}: {{Robust Approach}} to {{Audiovisual Emotion Recognition}}},
  shorttitle = {{{AuxFormer}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Goncalves, Lucas and Busso, Carlos},
  date = {2022},
  pages = {7357--7361},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747157},
  abstract = {A challenging task in audiovisual emotion recognition is to implement neural network architectures that can leverage and fuse multimodal information while temporally aligning modalities, handling missing modalities, and capturing information from all modalities without losing information during training. These requirements are important to achieve model robustness and to increase accuracy on the emotion recognition task. A recent approach to perform multimodal fusion is to use the transformer architecture to properly fuse and align the modalities. This study proposes the AuxFormer framework, which addresses in a principled way the aforementioned challenges. AuxFormer combines the transformer framework with auxiliary networks. It uses shared losses to infuse information from single-modality networks that are separately embedded. The extra layer of audiovisual information added to our main network retains information that would otherwise be lost during training. The results show that the AuxFormer architecture achieves macro and micro F1Scores of 71.3\% and 71.7\%, respectively, on the CREMA-D corpus. For the MSP-IMPROV corpus, AuxFormer achieves a macro and micro F1-Scores of 70.4\% and 76.5\%, respectively. The results for both corpora are significantly better than strong baselines, indicating that our framework benefits from auxiliary networks. We also show that under non-ideal conditions (e.g., missing modalities) our architecture is able to sustain strong performance under audio-only and video-only scenarios, benefiting from a optimized training strategy.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Audiovisual emotion recognition,auxiliary networks,Conferences,Emotion recognition,Fuses,multimodal fusion,Neural networks,shared losses,Signal processing,Training,transformers,Transformers},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Goncalves_Busso_2022_AuxFormer.pdf}
}

@inproceedings{goncalvesImprovingSpeechEmotion2022,
  title = {Improving {{Speech Emotion Recognition Using Self-Supervised Learning}} with {{Domain-Specific Audiovisual Tasks}}},
  booktitle = {Interspeech 2022},
  author = {Goncalves, Lucas and Busso, Carlos},
  date = {2022-09-18},
  pages = {1168--1172},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11012},
  url = {https://www.isca-speech.org/archive/interspeech_2022/goncalves22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Goncalves_Busso_2022_Improving Speech Emotion Recognition Using Self-Supervised Learning with.pdf}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  urldate = {2022-08-14},
  abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
  language = {en},
  keywords = {_Code,_readed,⛔ No DOI found},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Goodfellow et al_2014_Generative Adversarial Nets.pdf}
}

@article{goodmanValuingEmotionalControl2021,
  title = {Valuing Emotional Control in Social Anxiety Disorder: {{A}} Multimethod Study of Emotion Beliefs and Emotion Regulation},
  shorttitle = {Valuing Emotional Control in Social Anxiety Disorder},
  author = {Goodman, Fallon R. and Kashdan, Todd B. and \.Imamo\u{g}lu, Asl\i han},
  date = {2021},
  journaltitle = {Emotion},
  volume = {21},
  number = {4},
  pages = {842--855},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1931-1516},
  doi = {10.1037/emo0000750},
  abstract = {This study examines relationships between emotion beliefs and emotion regulation strategy use among people with social anxiety disorder (SAD) and a psychologically healthy control group. Using experience-sampling methodology, we tested group differences in 2 types of emotion beliefs (emotion control values and emotion malleability beliefs) and whether emotion beliefs predicted trait and daily use of cognitive reappraisal and emotion suppression. People with SAD endorsed higher emotion control values and lower emotion malleability beliefs than did healthy controls. Across groups, emotion control values were positively associated with suppression (but unrelated to reappraisal), and emotion malleability beliefs were negatively associated with suppression and positively associated with reappraisal. We also addressed 2 exploratory questions related to measurement. First, we examined whether trait and state measures of emotion regulation strategies were related to emotion control values in different ways and found similar associations across measures. Second, we examined whether explicit and implicit measures of emotion control values were related to daily emotion regulation strategy use in different ways\textemdash and found that an implicit measure was unrelated to strategy use. Results are discussed in the context of growing research on metaemotions and the measurement of complex features of emotion regulation. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  issue = {4},
  language = {en},
  keywords = {\#nosource,Cognitive Appraisal,Emotional Control,Emotional Regulation,Group Differences,Measurement,Social Phobia,Test Construction}
}

@article{gosslingPerformanceAnalysisExtended2021,
  title = {Performance {{Analysis}} of the {{Extended Binaural MVDR Beamformer With Partial Noise Estimation}}},
  author = {G\"o\ss ling, Nico and Marquardt, Daniel and Doclo, Simon},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {462--476},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3043674},
  abstract = {Besides reducing undesired noise sources and limiting speech distortion, another important objective of a binaural noise reduction algorithm is the preservation of the binaural cues of all sound sources in the acoustic scene. In this paper, we consider the binaural minimum variance distortionless response beamformer with partial noise estimation (BMVDR-N), which allows to trade off between noise reduction performance and binaural cue preservation of the noise component by mixing the output signals of the BMVDR beamformer with the noisy reference microphone signals. For a directional noise source, it has been shown that incorporating an external microphone in addition to the head-mounted microphones enables both the noise reduction performance as well as the interaural time and level difference cues of the noise component to be improved in the output signals. In this paper, we consider an arbitrary noise field and analytically show that incorporating an external microphone in the BMVDR-N beamformer enables 1) a larger output signal-to-noise ratio (SNR) for the same mixing parameter, 2) the same output SNR for a larger mixing parameter, and 3) the same desired output magnitude squared coherence (MSC) of the noise component for a smaller mixing parameter to be obtained. The derived analytical expressions are firstly validated using simulated anechoic acoustic transfer functions, where the listener's head is modelled as a rigid sphere. Experimental results using recorded signals for a binaural hearing device setup in a reverberant environment also show that in a realistic scenario incorporating an external microphone in the BMVDR-N beamformer significantly improves the output SNR and reduces the mixing parameter that is required to obtain a desired output MSC of the noise component compared to using only the head-mounted microphones.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustic distortion,Acoustics,Auditory system,Binaural cues,binaural noise reduction,external microphone,hearing devices,Microphones,Noise measurement,Noise reduction,Signal to noise ratio,speech enhancement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gößling et al_2021_Performance Analysis of the Extended Binaural MVDR Beamformer With Partial.pdf}
}

@article{gosztolyaEnsembleBagofAudioWordsRepresentation2021,
  title = {Ensemble {{Bag-of-Audio-Words Representation Improves Paralinguistic Classification Accuracy}}},
  author = {Gosztolya, G\'abor and Busa-Fekete, R\'obert},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {477--488},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3044465},
  abstract = {A recently introduced, effective feature extraction technique for computational paralinguistics is that of Bag-of-Audio-Words (BoAW), where we cluster the frame-level training vectors, and represent each speech utterance based on the cluster of its frames. Over the past few years, several improvements have been proposed for the original BoAW approach, but none of them has examined the impact of the stochastic nature of the clustering step. In this study we demonstrate experimentally that the random factor present in the BoAW clustering step is indeed propagated into the next classification step, eventually leading to suboptimal classification performance. As a solution, we propose to train an ensemble of classifiers; that is, we repeat the BoAW codebook selection step several times, train separate classifier models for these BoAW representation versions and combine their predictions. Our results, obtained for three different paralinguistic datasets, demonstrate that this ensemble technique makes the whole paralinguistic classification process more robust, and it leads to improvements in the classification performance. We tested this technique on three different paralinguistic datasets, and achieved the highest Unweighted Average Recall score reported so far on the iHEARu-EAT corpus.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Bag-of-Audio-Words representation,classification,Computational paralinguistics,Databases,ensemble learning,Feature extraction,Histograms,Speech coding,Speech processing,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gosztolya_Busa-Fekete_2021_Ensemble Bag-of-Audio-Words Representation Improves Paralinguistic.pdf}
}

@article{grasshofMultilinearModellingFaces2021,
  title = {Multilinear {{Modelling}} of {{Faces}} and {{Expressions}}},
  author = {Grasshof, Stella and Ackermann, Hanno and Brandt, Sami Sebastian and Ostermann, J\"orn},
  date = {2021},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {10},
  pages = {3540--3554},
  issn = {1939-3539},
  doi = {10.1109/tpami.2020.2986496},
  abstract = {In this work, we present a new versatile 3D multilinear statistical face model, based on a tensor factorisation of 3D face scans, that decomposes the shapes into person and expression subspaces. Investigation of the expression subspace reveals an inherent low-dimensional substructure, and further, a star-shaped structure. This is due to two novel findings. (1) Increasing the strength of one emotion approximately forms a linear trajectory in the subspace. (2) All these trajectories intersect at a single point \textendash{} not at the neutral expression as assumed by almost all prior works\textemdash but at an apathetic expression. We utilise these structural findings by reparameterising the expression subspace by the fourth-order moment tensor centred at the point of apathy. We propose a 3D face reconstruction method from single or multiple 2D projections by assuming an uncalibrated projective camera model. The non-linearity caused by the perspective projection can be neatly included into the model. The proposed algorithm separates person and expression subspaces convincingly, and enables flexible, natural modelling of expressions for a wide variety of human faces. Applying the method on independent faces showed that morphing between different persons and expressions can be performed without strong deformations.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  issue = {10},
  language = {en},
  keywords = {3D-reconstruction,Analytical models,Data models,expression transfer,HOSVD,person transfer,Shape,Solid modeling,Statistical shape model,Tensile stress,tensor model,Three-dimensional displays,Two dimensional displays},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Grasshof et al_2021_Multilinear Modelling of Faces and Expressions.pdf}
}

@article{guarasciBERTSyntacticTransfer2022,
  title = {{{BERT}} Syntactic Transfer: {{A}} Computational Experiment on {{Italian}}, {{French}} and {{English}} Languages},
  shorttitle = {{{BERT}} Syntactic Transfer},
  author = {Guarasci, Raffaele and Silvestri, Stefano and De Pietro, Giuseppe and Fujita, Hamido and Esposito, Massimo},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101261},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101261},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000681},
  urldate = {2021-09-28},
  abstract = {This paper investigates the ability of multilingual BERT (mBERT) language model to transfer syntactic knowledge cross-lingually, verifying if and to which extent syntactic dependency relationships learnt in a language are maintained in other languages. In detail, the main contributions of this paper are: (i) an analysis of the cross-lingual syntactic transfer capability of mBERT model; (ii) a detailed comparison of cross-language syntactic transfer among languages belonging to different branches of the Indo-European languages, namely English, Italian and French, which present very different syntactic constructions; (iii) a study on the transferability of a syntactic phenomenon peculiar of Italian language, namely the pronoun dropping (pro-drop), also known as omissibility of the subject. To this end, a structural probe devoted to reconstruct the dependency parse tree of a sentence has been exploited, representing the input sentences with the contextual embeddings from mBERT layers. The results of the experimental assessment have shown a transfer of syntactic knowledge of the mBERT model among these languages. Moreover, the behaviour of the probe in the transition from pro-drop to non-pro-drop languages and vice versa has proven to be more effective in case of languages sharing a common linguistic matrix. The possibility of transferring syntactical knowledge, especially in the case of specific phenomena, meets both a theoretical need and can have important practical implications in syntactic tasks, such as dependency parsing.},
  language = {en},
  keywords = {\#nosource,Cross language,Dependency Parse Tree,Language models,Multilingual BERT,Syntactic phenomena,Transfer learning}
}

@inproceedings{gudmalwarMagnitudePhaseBased2022,
  title = {The {{Magnitude}} and {{Phase}} Based {{Speech Representation Learning}} Using {{Autoencoder}} for {{Classifying Speech Emotions}} Using {{Deep Canonical Correlation Analysis}}},
  booktitle = {Interspeech 2022},
  author = {Gudmalwar, Ashishkumar and Basel, Biplove and Dutta, Anirban and Rama Rao, Ch V},
  date = {2022-09-18},
  pages = {1163--1167},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10769},
  url = {https://www.isca-speech.org/archive/interspeech_2022/gudmalwar22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gudmalwar et al_2022_The Magnitude and Phase based Speech Representation Learning using Autoencoder.pdf}
}

@thesis{GuoJiYuYuYinBiaoQingYuZiTaiDeDuoMoTaiQingGanShiBieSuanFaShiXian2017,
  type = {硕士},
  title = {基于语音、表情与姿态的多模态情感识别算法实现},
  author = {郭, 帅杰},
  date = {2017},
  institution = {{南京邮电大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1017859079.nh&uniplatform=NZKPT&v=i%25mmd2FC758VzSpGDFKkUXkV6ujY6CldMADKiQxEwNM3mOJ5vmavIo5q7Kihq2cQeGMEV},
  urldate = {2021-09-10},
  abstract = {在人类的社交活动以及日常交流过程中,语音、姿态和面部表情被认为是传达人类情感的主要渠道。近年来,人类情感识别领域的相关研究取得了长足的发展与进步,为人工智能在今后的普及奠定了坚实的理论基础。起初人们对人类情感识别的研究还仅仅停留在单模态的情感识别上,然而,近几年来,随着科学技术尤其是人工智能技术的不断发展,越来越多的研究人员将目光放在了基于多模态相互融合的情感识别研究上来。文中提出了一种基于语音、表情与姿态的三模态情感自动识别方法。本文的研究工作主要有:（1）各模态情感特征的提取。首先,对于语音信号,提取得到它的韵律特征和MFCC特征等情感特征;对于表情信号,利用Gabor小波变换提取得到它的Gabor特征,并利用主成分分析方法对其进行特征降维以去除冗余信息,同时使其更易于处理;对于姿态信号,则利用EyesWeb平台提取得到描述姿态情感特征的各种参数来组成姿态情感特征。（2）情感特征的融合。对提取得到的各模态的情感特征进行归一化,并利用判别多重典型相关分析（Discriminative Multiple Canonical Correlation Analysis,DMCCA）将三个模态的情感特征融合起来,接着利用特征选择算法ReliefF对融合后的情感特征进行特征选择以进一步去除那些对类别判定不利的特征。经过融合后的特征不仅更便于处理而且综合利用了三个模态的情感信息,更加利于情感类别的准确判定。（3）三模态情感识别。在得到融合后的情感特征后,利用训练样本的情感特征对基于支持向量机的分类器进行训练得到一个预测模型,并利用训练得到的预测模型对测试样本进行情感类别标签的判定。仿真实验表明,在利用多模态情感信息的情况下有着较单模态更好的识别率,达到了50.43\%,证明了DMCCA算法的有效性。},
  editora = {卢, 官明},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Feature dimension reduction,Feature fusion,Multimodal emotion recognition,Support vector machine,多模态情感识别,支持向量机,特征融合,特征降维},
  annotation = {10 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\郭_2017_基于语音、表情与姿态的多模态情感识别算法实现.pdf}
}

@thesis{GuoMianXiangShuoHuaRenQueRenDeLuYinHuiFangGongJiJianCe2020,
  type = {硕士},
  title = {面向说话人确认的录音回放攻击检测},
  author = {郭, 星辰},
  date = {2020},
  institution = {{苏州大学}},
  doi = {10.27351/d.cnki.gszhu.2020.000668},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020153058.nh&uniplatform=NZKPT&v=I1E33Ir8%25mmd2B%25mmd2B4E%25mmd2Bpo0Fnb15tsLXezPW6HfGT8z9RooEVX6nQXKRFRIcJR79p%25mmd2Bzds57},
  urldate = {2021-09-10},
  abstract = {说话人确认系统作为生物识别领域的研究热点近几十年来得到了迅速的发展。因其便捷、有效和低成本的特点,已经被广泛应用于需要身份认证的领域。但由于录音回放攻击语音的存在,说话人确认系统在走向实际应用的过程中,其安全性问题开始受到越来越多的关注。从应用安全的角度出发,提高说话人确认系统的安全性,抵御录音回放攻击语音成为亟须解决的问题。为有效抵御录音回放攻击语音,本文通过输入信道频率响应差异分析,提出了两种适用于录音回放攻击检测的特征提取方法。其一为信道频响差强化倒谱系数（Channel frequency response Difference Enhancement Cepstral Coefficient,CDECC）特征,该特征参数通过三阶多项式非线性频率尺度变换强化信道频率响应差异。其二为小波包熵密度（Wavelet Packet Entropy Density,WPED）特征,该特征参数通过小波包变换（Wavelet Packet Transform,WPT）和特定小波包节点的熵密度值计算强化输入信道频率响应差异,增大两类语音在特征空间中的距离。最后,将采用CDECC特征和WPED特征的录音回放攻击检测模块以串联方式嵌入说话人确认系统中,实现了具有录音回放攻击检测能力的说话人确认系统。基于ASVSpoof 2.0语料库的非特定说话人文本无关录音回放攻击检测实验表明,采用CDECC特征和WPED特征的录音回放攻击检测等错误率（Equal Error Rate,EER）分别为25.03\%和23.22\%,相比基线系统的EER相对下降了 10\%和16.50\%。采用CDECC特征和WPED特征作为检测特征的说话人确认系统的错误接受率（False Accept Rate,FAR）明显下降,EER 分别从 4.80\%下降为 1.07\%和 1.03\%。},
  editora = {俞, 一彪},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,CDECC,replay attack detection,speaker verification,WPED,录音回放攻击检测,说话人确认},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\郭_2020_面向说话人确认的录音回放攻击检测.pdf}
}

@inproceedings{guoRepresentationLearningSpectroTemporalChannel2021,
  title = {Representation {{Learning}} with {{Spectro-Temporal-Channel Attention}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Guo, Lili and Wang, Longbiao and Xu, Chenglin and Dang, Jianwu and Chng, Eng Siong and Li, Haizhou},
  date = {2021-06},
  pages = {6304--6308},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414006},
  abstract = {Convolutional neural network (CNN) is found to be effective in learning representation for speech emotion recognition. CNNs do not explicitly model the associations or relative importance of features in the spectral/temporal/channel-wise axes. In this paper, we propose an attention module, named spectro-temporal-channel (STC) attention module that is integrated with CNN to improve representation learning ability. Our module infers an attention map along the three dimensions, namely time, frequency, and CNN channel. Experiments are conducted on the IEMOCAP database to evaluate the effectiveness of the proposed representation learning method. The results demonstrate that the proposed method outperforms the traditional CNN method by an absolute increase of 3.13\% in terms of F1 score.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {channel attention,Conferences,Databases,Emotion recognition,Learning systems,representation learning,Signal processing,spectro-temporal attention,speech emotion recognition,Speech recognition,Time-frequency analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Guo et al_2021_Representation Learning with Spectro-Temporal-Channel Attention for Speech.pdf}
}

@inproceedings{guptaEstimationSpeakerAge2022,
  title = {Estimation of Speaker Age and Height from Speech Signal Using Bi-Encoder Transformer Mixture Model},
  booktitle = {Interspeech 2022},
  author = {Gupta, Tarun and Truong, Tuan Duc and Anh, Tran The and Chng, Eng Siong},
  date = {2022-09-18},
  pages = {1978--1982},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-567},
  url = {https://www.isca-speech.org/archive/interspeech_2022/gupta22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gupta et al_2022_Estimation of speaker age and height from speech signal using bi-encoder.pdf}
}

@article{gurunathshivakumarEndtoendNeuralSystems2022,
  title = {End-to-End Neural Systems for Automatic Children Speech Recognition: {{An}} Empirical Study},
  shorttitle = {End-to-End Neural Systems for Automatic Children Speech Recognition},
  author = {Gurunath Shivakumar, Prashanth and Narayanan, Shrikanth},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101289},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101289},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000905},
  urldate = {2021-09-28},
  abstract = {A key desiderata for inclusive and accessible speech recognition technology is ensuring its robust performance to children's speech. Notably, this includes the rapidly advancing neural network based end-to-end speech recognition systems. Children speech recognition is more challenging due to the larger intra-inter speaker variability in terms of acoustic and linguistic characteristics compared to adult speech. Furthermore, the lack of adequate and appropriate children speech resources adds to the challenge of designing robust end-to-end neural architectures. This study provides a critical assessment of automatic children speech recognition through an empirical study of contemporary state-of-the-art end-to-end speech recognition systems. Insights are provided on the aspects of training data requirements, adaptation on children data, and the effect of children age, utterance lengths, different architectures and loss functions for end-to-end systems and role of language models on the speech recognition performance.},
  language = {en},
  keywords = {\#nosource,Children speech recognition,End-to-end speech recognition,Residual network,Time depth separable convolutional network,Transformer}
}

@article{hamedInvestigationsSpeechRecognition2022,
  title = {Investigations on Speech Recognition Systems for Low-Resource Dialectal {{Arabic}}\textendash{{English}} Code-Switching Speech},
  author = {Hamed, Injy and Denisov, Pavel and Li, Chia-Yu and Elmahdy, Mohamed and Abdennadher, Slim and Vu, Ngoc Thang},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101278},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101278},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000814},
  urldate = {2021-09-28},
  abstract = {Code-switching (CS), defined as the mixing of languages in conversations, has become a worldwide phenomenon. The prevalence of CS has been recently met with a growing demand and interest to build CS automatic speech recognition (ASR) systems. In this paper, we present our work on code-switched Egyptian Arabic\textendash English ASR. We first contribute in filling the huge gap in resources by collecting, analyzing and publishing our spontaneous CS Egyptian Arabic\textendash English speech corpus. We build our ASR systems using DNN-based hybrid and Transformer-based end-to-end models. In this paper, we present a thorough comparison between both approaches under the setting of a low-resource, orthographically unstandardized, and morphologically rich language pair. We show that while both systems achieve comparable overall recognition results, the systems have complementary strengths. We show that recognition can be improved by combining the outputs of the two systems. We propose several effective system combination approaches, where hypotheses of both systems are merged on sentence- and word-levels. Our approaches result in overall WER relative improvement of 4.7\%, over a baseline performance of 32.1\% WER. In the case of intra-sentential CS sentences, we achieve WER relative improvement of 4.8\%. Our best performing system achieves 30.6\% WER on ArzEn test set.},
  language = {en},
  keywords = {_Waiting for read,Arabic–English,Code-switching,DNN-based ASR,End-to-end ASR,Low-resource,Speech corpus,Speech recognition,System combination},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hamed et al_2022_Investigations on speech recognition systems for low-resource dialectal.pdf}
}

@unpublished{hanCoteachingRobustTraining2018,
  title = {Co-Teaching: {{Robust Training}} of {{Deep Neural Networks}} with {{Extremely Noisy Labels}}},
  shorttitle = {Co-Teaching},
  author = {Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
  date = {2018-10-30},
  eprint = {1804.06872},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.06872},
  urldate = {2022-05-07},
  abstract = {Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called Co-teaching for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {_readed,⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Han et al_2018_Co-teaching.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\AQULTHKY\\1804.html}
}

@article{hanEmoBedStrengtheningMonomodal2021,
  title = {{{EmoBed}}: {{Strengthening Monomodal Emotion Recognition}} via {{Training}} with {{Crossmodal Emotion Embeddings}}},
  shorttitle = {{{EmoBed}}},
  author = {Han, Jing and Zhang, Zixing and Ren, Zhao and Schuller, Bj\"orn},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {553--564},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2928297},
  abstract = {Despite remarkable advances in emotion recognition, they are severely restrained from either the essentially limited property of the employed single modality, or the synchronous presence of all involved multiple modalities. Motivated by this, we propose a novel crossmodal emotion embedding framework called EmoBed, which aims to leverage the knowledge from other auxiliary modalities to improve the performance of an emotion recognition system at hand. The framework generally includes two main learning components, i.e., joint multimodal training and crossmodal training. Both of them tend to explore the underlying semantic emotion information but with a shared recognition network or with a shared emotion embedding space, respectively. In doing this, the enhanced system trained with this approach can efficiently make use of the complementary information from other modalities. Nevertheless, the presence of these auxiliary modalities is not demanded during inference. To empirically investigate the effectiveness and robustness of the proposed framework, we perform extensive experiments on the two benchmark databases RECOLA and OMG-Emotion for the tasks of dimensional emotion regression and categorical emotion classification, respectively. The obtained results show that the proposed framework significantly outperforms related baselines in monomodal inference, and are also competitive or superior to the recently reported systems, which emphasises the importance of the proposed crossmodal learning for emotion recognition.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_reading,Crossmodal learning,emotion embedding,emotion recognition,Emotion recognition,joint training,Knowledge engineering,Robustness,Speech recognition,Task analysis,Training,triplet loss,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Han et al_2021_EmoBed.pdf}
}

@thesis{HaoTongJiCanShuQingGanYuYinHeChengDeYanJiu2016,
  type = {硕士},
  title = {统计参数情感语音合成的研究},
  author = {郝, 东亮},
  date = {2016},
  institution = {{西北师范大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201701&filename=1016240502.nh&uniplatform=NZKPT&v=8pf94Bhe2H2S3LD0WuyNyHDiIyNzmTKQEI1fDf3T6He2R5dF%25mmd2ByZmhnuCTUCTMqvL},
  urldate = {2021-09-10},
  abstract = {随着语音合成技术的研究与发展,合成语音音质得到较大提升,但当前语音合成技术的研究仍以中性化语音为主,对情感语音合成的研究较少。人类生活对智能语音的需求不仅要涵盖基本的文字内容,还要承载丰富的情感信息,情感语音合成的研究将是智能语音研究领域的必然趋势。本文建立了一个多说话人的多种情感的情感语音语料库,针对汉语统计参数语音合成中的上下文相关标注生成,设计了一套包含6层上下文信息的标注格式,在此基础上,采用多说话人的情感语音数据和统计参数语音合成方法,利用说话人自适应训练算法训练了情感语音的声学模型,实现了情感语音的合成。论文的主要工作和创新如下:1.建立了一个多说话人的多种情感的语料库。在专业录音棚中,采用诱发方式激发录音人的情感,并进行录音。录制了7个男性说话人和7个女性说话人的11种典型情感的情感语音数据,并以Microsoft WAV格式（单通道、16bit、16kHz采样频率）进行保存。2.实现了一种面向普通话统计参数语音合成的标注生成算法。针对汉语统计参数语音合成中上下文相关标注的生成,设计了一套包含6层上下文相关信息的标注格式。以声韵母做为语音合成的合成基元,利用基于隐Markov模型（Hidden Markov Model,HMM）的统计参数语音合成方法,通过对合成语音音质的主、客观评测,验证了不同上下文信息对合成语音音质的影响。实验结果表明,本文设计的上下文相关的6层标注格式能够满足情感语音合成的需求。3.提出了一种利用多个说话人的多种情感训练语料,利用统计参数语音合成方法实现情感语音合成的方法。首先利用多个说话人的情感语音语料,通过说话人自适应训练（Speaker Adaptation Training,SAT）得到多个说话人情感语音的平均音模型,然后利用目标说话人的目标情感的训练语料,经过说话人自适应变换,得到目标说话人目标情感的声学模型,进而合成出目标说话人的目标情感语音。实验结果表明,本方法合成得到的情感语音具有较高的自然度和情感相似度。},
  editora = {杨, 鸿武},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,context-dependent information,emotional corpus,emotional speech synthesis,speaker adaptive training,statistical parametric speech synthesis,tagging format,上下文相关信息,情感语料库,标注格式,统计参数语音合成,说话人自适应训练},
  annotation = {4 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\郝_2016_统计参数情感语音合成的研究.pdf}
}

@article{haqueGuidedGenerativeAdversarial2021,
  title = {Guided {{Generative Adversarial Neural Network}} for {{Representation Learning}} and {{Audio Generation Using Fewer Labelled Audio Data}}},
  author = {Haque, Kazi Nazmul and Rana, Rajib and Liu, Jiajun and Hansen, John H. L. and Cummins, Nicholas and Busso, Carlos and Schuller, Bj\"orn W.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2575--2590},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3098764},
  abstract = {The Generation power of Generative Adversarial Neural Networks (GANs) has shown great promise to learn representations from unlabelled data while guided by a small amount of labelled data. We aim to utilise the generation power of GANs to learn Audio Representations. Most existing studies are, however, focused on images. Some studies use GANs for speech generation, but they are conditioned on text or acoustic features, limiting their use for other audio, such as instruments, and even for speech where transcripts are limited. This paper proposes a novel GAN-based model that we named Guided Generative Adversarial Neural Network (GGAN), which can learn powerful representations and generate good-quality samples using a small amount of labelled data as guidance. Experimental results based on a speech [Speech Command Dataset (S09)] and a non-speech [Musical Instrument Sound dataset (Nsyth)] dataset demonstrate that using only 5\% of labelled data as guidance, GGAN learns significantly better representations than the state-of-the-art models.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {and Generative Adversarial Neural Network,Audio Generation,Data models,Disentangled Representation Learning,Generative adversarial networks,Generators,Guided Representation Learning,Spectrogram,Speech processing,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Haque et al_2021_Guided Generative Adversarial Neural Network for Representation Learning and.pdf}
}

@unpublished{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2021-12-01},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {_reading,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\He et al_2015_Deep Residual Learning for Image Recognition.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\F2NLYINJ\\1512.html}
}

@article{heIdentityMappingsDeep2016,
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-03-16},
  url = {https://arxiv.org/abs/1603.05027v3},
  urldate = {2021-12-01},
  abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
  language = {en},
  keywords = {_reading,⛔ No DOI found},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\He et al_2016_Identity Mappings in Deep Residual Networks.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\QDHT8SX5\\1603.html}
}

@inproceedings{heImprovedStarGANEmotional2021,
  title = {An {{Improved StarGAN}} for {{Emotional Voice Conversion}}: {{Enhancing Voice Quality}} and {{Data Augmentation}}},
  shorttitle = {An {{Improved StarGAN}} for {{Emotional Voice Conversion}}},
  booktitle = {Interspeech 2021},
  author = {He, Xiangheng and Chen, Junjie and Rizos, Georgios and Schuller, Bj\"orn W.},
  date = {2021-08-30},
  pages = {821--825},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1253},
  url = {https://www.isca-speech.org/archive/interspeech_2021/he21b_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\He et al_2021_An Improved StarGAN for Emotional Voice Conversion.pdf}
}

@thesis{HeJiYuYuYinHeTuXiangDeDuoMoTaiQingGanShiBieYanJiu2017,
  type = {硕士},
  title = {基于语音和图像的多模态情感识别研究},
  author = {贺, 奇},
  date = {2017},
  institution = {{哈尔滨工业大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1017739526.nh&uniplatform=NZKPT&v=D2Cs%25mmd2FXwK2pJpX24DGA7g%25mmd2Fqdp2oLbpYRu2lVxfQ%25mmd2BIhdRsl4Cr7%25mmd2BP4gQwIXqM%25mmd2F5u8d},
  urldate = {2021-09-10},
  abstract = {随着人工智能的兴起,获得更加人性化、智能化的人机交互体验一直备受关注,这使得情感计算成为研究热点之一。作为情感计算研究领域的一个重要分支,情感识别近年来发展迅速,前景广阔。情感识别研究主要的方法有基于语音的情感识别研究、基于图像的情感识别研究和基于多模态融合的情感识别研究。由于单一的语音或图像模态信息所表达的情感信息是不完整的,不能完全满足人们的期望。而多模态融合的情感识别研究综合了各个模态信息,使各模态信息之间能够互补从而达到更好的识别效果。因此本文选择基于语音和图像的多模态情感识别研究。本文选择包含语音和人脸图像两种模态情感材料的英国萨里大学的Surrey Audio-Visual Expressed Emotion（SAVEE）Database作为标准源数据,进行七种情感（生气、厌恶、恐惧、平静、悲伤、惊讶）识别的相关研究,其主要研究内容如下:1)基于语音的情感识别研究。本文提取共92维语音情感特征,这些特征由短时能量、语音持续时间、基音频率、前三共振峰、梅尔频率倒谱系数（Mel-scale Frequency Cepstral Coeddicients,MFCC）的相关统计学参数组成。所有样本特征提取完成之后,在支持向量机（Support Vector Machine,SVM）上进行情感识别实验,得到了较好的分类结果。2)基于人脸图像的情感识别研究。本文分别提取语音段峰值图像的局部二值模式（Local Binary Pattern,LBP）以及序列图像脸部特征点的均值和标准差作为图像情感特征。在所有样本特征提取完成之后,通过SVM进行情感识别实验,并对在不同特征上得到的情感识别结果进行对比。最终基于序列图像脸部特征点特征提取方法取得的识别结果好于基于语音段峰值图像LBP特征提取方法。3)基于语音和图像的多模态融合情感识别研究。本文分别采用特征层融合和决策层融合策略对语音模态信息和图像模态信息进行融合,并在SVM上进行情感识别实验,将其得到的识别结果与单一模态情感识别结果进行对比,并比较特征层融合策略得到的识别结果与决策层融合策略得到的识别结果,验证了基于语音和图像的多模态情感识别比单一模态情感识别表现更佳,且决策层融合效果好于特征层融合,实验表明了决策层融合有助于提高恐惧类情感的识别率。},
  editora = {丁, 明理},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,decision-making layer fusion,Emotion recognition,facial expression feature,feature layer fusion,speech feature,Support Vector Machine(SVM),人脸图像特征,决策层融合,情感识别,支持向量机,特征层融合,语音特征},
  annotation = {5 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\贺_2017_基于语音和图像的多模态情感识别研究.pdf}
}

@inproceedings{heJointTemporalConvolutional2022,
  title = {Joint {{Temporal Convolutional Networks}} and {{Adversarial Discriminative Domain Adaptation}} for {{EEG-Based Cross-Subject Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {He, Zhipeng and Zhong, Yongshi and Pan, Jiahui},
  date = {2022},
  pages = {3214--3218},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746600},
  abstract = {Cross-subject emotion recognition is one of the most challenging tasks in electroencephalogram (EEG)-based emotion recognition. To guarantee the constancy of feature representations across domains and to eliminate differences between domains, we explored the feasibility of combining temporal convolutional networks (TCNs) and adversarial discriminative domain adaptation (ADDA) algorithms in solving the problem of domain shift in EEG-based cross-subject emotion recognition. In light of EEG signals that have specific temporal properties, we chose the temporal model TCN as the feature encoder. To verify the validity of the proposed method, we conducted experiments on two public datasets: DEAP and DREAMER. The experimental results show that for the leave-one-subject-out evaluation, average accuracies of 64.33\% (valence) and 63.25\% (arousal) were obtained on the DEAP dataset, and average accuracies of 66.56\% (valence) and 63.69\% (arousal) were achieved on the DREAMER dataset. Extensive experiments demonstrate that our method for EEG-based cross-subject emotion recognition is effective.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Adaptation models,Adversarial discriminative domain adaptation (ADDA),Brain modeling,Convolution,EEG,Electroencephalography,Emotion recognition,Signal processing algorithms,Speech recognition,Temporal convolutional network (TCN)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\He et al_2022_Joint Temporal Convolutional Networks and Adversarial Discriminative Domain.pdf}
}

@article{hernandez-castanedaLanguageindependentExtractiveAutomatic2022,
  title = {Language-Independent Extractive Automatic Text Summarization Based on Automatic Keyword Extraction},
  author = {Hern\'andez-Casta\~neda, \'Angel and Garc\'ia-Hern\'andez, Ren\'e Arnulfo and Ledeneva, Yulia and Mill\'an-Hern\'andez, Christian Eduardo},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101267},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101267},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000723},
  urldate = {2021-09-28},
  abstract = {This study proposes a language and domain independent approach for automatic extractive text summarization (EATS) tasks, which is based on a clustering scheme supported by a genetic algorithm (GA), to find an optimal grouping of sentences. Furthermore, our approach includes a topic modeling algorithm to find the key sentences in clusters based on automatically generated keywords. Our experimental results show that our system outperforms previous methods through the application of two general steps: clustering, which helps to increase coverage, and the addition of semantic information to the model, which facilitates the detection of the key sentences in the clusters and improves precision.},
  language = {en},
  keywords = {Automatic summarization,Extractive summaries,Genetic algorithm,Keywords,Topic modeling},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hernández-Castañeda et al_2022_Language-independent extractive automatic text summarization based on automatic.pdf}
}

@article{hoggOverlappingSpeakerSegmentation2021,
  title = {Overlapping {{Speaker Segmentation Using Multiple Hypothesis Tracking}} of {{Fundamental Frequency}}},
  author = {Hogg, Aidan O. T. and Evers, Christine and Moore, Alastair H. and Naylor, Patrick A.},
  date = {2021-01-01},
  pages = {1479--1490},
  url = {http://ieeexplore.ieee.org/document/9381673},
  abstract = {This paper demonstrates how the harmonic structure of voiced speech can be exploited to segment multiple overlapping speakers in a speaker diarization task. We explore how a change in the speaker can be inferred from a change in pitch. We show that voiced harmonics can be useful in detecting when more than one speaker is talking, such as during overlapping speaker activity. A novel system is proposed to track multiple harmonics simultaneously, allowing for the determination of onsets and end-points of a speaker's utterance in the presence of an additional active speaker. This system is bench-marked against a segmentation system from the literature that employs a bidirectional long short term memory network (BLSTM) approach and requires training. Experimental results highlight that the proposed approach outperforms the BLSTM baseline approach by 12.9\% in terms of HIT rate for speaker segmentation. We also show that the estimated pitch tracks of our system can be used as features to the BLSTM to achieve further improvements of 1.21\% in terms of coverage and 2.45\% in terms of purity.},
  language = {en},
  keywords = {Harmonic analysis,Hidden Markov models,Kalman filter,Kalman filters,Microphones,pitch tracking,Reliability,speaker seg-mentation,Speech processing,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hogg et al_2021_Overlapping Speaker Segmentation Using Multiple Hypothesis Tracking of.pdf}
}

@inproceedings{hossainFindingEmotionMultilingual2020,
  title = {Finding {{Emotion}} from {{Multi-lingual Voice Data}}},
  booktitle = {2020 {{IEEE}} 44th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  author = {Hossain, Nazia and Naznin, Mahmuda},
  date = {2020-07},
  pages = {408--417},
  publisher = {{IEEE}},
  location = {{Madrid, Spain}},
  doi = {10.1109/COMPSAC48688.2020.0-214},
  url = {https://ieeexplore.ieee.org/document/9202764/},
  urldate = {2021-09-10},
  abstract = {Human-Machine interaction through audio or speech is always challenging because of the difficulties in emotion detection from audio data. Emotion is a natural way to express the individual's mental state. When a person is in an unusual mental state, the voice changes accordingly in his or her subconsciousness. Emotion detection from continuous speech with multi-lingual platform is more difficult due to complicated feature extraction and feature modeling process. Voice features change due to physical, background and channel noise or sue to other articulatory obstacles. In spite of all these uncertainties, researchers find that among glottal wave forms (consists of glottal excitation and vocal tract filters), glottal pulses carry basic information of voice because these pulses are out of articulatory effects or inner-mouth changes. In this research, we capture real-time speech signal, separate the glottal pulses and buffer the signal into specified frames to identify the phonemes those are affected mostly in emotionally unstable conditions. In these frames, we identify the basic voice controlling features like pitch, intensity, jitter to get these phonemes. For this scheme, we use a real-life continuous dataset which is a multilingual corpus of emotional speech. After feature extraction from the dataset, different classification methodologies have been applied to predict the emotional state and we find our prediction model with around 83\% accuracy.},
  eventtitle = {2020 {{IEEE}} 44th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  isbn = {978-1-72817-303-0},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hossain_Naznin_2020_Finding Emotion from Multi-lingual Voice Data.pdf}
}

@inproceedings{houMultiModalEmotionRecognition2022,
  title = {Multi-{{Modal Emotion Recognition}} with {{Self-Guided Modality Calibration}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hou, Mixiao and Zhang, Zheng and Lu, Guangming},
  date = {2022},
  pages = {4688--4692},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747859},
  abstract = {Multi-modal emotion recognition aims to extract sentiment-related information from multiple sources and integrate different modal representations for sentiment analysis. Alignment is an effective strategy to achieve semantically consistent representations for multi-modal emotion recognition, while the current alignment models are jointly unable to maintain the dependence of word-to-sentence and independence of unimodal learning. In this paper, we propose a Self-guided Modality Calibration Network (SMCN) to realize multi-modal alignment which can capture the global connections without interfering with unimodal learning. While preserving unimodal learning without interference, our model leverages semantic sentiment-related features to guide modality-specific representation learning. On one hand, SMCN simulates human thinking by deriving a branch for acquiring knowledge of other modalities in unimodal learning. This branch aims to lean high-level semantic information of other modalities for realizing semantic alignment between modalities. On the other hand, we also provide an indirect interaction manner to integrate unimodal feature and calibrate features in different levels for avoiding unimodal features mixed with other clues. Experiments demonstrate that our approach outperforms the state-of-the-art methods on both IEMOCAP and MELD datasets.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_wreading,Emotion recognition,Feature Calibration,Feature Fusion,Indirect Interaction,Interference,Multi-modal Emotion Recognition,Representation learning,Semantics,Sentiment analysis,Signal processing,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hou et al_2022_Multi-Modal Emotion Recognition with Self-Guided Modality Calibration2.pdf}
}

@article{hsuSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition Considering Nonverbal Vocalization}} in {{Affective Conversations}}},
  author = {Hsu, Jia-Hao and Su, Ming-Hsiang and Wu, Chung-Hsien and Chen, Yi-Hsuan},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1675--1686},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3076364},
  abstract = {In real-life communication, nonverbal vocalization such as laughter, cries or other emotion interjections, within an utterance play an important role for emotion expression. In previous studies, only few emotion recognition systems consider nonverbal vocalization, which naturally exists in our daily conversation. In this work, both verbal and nonverbal sounds within an utterance are considered for emotion recognition of real-life affective conversations. Firstly, a support vector machine (SVM)-based verbal and nonverbal sound detector is developed. A prosodic phrase auto-tagger is further employed to extract the verbal/nonverbal sound segments. For each segment, the emotion and sound feature embeddings are respectively extracted using the deep residual networks (ResNets). Finally, a sequence of the extracted feature embeddings for the entire dialog turn are fed to an attentive long short-term memory (LSTM)-based sequence-to-sequence model to output an emotional sequence as recognition result. The NNIME corpus (The NTHU-NTUA Chinese interactive multimodal emotion corpus), which consists of verbal and nonverbal sounds, was adopted for system training and testing. 4766 single speaker dialogue turns in the audio data of the NNIME corpus were selected for evaluation. The experimental results showed that nonverbal vocalization was helpful for speech emotion recognition. For comparison, the proposed method based on decision-level fusion achieved an accuracy of 61.92\% for speech emotion recognition outperforming the traditional methods as well as the feature-level and model-level fusion approaches.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_reading,Artificial neural networks,convolutional neural network,Databases,Emotion recognition,Feature extraction,long-short term memory,nonverbal vocalization,prosodic phrase,sequence-to-sequence model,Speech emotion recognition,Speech processing,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hsu et al_2021_Speech Emotion Recognition Considering Nonverbal Vocalization in Affective.pdf}
}

@inproceedings{huangADFFAttentionBased2022,
  title = {{{ADFF}}: {{Attention Based Deep Feature Fusion Approach}} for {{Music Emotion Recognition}}},
  shorttitle = {{{ADFF}}},
  booktitle = {Interspeech 2022},
  author = {Huang, Zi and Ji, Shulei and Hu, Zhilan and Cai, Chuangjian and Luo, Jing and Yang, Xinyu},
  date = {2022-09-18},
  pages = {4152--4156},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-726},
  url = {https://www.isca-speech.org/archive/interspeech_2022/huang22d_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Huang et al_2022_ADFF.pdf}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  date = {2017-07},
  pages = {2261--2269},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.243},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  language = {en},
  keywords = {_reading,Convolution,Convolutional codes,Network architecture,Neural networks,Road transportation,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Huang et al_2017_Densely Connected Convolutional Networks.pdf}
}

@thesis{HuangJiYuYaSuoGanZhiDeYuYinQingGanShiBieJiShuDeYanJiu2017,
  type = {硕士},
  title = {基于压缩感知的语音情感识别技术的研究},
  author = {黄, 玉洁},
  date = {2017},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201802&filename=1018122203.nh&uniplatform=NZKPT&v=dmslavBes9zEy9ocyqbv981yQKkl4FvUj8UdMzQoaqDC8u0ivzyw6B3TKYQF5g2i},
  urldate = {2021-09-10},
  abstract = {随着信息技术和网络通信的快速发展,人们已经不仅仅局限于对信号的准确获取上,更期望对信号进行深度的挖掘,这是信号领域的一场革命。语音作为人类交流和人机交互最重要的一种工具,语音信号的识别和情感分析在互联网、通信、人工智能等等高科技领域中变的日益重要,因此专家们在语音信号识别和情感分析方面进行了大量的工作,在这些研究方向中,语音内容的识别日益趋于成熟且已经商用。虽然近些年针对语音情感识别的研究,相当多的研究成果得以发表,取得了很大的发展,但是语音情感识别研究仍处于初级阶段,一直没有形成一套被广泛认可的、系统的理论和研究方法。此外,实际生活中大多语音信号都处于噪声环境中,而传统的语音情感识别算法大多针对于纯净语音,因此寻找一种具有较好鲁棒性的语音情感识别技术迫在眉睫。压缩感知是近些年提出的一种新型采样技术,且基于压缩感知提出的稀疏表征识别算法在图像和语音识别领域展现出优异的识别性能。语音情感信号在小波等变换域中具有较好的稀疏性,因此将压缩感知技术应用于语音情感识别中,具有较强的理论基础。此外,压缩感知重构算法对信号中的噪声有一定的抑制作用,而针对噪声环境下语音情感识别的研究较少。基于上述分析,在现有研究的基础上,我们将压缩感知技术与语音情感识别相结合,使语音情感识别系统性能的进一步提升成为可能,具有重大的理论和实际应用价值。本文开展了针对纯净和噪声环境下,基于压缩感知的语音情感识别研究,具体工作内容如下:1)通过深入研究语音情感识别系统理论和压缩感知算法,将压缩感知应用于语音情感识别领域,研究了基于压缩感知的语音情感识别算法。仿真实验表明:与经典的GMM语音情感识别算法相比,基于压缩感知的语音情感识别系统在低信噪比时可以取得更好的识别结果。实验结果验证了将压缩感知应用于语音情感识别这一思路的正确性,拓宽了语音情感识别算法思路。2)为进一步提升纯净及噪声环境下语音情感识别系统的识别性能,本文研究了一种压缩感知抑噪语音情感识别系统。首先采用压缩感知技术对带噪语音进行稀疏重构,进而提取稀疏重构后信号的声学特征,最后将特征输入到传统的GMM分类器中。仿真实验表明:与不采用压缩感知语音抑噪的情感识别系统相比,本文提出的方法能够取得较大的识别增益,识别率提升了 5到32个百分点。3)为进一步提升语音情感识别系统的性能,本文针对压缩感知稀疏解存在的缺陷,提出了一种基于稀疏贝叶斯学习的语音情感识别算法。首先将语音信号提取参数特征,经稀疏贝叶斯学习进行参数向量重构,然后使用重构的参数向量与情感码本计算重构距离,进行情感分类。仿真实验表明:稀疏贝叶斯学习算法可以更加接近l0-范数的解,与FOCUSS、BP相比具有较小的重构误差。实验证明将稀疏贝叶斯学习算法应用于语音情感识别系统,提升了识别系统的性能,显示了该算法在语音情感识别领域的应用潜力。在本文最后部分对论文所有的研究和成果做了总结,并对日后的研究工作进行展望。},
  editora = {郭, 学雷},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Compression Sensing,Sparse Bayesian Learning,Sparse Representation,Speech Emotion Recognition,Speech Enhancement,压缩感知,稀疏表示,稀疏贝叶斯学习,语音增强,语音情感识别},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\黄_2017_基于压缩感知的语音情感识别技术的研究.pdf}
}

@article{huangPretrainingTechniquesSequencetoSequence2021,
  title = {Pretraining {{Techniques}} for {{Sequence-to-Sequence Voice Conversion}}},
  author = {Huang, Wen-Chin and Hayashi, Tomoki and Wu, Yi-Chiao and Kameoka, Hirokazu and Toda, Tomoki},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {745--755},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3049336},
  abstract = {Sequence-to-sequence (seq2seq) voice conversion (VC) models are attractive owing to their ability to convert prosody. Nonetheless, without sufficient data, seq2seq VC models can suffer from unstable training and mispronunciation problems in the converted speech, thus far from practical. To tackle these shortcomings, we propose to transfer knowledge from other speech processing tasks where large-scale corpora are easily available, typically text-to-speech (TTS) and automatic speech recognition (ASR). We argue that VC models initialized with such pretrained ASR or TTS model parameters can generate effective hidden representations for high-fidelity, highly intelligible converted speech. In this work, we examine our proposed method in a parallel, one-to-one setting. We employed recurrent neural network (RNN)-based and Transformer based models, and through systematical experiments, we demonstrate the effectiveness of the pretraining scheme and the superiority of Transformer based models over RNN-based models in terms of intelligibility, naturalness, and similarity.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Data models,Decoding,pretraining,sequence-to-sequence,Spectrogram,Speech processing,Task analysis,Training,Training data,transformer,Voice conversion},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Huang et al_2021_Pretraining Techniques for Sequence-to-Sequence Voice Conversion.pdf}
}

@article{huBayesianLearningLFMMI2021,
  title = {Bayesian {{Learning}} of {{LF-MMI Trained Time Delay Neural Networks}} for {{Speech Recognition}}},
  author = {Hu, Shoukang and Xie, Xurong and Liu, Shansong and Yu, Jianwei and Ye, Zi and Geng, Mengzhe and Liu, Xunying and Meng, Helen},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1514--1529},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3069080},
  abstract = {Discriminative training techniques define state-of-the-art performance for automatic speech recognition systems. However, they are inherently prone to overfitting, leading to poor generalization performance when using limited training data. In order to address this issue, this paper presents a full Bayesian framework to account for model uncertainty in sequence discriminative training of factored TDNN acoustic models. Several Bayesian learning based TDNN variant systems are proposed to model the uncertainty over weight parameters and choices of hidden activation functions, or the hidden layer outputs. Efficient variational inference approaches using as few as one single parameter sample ensure their computational cost in both training and evaluation time comparable to that of the baseline TDNN systems. Statistically significant word error rate (WER) reductions of 0.4\%\textendash 1.8\% absolute (5\%-11\% relative) were obtained over a state-of-the-art 900 h speed perturbed Switchboard corpus trained baseline LF-MMI factored TDNN system using multiple regularization methods including F-smoothing, L2 norm penalty, natural gradient, model averaging and dropout, in addition to i-Vector plus learning hidden unit contribution (LHUC) based speaker adaptation and RNNLM rescoring. The efficacy of the proposed Bayesian techniques is further demonstrated in a comparison against the state-of-the-art performance obtained on the same task using the most recent hybrid and end-to-end systems reported in the literature. Consistent performance improvements were also obtained on a 450-h HKUST conversational Mandarin telephone speech recognition task. On a third cross domain adaptation task requiring rapidly porting a 1000-h LibriSpeech data trained system to a small DementiaBank elderly speech corpus, the proposed Bayesian TDNN LF-MMI systems outperformed the baseline system using direct weight fine-tuning by up to 2.5\% absolute WER reduction.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Bayes methods,Bayesian learning,domain adaptation,gaussian process,Gaussian processes,Hidden Markov models,LF-MMI,Neural networks,Speech recognition,Training,Uncertainty,variational inference},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hu et al_2021_Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for Speech.pdf}
}

@inproceedings{huGraphIsomorphismNetwork2022,
  title = {A {{Graph Isomorphism Network}} with {{Weighted Multiple Aggregators}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Hu, Ying and Tang, Yuwu and Huang, Hao and He, Liang},
  date = {2022-09-18},
  pages = {4705--4709},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-637},
  url = {https://www.isca-speech.org/archive/interspeech_2022/hu22c_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hu et al_2022_A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech.pdf}
}

@article{huiDifferencesSpeechIntelligibility2021,
  title = {Differences in Speech Intelligibility in Noise between Native and Non-Native Listeners under Ambisonics-Based Sound Reproduction System},
  author = {Hui, C. T. Justine and Au, Eugena and Xiao, Shirley and Hioka, Yusuke and Masuda, Hinako and Watson, Catherine I.},
  date = {2021-12-15},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {184},
  pages = {108368},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2021.108368},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X2100462X},
  urldate = {2021-09-28},
  abstract = {The current paper examines how native and non-native listeners of New Zealand English differ in terms of speech intelligibility in noise in a number of room acoustics reproduced by a first-order Ambisonics-based sound reproduction system. Speech intelligibility test was conducted under three room acoustics environments (living room, lecture theatre and church) using the sound reproduction system, where a pink noise masker was played from one of five azimuthal angles (0, 45, 90, 135, 180\textdegree ) while the target speech was always played from 0\textdegree. We found significant two-way interactions between language nativeness and speech-noise separation, language nativeness and room acoustics, as well as between room acoustics and speech-noise separation. This suggests that native and non-native listeners respond differently to the virtually reproduced acoustic environments and they benefit from spatial release from masking in a different manner. Post-hoc results showed the native listeners performing significantly better than their non-native counterparts for all the angles of speech-noise separation and the room acoustics.},
  language = {en},
  keywords = {_Waiting for read,First-order ambisonics,Nativeness,Pink noise,Spatial release from masking,Speech intelligibility},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hui et al_2021_Differences in speech intelligibility in noise between native and non-native.pdf}
}

@inproceedings{huMMDFNMultimodalDynamic2022,
  title = {{{MM-DFN}}: {{Multimodal Dynamic Fusion Network}} for {{Emotion Recognition}} in {{Conversations}}},
  shorttitle = {{{MM-DFN}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hu, Dou and Hou, Xiaolong and Wei, Lingwei and Jiang, Lianxin and Mo, Yang},
  date = {2022},
  pages = {7037--7041},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747397},
  abstract = {Emotion Recognition in Conversations (ERC) has considerable prospects for developing empathetic machines. For multimodal ERC, it is vital to understand context and fuse modality information in conversations. Recent graph-based fusion methods generally aggregate multimodal information by exploring unimodal and cross-modal interactions in a graph. However, they accumulate redundant information at each layer, limiting the context understanding between modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize emotions by fully understanding multimodal conversational context. Specifically, we design a new graph-based dynamic fusion module to fuse multimodal context features in a conversation. The module reduces redundancy and enhances complementarity between modalities by capturing the dynamics of contextual information in different semantic spaces. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Conferences,dialogue systems,emotion recognition,Emotion recognition,emotion recognition in conversations,Fuses,Limiting,multimodal fusion,Redundancy,Semantics,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hu et al_2022_MM-DFN.pdf}
}

@inproceedings{huMultipleEnhancementsLSTM2022,
  title = {Multiple {{Enhancements}} to {{LSTM}} for {{Learning Emotion-Salient Features}} in {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Hu, Desheng and Hu, Xinhui and Xu, Xinkang},
  date = {2022-09-18},
  pages = {4720--4724},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-985},
  url = {https://www.isca-speech.org/archive/interspeech_2022/hu22e_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hu et al_2022_Multiple Enhancements to LSTM for Learning Emotion-Salient Features in Speech.pdf}
}

@article{husseinArabicSpeechRecognition2022,
  title = {Arabic Speech Recognition by End-to-End, Modular Systems and Human},
  author = {Hussein, Amir and Watanabe, Shinji and Ali, Ahmed},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101272},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101272},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000760},
  urldate = {2021-09-28},
  abstract = {Recent advances in automatic speech recognition (ASR) have achieved accuracy levels comparable to human transcribers, which led researchers to debate if the machine has reached human performance. Previous work focused on the English language and modular hidden Markov model-deep neural network (HMM\textendash DNN) systems. In this paper, we perform a comprehensive benchmarking for end-to-end transformer ASR, modular HMM\textendash DNN ASR, and human speech recognition (HSR) on the Arabic language and its dialects. For the HSR, we evaluate linguist performance and lay-native speaker performance on a new dataset collected as a part of this study. For ASR the end-to-end work led to 12.5\%, 27.5\% , 33.8\% WER; a new performance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our results suggest that human performance in the Arabic language is still considerably better than the machine with an absolute WER gap of 3.5\% on average.},
  language = {en},
  keywords = {_Waiting for read,Dialectal arabic,End-to-end speech recognition,Human speech recognition,Modern standard arabic,Transformer},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hussein et al_2022_Arabic speech recognition by end-to-end, modular systems and human.pdf}
}

@inproceedings{huTNTCTwoStreamNetwork2022,
  title = {{{TNTC}}: {{Two-Stream Network}} with {{Transformer-Based Complementarity}} for {{Gait-Based Emotion Recognition}}},
  shorttitle = {{{TNTC}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hu, Chuanfei and Sheng, Weijie and Dong, Bo and Li, Xinde},
  date = {2022},
  pages = {3229--3233},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746047},
  abstract = {Recognizing the human emotion automatically from visual characteristics plays a vital role in many intelligent applications. Recently, gait-based emotion recognition, especially gait skeletons-based characteristic, has attracted much attention, while many available methods have been proposed gradually. The popular pipeline is to first extract affective features from joint skeletons, and then aggregate the skeleton joint and affective features as the feature vector for classifying the emotion. However, the aggregation procedure of these emerged methods might be rigid, resulting in insufficiently exploiting the complementary relationship between skeleton joint and affective features. Meanwhile, the long range dependencies in both spatial and temporal domains of the gait sequence are scarcely considered. To address these issues, we propose a novel two-stream network with transformer-based complementarity, termed as TNTC. Skeleton joint and affective features are encoded into two individual images as the inputs of two streams, respectively. A new transformer-based complementarity module (TCM) is proposed to bridge the complementarity between two streams hierarchically via capturing long range dependencies. Experimental results demonstrate that TNTC outperforms state-of-the-art methods on the latest dataset in terms of accuracy.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {complementarity,convolutional neural network,Emotion recognition,Gait-based emotion recognition,Pipelines,Signal processing,Speech recognition,Streaming media,transformer,Transformers,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hu et al_2022_TNTC.pdf}
}

@inproceedings{iakubchikEmotionalVoiceReconstruction2021,
  title = {Emotional {{Voice Reconstruction Based}} on {{Intonation Alteration}}},
  booktitle = {2021 {{IEEE Conference}} of {{Russian Young Researchers}} in {{Electrical}} and {{Electronic Engineering}} ({{ElConRus}})},
  author = {Iakubchik, Ian and Iakubchik, Anna and Lipin, Yuri},
  date = {2021-01-26},
  pages = {390--395},
  publisher = {{IEEE}},
  location = {{St. Petersburg, Moscow, Russia}},
  doi = {10.1109/ElConRus51938.2021.9396450},
  url = {https://ieeexplore.ieee.org/document/9396450/},
  urldate = {2021-09-10},
  abstract = {Emotion recognition and voice emotional alteration are major topics in the field of artificial intelligence. Their possible applications include driver's physical state detection, humanmachine interaction, monitoring of students, voice training and robotics. There are many models to express a wide variety of emotions felt or expressed by humans. While there is much work regarding emotion detection, our work will focus on creating emotional voice by altering intonation and rhythm by methods such as overlap-add. We succeeded in detecting an emotional intonation pattern and reconstructing it on a same phrase said by a different voice.},
  eventtitle = {2021 {{IEEE Conference}} of {{Russian Young Researchers}} in {{Electrical}} and {{Electronic Engineering}} ({{ElConRus}})},
  isbn = {978-1-66540-476-1},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Iakubchik et al_2021_Emotional Voice Reconstruction Based on Intonation Alteration.pdf}
}

@article{inoueModelArchitecturesExtrapolate2021,
  title = {Model Architectures to Extrapolate Emotional Expressions in {{DNN-based}} Text-to-Speech},
  author = {Inoue, Katsuki and Hara, Sunao and Abe, Masanobu and Hojo, Nobukatsu and Ijima, Yusuke},
  date = {2021-02},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {126},
  pages = {35--43},
  issn = {01676393},
  doi = {10.1016/j.specom.2020.11.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639320302958},
  urldate = {2021-09-10},
  abstract = {This paper proposes architectures that facilitate the extrapolation of emotional expressions in deep neural network (DNN)-based text-to-speech (TTS). In this study, the meaning of ``extrapolate emotional expressions'' is to borrow emotional expressions from others, and the collection of emotional speech uttered by target speakers is unnecessary. Although a DNN has potential power to construct DNN-based TTS with emotional expressions and some DNN-based TTS systems have demonstrated satisfactory performances in the expression of the diversity of human speech, it is necessary and troublesome to collect emotional speech uttered by target speakers. To solve this issue, we propose architectures to separately train the speaker feature and the emotional feature and to synthesize speech with any combined quality of speakers and emotions. The architectures are parallel model (PM), serial model (SM), auxiliary input model (AIM), and hybrid models (PM\&AIM and SM\&AIM). These models are trained through emotional speech uttered by few speakers and neutral speech uttered by many speakers. Objective evaluations demonstrate that the performances in the open-emotion test provide insufficient information. They make a comparison with those in the closed-emotion test, but each speaker has their own manner of expressing emotion. However, subjective evaluation results indicate that the proposed models could convey emotional information to some extent. Notably, the PM can correctly convey sad and joyful emotions at a rate of {$>$}60\%.},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Inoue et al_2021_Model architectures to extrapolate emotional expressions in DNN-based.pdf}
}

@inproceedings{itoAudioVisualSpeechEmotion2021,
  title = {Audio-{{Visual Speech Emotion Recognition}} by {{Disentangling Emotion}} and {{Identity Attributes}}},
  booktitle = {Interspeech 2021},
  author = {Ito, Koichiro and Fujioka, Takuya and Sun, Qinghua and Nagamatsu, Kenji},
  date = {2021-08-30},
  pages = {4493--4497},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-809},
  url = {https://www.isca-speech.org/archive/interspeech_2021/ito21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ito et al_2021_Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity.pdf}
}

@article{itoJointDiagonalizationBased2021,
  title = {A {{Joint Diagonalization Based Efficient Approach}} to {{Underdetermined Blind Audio Source Separation Using}} the {{Multichannel Wiener Filter}}},
  author = {Ito, Nobutaka and Ikeshita, Rintaro and Sawada, Hiroshi and Nakatani, Tomohiro},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1950--1965},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3079815},
  abstract = {Blind source separation (BSS) of audio signals aims to separate original source signals from their mixtures recorded by microphones. The applications include automatic speech recognition in a noisy/multi-speaker environment, hearing aids, and music analysis. Independent component analysis (ICA) can perform BSS efficiently, but it is basically inapplicable to the underdetermined case\textemdash the number of sources \$\textbackslash boldsymbol{$>\$$} the number of microphones. In contrast, a BSS approach using the multichannel Wiener filter (MWF) is applicable even to the underdetermined case, but conventional methods based on this approach\textemdash including full-rank spatial covariance analysis (FCA)\textemdash are highly inefficient. This is because these methods require massive numbers of matrix inversions to design the MWF. To obtain the best of both worlds, we take a joint diagonalization approach: We restrict spatial covariance matrices of all sources to the class of jointly diagonalizable matrices. This enables the above matrix inversions to be replaced by mere scalar inversions of the diagonal elements of diagonal matrices. Based on this, we present FastFCA and FastMNMF\textemdash efficient methods for underdetermined BSS. In an experiment, FastFCA was several orders of magnitude faster than FCA without sacrificing separation performance. We also present a unified framework for underdetermined and determined BSS, which highlights theoretical connections between various methods including ours. The efficiency of our BSS methods makes them suitable for large data (e.g., data augmentation for machine learning) or limited computational resources encountered in, e.g., hearing aids, distributed microphone arrays, and online BSS.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Blind source separation,Covariance matrices,Gaussian distribution,Indexes,joint diagonalization,maximum likelihood estimation,microphone arrays,multichannel Wiener filter,Source separation,Speech processing,Training data,Wiener filters},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ito et al_2021_A Joint Diagonalization Based Efficient Approach to Underdetermined Blind Audio.pdf}
}

@article{itzhakDesignDifferentialKronecker2021,
  title = {On the {{Design}} of {{Differential Kronecker Product Beamformers}}},
  author = {Itzhak, Gal and Benesty, Jacob and Cohen, Israel},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1397--1410},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3069089},
  abstract = {In this paper, we present a generalized approach for differential microphone array (DMA) beamforming in the short-time Fourier transform (STFT) domain. We propose a multistage beamforming approach, which considers a Kronecker product (KP) decomposition of the global beamformer into two independent sub-beamformers. We derive differential KP beamformers according to different criteria and analyze their performances, which are tuned by three design parameters. These parameters allow a high beamforming design flexibility; in particular, non-differential or non-KP beamformers may be obtained as special cases. Depending on the selection of parameters, we demonstrate a preferable performance with the new approach with respect to the white noise gain and directivity factor measures. In addition, we consider the task of speech enhancement. We show that differential KP beamformers perform better than non-differential and non-KP beamformers in terms of the quality and intelligibility of their respective time-domain enhanced signals, particularly in moderately reverberant environments.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Array signal processing,Differential beamforming,kronecker product decomposition,Length measurement,microphone arrays,Microphone arrays,Noise measurement,optimal beamformer,Sensors,Signal to noise ratio,Time-domain analysis,uniform linear arrays},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Itzhak et al_2021_On the Design of Differential Kronecker Product Beamformers.pdf}
}

@article{jangSentenceTransitionMatrix2022,
  title = {Sentence Transition Matrix: {{An}} Efficient Approach That Preserves Sentence Semantics},
  shorttitle = {Sentence Transition Matrix},
  author = {Jang, Myeongjun and Kang, Pilsung},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101266},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101266},
  url = {https://www.sciencedirect.com/science/article/pii/S088523082100070X},
  urldate = {2021-09-28},
  abstract = {Sentence embedding is an influential research topic in natural language processing (NLP). Generation of sentence vectors that reflect the intrinsic meaning of sentences is crucial for improving performance in various NLP tasks. Therefore, numerous supervised and unsupervised sentence-representation approaches have been proposed since the advent of the distributed representation of words. These approaches have been evaluated on semantic textual similarity (STS) tasks designed to measure the degree of semantic information preservation; neural network-based supervised embedding models typically deliver state-of-the-art performance. However, these models have limitations in that they have numerous learnable parameters and thus require large amounts of specific types of labeled training data. Pretrained language model-based approaches, which have become a predominant trend in the NLP field, alleviate this issue to some extent; however, it is still necessary to collect sufficient labeled data for the fine-tuning process is still necessary. Herein, we propose an efficient approach that learns a transition matrix tuning a sentence embedding vector to capture the latent semantic meaning. Our proposed method has two practical advantages: (1) it can be applied to any sentence embedding method, and (2) it can deliver robust performance in STS tasks with only a few training examples.},
  language = {en},
  keywords = {Natural language processing,Paraphrase,Sentence embedding,Sentence semantics,Transition matrix},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Jang_Kang_2022_Sentence transition matrix.pdf}
}

@inproceedings{jiaDesignEvaluationAdult2020,
  title = {Design and {{Evaluation}} of {{Adult Emotional Speech Corpus}} for {{Natural Environment}}},
  booktitle = {2020 12th {{International Conference}} on {{Intelligent Human-Machine Systems}} and {{Cybernetics}} ({{IHMSC}})},
  author = {Jia, Ning and Zheng, Chunjun and Sun, Wei},
  date = {2020-08},
  pages = {53--56},
  publisher = {{IEEE}},
  location = {{Hangzhou, China}},
  doi = {10.1109/IHMSC49165.2020.00020},
  url = {https://ieeexplore.ieee.org/document/9204189/},
  urldate = {2021-09-10},
  abstract = {Nowadays, speech corpus plays a fundamental role in the research and development of speech processing technology. This paper mainly focuses on the research and analysis of adult emotional speech corpus construction methods. According to the needs of the research on the emotion recognition of reading speech and spoken speech, this paper studies the construction standard and the corresponding annotation standard of the emotion speech recognition for the natural environment, and designs the specific scheme for the evaluation of the effectiveness of the corpus. Based on this, an emotional speech corpus with multi-level annotation information is constructed. Experiments show that the corpus has balanced coverage of local and global emotional expression while retaining the natural attributes of emotion, which provides a reliable data support for the research of emotion based on speech recognition technology.},
  eventtitle = {2020 12th {{International Conference}} on {{Intelligent Human-Machine Systems}} and {{Cybernetics}} ({{IHMSC}})},
  isbn = {978-1-72816-517-2},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Jia et al_2020_Design and Evaluation of Adult Emotional Speech Corpus for Natural Environment.pdf}
}

@article{jiaMultiSourceDOAEstimation2021,
  title = {Multi-{{Source DOA Estimation}} in {{Reverberant Environments}} by {{Jointing Detection}} and {{Modeling}} of {{Time-Frequency Points}}},
  author = {Jia, Maoshen and Wu, Yuxuan and Bao, Changchun and Ritz, Christian},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {379--392},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3042705},
  abstract = {In this article, the direction of arrival (DOA) estimation of multiple speech sources in reverberant environments is investigated based on the recording of a soundfield microphone. First, the recordings are analyzed in the time-frequency (T-F) domain to detect both ``points'' (single T-F points) and ``regions'' (multiple, adjacent T-F points) corresponding to a single source with low reverberation (known as low-reverberant-single-source (LRSS) points). Then, a LRSS point detection algorithm is proposed based on a joint dominance measure and instantaneous single-source point (SSP) identification. Following this, initial DOA estimates obtained for the detected LRSS points are analyzed using a Gaussian Mixture Model (GMM) derived by the Expectation-Maximization (EM) algorithm to cluster components into sources or outliers using a rule-based method. Finally, the DOA of each actual source is obtained from the estimated source components. Experiments on both simulated data and data recorded in an actual acoustic chamber demonstrate that the proposed algorithm exhibits improved performance for the DOA estimation in reverberant environments when compared to several existing approaches.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Direction-of-arrival estimation,DOA estimation,Estimation,LRSS point,Microphone arrays,Reflection,reverberant environments,Reverberation,Speech processing,Time-frequency analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Jia et al_2021_Multi-Source DOA Estimation in Reverberant Environments by Jointing Detection.pdf}
}

@article{jiangVectorBasedFeatureRepresentations2021,
  title = {Vector-{{Based Feature Representations}} for {{Speech Signals}}: {{From Supervector}} to {{Latent Vector}}},
  shorttitle = {Vector-{{Based Feature Representations}} for {{Speech Signals}}},
  author = {Jiang, Yuechi and Leung, Frank H. F.},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {2641--2655},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.3014559},
  abstract = {There are two basic types of feature representations for speech signals. The first type refers to probabilistic models, such as the Gaussian mixture model (GMM). The second type refers to vector-based feature representations, such as the Gaussian supervector (GSV). Since vector-based feature representations are easier to use and process, they are more popular than probabilistic model-based feature representations. In this paper, we begin by explaining the rationale behind two widely used vector-based feature representations, viz. GSV and the i-vector, and then make extensions. GSV is a supervector (SV) based on maximum a posteriori (MAP) adaptation. Its computation is simple and fast, but its dimensionality is high and fixed. While the i-vector is a latent vector (LV) based on factor analysis (FA). Although the computation can be time-consuming because of additional model parameters, its dimensionality is changeable. To generalize GSV, we propose the MAP SV, which is also based on MAP adaptation but can have an even higher dimensionality and thus carry more information. To boost the computational efficiency of the i-vector, we adopt the concept of the mixture of factor analyzers (MFA) and propose the MFA LV, which exhibits a similar flexibility in dimensionality but is faster in computation. The experimental results for speaker identification and verification tasks demonstrate that, MAP SV can be more robust than GSV, and MFALV is comparable to or even better than the i-vector in effectiveness and meanwhile maintains a higher computational efficiency. With a powerful backend, GSV and MAP SV are comparable to the i-vector and MFALV, but the latter two are more flexible in dimensionality.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {_Waiting for read,Acoustic and speech signal processing,Acoustics,Adaptation models,Computational efficiency,Computational modeling,gaussian supervector,i-vector,Probabilistic logic,Speaker recognition,supervector and latent vector,Task analysis,vector-based feature representation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Jiang_Leung_2021_Vector-Based Feature Representations for Speech Signals.pdf}
}

@thesis{JinXiJinPingWaiJiaoSiXiangYanJiu2019,
  type = {硕士},
  title = {习近平外交思想研究},
  author = {金, 祎},
  date = {2019},
  institution = {{中共辽宁省委党校}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019177417.nh&uniplatform=NZKPT&v=4s7FOpMcrPTkWfEMGb1F1fnzVVTO%25mmd2F8zNOwv8%25mmd2FMilTNXXZKRPaDO%25mmd2BCe1IpDFCB5QB},
  urldate = {2021-11-21},
  abstract = {党的十八大以来,中国特色社会主义进入新时代,面对世界百年未有之大变局,以习近平同志为核心的党中央进行了一系列重大理论和实践创新,形成了习近平外交思想。习近平外交思想为解决当前国际社会共同面临的问题提出了新见解、新思路,贡献了中国智慧,提升了中国的国际地位和形象,推动了中华民族伟大复兴的历史进程。在这一思想指引下,我们开创了中国特色外交工作的新局面,走出了一条具有中国特色的外交道路。本论文正文包含六大部分内容,具体如下:第一部分,绪论。包括四个方面内容:一是阐述研究背景和研究意义。研究背景主要从国际国内两个视角切入,研究意义主要从理论意义和实践意义两个方面展开;二是文献综述。主要是对学术界既有的...},
  language = {zh-CN},
  keywords = {diplomatic thoughts,Great power diplomacy with Chinese characteristics,Xi Jinping,中国特色大国外交,习近平,外交思想},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\金_2019_习近平外交思想研究.pdf}
}

@article{jooEffectiveEmotionTransplantation2020,
  title = {Effective {{Emotion Transplantation}} in an {{End-to-End Text-to-Speech System}}},
  author = {Joo, Young-Sun and Bae, Hanbin and Kim, Young-Ik and Cho, Hoon-Young and Kang, Hong-Goo},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {161713--161719},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3021758},
  url = {https://ieeexplore.ieee.org/document/9186605/},
  urldate = {2021-09-10},
  abstract = {In this paper, we propose an effective technique to transplant a source speaker's emotional expression to a new target speaker's voice within an end-to-end text-to-speech (TTS) framework. We modify an expressive TTS model pre-trained using a source speaker's emotional speech database to reflect the voice characteristics of a target speaker for which only a neutral speech database is available. We set two adaptation criteria to achieve this. One criterion is to minimize the reconstruction loss between the target speaker's recorded and synthesized speech, such that the synthesized speech has the target speaker's voice characteristics. The other criterion is to minimize the emotion loss between the emotion embedding vectors extracted from the reference expressive speech and the target speaker's synthesized expressive speech, which is essential to preserve expressiveness. Since the two criteria are applied alternately in the adaptation process, we are able to avoid the kind of bias issues frequently encountered in similar tasks. The proposed adaptation technique demonstrates more effective performance compared to conventional approaches in both quantitative and qualitative evaluations.},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Joo et al_2020_Effective Emotion Transplantation in an End-to-End Text-to-Speech System.pdf}
}

@article{k.sreenivasaraoEmotionRecognitionSpeech2013,
  title = {Emotion Recognition from Speech Using Global and Local Prosodic Features},
  author = {{K. Sreenivasa Rao} and {Shashidhar G. Koolagudi} and {Ramu Reddy Vempada}},
  date = {2013},
  journaltitle = {International Journal of Speech Technology},
  doi = {10.1007/s10772-012-9172-2},
  url = {http://www.researchgate.net/publication/257571796_Emotion_recognition_from_speech_using_global_and_local_prosodic_features},
  urldate = {2021-09-15},
  abstract = {In this paper, global and local prosodic features extracted from sentence, word and syllables are proposed for speech emotion or affect recognition. In this work, duration, pitch, and energy values are used to represent the prosodic information, for recognizing the emotions from speech. Global prosodic features represent the gross statistics such as mean, minimum, maximum, standard deviation, and slope of the prosodic contours. Local prosodic features represent the temporal dynamics in the prosody. In this work, global and local prosodic features are analyzed separately and in combination at different levels for the recognition of emotions. In this study, we have also explored the words and syllables at different positions (initial, middle, and final) separately, to analyze their contribution towards the recognition of emotions. In this paper, all the studies are carried out using simulated Telugu emotion speech corpus (IITKGP-SESC). These results are compared with the results of internationally known Berlin emotion speech corpus (Emo-DB). Support vector machines are used to develop the emotion recognition models. The results indicate that, the recognition performance using local prosodic features is better compared to the performance of global prosodic features. Words in the final position of the sentences, syllables in the final position of the words exhibit more emotion discriminative information compared to the words and syllables present in the other positions.},
  language = {en},
  keywords = {\#nosource,emo-db,emotion recognition,global prosodic features,iitkgp-sesc,local prosodic features,region-wise emotion recognition,segment-wise emotion recognition,vowel onset point}
}

@inproceedings{kadiriComparisonGlottalClosure2020,
  title = {Comparison of {{Glottal Closure Instants Detection Algorithms}} for {{Emotional Speech}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kadiri, Sudarsana Reddy and Alku, Paavo and Yegnanarayana, B.},
  date = {2020-05},
  pages = {7379--7383},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9054737},
  url = {https://ieeexplore.ieee.org/document/9054737/},
  urldate = {2021-09-10},
  abstract = {In production of voiced speech, epochs or glottal closure instants (GCIs) refer to the instants of significant excitation of the vocal tract. Extraction of GCIs is used as a pre-processing stage in many areas of speech technology, such as in prosody modification, speech synthesis and voice source analysis. In the past decades, several GCI detection algorithms have been developed and most of them provide excellent results for speech signals produced using modal (normal) type of phonation. There are, however, no studies comparing multiple state-of-the-art GCI detection methods in emotional speech. In this paper, we compare six GCI detection algorithms using emotional speech and known evaluation metrics. We use the Berlin EMO-DB acted emotional speech database which contains seven emotions and simultaneous electroglottography (EGG) recordings as ground truth. The results show that all six GCI detection algorithms give best performance in processing speech of neutral emotion and that the performance degrade particularly in emotions of high arousal (anger and joy). To improve the performance of GCI detection in emotional speech, the study underlines the importance of local average pitch period estimates.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kadiri et al_2020_Comparison of Glottal Closure Instants Detection Algorithms for Emotional Speech.pdf}
}

@article{kameokaManytoManyVoiceTransformer2021,
  title = {Many-to-{{Many Voice Transformer Network}}},
  author = {Kameoka, Hirokazu and Huang, Wen-Chin and Tanaka, Kou and Kaneko, Takuhiro and Hojo, Nobukatsu and Toda, Tomoki},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {656--670},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3047262},
  abstract = {This paper proposes a voice conversion (VC) method based on a sequence-to-sequence (S2S) learning framework, which enables simultaneous conversion of the voice characteristics, pitch contour, and duration of input speech. We previously proposed an S2S-based VC method using a transformer network architecture called the voice transformer network (VTN). The original VTN was designed to learn only a mapping of speech feature sequences from one speaker to another. Here, the main idea we propose is an extension of the original VTN that can simultaneously learn mappings among multiple speakers. This extension, called the many-to-many VTN, enables us to fully use available training data collected from multiple speakers by capturing common latent features that can be shared across different speakers. It also allows us to introduce a training loss called the identity mapping loss to ensure that the input feature sequence will remain unchanged when the source and target speaker indices are the same. Using this particular loss for model training has been found to be extremely effective in improving the performance of the model at test time. We conducted speaker identity conversion experiments and found that our model obtained higher sound quality and speaker similarity than baseline methods. We also found that our model, with a slight modification to its architecture, can handle any-to-many conversion tasks reasonably well.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Attention,Computational modeling,Computer architecture,Data models,Decoding,many-to-many VC,sequence-to-sequence learning,Training,Training data,transformer network,voice conversion (VC)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kameoka et al_2021_Many-to-Many Voice Transformer Network.pdf}
}

@article{kamperImprovedAcousticWord2021,
  title = {Improved {{Acoustic Word Embeddings}} for {{Zero-Resource Languages Using Multilingual Transfer}}},
  author = {Kamper, Herman and Matusevych, Yevgen and Goldwater, Sharon},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1107--1118},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3060805},
  abstract = {Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. Such embeddings can form the basis for speech search, indexing and discovery systems when conventional speech recognition is not possible. In zero-resource settings where unlabelled speech is the only available resource, we need a method that gives robust embeddings on an arbitrary language. Here we explore multilingual transfer: we train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. We consider three multilingual recurrent neural network (RNN) models: a classifier trained on the joint vocabularies of all training languages; a Siamese RNN trained to discriminate between same and different words from multiple languages; and a correspondence autoencoder (CAE) RNN trained to reconstruct word pairs. In a word discrimination task on six target languages, all of these models outperform state-of-the-art unsupervised models trained on the zero-resource languages themselves, giving relative improvements of more than 30\% in average precision. When using only a few training languages, the multilingual CAE-RNN performs better, but with more training languages the other multilingual models perform similarly. Using more training languages is generally beneficial, but improvements are marginal on some languages. We present probing experiments which show that the CAE-RNN encodes more phonetic, word duration, language identity and speaker information than the other multilingual models.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustic word embeddings,Acoustics,Data models,Hidden Markov models,multilingual models,Speech processing,Training,transfer learning,Transfer learning,Vocabulary,zero-resource speech processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kamper et al_2021_Improved Acoustic Word Embeddings for Zero-Resource Languages Using.pdf}
}

@inproceedings{kanekoCycleganVC2ImprovedCycleganbased2019,
  title = {Cyclegan-{{VC2}}: {{Improved Cyclegan-based Non-parallel Voice Conversion}}},
  shorttitle = {Cyclegan-{{VC2}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kaneko, Takuhiro and Kameoka, Hirokazu and Tanaka, Kou and Hojo, Nobukatsu},
  date = {2019},
  pages = {6820--6824},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8682897},
  abstract = {Non-parallel voice conversion (VC) is a technique for learning the mapping from source to target speech without relying on parallel data. This is an important task, but it has been challenging due to the disadvantages of the training conditions. Recently, CycleGAN-VC has provided a breakthrough and performed comparably to a parallel VC method without relying on any extra data, modules, or time alignment procedures. However, there is still a large gap between the real target and converted speech, and bridging this gap remains a challenge. To reduce the gap, we propose CycleGAN-VC2, which is an improved version of CycleGAN-VC incorporating three new techniques: an improved objective (two-step adversarial losses), improved generator (2-1-2D CNN), and improved discriminator (PatchGAN). We evaluated our method on a non-parallel VC task and analyzed the effect of each technique in detail. An objective evaluation showed that these techniques help bring the converted feature sequence closer to the target in terms of both global and local structures, which we assess by using Mel-cepstral distortion and modulation spectra distance, respectively. A subjective evaluation showed that CycleGAN-VC2 outperforms CycleGAN-VC in terms of naturalness and similarity for every speaker pair, including intra-gender and inter-gender pairs.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustics,Artificial neural networks,Convolution,CycleGAN,CycleGAN-VC,generative adversarial networks (GANs),Generators,non-parallel VC,Task analysis,Training,Two dimensional displays,Voice conversion (VC)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kaneko et al_2019_Cyclegan-VC2.pdf}
}

@inproceedings{kanekoCycleGANVC3ExaminingImproving2020,
  title = {{{CycleGAN-VC3}}: {{Examining}} and {{Improving CycleGAN-VCs}} for {{Mel-Spectrogram Conversion}}},
  shorttitle = {{{CycleGAN-VC3}}},
  booktitle = {Interspeech 2020},
  author = {Kaneko, Takuhiro and Kameoka, Hirokazu and Tanaka, Kou and Hojo, Nobukatsu},
  date = {2020-10-25},
  pages = {2017--2021},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2020-2280},
  url = {https://www.isca-speech.org/archive/interspeech_2020/kaneko20_interspeech.html},
  urldate = {2022-08-14},
  eventtitle = {Interspeech 2020},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kaneko et al_2020_CycleGAN-VC3.pdf}
}

@inproceedings{kangSpeechEQSpeechEmotion2022,
  title = {{{SpeechEQ}}: {{Speech Emotion Recognition}} Based on {{Multi-scale Unified Datasets}} and {{Multitask Learning}}},
  shorttitle = {{{SpeechEQ}}},
  booktitle = {Interspeech 2022},
  author = {Kang, Zuheng and Peng, Junqing and Wang, Jianzong and Xiao, Jing},
  date = {2022-09-18},
  pages = {4745--4749},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11456},
  url = {https://www.isca-speech.org/archive/interspeech_2022/kang22d_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kang et al_2022_SpeechEQ.pdf}
}

@unpublished{karmakarThankYouAttention2021,
  title = {Thank You for {{Attention}}: {{A}} Survey on {{Attention-based Artificial Neural Networks}} for {{Automatic Speech Recognition}}},
  shorttitle = {Thank You for {{Attention}}},
  author = {Karmakar, Priyabrata and Teng, Shyh Wei and Lu, Guojun},
  date = {2021-02-14},
  eprint = {2102.07259},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2102.07259},
  urldate = {2021-09-10},
  abstract = {Attention is a very popular and effective mechanism in artificial neural network-based sequence-to-sequence models. In this survey paper, a comprehensive review of the different attention models used in developing automatic speech recognition systems is provided. The paper focuses on the development and evolution of attention models for offline and streaming speech recognition within recurrent neural network- and Transformerbased architectures.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Karmakar et al_2021_Thank you for Attention.pdf}
}

@inproceedings{karrasStyleBasedGeneratorArchitecture2019,
  title = {A {{Style-Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  date = {2019-06},
  pages = {4396--4405},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00453},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  language = {en},
  keywords = {Deep Learning,Image and Video Synthesis,Representation Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf}
}

@inproceedings{kasunDiscriminativeAdversarialLearning2022,
  title = {Discriminative {{Adversarial Learning}} for {{Speaker Independent Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Kasun, Chamara and Ahn, Chung Soo and Rajapakse, Jagath and Lin, Zhiping and Huang, Guang-Bin},
  date = {2022-09-18},
  pages = {4975--4979},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-285},
  url = {https://www.isca-speech.org/archive/interspeech_2022/kasun22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kasun et al_2022_Discriminative Adversarial Learning for Speaker Independent Emotion Recognition.pdf}
}

@inproceedings{kazakosSlowFastAuditoryStreams2021,
  title = {Slow-{{Fast Auditory Streams}} for {{Audio Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  date = {2021-06},
  pages = {855--859},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413376},
  abstract = {We propose a two-stream convolutional network for audio recognition, that operates on time-frequency spectrogram inputs. Following similar success in visual recognition, we learn Slow-Fast auditory streams with separable convolutions and multi-level lateral connections. The Slow pathway has high channel capacity while the Fast pathway operates at a fine-grained temporal resolution. We showcase the importance of our two-stream proposal on two diverse datasets: VGG-Sound and EPIC-KITCHENS-100, and achieve state- of-the-art results on both.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,action recognition,audio recognition,Channel capacity,Conferences,Convolution,fusion,multi-stream networks,Speech recognition,Time-frequency analysis,Training,Visualization},
  annotation = {ECC: No Data (logprob: -86.457)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kazakos et al_2021_Slow-Fast Auditory Streams for Audio Recognition.pdf}
}

@inproceedings{keesingAcousticFeaturesNeural2021,
  title = {Acoustic {{Features}} and {{Neural Representations}} for {{Categorical Emotion Recognition}} from {{Speech}}},
  booktitle = {Interspeech 2021},
  author = {Keesing, Aaron and Koh, Yun Sing and Witbrock, Michael},
  date = {2021-08-30},
  pages = {3415--3419},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-2217},
  url = {https://www.isca-speech.org/archive/interspeech_2021/keesing21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Keesing et al_2021_Acoustic Features and Neural Representations for Categorical Emotion.pdf}
}

@article{kellyAnalysisCalibrationLombard2021,
  title = {Analysis and {{Calibration}} of {{Lombard Effect}} and {{Whisper}} for {{Speaker Recognition}}},
  author = {Kelly, Finnian and Hansen, John H.L.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {927--942},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3053388},
  abstract = {Variations in vocal effort can create challenges for speaker recognition systems that are optimized for use with neutral speech. The Lombard effect and whisper are two commonly-occurring forms of vocal effort variation that result in non-neutral speech, the first due to noise exposure and the second due to intentional adjustment on the part of the speaker. In this article, a comparative evaluation of speaker recognition performance in non-neutral conditions is presented using multiple Lombard effect and whisper corpora. The detrimental impact of these vocal effort variations on discrimination and calibration performance on global, per-corpus, and per-speaker levels is explored using conventional error metrics, along with visual representations of the model and score spaces. A non-neutral speech detector is subsequently introduced and used to inform score calibration in several ways. Two calibration approaches are proposed and shown to reduce error to the same level as an optimal calibration approach that relies on ground-truth vocal effort information. This article contributes a generalizable methodology towards detecting vocal effort variation and using this knowledge to inform and advance speaker recognition system behavior.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Calibration,Headphones,Lombard effect,Microphones,speaker recognition,Speaker recognition,speech processing,Speech processing,Speech recognition,Stress,whisper/vocal effort},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kelly_Hansen_2021_Analysis and Calibration of Lombard Effect and Whisper for Speaker Recognition.pdf}
}

@inproceedings{kimDNNbasedEmotionRecognition2019,
  title = {{{DNN-based Emotion Recognition Based}} on {{Bottleneck Acoustic Features}} and {{Lexical Features}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kim, Eesung and Shin, Jong Won},
  date = {2019-05},
  pages = {6720--6724},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8683077},
  abstract = {In this paper, we propose a novel emotion recognition method to reflect affect salient information using acoustic and lexical features. The acoustic features are extracted from the speech signal by applying statistical functionals of emotionally high-level features derived from Deep Neural Network (DNN). These acoustic features are early fused with two types of lexical features extracted from the text transcription of the speech signal, which are the distributed representation and affective lexicon-based dimensions. The fused features are fed to another DNN for utterance-level emotion classification. Experimental results on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) multimodal dataset showed 75.5\% in unweighted accuracy recall, which outperformed the best results reported previously in the multimodal emotion recognition using acoustic and lexical features.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Acoustic feature,Data mining,DNN-based emotion recognition,Emotion recognition,Feature extraction,Lexical feature,Mel frequency cepstral coefficient,Multimodal emotion recognition,Neural networks,Speech recognition},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\3TU8KZ9C\\8683077.html}
}

@article{kimEditorialSpecialIssue2021,
  title = {Editorial: {{Special Issue}} on the {{Eighth Dialog System Technology Challenge}}},
  shorttitle = {Editorial},
  author = {Kim, Seokhwan and Schulz, Hannes and Gunasekara, Chulaka and Hori, Chiori and Rastogi, Abhinav and D'Haro, Luis Fernando},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2434--2436},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3097842},
  abstract = {The 11 papers in this special section that were part of the Dialog System Technology Challenge. Research competitions have been a long-standing and valuable tradition in the speech and language community. They accelerate the development of new technologies by establishing a shared testbed, cross-validating many potential approaches, and attracting new researchers to the field. DSTC, the Dialog System Technology Challenge, has been a premier research competition for Dialog Systems since its inception in 2013. The earlier Dialog State Tracking Challenges focused on developing a single component for dialog state tracking on goaloriented human-machine conversations. Then, DSTC4 (2015) and DSTC5 (2016) introduced human-human conversations and started to offer multiple tasks not only for dialog state tracking, but also for other components in dialog systems as the pilot tasks. From the sixth challenge (2017), the DSTC has rebranded itself as ``Dialog System Technology Challenge'' and organized multiple main tracks in parallel to address a wider variety of end-to-end dialog related problems. In line with recent editions of this challenge, DSTC8 focuses on applying end-to-end technologies to Dialog Systems in a pragmatic way. We opened the call for track proposals to the dialog research community and finally held the four parallel main tracks: (1) Multi-domain Task Completion, (2) NOESIS II: Predicting Responses, Identifying Success, and Managing Complexity in Task-Oriented Dialogue, (3) Audio Visual SceneAware Dialog Track, and (4) Scalable Schema-Guided Dialogue State Tracking. This special issue presents more in-depth details of the challenge outcomes.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Audio systems,Special issues and sections,Speech processing,Technological innovation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kim et al_2021_Editorial.pdf}
}

@article{kimHowDoesAttention2021,
  title = {How Does the Attention System Learn from Aversive Outcomes?},
  author = {Kim, Haena and Anderson, Brian A.},
  date = {2021},
  journaltitle = {Emotion},
  volume = {21},
  number = {4},
  pages = {898--903},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1931-1516},
  doi = {10.1037/emo0000757},
  abstract = {Learning about aversive outcomes plays a role in the guidance of attention. Classical conditioning generates a bias to predictors of aversive outcomes, whereas instrumental learning potentiates a negatively reinforced avoidance behavior, which can be difficult to dissociate in the case of attention to aversively conditioned stimuli. The present study examined the relative contribution from these two learning processes to the control of attention. Participants were first provided an opportunity to avoid an electric shock by generating a saccade in the direction opposite one of two stimuli. In contradiction to the practiced avoidance behavior, such training resulted in a bias to orient toward the shock-associated stimulus, indicative of a more dominant role of classical conditioning in the control of attention. The findings are in parallel with the influence of positive reinforcement on attention, suggesting that the attention system may be guided by motivational relevance rather than a particular emotional valence. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  issue = {4},
  language = {en},
  keywords = {_Waiting for read,\#nosource,Aversion,Aversion Conditioning,Avoidance,Behavior,Classical Conditioning,Learning,Negative Reinforcement,Training}
}

@inproceedings{kimImprovingSpeechEmotion2022,
  title = {Improving {{Speech Emotion Recognition Through Focus}} and {{Calibration Attention Mechanisms}}},
  booktitle = {Interspeech 2022},
  author = {Kim, Junghun and An, Yoojin and Kim, Jihie},
  date = {2022-09-18},
  pages = {136--140},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-299},
  url = {https://www.isca-speech.org/archive/interspeech_2022/kim22d_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kim et al_2022_Improving Speech Emotion Recognition Through Focus and Calibration Attention.pdf}
}

@article{kolliasExploitingMultiCNNFeatures2021,
  title = {Exploiting {{Multi-CNN Features}} in {{CNN-RNN Based Dimensional Emotion Recognition}} on the {{OMG}} in-the-{{Wild Dataset}}},
  author = {Kollias, Dimitrios and Zafeiriou, Stefanos},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {595--606},
  issn = {1949-3045},
  doi = {10.1109/taffc.2020.3014171},
  abstract = {This article presents a novel CNN-RNN based approach, which exploits multiple CNN features for dimensional emotion recognition in-the-wild, utilizing the One-Minute Gradual-Emotion (OMG-Emotion) dataset. Our approach includes first pre-training with the relevant and large in size, Aff-Wild and Aff-Wild2 emotion databases. Low-, mid- and high-level features are extracted from the trained CNN component and are exploited by RNN subnets in a multi-task framework. Their outputs constitute an intermediate level prediction; final estimates are obtained as the mean or median values of these predictions. Fusion of the networks is also examined for boosting the obtained performance, at Decision-, or at Model-level; in the latter case a RNN was used for the fusion. Our approach, although using only the visual modality, outperformed state-of-the-art methods that utilized audio and visual modalities. Some of our developments have been submitted to the OMG-Emotion Challenge, ranking second among the technologies which used only visual information for valence estimation; ranking third overall. Through extensive experimentation, we further show that arousal estimation is greatly improved when low-level features are combined with high-level ones.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_Waiting for read,AffWild and AffWild2 emotion databases,AffWildNet,arousal,CNN plus Multi RNN,Computer architecture,Databases,Deep convolutional and recurrent neural architectures,Emotion recognition,emotion recognition in-the-wild,Estimation,facial image analysis,Feature extraction,low-; mid-; high-level features,Machine learning,multi-CNN feature extraction and aggregation,multi-task learning,OMG-Emotion database and challenge,valence,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kollias_Zafeiriou_2021_Exploiting Multi-CNN Features in CNN-RNN Based Dimensional Emotion Recognition.pdf}
}

@inproceedings{konsExtendingRNNTbasedSpeech2022,
  title = {Extending {{RNN-T-based}} Speech Recognition Systems with Emotion and Language Classification},
  booktitle = {Interspeech 2022},
  author = {Kons, Zvi and Aronowitz, Hagai and Morais, Edmilson and Damasceno, Matheus and Kuo, Hong-Kwang and Thomas, Samuel and Saon, George},
  date = {2022-09-18},
  pages = {546--549},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10480},
  url = {https://www.isca-speech.org/archive/interspeech_2022/kons22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kons et al_2022_Extending RNN-T-based speech recognition systems with emotion and language.pdf}
}

@article{koseMultimodalRepresentationsSynchronized2021,
  title = {Multimodal {{Representations}} for {{Synchronized Speech}} and {{Real-Time MRI Video Processing}}},
  author = {K\"ose, \"Oyk\"u Deniz and Sara\c{c}lar, Murat},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1912--1924},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3084099},
  abstract = {Representations for data subunits can help with recent data accumulation by enabling efficient storage and retrieval systems. In this paper, we investigate the problem of representation generation for phone classification and cross-modal same-different word discrimination tasks. The benefits of utilizing multimodal data on these tasks are examined together with different data fusion schemes. Mainly, the paper considers two different data modalities, upper airway mid-sagittal plane real-time magnetic resonance imaging (rtMRI) videos and the corresponding speech waveforms, and experiments on USC-TIMIT rtMRI dataset. For the phone classification task, two unimodal neural networks are designed, and these separate systems are merged in two different ways that provide data fusion between two modalities. The proposed networks differ in their stages in which they perform the data fusion. As hypothesized, our results show that data fusion indeed brings a performance improvement over both unimodal approaches, and performing fusion in earlier stages with cross-connections yields better results than fusing the data in later stages. In addition to the proposed phone classification schemes, different unimodal and multimodal systems are designed to obtain phone recognition results on USC-TIMIT rtMRI dataset. Phone representations generated for the phone classification task are also utilized in the phone recognition task, and their representative power is illustrated. Finally, we define a cross-view same-different word discrimination task on USC-TIMIT. We propose two different schemes to tackle this task, and find that for cross-view same-different discrimination, generating representations with the help of cross-modality yields better accuracy than a system employing independently created representations.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {cross-modality,Data integration,deep learning,Machine learning,Magnetic resonance imaging,multi-modal information,Neural networks,Phonetics,rtMRI-TIMIT,Speech processing,Speech recognition,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Köse_Saraçlar_2021_Multimodal Representations for Synchronized Speech and Real-Time MRI Video.pdf}
}

@article{kourkounakisFluentNetEndtoEndDetection2021,
  title = {{{FluentNet}}: {{End-to-End Detection}} of {{Stuttered Speech Disfluencies With Deep Learning}}},
  shorttitle = {{{FluentNet}}},
  author = {Kourkounakis, Tedd and Hajavi, Amirhossein and Etemad, Ali},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2986--2999},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3110146},
  abstract = {Millions of people are affected by stuttering and other speech disfluencies, with the majority of the world having experienced mild stutters while communicating under stressful conditions. While there has been much research in the field of automatic speech recognition and language models, stutter detection and recognition has not received as much attention. To this end, we propose an end-to-end deep neural network, FluentNet, capable of detecting a number of different stutter types. FluentNet consists of a Squeeze-and-Excitation Residual convolutional neural network which facilitate the learning of strong spectral frame-level representations, followed by a set of bidirectional long short-term memory layers that aid in learning effective temporal relationships. Lastly, FluentNet uses an attention mechanism to focus on the important parts of speech to obtain a better performance. We perform a number of different experiments, comparisons, and ablation studies to evaluate our model. Our model achieves state-of-the-art results by outperforming other solutions in the field on the publicly available UCLASS dataset. Additionally, we present LibriStutter: a stuttered speech dataset based on the public LibriSpeech dataset with synthesized stutters. We also evaluate FluentNet on this dataset, showing the strong performance of our model versus a number of baseline and state-of-the-art techniques.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Attention,Benchmark testing,BLSTM,deep learning,Deep learning,disfluency,Residual neural networks,Speaker recognition,speech,Speech processing,squeeze-and-excitation,stutter,Tools,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kourkounakis et al_2021_FluentNet.pdf}
}

@article{koutiniReceptiveFieldRegularization2021,
  title = {Receptive {{Field Regularization Techniques}} for {{Audio Classification}} and {{Tagging With Deep Convolutional Neural Networks}}},
  author = {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1987--2000},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2021.3082307},
  abstract = {In this paper, we study the performance of variants of well-known Convolutional Neural Network (CNN) architectures on different audio tasks. We show that tuning the Receptive Field (RF) of CNNs is crucial to their generalization. An insufficient RF limits the CNN's ability to fit the training data. In contrast, CNNs with an excessive RF tend to over-fit the training data and fail to generalize to unseen testing data. As state-of-the-art CNN architectures \textendash{} in computer vision and other domains \textendash{} tend to go deeper in terms of number of layers, their RF size increases and therefore they degrade in performance in several audio classification and tagging tasks. We study well-known CNN architectures and how their building blocks affect their receptive field. We propose several systematic approaches to control the RF of CNNs and systematically test the resulting architectures on different audio classification and tagging tasks and datasets. The experiments show that regularizing the RF of CNNs using our proposed approaches can drastically improve the generalization of models, out-performing complex architectures and pre-trained models on larger datasets. The proposed CNNs achieve state-of-the-art results in multiple tasks, from acoustic scene classification to emotion and theme detection in music to instrument recognition, as demonstrated by top ranks in several pertinent challenges (DCASE, MediaEval).},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_reading,acoustic scene classification,Computer architecture,Convolutional neural networks,emotion detection,Feature extraction,instrument detection,Neurons,Radio frequency,receptive field regularization,Speech processing,Tagging,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Koutini et al_2021_Receptive Field Regularization Techniques for Audio Classification and Tagging.pdf}
}

@article{krishnasivaprasadExploringIntrinsicInformation2022,
  title = {Exploring Intrinsic Information Content Models for Addressing the Issues of Traditional Semantic Measures to Evaluate Verb Similarity},
  author = {Krishna Siva Prasad, M. and Sharma, Poonam},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101280},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101280},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000838},
  urldate = {2021-09-28},
  abstract = {Semantic similarity measures play an important role in many natural language processing and information retrieval activities. It is highly challenging to measure semantic similarity with higher accuracy. A notable branch of semantic similarity evaluation based on information content (IC) is popular in this aspect. Intrinsic information content (IIC) models are another wing of IC based evaluation. Both IC based and IIC based approaches majorly handled similarity evaluation of nouns. Research related to semantic similarity assessment of verb pairs are rarely discussed. To bridge this gap, this work examines various IC based, IIC based approaches on verb pairs. A detailed discussion of the existing measures and their drawbacks are mentioned in this work. Strategies based on information content, length and depth of the concepts are discussed and tested on benchmark datasets. Existing intrinsic information content models are enhanced by addressing various issues like (a) dealing concepts with no path in WordNet and (b) handling the synonym sets of verb concepts. Measures based on path length, intrinsic information content, combined strategies and non-linear strategies for verb pairs are thoroughly inspected. This paper also presents novel strategies to understand novel aspects that are not addressed before. The strategies are experimented by generating the synonym sets of required parts-of-speech which proved very effective in improving the correlation with human judgment. Results on benchmark datasets specify that the proposed approaches for verb similarity will be a guiding factor for understanding the natural language processing tasks.},
  language = {en},
  keywords = {Depth,Information content,Intrinsic information content model,Path length,Semantic similarity},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Krishna Siva Prasad_Sharma_2022_Exploring intrinsic information content models for addressing the issues of.pdf}
}

@inproceedings{kumarExplainabilityMultimodalSpeech2021,
  title = {Towards the {{Explainability}} of {{Multimodal Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Kumar, Puneet and Kaushik, Vishesh and Raman, Balasubramanian},
  date = {2021-08-30},
  pages = {1748--1752},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1718},
  url = {https://www.isca-speech.org/archive/interspeech_2021/kumar21d_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kumar et al_2021_Towards the Explainability of Multimodal Speech Emotion Recognition.pdf}
}

@inproceedings{kumawatApplyingTDNNArchitectures2021,
  title = {Applying {{TDNN Architectures}} for {{Analyzing Duration Dependencies}} on {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Kumawat, Pooja and Routray, Aurobinda},
  date = {2021-08-30},
  pages = {3410--3414},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-2168},
  url = {https://www.isca-speech.org/archive/interspeech_2021/kumawat21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Kumawat_Routray_2021_Applying TDNN Architectures for Analyzing Duration Dependencies on Speech.pdf}
}

@article{landiniBayesianHMMClustering2022,
  title = {Bayesian {{HMM}} Clustering of X-Vector Sequences ({{VBx}}) in Speaker Diarization: {{Theory}}, Implementation and Analysis on Standard Tasks},
  shorttitle = {Bayesian {{HMM}} Clustering of X-Vector Sequences ({{VBx}}) in Speaker Diarization},
  author = {Landini, Federico and Profant, J\'an and Diez, Mireia and Burget, Luk\'a\v{s}},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101254},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101254},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000619},
  urldate = {2021-09-28},
  abstract = {The recently proposed VBx diarization method uses a Bayesian hidden Markov model to find speaker clusters in a sequence of x-vectors. In this work we perform an extensive comparison of performance of the VBx diarization with other approaches in the literature and we show that VBx achieves superior performance on three of the most popular datasets for evaluating diarization: CALLHOME, AMI and DIHARD II datasets. Further, we present for the first time the derivation and update formulae for the VBx model, focusing on the efficiency and simplicity of this model as compared to the previous and more complex BHMM model working on frame-by-frame standard Cepstral features. Together with this publication, we release the recipe for training the x-vector extractors used in our experiments on both wide and narrowband data, and the VBx recipes that attain state-of-the-art performance on all three datasets. Besides, we point out the lack of a standardized evaluation protocol for AMI dataset and we propose a new protocol for both Beamformed and Mix-Headset audios based on the official AMI partitions and transcriptions.},
  language = {en},
  keywords = {_Waiting for read,AMI,HMM,Speaker diarization,Variational Bayes,x-vector},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Landini et al_2022_Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization.pdf}
}

@inproceedings{leeEmotionNotOnehot2022,
  title = {The {{Emotion}} Is {{Not One-hot Encoding}}: {{Learning}} with {{Grayscale Label}} for {{Emotion Recognition}} in {{Conversation}}},
  shorttitle = {The {{Emotion}} Is {{Not One-hot Encoding}}},
  booktitle = {Interspeech 2022},
  author = {Lee, Joosung},
  date = {2022-09-18},
  pages = {141--145},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-551},
  url = {https://www.isca-speech.org/archive/interspeech_2022/lee22e_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lee_2022_The Emotion is Not One-hot Encoding.pdf}
}

@article{leeGatedRecurrentContext2021,
  title = {Gated {{Recurrent Context}}: {{Softmax-Free Attention}} for {{Online Encoder-Decoder Speech Recognition}}},
  shorttitle = {Gated {{Recurrent Context}}},
  author = {Lee, Hyeonseung and Kang, Woo Hyun and Cheon, Sung Jun and Kim, Hyeongju and Kim, Nam Soo},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {710--719},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3049344},
  abstract = {Recently, attention-based encoder-decoder (AED) models have shown state-of-the-art performance in automatic speech recognition (ASR). As the original AED models with global attentions are not capable of online inference, various online attention schemes have been developed to reduce ASR latency for better user experience. However, a common limitation of the conventional softmax-based online attention approaches is that they introduce an additional hyperparameter related to the length of the attention window, requiring multiple trials of model training for tuning the hyperparameter. In order to deal with this problem, we propose a novel softmax-free attention method and its modified formulation for online attention, which does not need any additional hyperparameter at the training phase. Through a number of ASR experiments, we demonstrate the tradeoff between the latency and performance of the proposed online attention technique can be controlled by merely adjusting a threshold at the test phase. Furthermore, the proposed methods showed competitive performance to the conventional global and online attentions in terms of word-error-rates (WERs).},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Attention-based encoder-decoder model,automatic speech recognition,Decoding,Hidden Markov models,Logic gates,online speech recognition,Predictive models,Speech recognition,Training,Tuning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lee et al_2021_Gated Recurrent Context.pdf}
}

@inproceedings{leemNotAllFeatures2022,
  title = {Not {{All Features}} Are {{Equal}}: {{Selection}} of {{Robust Features}} for {{Speech Emotion Recognition}} in {{Noisy Environments}}},
  shorttitle = {Not {{All Features}} Are {{Equal}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Leem, Seong-Gyun and Fulford, Daniel and Onnela, Jukka-Pekka and Gard, David and Busso, Carlos},
  date = {2022},
  pages = {6447--6451},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747705},
  abstract = {Speech emotion recognition (SER) system deployed in real-world applications often encounters noisy speech. While most noise compensation techniques consider all acoustic features to have equal impact on the SER model, some acoustic features may be more sensitive to noisy conditions. This paper investigates the noise robustness of each feature in the acoustic feature set. We focus on low-level descriptors (LLDs) commonly used in SER systems. We firstly train SER models with clean speech by only using a single LLD. Then, we rank each LLD with respect to the absolute performance on a development set contaminated with noise, and the relative performance decrease from the results from the models trained with the clean set. Our experiment shows that using all the LLDs leads to worse performance than training the system with a single robust LLD. We propose to select a group of robust features according to their performance and robustness in noisy condition. Without using any compensation method, our feature selection methods improve the performance by 24.4\% (arousal), 23.9\% (dominance), and 43.2\% (valence) in the 10dB noisy condition. Moreover, even though the selection is conducted with the 10dB condition, our selection methods also yield performance improvements in unseen noisy recording conditions.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {acoustic feature,Emotion recognition,Feature extraction,feature selection,Noise robustness,noisy speech,Recording,Robustness,Speech emotion recognition,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Leem et al_2022_Not All Features are Equal.pdf}
}

@inproceedings{leemSeparationEmotionalReconstruction2021,
  title = {Separation of {{Emotional}} and {{Reconstruction Embeddings}} on {{Ladder Network}} to {{Improve Speech Emotion Recognition Robustness}} in {{Noisy Conditions}}},
  booktitle = {Interspeech 2021},
  author = {Leem, Seong-Gyun and Fulford, Daniel and Onnela, Jukka-Pekka and Gard, David and Busso, Carlos},
  date = {2021-08-30},
  pages = {2871--2875},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1438},
  url = {https://www.isca-speech.org/archive/interspeech_2021/leem21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Leem et al_2021_Separation of Emotional and Reconstruction Embeddings on Ladder Network to.pdf}
}

@unpublished{leeSTYLERStyleFactor2021,
  title = {{{STYLER}}: {{Style Factor Modeling}} with {{Rapidity}} and {{Robustness}} via {{Speech Decomposition}} for {{Expressive}} and {{Controllable Neural Text}} to {{Speech}}},
  shorttitle = {{{STYLER}}},
  author = {Lee, Keon and Park, Kyumin and Kim, Daeyoung},
  date = {2021-06-24},
  eprint = {2103.09474},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.09474},
  urldate = {2021-09-10},
  abstract = {Previous works on neural text-to-speech (TTS) have been addressed on limited speed in training and inference time, robustness for difficult synthesis conditions, expressiveness, and controllability. Although several approaches resolve some limitations, there has been no attempt to solve all weaknesses at once. In this paper, we propose STYLER, an expressive and controllable TTS framework with high-speed and robust synthesis. Our novel audio-text aligning method called Mel Calibrator and excluding autoregressive decoding enable rapid training and inference and robust synthesis on unseen data. Also, disentangled style factor modeling under supervision enlarges the controllability in synthesizing process leading to expressive TTS. On top of it, a novel noise modeling pipeline using domain adversarial training and Residual Decoding empowers noise-robust style transfer, decomposing the noise without any additional label. Various experiments demonstrate that STYLER is more effective in speed and robustness than expressive TTS with autoregressive decoding and more expressive and controllable than reading style non-autoregressive TTS. Synthesis samples and experiment results are provided via our demo page1, and code is available publicly2.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lee et al_2021_STYLER.pdf}
}

@inproceedings{leiFineGrainedEmotionStrength2021,
  title = {Fine-{{Grained Emotion Strength Transfer}}, {{Control}} and {{Prediction}} for {{Emotional Speech Synthesis}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Lei, Yi and Yang, Shan and Xie, Lei},
  date = {2021-01-19},
  pages = {423--430},
  publisher = {{IEEE}},
  location = {{Shenzhen, China}},
  doi = {10.1109/SLT48900.2021.9383524},
  url = {https://ieeexplore.ieee.org/document/9383524/},
  urldate = {2021-09-10},
  abstract = {This paper proposes a unified model to conduct emotion transfer, control and prediction for sequence-to-sequence based fine-grained emotional speech synthesis. Conventional emotional speech synthesis often needs manual labels or reference audio to determine the emotional expressions of synthesized speech. Such coarse labels cannot control the details of speech emotion, often resulting in an averaged emotion expression delivery, and it is also hard to choose suitable reference audio during inference. To conduct fine-grained emotion expression generation, we introduce phoneme-level emotion strength representations through a learned ranking function to describe the local emotion details, and the sentence-level emotion category is adopted to render the global emotions of synthesized speech. With the global render and local descriptors of emotions, we can obtain fine-grained emotion expressions from reference audio via its emotion descriptors (for transfer) or directly from phoneme-level manual labels (for control). As for the emotional speech synthesis with arbitrary text inputs, the proposed model can also predict phoneme-level emotion expressions from texts, which does not require any reference audio or manual label.},
  eventtitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  isbn = {978-1-72817-066-4},
  language = {en},
  annotation = {ECC: 0000005},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lei et al_2021_Fine-Grained Emotion Strength Transfer, Control and Prediction for Emotional.pdf}
}

@inproceedings{leiNovelSchemeSpeaker2014,
  title = {A Novel Scheme for Speaker Recognition Using a Phonetically-Aware Deep Neural Network},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lei, Yun and Scheffer, Nicolas and Ferrer, Luciana and McLaren, Mitchell},
  date = {2014-05},
  pages = {1695--1699},
  publisher = {{IEEE}},
  location = {{Florence, Italy}},
  doi = {10.1109/ICASSP.2014.6853887},
  url = {http://ieeexplore.ieee.org/document/6853887/},
  urldate = {2023-01-19},
  eventtitle = {{{ICASSP}} 2014 - 2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-2893-4},
  language = {en},
  keywords = {deep neural network,Hidden Markov models,Mathematical model,NIST,speaker recognition,Speaker recognition,Speech,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lei et al_2014_A novel scheme for speaker recognition using a phonetically-aware deep neural.pdf}
}

@inproceedings{liActedVsImprovised2021,
  title = {Acted vs. {{Improvised}}: {{Domain Adaptation}} for {{Elicitation Approaches}} in {{Audio-Visual Emotion Recognition}}},
  shorttitle = {Acted vs. {{Improvised}}},
  booktitle = {Interspeech 2021},
  author = {Li, Haoqi and Kim, Yelin and Kuo, Cheng-Hao and Narayanan, Shrikanth S.},
  date = {2021-08-30},
  pages = {3395--3399},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-666},
  url = {https://www.isca-speech.org/archive/interspeech_2021/li21k_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Acted vs2.pdf}
}

@article{liAdaptivelyLearningFacial2021,
  title = {Adaptively {{Learning Facial Expression Representation}} via {{C-F Labels}} and {{Distillation}}},
  author = {Li, Hangyu and Wang, Nannan and Ding, Xinpeng and Yang, Xi and Gao, Xinbo},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {2016--2028},
  issn = {1941-0042},
  doi = {10.1109/tip.2021.3049955},
  abstract = {Facial expression recognition is of significant importance in criminal investigation and digital entertainment. Under unconstrained conditions, existing expression datasets are highly class-imbalanced, and the similarity between expressions is high. Previous methods tend to improve the performance of facial expression recognition through deeper or wider network structures, resulting in increased storage and computing costs. In this paper, we propose a new adaptive supervised objective named AdaReg loss, re-weighting category importance coefficients to address this class imbalance and increasing the discrimination power of expression representations. Inspired by human beings' cognitive mode, an innovative coarse-fine (C-F) labels strategy is designed to guide the model from easy to difficult to classify highly similar representations. On this basis, we propose a novel training framework named the emotional education mechanism (EEM) to transfer knowledge, composed of a knowledgeable teacher network (KTN) and a self-taught student network (STSN). Specifically, KTN integrates the outputs of coarse and fine streams, learning expression representations from easy to difficult. Under the supervision of the pre-trained KTN and existing learning experience, STSN can maximize the potential performance and compress the original KTN. Extensive experiments on public benchmarks demonstrate that the proposed method achieves superior performance compared to current state-of-the-art frameworks with 88.07\% on RAF-DB, 63.97\% on AffectNet and 90.49\% on FERPlus.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  language = {en},
  keywords = {_Waiting for read,Adaptation models,adaptive regular loss,coarse-fine labels,emotional education mechanism,Face recognition,Faces,Facial expression recognition,Feature extraction,Image coding,knowledge distillation,Mouth,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Adaptively Learning Facial Expression Representation via C-F Labels and.pdf}
}

@article{lianCTNetConversationalTransformer2021,
  title = {{{CTNet}}: {{Conversational Transformer Network}} for {{Emotion Recognition}}},
  shorttitle = {{{CTNet}}},
  author = {Lian, Zheng and Liu, Bin and Tao, Jianhua},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {985--1000},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3049898},
  abstract = {Emotion recognition in conversation is a crucial topic for its widespread applications in the field of human-computer interactions. Unlike vanilla emotion recognition of individual utterances, conversational emotion recognition requires modeling both context-sensitive and speaker-sensitive dependencies. Despite the promising results of recent works, they generally do not leverage advanced fusion techniques to generate the multimodal representations of an utterance. In this way, they have limitations in modeling the intra-modal and cross-modal interactions. In order to address these problems, we propose a multimodal learning framework for conversational emotion recognition, called conversational transformer network (CTNet). Specifically, we propose to use the transformer-based structure to model intra-modal and cross-modal interactions among multimodal features. Meanwhile, we utilize word-level lexical features and segment-level acoustic features as the inputs, thus enabling us to capture temporal information in the utterance. Additionally, to model context-sensitive and speaker-sensitive dependencies, we propose to use the multihead attention based bi-directional GRU component and speaker embeddings. Experimental results on the IEMOCAP and MELD datasets demonstrate the effectiveness of the proposed method. Our method shows an absolute 2.1 6.2\% performance improvement on weighted average F1 over state-of-the-art strategies.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Bidirectional control,Context modeling,Context-sensitive modeling,conversational emotion recognition,conversational transformer network (CTNet),Data models,Emotion recognition,Feature extraction,Fuses,multimodal fusion,speaker-sensitive modeling,Speech processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lian et al_2021_CTNet.pdf}
}

@article{liangEvaluatingVoiceassistantCommands2021,
  title = {Evaluating Voice-Assistant Commands for Dementia Detection},
  author = {Liang, Xiaohui and Batsis, John A. and Zhu, Youxiang and Driesse, Tiffany M. and Roth, Robert M. and Kotz, David and MacWhinney, Brian},
  date = {2021-09-22},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  pages = {101297},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101297},
  url = {https://www.sciencedirect.com/science/article/pii/S088523082100098X},
  urldate = {2021-09-28},
  abstract = {Early detection of cognitive decline involved in Alzheimer's Disease and Related Dementias (ADRD) in older adults living alone is essential for developing, planning, and initiating interventions and support systems to improve users' everyday function and quality of life. In this paper, we explore the voice commands using a Voice-Assistant System (VAS), i.e., Amazon Alexa, from 40 older adults who were either Healthy Control (HC) participants or Mild Cognitive Impairment (MCI) participants, age 65 or older. We evaluated the data collected from voice commands, cognitive assessments, and interviews and surveys using a structured protocol. We extracted 163 unique command-relevant features from each participant's use of the VAS. We then built machine-learning models including 1-layer/2-layer neural networks, support vector machines, decision tree, and random forest, for classification and comparison with standard cognitive assessment scores, e.g., Montreal Cognitive Assessment (MoCA). Our classification models using fusion features achieved an accuracy of 68\%, and our regression model resulted in a Root-Mean-Square Error (RMSE) score of 3.53. Our Decision Tree (DT) and Random Forest (RF) models using selected features achieved higher classification accuracy 80\%\textendash 90\%. Finally, we analyzed the contribution of each feature set to the model output, thus revealing the commands and features most useful in inferring the participants' cognitive status. We found that features of overall performance, features of music-related commands, features of call-related commands, and features from Automatic Speech Recognition (ASR) were the top-four feature sets most impactful on inference accuracy. The results from this controlled study demonstrate the promise of future home-based cognitive assessments using Voice-Assistant Systems.},
  language = {en},
  keywords = {Alzheimer’s disease,Cognitive decline,Machine learning,Speech analysis,Voice assistant},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\IPVRAZ4M\\S088523082100098X.html}
}

@inproceedings{liangUnsupervisedLearningMultiStyle2021,
  title = {Unsupervised {{Learning}} for {{Multi-Style Speech Synthesis}} with {{Limited Data}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liang, Shuang and Miao, Chenfeng and Chen, Minchuan and Ma, Jun and Wang, Shaojun and Xiao, Jing},
  date = {2021-06-06},
  pages = {6583--6587},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9414220},
  url = {https://ieeexplore.ieee.org/document/9414220/},
  urldate = {2021-09-10},
  abstract = {Existing multi-style speech synthesis methods require either style labels or large amounts of unlabeled training data, making data acquisition difficult. In this paper, we present an unsupervised multi-style speech synthesis method that can be trained with limited data. We leverage instance discriminator to guide a style encoder to learn meaningful style representations from a multi-style dataset. Furthermore, we employ information bottleneck to filter out style-irrelevant information in the representations, which can improve speech quality and style similarity. Our method is able to produce desirable speech using a fairly small dataset, where the baseline GSTTacotron fails. ABX tests show that our model significantly outperforms GST-Tacotron in both emotional speech synthesis task and multi-speaker speech synthesis task. In addition, we demonstrate that our method is able to learn meaningful style features with only 50 training samples per style.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liang et al_2021_Unsupervised Learning for Multi-Style Speech Synthesis with Limited Data.pdf}
}

@article{liAttendingForesightNovel2021,
  title = {Attending {{From Foresight}}: {{A Novel Attention Mechanism}} for {{Neural Machine Translation}}},
  shorttitle = {Attending {{From Foresight}}},
  author = {Li, Xintong and Liu, Lemao and Tu, Zhaopeng and Li, Guanlin and Shi, Shuming and Meng, Max Q.-H.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2606--2616},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3097939},
  abstract = {Machines translation (MT) is an essential task in natural language processing or even in artificial intelligence. Statistical machine translation has been the dominant approach to MT for decades, but recently neural machine translation achieves increasing interest because of its appealing model architecture and impressive translation performance. In neural machine translation, an attention model is used to identify the aligned source words for the next target word, i.e., target foresight word, to select translation context. However, it does not make use of any information about this target foresight word at all. Previous work proposed an approach to improve the attention model by explicitly accessing this target foresight word and demonstrating substantial alignment tasks. However, this approach cannot be applied in machine translation tasks where the target foresight word is unavailable. This paper proposes several novel enhanced attention models by introducing hidden information (such as part-of-speech) of the target foresight word for the translation task. We incorporate the novel enhanced attention employing hidden information about the target foresight word into both recurrent and self-attention-based neural translation models and theoretically justify that such hidden information can make translation prediction easier. Empirical experiments on four datasets further verify that the proposed attention models deliver significant improvements in translation quality.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Attention,Context modeling,Decoding,Machine translation,NMT,Predictive models,Recurrent neural networks,Task analysis,Unemployment,Word Alignment}
}

@inproceedings{liAttentionPoolingBased2018,
  title = {An {{Attention Pooling}} Based {{Representation Learning Method}} for {{Speech Emotion Recognition}}},
  author = {Li, Pengcheng and Song, Yan and McLoughlin, Ian Vince and Guo, Wu and Dai, Li-Rong},
  date = {2018-09-06},
  publisher = {{International Speech Communication Association}},
  location = {{Hyderabad, India}},
  issn = {1990-9772},
  doi = {10.21437/Interspeech.2018-1242},
  url = {http://dx.doi.org/10.21437/Interspeech.2018-1242},
  urldate = {2022-04-28},
  abstract = {This paper proposes an attention pooling based representation learning method for speech emotion recognition (SER). The emotional representation is learned in an end-to-end fashion by applying a deep convolutional neural network (CNN) directly to spectrograms extracted from speech utterances. Motivated by the success of GoogleNet, two groups of filters with different shapes are designed to capture both temporal and frequency domain context information from the input spectrogram.  The learned features are concatenated and fed into the subsequent convolutional layers. To learn the final emotional representation, a novel attention pooling method is further proposed. Compared with the existing pooling methods, such as max-pooling and average-pooling, the proposed attention pooling can effectively incorporate class-agnostic bottom-up, and class-specific top-down, attention maps.  We conduct extensive evaluations on benchmark IEMOCAP data to assess the effectiveness of the proposed representation. Results demonstrate a recognition performance of 71.8\% weighted accuracy (WA) and 68\%  unweighted accuracy (UA) over four emotions, which outperforms the state-of-the-art method by about 3\% absolute for WA and 4\% for UA.},
  eventtitle = {Interspeech 2018},
  language = {en},
  keywords = {_readed},
  annotation = {Accepted: 2018-06-03},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2018_An Attention Pooling based Representation Learning Method for Speech Emotion.pdf}
}

@article{liberatoreNativeNonnativeVoiceConversion2021,
  title = {Native-{{Nonnative Voice Conversion}} by {{Residual Warping}} in a {{Sparse}}, {{Anchor-Based Representation}}},
  author = {Liberatore, Christopher},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {3040--3051},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2021.3111568},
  abstract = {Voice conversion (VC) techniques can be used to synthesize utterances from second language learners to appear as if they have a native accent, providing learners with an ideal target to imitate in pronunciation training. In prior work, we presented a low-resource technique called SABR (Sparse, Anchor-Based Representation of Speech), which uses acoustic ``anchors''\textemdash one per English phoneme\textemdash to represent an utterance as a sparse, linear combination of nonnegative weights. SABR produces intelligible speech, but its compact size limits the acoustic quality of the synthesis, in large part due to the significant residual left out by the compact model. In this article, we propose SABR+Res, which uses a linear combination of frequency warp transforms to convert the source residual spectrum to be closer to that of the target speaker and use it in synthesis. We evaluate the proposed method on speakers from the ARCTIC and L2-ARCTIC databases and compare them to state-of-the-art exemplar and frequency-warping VC methods. We find that SABR+Res had the lowest objective VC error for native-to-nonnative conversion and was preferred in subjective tests. Additionally, when compared to the baseline systems, SABR+Res had a much higher synthesis quality on native-to-nonnative speakers, performing similarly to native-to-native speaker pairs. We discuss the implications for the residual warping system and applying the residual transform to other exemplar-based systems.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {\#nosource,Acoustics,Dictionaries,Exemplar,Frequency conversion,Frequency synthesizers,frequency warping,residual,sparse representation,Speech processing,Training,Transforms,voice conversion}
}

@inproceedings{liConfidenceEstimationSpeech2022,
  title = {Confidence {{Estimation}} for {{Speech Emotion Recognition Based}} on the {{Relationship Between Emotion Categories}} and {{Primitives}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Li, Yang and Papayiannis, Constantinos and Rozgic, Viktor and Shriberg, Elizabeth and Wang, Chao},
  date = {2022},
  pages = {7352--7356},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746930},
  abstract = {Confidence estimation for Speech Emotion Recognition (SER) is instrumental in improving the reliability in the behavior of downstream applications. In this work we propose (1) a novel confidence metric for SER based on the relationship between emotion primitives: arousal, valence, and dominance (AVD) and emotion categories (ECs), (2) EmoConfidNet - a DNN trained alongside the EC recognizer to predict the proposed confidence metric, and (3) a data filtering technique used to enhance the training of EmoConfidNet and the EC recognizer. For each training sample, we calculate distances from corresponding AVD annotation vectors to centroids of each EC in the AVD space, and define EC confidences as functions of the evaluated distances. EmoConfidNet is trained to predict confidence from the same acoustic representations used to train the EC recognizer. EmoConfidNet outperforms state-of-the-art confidence estimation methods on the MSP-Podcast and IEMOCAP datasets. For a fixed EC recognizer, after we reject the same number of low confidence predictions using EmoConfidNet, we achieve a higher F1 and unweighted average recall (UAR) than when rejecting using other methods.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Annotations,Confidence estimation,Data preprocessing,Emotion categories,Emotion primitives,Emotion recognition,Estimation,Instruments,Signal processing,Speech emotion recognition,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2022_Confidence Estimation for Speech Emotion Recognition Based on the Relationship.pdf}
}

@inproceedings{liContextawareMultimodalFusion2022,
  title = {Context-Aware {{Multimodal Fusion}} for {{Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Li, Jinchao and Wang, Shuai and Chao, Yang and Liu, Xunying and Meng, Helen},
  date = {2022-09-18},
  pages = {2013--2017},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10592},
  url = {https://www.isca-speech.org/archive/interspeech_2022/li22v_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2022_Context-aware Multimodal Fusion for Emotion Recognition.pdf}
}

@inproceedings{liContrastiveUnsupervisedLearning2021,
  title = {Contrastive {{Unsupervised Learning}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Li, Mao and Yang, Bo and Levy, Joshua and Stolcke, Andreas and Rozgic, Viktor and Matsoukas, Spyros and Papayiannis, Constantinos and Bone, Daniel and Wang, Chao},
  date = {2021-06},
  pages = {6329--6333},
  issn = {2379-190X},
  doi = {10/gmr2jp},
  abstract = {Speech emotion recognition (SER) is a key technology to enable more natural human-machine communication. However, SER has long suffered from a lack of public large-scale labeled datasets. To circumvent this problem, we investigate how unsupervised representation learning on unlabeled datasets can benefit SER. We show that the contrastive predictive coding (CPC) method can learn salient representations from unlabeled datasets, which improves emotion recognition performance. In our experiments, this method achieved state-of-the-art concordance correlation coefficient (CCC) performance for all emotion primitives (activation, valence, and dominance) on IEMOCAP. Additionally, on the MSP-Podcast dataset, our method obtained considerable performance improvements compared to baselines.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Conferences,Contrastive predictive coding,Correlation coefficient,Emotion recognition,Predictive coding,Signal processing,Speech emotion recognition,Speech recognition,Unsupervised pre-training,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Contrastive Unsupervised Learning for Speech Emotion Recognition.pdf}
}

@inproceedings{liControllableEmotionTransfer2021,
  title = {Controllable {{Emotion Transfer For End-to-End Speech Synthesis}}},
  booktitle = {2021 12th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  author = {Li, Tao and Yang, Shan and Xue, Liumeng and Xie, Lei},
  date = {2021-01-24},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Hong Kong}},
  doi = {10.1109/ISCSLP49672.2021.9362069},
  url = {https://ieeexplore.ieee.org/document/9362069/},
  urldate = {2021-09-10},
  abstract = {Emotion embedding space learned from references is a straightforward approach for emotion transfer in encoder-decoder structured emotional text to speech (TTS) systems. However, the transferred emotion in the synthetic speech is not accurate and expressive enough with emotion category confusions. Moreover, it is hard to select an appropriate reference to deliver desired emotion strength. To solve these problems, we propose a novel approach based on Tacotron. First, we plug two emotion classifiers \textendash{} one after the reference encoder, one after the decoder output \textendash{} to enhance the emotion-discriminative ability of the emotion embedding and the predicted mel-spectrum. Second, we adopt style loss to measure the difference between the generated and reference mel-spectrum. The emotion strength in the synthetic speech can be controlled by adjusting the value of the emotion embedding as the emotion embedding can be viewed as the feature map of the mel-spectrum. Experiments on emotion transfer and strength control have shown that the synthetic speech of the proposed method is more accurate and expressive with less emotion category confusions and the control of emotion strength is more salient to listeners.},
  eventtitle = {2021 12th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  isbn = {978-1-72816-994-1},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Controllable Emotion Transfer For End-to-End Speech Synthesis.pdf}
}

@inproceedings{liCoupledDiscriminantSubspace2022,
  title = {Coupled {{Discriminant Subspace Alignment}} for {{Cross-database Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Li, Shaokai and Song, Peng and Zhao, Keke and Zhang, Wenjing and Zheng, Wenming},
  date = {2022-09-18},
  pages = {4695--4699},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-40},
  url = {https://www.isca-speech.org/archive/interspeech_2022/li22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2022_Coupled Discriminant Subspace Alignment for Cross-database Speech Emotion.pdf}
}

@article{liDetectionMultipleSteganography2021,
  title = {Detection of {{Multiple Steganography Methods}} in {{Compressed Speech Based}} on {{Code Element Embedding}}, {{Bi-LSTM}} and {{CNN With Attention Mechanisms}}},
  author = {Li, Songbin and Wang, Jingang and Liu, Peng and Wei, Miao and Yan, Qiandong},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1556--1569},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3074752},
  abstract = {Steganographic algorithms in low-bit-rate compressed speech bring convenience to realize covert communication, meanwhile result in safety issues. The existing steganalysis methods are normally designed for one specific category of steganographic methods, thus lacking generalization capability. In this paper, we propose a general steganalysis method based on code element (CE) embedding, Bi-LSTM and CNN with attention mechanisms. Firstly, CEs in each frame are converted to a multi-hot vector. And each multi-hot vector will be mapped into a fixed-length embedding vector to get a more compact representation by utilizing dictionaries. Then, Bi-LSTM and CNN are applied to extract the contextual information and the local characteristics respectively of these embedding vectors. In addition, the attention mechanisms are introduced in different layers of the network to assign different weights to the output feature within each layer. Finally, the prediction results can be generated by the fully connected layer. Experimental results show that our method performs better than the existing steganalysis methods for detecting multiple steganography methods in the low-bit-rate compressed speech streams.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Bi-LSTM,CNN,Compressed speech,Correlation,Dictionaries,Feature extraction,Indexes,Speech coding,Speech processing,steganalysis,Support vector machines},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Detection of Multiple Steganography Methods in Compressed Speech Based on Code.pdf}
}

@inproceedings{liFusingASROutputs2022,
  title = {Fusing {{ASR Outputs}} in {{Joint Training}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Li, Yuanchao and Bell, Peter and Lai, Catherine},
  date = {2022},
  pages = {7362--7366},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746289},
  abstract = {Alongside acoustic information, linguistic features based on speech transcripts have been proven useful in Speech Emotion Recognition (SER). However, due to the scarcity of emotion labelled data and the difficulty of recognizing emotional speech, it is hard to obtain reliable linguistic features and models in this research area. In this paper, we propose to fuse Automatic Speech Recognition (ASR) outputs into the pipeline for joint training SER. The relationship between ASR and SER is understudied, and it is unclear what and how ASR features benefit SER. By examining various ASR outputs and fusion methods, our experiments show that in joint ASR-SER training, incorporating both ASR hidden and text output using a hierarchical co-attention fusion approach improves the SER performance the most. On the IEMOCAP corpus, our approach achieves 63.4\% weighted accuracy, which is close to the baseline results achieved by combining ground-truth transcripts. In addition, we also present novel word error rate analysis on IEMOCAP and layer-difference analysis of the Wav2vec 2.0 model to better understand the relationship between ASR and SER.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Automatic speech recognition,Emotion recognition,Fuses,Linguistics,Multi-task learning,Pipelines,Signal processing,Speech emotion recognition,Speech recognition,Training,Wav2vec 2.0},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2022_Fusing ASR Outputs in Joint Training for Speech Emotion Recognition.pdf}
}

@article{liIntelligibilityEnhancementNormaltoLombard2021,
  title = {Intelligibility {{Enhancement Via Normal-to-Lombard Speech Conversion With Long Short-Term Memory Network}} and {{Bayesian Gaussian Mixture Model}}},
  author = {Li, Gang and Wang, Xiaochen and Hu, Ruimin and Zhang, Huyin and Ke, Shanfa},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {3035--3047},
  issn = {1941-0077},
  doi = {10.1109/tmm.2021.3068565},
  abstract = {Speech communications and interactions frequently occur in a variety of environments. Noise in the environment significantly degrades speech intelligibility when speaking and listening. Especially in the listening stage, even if the multimedia terminal outputs clean speech, it is still difficult for listeners to obtain information. Intelligibility enhancement (IENH) of speech is a technique for overcoming the environmental noise in the listening stage. It implements a perceptual enhancement of non-noisy speech. This study focuses on IENH via normal-to-Lombard speech conversion, inspired by a well known acoustic mechanism named the Lombard effect. Our method combines the long short-term memory (LSTM) network and Bayesian Gaussian mixture model (BGMM) to build a conversion architecture. Compared with baselines, it has three main advantages: 1) an LSTM network is used for spectral tilt mapping with fully considering short-term correlations and high-dimensional expression abilities; 2) the aperiodicity (AP) is mapped together with the fundamental frequency (\$F\_0\$) by a BGMM, which considers their relevance constraints and the importance of APs; 3) the gender-dependent mapping is used for \$F\_0\$ and APs to consider distribution differences between genders. Experiments indicate that our method gets better performance in both objective and subjective tests.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {_Waiting for read,Computer architecture,Delays,Environmental noise,feature mapping,intelligibility enhancement (IENH),lombard effect,Real-time systems,speech conversion,Speech enhancement,Telephone sets,Vocoders,Working environment noise},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Intelligibility Enhancement Via Normal-to-Lombard Speech Conversion With Long.pdf}
}

@article{LiJiYuDuoQingGanShuoHuaRenZiGuaYingDeQingGanYuYinHeChengYanJiu2018,
  title = {基于多情感说话人自适应的情感语音合成研究},
  author = {李, 葵 and 徐, 海青 and 吴, 立刚 and 梁, 翀},
  date = {2018},
  journaltitle = {湘潭大学自然科学学报},
  volume = {40},
  number = {04},
  pages = {39--44},
  issn = {1000-5900},
  doi = {10.13715/j.cnki.nsjxu.2018.04.009},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=XYDZ201804009&uniplatform=NZKPT&v=5TKuaK5bBK4mRIwG8n%25mmd2FRzS7HaMJl2zLPY7LFeVLV%25mmd2FjxlACL2MnS1NENJJ6DssOQk},
  urldate = {2021-09-10},
  abstract = {提出了一种基于多情感自适应的情感语音合成方法,其创新点在于,通过SAT过程从多个说话人的情感语音语料中获得情感语音的平均音模型,对目标说话人的情感数据进行自适应变换,构建目标情感的声学参数模型,从而达到合成出目标说话人的情感语音的效果.实验表明,本方提出的方法能够获得自然度和情感相似度均较好的合成情感语音.},
  issue = {04},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,emotional corpus,emotional speech synthesis,SAT,statistical parametric speech synthesis,情感语料库,统计参数语音合成},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\李 et al_2018_基于多情感说话人自适应的情感语音合成研究.pdf}
}

@article{LiJiYuPSOLAYuDCTDeQingGanYuYinHeChengFangFa2017,
  title = {基于PSOLA与DCT的情感语音合成方法},
  author = {李, 勇 and 魏, 珰 and 王, 柳渝},
  date = {2017},
  journaltitle = {计算机工程},
  volume = {43},
  number = {12},
  pages = {278-282+291},
  issn = {1000-3428},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2018&filename=JSJC201712050&uniplatform=NZKPT&v=0WWGj653Dv4t%25mmd2BUjeyt3FF6KEORRDRq0FbcVrwbYCCGYzIXSBFB%25mmd2F%25mmd2Fn4IEsZiYTgo%25mmd2F},
  urldate = {2021-09-10},
  abstract = {情感语音合成可以增强语音的表现力,为使合成的情感语音更自然,提出一种结合时域基音同步叠加（PSOLA）和离散余弦变换（DCT）的情感语音合成方法。根据情感语音数据库中的高兴、悲伤、中性语音进行韵律参数分析归纳情感规则,调整中性语音各音节的基音频率、能量和时长。使用DCT方法对基音标记过的语音段进行基音频率的调整,并利用PSOLA算法修改基音频率使其逼近目标情感语音的基频。实验结果表明,该方法比单独使用PSOLA算法合成的情感语音更具情感色彩,其主观情感的识别率更高,合成的情感语音质量更好。},
  issue = {12},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,Discrete Cosine Transform(DCT),duration,emotional speech synthesis,energy,fundamental frequency,Pitch Synchronous Overlap Add(PSOLA),基音同步叠加,基频,时长,离散余弦变换,能量},
  annotation = {7 citations(CNKI)[2021-9-10]{$<$}北大核心, CSCD{$>$}},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\李 et al_2017_基于PSOLA与DCT的情感语音合成方法.pdf}
}

@thesis{LiJiYuShenDuXueXiDeYuYinQingGanShiBieYanJiu2019,
  type = {硕士},
  title = {基于深度学习的语音情感识别研究},
  author = {李, 鹏程},
  date = {2019},
  institution = {{中国科学技术大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201902&filename=1019074507.nh&uniplatform=NZKPT&v=s9jxo9yqL5uhqDOb%25mmd2FbCH4GJysNCJdhXz87i7K3VKv0qugNkAq%25mmd2FbEhNYJJNSsRLtw},
  urldate = {2021-09-10},
  abstract = {语音情感识别是从给定语音段中自动获取情感类型的技术,随着商业和教育等领域对情感识别需求的不断增加,发展高准确率的语音情感识别系统成为了语音领域的一个热门研究方向。而基于深度学习的语音情感识别方法,特别是卷积神经网络（Convolutional Neural Network,CNN）在这一领域得到了研究者们的重视,一些初步研究取得了良好效果,表现出比较大的研究潜力。然而对于CNN语音情感识别模型仍然有几个方面需要研究和探索,首先,什么样的特征更适合于CNN语音情感识别模型。其次,如何合理地设计网络结构,使其能有效学习到情感区分性信息,最后数据不足的问题是制约语音情感识别发展的一大难题,因此如何利用更多辅助数据来提高识别准确率的方法有待于去探索。针对这这些问题,本文展开了研究和实验。为了探究特征对情感识别的影响,本文首先建立了一个端到端CNN语音情感识别系统,并且在多种特征上进行实验,发现语谱图特征对于语音情感识别有着最佳的性能。在此基础之上本文进一步对于语谱图的不同频域段进行了探索,发现低频率段对于情感识别有着重要的作用。同时本文还对不同情感类别的CNN的激活值进行了研究,以探索不同高层特征区别。这些研究有助于理解不同情感在时频区域上的分布特性。为了进一步利用CNN模型所输出的高层时频信息,产生更有效的情感区分性特征,本文使用双线性池化方法来对CNN语音情感识别模型的高层特征进行建模,它能计算情感高层特征各维度之间的相关性,产生更为丰富的情感特征表示。然而由于情感数据集规模的限制,造成双线性池化的训练较为困难,因此本文使用分解双线性池化对输出特征进行降维,明显提升了语音情感识别的准确率。基于双线性池化理论,本文又进一步提出了基于注意力机制的注意力池化语音情感识别模型,通过引入自顶向下和自底向上注意力权重图,使得各个情感类别得到更好的区分,提升了识别性能。为了利用额外信息,解决情感识别数据不足的问题,同时提高识别准确率,本文提出了利用音素信息和说话人信息的语音情感识别方法。针对音素特征,使用了具有两个分支的CNN网络来协同训练语音和音素特征。针对说话人信息,本文提出了使用残差适应模型进行说话人到情感的域适应。这一方法先通过带有说话人标签的数据集训练一个深度残差网络,再使用情感数据集训练适应模块,以此利用说话人数据集提供的辅助信息来提升情感识别性能。实验验证表明利用音素和说话人的情感识别模型均大幅超过仅利用语音特征的模型。},
  editora = {宋, 彦},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,auxiliary of phoneme and speaker,bilinear pooling,deep learning,Speech emotion recognition,time-frequency information utilization,双线性池化,时频信息利用,深度学习,语音情感识别,音素和说话人辅助},
  annotation = {5 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\李_2019_基于深度学习的语音情感识别研究.pdf}
}

@article{liJointLocalGlobal2021,
  title = {Joint {{Local}} and {{Global Information Learning With Single Apex Frame Detection}} for {{Micro-Expression Recognition}}},
  author = {Li, Yante and Huang, Xiaohua and Zhao, Guoying},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {249--263},
  issn = {1941-0042},
  doi = {10.1109/tip.2020.3035042},
  abstract = {Micro-expressions (MEs) are rapid and subtle facial movements that are difficult to detect and recognize. Most recent works have attempted to recognize MEs with spatial and temporal information from video clips. According to psychological studies, the apex frame conveys the most emotional information expressed in facial expressions. However, it is not clear how the single apex frame contributes to micro-expression recognition. To alleviate that problem, this paper firstly proposes a new method to detect the apex frame by estimating pixel-level change rates in the frequency domain. With frequency information, it performs more effectively on apex frame spotting than the currently existing apex frame spotting methods based on the spatio-temporal change information. Secondly, with the apex frame, this paper proposes a joint feature learning architecture coupling local and global information to recognize MEs, because not all regions make the same contribution to ME recognition and some regions do not even contain any emotional information. More specifically, the proposed model involves the local information learned from the facial regions contributing major emotion information, and the global information learned from the whole face. Leveraging the local and global information enables our model to learn discriminative ME representations and suppress the negative influence of unrelated regions to MEs. The proposed method is extensively evaluated using CASME, CASME II, SAMM, SMIC, and composite databases. Experimental results demonstrate that our method with the detected apex frame achieves considerably promising ME recognition performance, compared with the state-of-the-art methods employing the whole ME sequence. Moreover, the results indicate that the apex frame can significantly contribute to micro-expression recognition.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  language = {en},
  keywords = {_Waiting for read,3D FFT,Character recognition,Databases,deep learning,Deep learning,Face recognition,facial expression recognition,Feature extraction,Frequency-domain analysis,Micro-expression,multi-instance learning,Optical imaging},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Joint Local and Global Information Learning With Single Apex Frame Detection.pdf}
}

@inproceedings{liLearningFineGrainedCross2021,
  title = {Learning {{Fine-Grained Cross Modality Excitement}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Li, Hang and Ding, Wenbiao and Wu, Zhongqin and Liu, Zitao},
  date = {2021-08-30},
  pages = {3375--3379},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-158},
  url = {https://www.isca-speech.org/archive/interspeech_2021/li21j_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition.pdf}
}

@inproceedings{liMultilingualPhoneticDataset2021,
  title = {Multilingual {{Phonetic Dataset}} for {{Low Resource Speech Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Li, Xinjian and Mortensen, David R. and Metze, Florian and Black, Alan W},
  date = {2021-06},
  pages = {6958--6962},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413720},
  abstract = {Phone Recognition is one of the most important tasks in the field of multilingual speech recognition, especially for low-resource languages whose orthographies are not available. However, most speech recognition datasets so far only focus on high-resource languages, there are very few datasets available for low-resource languages, especially datasets with detailed phone annotation. In this work, we present a large multilingual phonetic dataset, which is preprocessed and aligned from the UCLA phonetic dataset. The dataset contains around 100 low-resource languages and 7000 utterances in total. This dataset would provide an ideal training/evaluation set for universal phone recognition.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_reading,Acoustics,Annotations,Conferences,Low-Resource Speech recognition,Multilingual Phonetic Dataset,Multilingual Speech Alignment,Phonetics,Signal processing,Speech processing,Speech recognition},
  annotation = {ECC: 0000001},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Multilingual Phonetic Dataset for Low Resource Speech Recognition.pdf}
}

@article{liMultiMetricOptimizationUsing2021,
  title = {Multi-{{Metric Optimization Using Generative Adversarial Networks}} for {{Near-End Speech Intelligibility Enhancement}}},
  author = {Li, Haoyu and Yamagishi, Junichi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {3000--3011},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3111566},
  abstract = {The intelligibility of speech severely degrades in the presence of environmental noise and reverberation. In this paper, we propose a novel deep learning based system for modifying the speech signal to increase its intelligibility under the equal-power constraint, i.e., signal power before and after modification must be the same. To achieve this, we use generative adversarial networks (GANs) to obtain time-frequency dependent amplification factors, which are then applied to the input raw speech to reallocate the speech energy. Instead of optimizing only a single, simple metric, we train a deep neural network (DNN) model to simultaneously optimize multiple advanced speech metrics, including both intelligibility- and quality-related ones, which results in notable improvements in performance and robustness. Our system can not only work in non-real-time mode for offline audio playback but also support practical real-time speech applications. Experimental results using both objective measurements and subjective listening tests indicate that the proposed system significantly outperforms state-of-the-art baseline systems under various noisy and reverberant listening conditions.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Generative adversarial networks,Indexes,Measurement,multi-metric optimization,Noise measurement,Optimization,Reverberation,Speech enhancement,speech intelligibility},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li_Yamagishi_2021_Multi-Metric Optimization Using Generative Adversarial Networks for Near-End.pdf;C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li_Yamagishi_2021_Multi-Metric Optimization Using Generative Adversarial Networks for Near-End2.pdf}
}

@article{linATCSpeechNetMultilingualEndtoend2021,
  title = {{{ATCSpeechNet}}: {{A}} Multilingual End-to-End Speech Recognition Framework for Air Traffic Control Systems},
  shorttitle = {{{ATCSpeechNet}}},
  author = {Lin, Yi and Yang, Bo and Li, Linchao and Guo, Dongyue and Zhang, Jianwei and Chen, Hu and Zhang, Yi},
  date = {2021-11-01},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {112},
  pages = {107847},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2021.107847},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494621007699},
  urldate = {2021-09-28},
  abstract = {In this paper, a multilingual end-to-end framework, called ATCSpeechNet, is proposed to tackle the issue of translating communication speech into human-readable text in air traffic control (ATC) systems. In the proposed framework, we focus on integrating multilingual automatic speech recognition (ASR) into one model, in which an end-to-end paradigm is developed to convert speech waveforms into text directly, without any feature engineering or lexicon. To compensate the deficiency of handcrafted feature engineering caused by ATC challenges, including multilingual, multispeaker dialog and unstable speech rates, a speech representation learning (SRL) network is proposed to capture robust and discriminative speech representations from raw waves. The self-supervised training strategy is adopted to optimize the SRL network from unlabeled data, and to further predict the speech features, i.e., wave-to-feature. An end-to-end architecture is improved to complete the ASR task, in which a grapheme-based modeling unit is applied to address the multilingual ASR issue. Facing the problem of small transcribed samples in the ATC domain, an unsupervised approach with mask prediction is applied to pretrain the backbone network of the ASR model on unlabeled data by a feature-to-feature process. Finally, by integrating the SRL with ASR, an end-to-end multilingual ASR framework is formulated in a supervised manner, which is able to translate the raw wave into text in one model, i.e., wave-to-text. Experimental results on the ATCSpeech corpus demonstrate that the proposed approach achieves high performance with a very small labeled corpus and less resource consumption, only a 4.20\% label error rate on the 58-hour transcribed corpus. Compared to the baseline model, the proposed approach obtains over 100\% relative performance improvement which can be further enhanced with increasing size of the transcribed samples. It is also confirmed that the proposed SRL and training strategies make significant contributions to improving the final performance. In addition, the effectiveness of the proposed framework is also validated on common corpora (AISHELL, LibriSpeech, and cv-fr). More importantly, the proposed multilingual framework not only reduces the system complexity but also obtains higher accuracy compared to that of the independent monolingual ASR models. The proposed approach can also greatly reduce the cost of annotating samples, which benefits to advance the ASR technique to industrial applications.},
  language = {en},
  keywords = {_Waiting for read,Air traffic control,End-to-end,Multilingual,Pretraining,Representation learning,Small samples,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lin et al_2021_ATCSpeechNet.pdf}
}

@inproceedings{linDeepemoclusterSemiSupervisedFramework2021,
  title = {Deepemocluster: A {{Semi-Supervised Framework}} for {{Latent Cluster Representation}} of {{Speech Emotions}}},
  shorttitle = {Deepemocluster},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lin, Wei-Cheng and Sridhar, Kusha and Busso, Carlos},
  date = {2021-06},
  pages = {7263--7267},
  issn = {2379-190X},
  doi = {10/gmr2js},
  abstract = {Semi-supervised learning (SSL) is an appealing approach to resolve generalization problem for speech emotion recognition (SER) systems. By utilizing large amounts of unlabeled data, SSL is able to gain extra information about the prior distribution of the data. Typically, it can lead to better and robust recognition performance. Existing SSL approaches for SER include variations of encoder-decoder model structures such as autoencoder (AE) and variational autoecoders (VAEs), where it is difficult to interpret the learning mechanism behind the latent space. In this study, we introduce a new SSL framework, which we refer to as the DeepEmoCluster framework, for attribute-based SER tasks. The DeepEmoCluster framework is an end-to-end model with mel-spectrogram inputs, which combines a self-supervised pseudo labeling classification network with a supervised emotional attribute regressor. The approach encourages the model to learn latent representations by maximizing the emotional separation of K-means clusters. Our experimental results based on the MSP-Podcast corpus indicate that the DeepEmoCluster framework achieves competitive prediction performances in fully supervised scheme, outperforming baseline methods in most of the conditions. The approach can be further improved by incorporating extra unlabeled set. Moreover, our experimental results explicitly show that the latent clusters have emotional dependencies, enriching the geometric interpretation of the clusters.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_reading,Conferences,Emotion recognition,Labeling,Semi-supervised learning (SSL),Semisupervised learning,Signal processing,speech emotion recognition (SER),Speech recognition,Training,unsupervised clusters},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lin et al_2021_Deepemocluster.pdf}
}

@inproceedings{linSpeechEmotionRecognition2005,
  title = {Speech Emotion Recognition Based on {{HMM}} and {{SVM}}},
  booktitle = {2005 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  author = {Lin, Yi-Lin and Wei, Gang},
  date = {2005-08},
  volume = {8},
  pages = {4898-4901 Vol. 8},
  issn = {2160-1348},
  doi = {10.1109/ICMLC.2005.1527805},
  abstract = {Automatic emotion recognition in speech is a current research area with a wide range of applications in human-machine interactions. This paper uses two classification methods, the hidden Markov model (HMM) and the support vector machine (SVM), to classify five emotional states: anger, happiness, sadness, surprise and a neutral state. In the HMM method, 39 candidate instantaneous features were extracted, and the sequential forward selection (SFS) method was used to find the best feature subset. The classification performance of the selected feature subset was then compared with that of the Mel frequency cepstrum coefficients (MFCC). Within the method based on SVM, a new vector measuring the difference between Mel frequency scale sub-bands energies is proposed. The performance of the K-nearest neighbors (KNN) classifier using the proposed vector was also investigated. Both gender dependent and gender independent experiments were conducted on the Danish emotional speech (DES) database. The recognition rates by the HMM classifier were 98.9\% for female subjects, 100\% for male subjects, and 99.5\% for gender independent cases. When the SVM classifier and the proposed feature vector were employed, correct classification rates of 89.4\%, 93.6\% and 88.9\% were obtained for male, female and gender independent cases respectively.},
  eventtitle = {2005 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  keywords = {Cepstrum,Emotion recognition,Energy measurement,Feature extraction,Hidden Markov Model,Hidden Markov models,Man machine systems,Mel energy spectrum dynamics coefficients,Mel frequency cepstral coefficient,Sequential Forward Selection,Speech,Support Vector Machine,Support vector machine classification,Support vector machines},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\2ICCL6H9\\1527805.html}
}

@article{liRecurrentNeuralNetworks2021,
  title = {Recurrent {{Neural Networks}} and {{Acoustic Features}} for {{Frame-Level Signal-to-Noise Ratio Estimation}}},
  author = {Li, Hao and Wang, DeLiang and Zhang, Xueliang and Gao, Guanglai},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2878--2887},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3107617},
  abstract = {It is important to know the presence and the relative level of background noise for many speech processing tasks. Frame-level signal-to-noise ratio (SNR) provides a measure of instantaneous noise level of a noisy signal, and its estimation has been researched for decades. This problem can be approached from a supervised learning perspective by predicting SNR from features of noisy speech. In this study, we introduce a deep learning algorithm for frame-level SNR estimation. The proposed algorithm employs recurrent neural networks (RNNs) with long short-term memory (LSTM) to leverage contextual information. We also systematically examine a range of acoustic features and investigate feature combinations using Group Lasso and sequential floating forward selection (SFFS). The proposed algorithm naturally leads to an utterance-level SNR estimator. Systematical evaluations show that the proposed algorithm provides an accurate estimate of frame-level SNR, as well as utterance-level SNR, under different noise conditions, outperforming other estimators.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,Estimation,feature combination,Feature extraction,Frame-level SNR estimation,long short-term memory,Mel frequency cepstral coefficient,Noise measurement,recurrent neural networks,Signal to noise ratio,Speech enhancement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Recurrent Neural Networks and Acoustic Features for Frame-Level Signal-to-Noise.pdf}
}

@article{liSpeakerClusteringCoOptimizing2021,
  title = {Speaker {{Clustering}} by {{Co-Optimizing Deep Representation Learning}} and {{Cluster Estimation}}},
  author = {Li, Yanxiong and Wang, Wucheng and Liu, Mingle and Jiang, Zhongjie and He, Qianhua},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {3377--3387},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.3024667},
  abstract = {Speaker clustering is a task to merge speech segments uttered by the same speaker into a single cluster, which is an effective tool for alleviating the management of massive amount of audio documents. In this paper, we present a work for co-optimizing the two main steps of speaker clustering, namely, feature learning and cluster estimation. In our method, the deep representation feature is learned by a deep convolutional autoencoder network (DCAN), while the cluster estimation is realized by a softmax layer that is combined with the DCAN. We devise an integrated loss function to simultaneously minimize the reconstruction loss (for deep representation learning) and the clustering loss (for cluster estimation). Many state-of-the-art audio features and clustering methods are evaluated on experimental datasets selected from two publicly available speech corpora (the AISHELL-2 and the VoxCeleb1). The results show that the proposed method exceeds other speaker clustering methods in regard to the normalized mutual information (NMI) and the clustering accuracy (CA). Additionally, the proposed deep representation feature outperforms other features that were widely used in previous works, in terms of both NMI and CA.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {_Waiting for read,audio document analysis,Clustering algorithms,Clustering methods,Decoding,deep convolutional autoencoder network,deep representation,Estimation,Feature extraction,Neural networks,Speaker clustering,Speaker recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Speaker Clustering by Co-Optimizing Deep Representation Learning and Cluster.pdf}
}

@inproceedings{liSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition}} via {{Multi-Level Cross-Modal Distillation}}},
  booktitle = {Interspeech 2021},
  author = {Li, Ruichen and Zhao, Jinming and Jin, Qin},
  date = {2021-08-30},
  pages = {4488--4492},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-785},
  url = {https://www.isca-speech.org/archive/interspeech_2021/li21p_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Speech Emotion Recognition via Multi-Level Cross-Modal Distillation.pdf}
}

@article{liTwoHeadsAre2021,
  title = {Two {{Heads}} Are {{Better Than One}}: {{A Two-Stage Complex Spectral Mapping Approach}} for {{Monaural Speech Enhancement}}},
  shorttitle = {Two {{Heads}} Are {{Better Than One}}},
  author = {Li, Andong and Liu, Wenzhe and Zheng, Chengshi and Fan, Cunhang and Li, Xiaodong},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1829--1843},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3079813},
  abstract = {For challenging acoustic scenarios as low signal-to-noise ratios, current speech enhancement systems usually suffer from performance bottleneck in extracting the target speech from the mixtures within one step. To address this issue, we propose a novel complex spectral mapping approach with a two-stage pipeline for monaural speech enhancement in the time-frequency domain. The proposed algorithm aims to decouple the primal problem into multiple sub-problems, which follows the classic proverb, ``two heads are better than one''. More specifically, in the first stage, only magnitude is estimated, which is incorporated with the noisy phase to obtain a coarse complex spectrum estimation. To facilitate the previous estimation, in the second stage, an auxiliary network serves as the post-processing module, where residual noise is further suppressed and the phase information is effectively modified. The global residual connection strategy is adopted in the second stage to accelerate the training convergence speed. To alleviate the parameter burden caused by the multi-stage pipeline, we propose a light-weight temporal convolutional module, which substantially decreases the trainable parameters and obtains even better objective performance over the original version. We conduct extensive experiments on three standard corpora, including WSJ0-SI84, DNS Challenge dataset, and Voice Bank + DEMAND dataset. Objective test results demonstrate that our proposed approach achieves state-of-the-art performance over previous advanced systems under various conditions. Meanwhile, subjective listening test results further validate the superiority of our proposed method in terms of subjective quality.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {complex domain,Estimation,global residual connection,Monaural speech enhancement,phase estimation,Pipelines,Reverberation,Speech enhancement,Task analysis,Time-frequency analysis,Training,two-stage},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Two Heads are Better Than One.pdf}
}

@article{liuAnytoManyVoiceConversion2021,
  title = {Any-to-{{Many Voice Conversion With Location-Relative Sequence-to-Sequence Modeling}}},
  author = {Liu, Songxiang and Cao, Yuewen and Wang, Disong and Wu, Xixin and Liu, Xunying and Meng, Helen},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1717--1728},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3076867},
  abstract = {This paper proposes an any-to-many location-relative, sequence-to-sequence (seq2seq), non-parallel voice conversion approach, which utilizes text supervision during training. In this approach, we combine a bottle-neck feature extractor (BNE) with a seq2seq synthesis module. During the training stage, an encoder-decoder-based hybrid connectionist-temporal-classification-attention (CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck layer. A BNE is obtained from the phoneme recognizer and is utilized to extract speaker-independent, dense and rich spoken content representations from spectral features. Then a multi-speaker location-relative attention based seq2seq synthesis model is trained to reconstruct spectral features from the bottle-neck features, conditioning on speaker representations for speaker identity control in the generated speech. To mitigate the difficulties of using seq2seq models to align long sequences, we down-sample the input spectral feature along the temporal dimension and equip the synthesis model with a discretized mixture of logistic (MoL) attention mechanism. Since the phoneme recognizer is trained with large speech recognition data corpus, the proposed approach can conduct any-to-many voice conversion. Objective and subjective evaluations show that the proposed any-to-many approach has superior voice conversion performance in terms of both naturalness and speaker similarity. Ablation studies are conducted to confirm the effectiveness of feature selection and model design strategies in the proposed approach. The proposed VC approach can readily be extended to support any-to-any VC (also known as one/few-shot VC), and achieve high performance according to objective and subjective evaluations.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Any-to-many,Computational modeling,Decoding,Feature extraction,Hidden Markov models,location relative attention,Pipelines,sequence-to-sequence modeling,Training,voice conversion},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Any-to-Many Voice Conversion With Location-Relative Sequence-to-Sequence.pdf}
}

@article{liuCombiningContextrelevantFeatures2022,
  title = {Combining Context-Relevant Features with Multi-Stage Attention Network for Short Text Classification},
  author = {Liu, Yingying and Li, Peipei and Hu, Xuegang},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101268},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101268},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000735},
  urldate = {2021-09-28},
  abstract = {Short text classification is a challenging task in natural language processing. Existing traditional methods using external knowledge to deal with the sparsity and ambiguity of short texts have achieved good results, but accuracy still needs to be improved because they ignore the context-relevant features. Deep learning methods based on RNN or CNN are hence becoming more and more popular in short text classification. However, RNN based methods cannot perform well in the parallelization which causes the lower efficiency, while CNN based methods ignore sequences and relationships between words, which causes the poorer effectiveness. Motivated by this, we propose a novel short text classification approach combining Context-Relevant Features with multi-stage Attention model based on Temporal Convolutional Network (TCN) and CNN, called CRFA. In our approach, we firstly use Probase as external knowledge to enrich the semantic representation for the solution to the data sparsity and ambiguity of short texts. Secondly, we design a multi-stage attention model based on TCN and CNN, where TCN is introduced to improve the parallelization of the proposed model for higher efficiency, and discriminative features are obtained at each stage through the fusion of attention and different-level CNN for a higher accuracy. Specifically, TCN is adopted to capture context-related features at word and concept levels, and meanwhile, in order to measure the importance of features, Word-level TCN (WTCN) based attention, Concept-level TCN (CTCN) based attention and different-level CNN are used at each stage to focus on the information of more important features. Finally, experimental studies demonstrate the effectiveness and efficiency of our approach in the short text classification compared to several well-known short text classification approaches based on CNN and RNN.},
  language = {en},
  keywords = {Attention mechanism,Deep learning,Short text classification,Temporal Convolutional Network (TCN)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2022_Combining context-relevant features with multi-stage attention network for.pdf}
}

@inproceedings{liuDiscriminativeFeatureRepresentation2022,
  title = {Discriminative {{Feature Representation Based}} on {{Cascaded Attention Network}} with {{Adversarial Joint Loss}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Liu, Yang and Sun, Haoqin and Guan, Wenbo and Xia, Yuqi and Zhao, Zhen},
  date = {2022-09-18},
  pages = {4750--4754},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11480},
  url = {https://www.isca-speech.org/archive/interspeech_2022/liu22aa_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2022_Discriminative Feature Representation Based on Cascaded Attention Network with.pdf}
}

@article{liuExploitingMorphologicalPhonological2021,
  title = {Exploiting {{Morphological}} and {{Phonological Features}} to {{Improve Prosodic Phrasing}} for {{Mongolian Speech Synthesis}}},
  author = {Liu, Rui and Sisman, Berrak and Bao, Feilong and Yang, Jichen and Gao, Guanglai and Li, Haizhou},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {274--285},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3040523},
  abstract = {Prosodic phrasing is an important factor that affects naturalness and intelligibility in text-to-speech synthesis. Studies show that deep learning techniques improve prosodic phrasing when large text and speech corpus are available. However, for low-resource languages, such as Mongolian, prosodic phrasing remains a challenge for various reasons. First, the database suitable for system training is limited. Second, word composition knowledge that is prosody-informing has not been used in prosodic phrase modeling. To address these problems, in this article, we propose a feature augmentation method in conjunction with a self-attention neural classifier. We augment input text with morphological and phonological decompositions of words to enhance the text encoder. We study the use of self-attention classifier, that makes use of global context of a sentence, as a decoder for phrase break prediction. Both objective and subjective evaluations validate the effectiveness of the proposed phrase break prediction framework, that consistently improves voice quality in a Mongolian text-to-speech synthesis system.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Context modeling,Hidden Markov models,Linguistics,Mongolian speech synthesis,morphological and phonological features,phrase break prediction,Predictive models,prosodic phrasing,Recurrent neural networks,self-attention,Speech synthesis,Syntactics},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Exploiting Morphological and Phonological Features to Improve Prosodic Phrasing.pdf}
}

@article{liuExpressiveTTSTraining2021,
  title = {Expressive {{TTS Training With Frame}} and {{Style Reconstruction Loss}}},
  author = {Liu, Rui and Sisman, Berrak and Gao, Guanglai and Li, Haizhou},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1806--1818},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3076369},
  abstract = {We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system that improves the speech styling at utterance level. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn't require prosody annotations from training data. It doesn't attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. This study marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. It adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms the state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Decoding,emotion recognition,Expressive speech synthesis,frame and style reconstruction loss,Hidden Markov models,Loss measurement,Stress,tacotron,Task analysis,Training,Training data},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Expressive TTS Training With Frame and Style Reconstruction Loss.pdf}
}

@inproceedings{liuGraphIsomorphismNetwork2021,
  title = {Graph {{Isomorphism Network}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Liu, Jiawang and Wang, Haoxiang},
  date = {2021-08-30},
  pages = {3405--3409},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1154},
  url = {https://www.isca-speech.org/archive/interspeech_2021/liu21k_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu_Wang_2021_Graph Isomorphism Network for Speech Emotion Recognition.pdf}
}

@article{LiuJiYuTingGanLiangHuaBianMaDeShenJingWangLuoYuYinHeChengFangFaYanJiu2019,
  title = {基于听感量化编码的神经网络语音合成方法研究},
  author = {刘, 庆峰 and 江, 源 and 胡, 亚军 and 刘, 利娟},
  date = {2019},
  journaltitle = {电子科技},
  volume = {32},
  number = {09},
  pages = {76--79},
  issn = {1007-7820},
  doi = {10.16180/j.cnki.issn1007-7820.2019.09.016},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=DZKK201909017&uniplatform=NZKPT&v=tkYSEIMU%25mmd2BIezlc439UowbIzqoXWW1nxeeXdk5A8YWfnSgkoqKPREOL5AVW65KGxr},
  urldate = {2021-09-10},
  abstract = {针对当前神经网络声学建模中数据混用困难的问题,文中提出了一种基于听感量化编码的神经网络语音合成方法。通过设计听感量化编码模型学习海量语音在音色、语种、情感上的不同差异表征,构建统一的多人数据混合训练的神经网络声学模型。在统一的听感量化编码声学模型内通过数据共享和迁移学习,可以显著降低合成系统搭建的数据量要求,并实现对合成语音的音色、语种、情感等属性的有效控制。提升了神经网络语音合成的质量和灵活性,一小时数据构建语音合成系统自然度可达到4.0MOS分,达到并超过普通说话人水平。},
  issue = {09},
  language = {zh-CN},
  keywords = {_readed,_中文,cross-language,limited data,neural networks,perception quantification,speech synthesis,style control,听感量化编码,少数据量合成,情感控制,神经网络,语音合成,跨语种合成},
  annotation = {ECC: 0033085},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\刘 et al_2019_基于听感量化编码的神经网络语音合成方法研究.pdf}
}

@article{liuLanguageIndependentApproachAutomatic2021,
  title = {Language-{{Independent Approach}} for {{Automatic Computation}} of {{Vowel Articulation Features}} in {{Dysarthric Speech Assessment}}},
  author = {Liu, Yuanyuan and Penttil\"a, Nelly and Ihalainen, Tiina and Lintula, Juulia and Convey, Rachel and R\"as\"anen, Okko},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2228--2243},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3090973},
  abstract = {Imprecise vowel articulation can be observed in people with Parkinson's disease (PD). Acoustic features measuring vowel articulation have been demonstrated to be effective indicators of PD in its assessment. Standard clinical vowel articulation features of vowel working space area (VSA), vowel articulation index (VAI) and formants centralization ratio (FCR), are derived the first two formants of the three corner vowels /a/, /i/ and /u/. Conventionally, manual annotation of the corner vowels from speech data is required before measuring vowel articulation. This process is time-consuming. The present work aims to reduce human effort in clinical analysis of PD speech by proposing an automatic pipeline for vowel articulation assessment. The method is based on automatic corner vowel detection using a language universal phoneme recognizer, followed by statistical analysis of the formant data. The approach removes the restrictions of prior knowledge of speaking content and the language in question. Experimental results on a Finnish PD speech corpus demonstrate the efficacy and reliability of the proposed automatic method in deriving VAI, VSA, FCR and F2i/F2u (the second formant ratio for vowels /i/ and /u/). The automatically computed parameters are shown to be highly correlated with features computed with manual annotations of corner vowels. In addition, automatically and manually computed vowel articulation features have comparable correlations with experts' ratings on speech intelligibility, voice impairment and overall severity of communication disorder. Language-independence of the proposed approach is further validated on a Spanish PD database, PC-GITA, as well as on TORGO corpus of English dysarthric speech.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Annotations,automatic corner vowels detection,Diseases,dysarthria,Feature extraction,Manuals,Parkinson's diseases,phoneme recognition,Speech processing,Task analysis,vowel articulation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Language-Independent Approach for Automatic Computation of Vowel Articulation.pdf}
}

@inproceedings{liuMultimodalEmotionRecognition2021,
  title = {Multimodal {{Emotion Recognition}} with {{Capsule Graph Convolutional Based Representation Fusion}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liu, Jiaxing and Chen, Sen and Wang, Longbiao and Liu, Zhilei and Fu, Yahui and Guo, Lili and Dang, Jianwu},
  date = {2021-06},
  pages = {6339--6343},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413608},
  abstract = {Due to the more robust characteristics compared to unimodal, audio-video multimodal emotion recognition (MER) has attracted a lot of attention. The efficiency of representation fusion algorithm often determines the performance of MER. Although there are many fusion algorithms, information redundancy and information complementarity are usually ignored. In this paper, we propose a novel representation fusion method, Capsule Graph Convolutional Network (CapsGCN). Firstly, after unimodal representation learning, the extracted audio and video representations are distilled by capsule network and encapsulated into multimodal capsules respectively. Multimodal capsules can effectively reduce data redundancy by the dynamic routing algorithm. Secondly, the multimodal capsules with their inter-relations and intra-relations are treated as a graph structure. The graph structure is learned by Graph Convolutional Network (GCN) to get hidden representation which is a good supplement for information complementarity. Finally, the multimodal capsules and hidden relational representation learned by CapsGCN are fed to multihead self-attention to balance the contributions of source representation and relational representation. To verify the performance, visualization of representation, the results of commonly used fusion methods, and ablation studies of the proposed CapsGCN are provided. Our proposed fusion method achieves 80.83\% accuracy and 80.23\% F1 score on eNTERFACE05'.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {capsule networks,Conferences,Convolution,Emotion recognition,graph convolutional,Heuristic algorithms,multimodal emotion recognition,Redundancy,Sensitivity,Signal processing algorithms,VGG-16},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Multimodal Emotion Recognition with Capsule Graph Convolutional Based.pdf}
}

@misc{liUniFormerUnifiedTransformer2022,
  title = {{{UniFormer}}: {{Unified Transformer}} for {{Efficient Spatiotemporal Representation Learning}}},
  shorttitle = {{{UniFormer}}},
  author = {Li, Kunchang and Wang, Yali and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  date = {2022-02-08},
  number = {arXiv:2201.04676},
  eprint = {2201.04676},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.04676},
  url = {http://arxiv.org/abs/2201.04676},
  urldate = {2022-11-12},
  abstract = {It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1\&V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9\%/84.8\% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9\% and 71.2\% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer.},
  archiveprefix = {arXiv},
  language = {en},
  version = {3},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2022_UniFormer.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\4PGFNSDQ\\2201.html}
}

@article{liuRecentProgressCUHK2021,
  title = {Recent {{Progress}} in the {{CUHK Dysarthric Speech Recognition System}}},
  author = {Liu, Shansong and Geng, Mengzhe and Hu, Shoukang and Xie, Xurong and Cui, Mingyu and Yu, Jianwei and Liu, Xunying and Meng, Helen},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2267--2281},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3091805},
  abstract = {Despite the rapid progress of automatic speech recognition (ASR) technologies in the past few decades, recognition of disordered speech remains a highly challenging task to date. Disordered speech presents a wide spectrum of challenges to current data intensive deep neural networks (DNNs) based ASR technologies that predominantly target normal speech. This paper presents recent research efforts at the Chinese University of Hong Kong (CUHK) to improve the performance of disordered speech recognition systems on the largest publicly available UASpeech dysarthric speech corpus. A set of novel modelling techniques including neural architectural search, data augmentation using spectra-temporal perturbation, model based speaker adaptation and cross-domain generation of visual features within an audio-visual speech recognition (AVSR) system framework were employed to address the above challenges. The combination of these techniques produced the lowest published word error rate (WER) of 25.21\% on the UASpeech test set 16 dysarthric speakers, and an overall WER reduction of 5.4\% absolute (17.6\% relative) over the CUHK 2018 dysarthric speech recognition system featuring a 6-way DNN system combination and cross adaptation of out-of-domain normal speech data trained systems. Bayesian model adaptation further allows rapid adaptation to individual dysarthric speakers to be performed using as little as 3.06 seconds of speech. The efficacy of these techniques were further demonstrated on a CUDYS Cantonese dysarthric speech recognition task.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Adaptation models,data augmentation,Data models,Disordered speech recognition,multimodal speech recognition,Phonetics,speaker adaptation,Speech processing,Speech recognition,Task analysis,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Recent Progress in the CUHK Dysarthric Speech Recognition System.pdf}
}

@unpublished{liuReinforcementLearningEmotional2021,
  title = {Reinforcement {{Learning}} for {{Emotional Text-to-Speech Synthesis}} with {{Improved Emotion Discriminability}}},
  author = {Liu, Rui and Sisman, Berrak and Li, Haizhou},
  date = {2021-06-13},
  eprint = {2104.01408},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.01408},
  urldate = {2021-09-10},
  abstract = {Emotional text-to-speech synthesis (ETTS) has seen much progress in recent years. However, the generated voice is often not perceptually identifiable by its intended emotion category. To address this problem, we propose a new interactive training paradigm for ETTS, denoted as i-ETTS, which seeks to directly improve the emotion discriminability by interacting with a speech emotion recognition (SER) model. Moreover, we formulate an iterative training strategy with reinforcement learning to ensure the quality of i-ETTS optimization. Experimental results demonstrate that the proposed i-ETTS outperforms the state-of-the-art baselines by rendering speech with more accurate emotion style. To our best knowledge, this is the first study of reinforcement learning in emotional text-to-speech synthesis. Index Terms: Reinforcement Learning, Emotional Text-toSpeech Synthesis, Speech Emotion Recognition.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved.pdf}
}

@inproceedings{liuReinforcementLearningEmotional2021a,
  title = {Reinforcement {{Learning}} for {{Emotional Text-to-Speech Synthesis}} with {{Improved Emotion Discriminability}}},
  booktitle = {Interspeech 2021},
  author = {Liu, Rui and Sisman, Berrak and Li, Haizhou},
  date = {2021-08-30},
  pages = {4648--4652},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1236},
  url = {https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved2.pdf}
}

@article{liuReSynchronizationUsingHand2021,
  title = {Re-{{Synchronization Using}} the {{Hand Preceding Model}} for {{Multi-Modal Fusion}} in {{Automatic Continuous Cued Speech Recognition}}},
  author = {Liu, Li and Feng, Gang and Beautemps, Denis and Zhang, Xiao-Ping},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {292--305},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.2976493},
  abstract = {Cued Speech (CS) is an augmented lip reading system complemented by hand coding, and it is very helpful to the deaf people. Automatic CS recognition can help communications between the deaf people and others. Due to the asynchronous nature of lips and hand movements, fusion of them in automatic CS recognition is a challenging problem. In this work, we propose a novel re-synchronization procedure for multi-modal fusion, which aligns the hand features with lips feature. It is realized by delaying hand position and hand shape with their optimal hand preceding time which is derived by investigating the temporal organizations of hand position and hand shape movements in CS. This re-synchronization procedure is incorporated into a practical continuous CS recognition system that combines convolutional neural network (CNN) with multi-stream hidden markov model (MSHMM). A significant improvement of about 4.6\% has been achieved retaining 76.6\% CS phoneme recognition correctness compared with the state-of-the-art architecture (72.04\%), which did not take into account the asynchrony issue of multi-modal fusion in CS. To our knowledge, this is the first work to tackle the asynchronous multi-modal fusion in the automatic continuous CS recognition.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {_Waiting for read,automatic CS recognition,CNN,Cued speech,Encoding,Feature extraction,Hidden Markov models,Lips,MSHMM,multi-modal fusion,Organizations,re-synchronization procedure,Shape,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Re-Synchronization Using the Hand Preceding Model for Multi-Modal Fusion in.pdf}
}

@article{LiuShenKeBaWoXiJinPingWaiJiaoSiXiangDeShiDaiBeiJingKeXueNeiHanHeShiJianYaoQiu2021,
  title = {深刻把握习近平外交思想的时代背景、科学内涵和实践要求},
  author = {刘, 世强 and 朱, 光},
  date = {2021-11-21},
  journaltitle = {江苏大学学报(社会科学版)},
  pages = {1--17},
  issn = {1671-6604},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=JSLD20211109000&uniplatform=NZKPT&v=n87C4i8tN4eK9UfZKp7NxCY%25mmd2Bn%25mmd2FAZXGlbik3trtlYwTlo24tMaWH9lTmuu4%25mmd2BOu%25mmd2F%25mmd2Fz},
  urldate = {2021-11-21},
  abstract = {习近平外交思想是习近平新时代中国特色社会主义思想的重要组成部分。党的十八大以来,以习近平同志为核心的党中央统筹中华民族伟大复兴战略全局和世界百年未有之大变局,高举和平、发展、合作、共赢旗帜,大力推进外交理论与实践创新,形成了习近平外交思想这一马克思主义中国化的重要理论成果。习近平外交思想包括了我国对外工作的根本保证、历史使命、战略路径、外交布局、政治要求、精神追求等内容,为全方位推进新时代中国特色大国外交指明了方向。面对世界进入动荡变革期带来的新挑战和全面建设社会主义现代化国家开启的新征程,我们要以习近平外交思想为指引,进一步保持战略定力,坚定不移地走中国特色大国外交之路;进一步加强形势研判,...},
  language = {zh-CN},
  keywords = {⛔ No DOI found,global governance,international order,major-country diplomacy with Chinese characteristics,Xi Jinping Thought on Diplomacy,中国特色大国外交,习近平外交思想,全球治理,国际秩序},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\刘_朱_2021_深刻把握习近平外交思想的时代背景、科学内涵和实践要求.pdf}
}

@inproceedings{liuSpeechEmotionRecognition2021,
  title = {A {{Speech Emotion Recognition Framework}} for {{Better Discrimination}} of {{Confusions}}},
  booktitle = {Interspeech 2021},
  author = {Liu, Jiawang and Wang, Haoxiang},
  date = {2021-08-30},
  pages = {4483--4487},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-718},
  url = {https://www.isca-speech.org/archive/interspeech_2021/liu21n_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\FQSJCGKG\\Liu 和 Wang - 2021 - A Speech Emotion Recognition Framework for Better .pdf}
}

@article{liuSpeechPersonalityRecognition2021,
  title = {Speech {{Personality Recognition Based}} on {{Annotation Classification Using Log-Likelihood Distance}} and {{Extraction}} of {{Essential Audio Features}}},
  author = {Liu, Zhen-Tao and Rehman, Abdul and Wu, Min and Cao, Wei-Hua and Hao, Man},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {3414--3426},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.3025108},
  abstract = {Speech personality recognition relies on training models that require an excessive number of features and are, in most cases, designed specifically for certain databases. As a result, when tested on different datasets, overfitted classifier models are not always reliable because their accuracy changes with changes in the domain of speakers. Moreover, personality annotations are often subjective, which creates variability in raters perception during labeling. These problems inhibit the effectiveness of speech personality recognition applications. To reduce the unexplained variance caused by unknown differences in raters perception, a structure that uses Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm is proposed. Furthermore, a feature extraction method is proposed to filter out undesirable adulterations be it noise, silence, or uncertain pitch segments, while extracting essential audio features, i.e., signal power roll-off, pitch, and pause rate. Experiments on the standard SSPNet dataset records a relative 4\% increase in overall accuracy when log-likelihood based annotations are used. Moreover, improved consistency in accuracy is observed when this method is tested on male and female subsets.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {_Waiting for read,acoustic features,annotation clustering,Emotion recognition,Feature extraction,Human computer interaction,Personality analysis,Reliability,speech emotion recognition,Speech recognition,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Speech Personality Recognition Based on Annotation Classification Using.pdf}
}

@article{liuTERASelfSupervisedLearning2021,
  title = {{{TERA}}: {{Self-Supervised Learning}} of {{Transformer Encoder Representation}} for {{Speech}}},
  shorttitle = {{{TERA}}},
  author = {Liu, Andy T. and Li, Shang-Wen and Lee, Hung-yi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2351--2366},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3095662},
  abstract = {We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn by using a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous methods, we use alteration along three orthogonal axes to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from their altered counterpart, where we use a stochastic policy to alter along various dimensions: time, frequency, and magnitude. TERA can be used for speech representations extraction or fine-tuning with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, keyword spotting, speaker recognition, and speech recognition. We present a large-scale comparison of various self-supervised models. TERA achieves strong performance in the comparison by improving upon surface features and outperforming previous models. In our experiments, we study the effect of applying different alteration techniques, pre-training on more data, and pre-training on various features. We analyze different model sizes and find that smaller models are strong representation learners than larger models, while larger models are more effective for downstream fine-tuning than smaller models. Furthermore, we show the proposed method is transferable to downstream datasets not used in pre-training.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Bit error rate,Data models,pre-training,Predictive models,representation,Self-supervised,Speech processing,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_TERA.pdf}
}

@inproceedings{liuTimeFrequencyRepresentationLearning2021,
  title = {Time-{{Frequency Representation Learning}} with {{Graph Convolutional Network}} for {{Dialogue-Level Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Liu, Jiaxing and Song, Yaodong and Wang, Longbiao and Dang, Jianwu and Yu, Ruiguo},
  date = {2021-08-30},
  pages = {4523--4527},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-2067},
  url = {https://www.isca-speech.org/archive/interspeech_2021/liu21o_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Liu et al_2021_Time-Frequency Representation Learning with Graph Convolutional Network for.pdf}
}

@article{LiuYuYinQingGanTeZhengTiQuJiQiJiangWeiFangFaZongShu2017,
  title = {语音情感特征提取及其降维方法综述},
  author = {刘, 振焘；徐建平；吴敏；曹卫华；陈略峰；丁学文；郝曼；谢桥},
  date = {2017},
  journaltitle = {计算机学报},
  pages = {1--22},
  issn = {0254-4164},
  url = {https://kns.cnki.net/kcms/detail/11.1826.TP.20170813.1200.006.html},
  urldate = {2021-09-15},
  abstract = {情感智能是人工智能的重要发展方向,随着人工智能的迅速发展,情感智能已成为当前人机交互领域的研究热点。语音情感是人们相互情感交流最直接、最高效的途径,越来越多的研究者投入到语音情感识别的研究中。本文综述了国内外近几年语音情感特征提取及降维领域的最新进展。首先,介绍了语音情感识别中常用的特征,将语音特征分为韵律特征、基于谱的特征等,并提出以个性化与非个性化的方式对语音情感特征进行分类。然后,对其中广泛应用的特征提取方法进行了详细地比较与分析,阐述了各类方法的优缺点,并对最新的基于深度学习方法的语音特征提取相关研究进行了介绍。同时,介绍了常用的语音情感特征降维方法,并在此基础上分析了这些特征降维方法时间复杂度,对比了各类方法的优缺点。最后,对当前语音情感识别领域的研究现状与难点进行了讨论与展望。},
  language = {zh-CN},
  keywords = {⛔ No DOI found,情感特征提取,特征分类,特征降维,语音,非个性化特征},
  annotation = {{$<$}北大核心, EI, CSCD{$>$}},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\刘_2017_语音情感特征提取及其降维方法综述.pdf}
}

@thesis{LiXiJinPingXinShiDaiZhongGuoTeSeDaGuoWaiJiaoDeZheXueLiNianYanJiu2021,
  type = {硕士},
  title = {习近平新时代中国特色大国外交的哲学理念研究},
  author = {李, 秀秀},
  date = {2021},
  institution = {{延安大学}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFDTEMP&filename=1021622440.nh&uniplatform=NZKPT&v=JMHH%25mmd2Bx5thBsneW9b43LKiZjvGm08%25mmd2BWsgLT%25mmd2BFg02Z5c8ucAE72lQwPQf04KOUt7an},
  urldate = {2021-11-21},
  abstract = {十九大的胜利召开,意味着中国特色社会主义进入了新时代,并向世界宣示了中国新的历史方位。以习近平同志为核心的党中央领导集体在总结前人历史经验的基础上,统筹国内国际两个大局,深刻把握中国和世界发展大势,顺应时代潮流、审时度势、高瞻远瞩地提出了一系列外交新思想、新理念、新战略,形成了习近平新时代中国特色大国外交思想,并在2018年中央外事工作会议上将其确立为新时期我国对外工作的指导思想,为开创中国特色大国外交新局面指明了前进方向。在百年未有之大变局的时代背景下,由习近平开创的中国特色大国外交以``天下大同''、``美美与共''为思想遵从,以构建人类命运共同体为目标,阐述了习近平对当前世界局势的鲜明立场和明确...},
  language = {zh-CN},
  keywords = {Major-country diplomacy,New era,philosophy,Xi Jinping,习近平,哲学理念,大国外交,新时代},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\李_2021_习近平新时代中国特色大国外交的哲学理念研究.pdf}
}

@article{LiYuYinQingGanShiBieYanJiuJinZhanFenXi2020,
  title = {语音情感识别研究进展分析},
  author = {李, 晓宇 and 徐, 勇 and 张, 心蕊 and 汪, 倩 and 武, 雅利},
  date = {2020},
  journaltitle = {现代计算机},
  number = {20},
  pages = {44--47},
  issn = {1007-1423},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=XDJS202020008&uniplatform=NZKPT&v=TvxEf3bHZ8HNwbNFkrn79ujWs5Rpv8S%25mmd2FbZaXtlGJIRSHgTeNUmWG%25mmd2F7Y%25mmd2BLzU01my7},
  urldate = {2021-09-10},
  abstract = {语音情感识别的研究对于促进人工智能的发展具有重要的意义,分析其研究进展有助于了解目前的发展方向和研究热点。使用文献计量的方法,借助CiteSpace与Python语言对2010年来知网中有关语音情感识别的文献进行分析。分别研究相关文献的发表时间、作者与研究机构、关键词时间分布与词频分布,并对关键词频统计方法做出改进,提出基于文献重要度的关键词分析方法。主要研究热点可以归纳为对语音情感特征的研究、分类方法的研究以及应用方向的研究三个方面。此外,"情感智能"、"多模态融合"、"非个性化特征"和"情感语音合成"等也是近年值得关注的研究主题。},
  issue = {20},
  language = {zh-CN},
  keywords = {_中文,Bibliometrics,Keyword Analysis,Literature Importance,Speech Emotion,关键词分析,文献计量,文献重要度,语音情感},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\李 et al_2020_语音情感识别研究进展分析.pdf}
}

@article{lopez-espejoNovelLossFunction2021,
  title = {A {{Novel Loss Function}} and {{Training Strategy}} for {{Noise-Robust Keyword Spotting}}},
  author = {L\'opez-Espejo, Iv\'an and Tan, Zheng-Hua and Jensen, Jesper},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2254--2266},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3092567},
  abstract = {The development of keyword spotting (KWS) systems that are accurate in noisy conditions remains a challenge. Towards this goal, in this paper we propose a novel training strategy relying on multi-condition training for noise-robust KWS. By this strategy, we think of the state-of-the-art KWS models as the composition of a keyword embedding extractor and a linear classifier that are successively trained. To train the keyword embedding extractor, we also propose a new (\$C\_N,2+1\$)-pair loss function extending the concept behind related loss functions like triplet and \$N\$-pair losses to reach larger inter-class and smaller intra-class variation. Experimental results on a noisy version of the Google Speech Commands Dataset show that our proposal achieves around 12\% KWS accuracy relative improvement with respect to standard end-to-end multi-condition training when speech is distorted by unseen noises. This performance improvement is achieved without increasing the computational complexity of the KWS model.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustic distortion,Computational modeling,Deep learning,deep metric learning,keyword embedding,Keyword spotting,loss function,multi-condition training,Noise measurement,noise robustness,Noise robustness,Proposals,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\López-Espejo et al_2021_A Novel Loss Function and Training Strategy for Noise-Robust Keyword Spotting.pdf}
}

@article{lorenzo-truebaInvestigatingDifferentRepresentations2018,
  title = {Investigating Different Representations for Modeling and Controlling Multiple Emotions in {{DNN-based}} Speech Synthesis},
  author = {Lorenzo-Trueba, Jaime and Eje Henter, Gustav and Takaki, Shinji and Yamagishi, Junichi and Morino, Yosuke and Ochiai, Yuta},
  date = {2018-05},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {99},
  pages = {135--143},
  issn = {01676393},
  doi = {10.1016/j.specom.2018.03.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639317303096},
  urldate = {2021-09-10},
  abstract = {In this paper, we investigate the simultaneous modeling of multiple emotions in DNN-based expressive speech synthesis, and how to represent the emotional labels, such as emotional class and strength, for this task. Our goal is to answer two questions: First, what is the best way to annotate speech data with multiple emotions \textendash{} should we use the labels that the speaker intended to express, or labels based on listener perception of the resulting speech signals? Second, how should the emotional information be represented as labels for supervised DNN training, e.g., should emotional class and emotional strength be factorized into separate inputs or not? We evaluate on a large-scale corpus of emotional speech from a professional voice actress, additionally annotated with perceived emotional labels from crowdsourced listeners. By comparing DNN-based speech synthesizers that utilize different emotional representations, we assess the impact of these representations and design decisions on human emotion recognition rates, perceived emotional strength, and subjective speech quality. Simultaneously, we also study which representations are most appropriate for controlling the emotional strength of synthetic speech.},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lorenzo-Trueba et al_2018_Investigating different representations for modeling and controlling multiple.pdf}
}

@article{lotfianBuildingNaturalisticEmotionally2019,
  title = {Building {{Naturalistic Emotionally Balanced Speech Corpus}} by {{Retrieving Emotional Speech}} from {{Existing Podcast Recordings}}},
  author = {Lotfian, Reza and Busso, Carlos},
  date = {2019-10},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {10},
  number = {4},
  pages = {471--483},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2017.2736999},
  abstract = {The lack of a large, natural emotional database is one of the key barriers to translate results on speech emotion recognition in controlled conditions into real-life applications. Collecting emotional databases is expensive and time demanding, which limits the size of existing corpora. Current approaches used to collect spontaneous databases tend to provide unbalanced emotional content, which is dictated by the given recording protocol (e.g., positive for colloquial conversations, negative for discussion or debates). The size and speaker diversity are also limited. This paper proposes a novel approach to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. It relies on existing spontaneous recordings obtained from audio-sharing websites. The proposed approach combines machine learning algorithms to retrieve recordings conveying balanced emotional content with a cost effective annotation process using crowdsourcing, which make it possible to build a large scale speech emotional database. This approach provides natural emotional renditions from multiple speakers, with different channel conditions and conveying balanced emotional content that are difficult to obtain with alternative data collection protocols.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {4},
  language = {en},
  keywords = {_readed,Affective corpus,Digital audio broadcasting,emotion ranking,emotion recognition,Emotion recognition,expressive speech,information retrieval,Information retrieval,Machine learning algorithms,Speech processing,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lotfian_Busso_2019_Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving.pdf}
}

@inproceedings{luMultiSpeakerEmotionalSpeech2021,
  title = {Multi-{{Speaker Emotional Speech Synthesis}} with {{Fine-Grained Prosody Modeling}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lu, Chunhui and Wen, Xue and Liu, Ruolan and Chen, Xiao},
  date = {2021-06-06},
  pages = {5729--5733},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9413398},
  url = {https://ieeexplore.ieee.org/document/9413398/},
  urldate = {2021-09-10},
  abstract = {We present an end-to-end system for multi-speaker emotional speech synthesis. In particular, our system learns emotion classes from just two speakers then generalizes these classes to other speakers from whom no emotional data was seen. We address the problem by integrating disentangled, fine-grained prosody features with global, sentence-level emotion embedding. These fine-grained features learn to represent local prosodic variations disentangled from speaker, tone and global emotion label. Compared to systems that model emotions at sentence level only, our method achieves higher ratings in naturalness and expressiveness, while retaining comparable speaker similarity ratings.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  language = {en},
  keywords = {_readed,Acoustics,Conferences,Controllability,emotional speech synthesis,fine-grained,Multi-speaker,Predictive models,prosody modeling,Runtime,Signal processing,Synthesizers},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lu et al_2021_Multi-Speaker Emotional Speech Synthesis with Fine-Grained Prosody Modeling.pdf}
}

@article{luoGroupCommunicationContext2021,
  title = {Group {{Communication With Context Codec}} for {{Lightweight Source Separation}}},
  author = {Luo, Yi and Han, Cong and Mesgarani, Nima},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1752--1761},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3078640},
  abstract = {Despite the recent progress on neural network architectures for speech separation, the balance between the model size, model complexity and model performance is still an important and challenging problem for the deployment of such models to low-resource platforms. In this paper, we propose two simple modules, group communication and context codec, that can be easily applied to a wide range of architectures to jointly decrease the model size and complexity without sacrificing the performance. A group communication module splits a high-dimensional feature into groups of low-dimensional features and captures the inter-group dependency. A separation module with a significantly smaller model size can then be shared by all the groups. A context codec module, containing a context encoder and a context decoder, is designed as a learnable downsampling and upsampling module to decrease the length of a sequential feature processed by the separation module. The combination of the group communication and the context codec modules is referred to as the GC3 design. Experimental results show that applying GC3 on multiple network architectures for speech separation can achieve on-par or better performance with as small as 2.5\% model size and 17.6\% model complexity, respectively.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Codecs,Complexity theory,context codec,Context modeling,Convolutional codes,Decoding,deep learning,group communication,lightweight,Neural networks,Pipelines,Source separation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Luo et al_2021_Group Communication With Context Codec for Lightweight Source Separation.pdf}
}

@article{madmoniEffectPartialTimeFrequency2021,
  title = {The {{Effect}} of {{Partial Time-Frequency Masking}} of the {{Direct Sound}} on the {{Perception}} of {{Reverberant Speech}}},
  author = {Madmoni, Lior and Tibor, Shir and Nelken, Israel and Rafaely, Boaz},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2037--2047},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3084742},
  abstract = {The perception of sound in real-life acoustic environments, such as enclosed rooms or open spaces with reflective objects, is affected by reverberation. Hence, reverberation is extensively studied in the context of auditory perception, with many studies highlighting the importance of the direct sound for perception. Based on this insight, speech processing methods often use time-frequency (TF) analysis to detect TF bins that are dominated by the direct sound, and then use the detected bins to reproduce or enhance the speech signals. The detection of bins dominated by the direct sound is typically based on an objective measure, such as the direct-to-reverberant ratio (DRR). However, the relation between the DRR in the TF bins and the spatial perception of the reverberant sound which is reproduced from these bins is still not clear. It is the aim of this paper to provide some insights into this relation, specifically for reverberant speech, focusing on bins with high DRR. This is performed using a listening experiment, where high DRR bins within a reverberant speech signal have been masked in the TF domain, based on various DRR thresholds. The results show that the percentage of high-DRR TF bins that were masked may better indicate the quality of spatial perception, compared to the specific value of the DRR threshold. The insights from this work could be incorporated into spatial audio techniques that reproduce the direct sound of reverberant speech, and potentially improve spatial perception. This was illustrated with an implementation of directional audio coding that was studied with an additional listening experiment supporting the previously described results.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Audio coding,binaural reproduction,direct-to-reverberant ratio,Power measurement,reverberant speech,Reverberation,Spatial perception,Speech coding,Speech enhancement,Time-frequency analysis,Wavelength measurement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Madmoni et al_2021_The Effect of Partial Time-Frequency Masking of the Direct Sound on the.pdf}
}

@thesis{MaDuoFengGeYuYinHeChengMoXingDeYanJiu2019,
  type = {硕士},
  title = {多风格语音合成模型的研究},
  author = {马, 珍},
  date = {2019},
  institution = {{华中科技大学}},
  doi = {10.27157/d.cnki.ghzku.2019.001000},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019919227.nh&uniplatform=NZKPT&v=slmPgim7Pa3SIluSiXwL1hG3DGfi7jdBnEoMB7zZKhaOlg5%25mmd2Fw4Plc8TAmCcxL79W},
  urldate = {2021-09-28},
  abstract = {语音合成是将文字合成语音的技术,在很多应用产品中扮演了很重要的角色,例如导航系统、语音助手（谷歌助手,苹果Siri,微软Cortana等）、语音到语音翻译系统等。理想地,合成的语音应该传达文字信息（可理解性）,同时听起来像人类发出的声音（自然性）,并且带有不同风格（多样性）。然而,大多语音合成系统主要关注在可理解性和自然性上。近些年来,基于深度学习的模型在很多领域取得了巨大的成功,我们见证了深度学习技术给语音合成领域带来了激动人心的发展。第一,基于深度学习的语音合成系统消除了大量人工标注的特征工程工作,让机器能够自动地从原始数据中提取抽象、显著的特征;第二,基于深度学习的语音合成系统能够在不同的输入下,控制合成不同风格的语音,例如,不同语速、不同说话人、不同情绪等。第三,基于深度学习的语音合成系统适应性更强,将设计的模型应用到新的数据集上,不需要过多的人工特征设计的工作。最后,端到端的语音合成系统是整体来训练的,相较于传统的多个独立训练阶段,模型的鲁棒性更强。在这篇论文中,我们主要利用深度神经网络来实现多风格语音的合成。我们的贡献主要分为两个方面。第一,为了覆盖更丰富和更多样的语音风格,我们从双语版动画电影中利用字幕信息来切割音频,制作出了多风格语音数据集和跨语言多风格数据集。第二,我们设计了两种不同的多风格语音合成模型,分别为多风格语音合成模型和跨语言多风格语音合成模型,这些模型能自动地从参照音频中提取语音风格特征,并且这些模型能够以随机初始化的方式从头开始训练,消除了传统模型需要人工设计特征的工作,提高了模型的泛化能力。在论文的实验部分,由于自制的数据集中存在背景噪声,我们采用巧妙的训练策略来使模型的训练更简单和更稳定。最后,我们做了一系列的实验来验证和解释训练好的模型。},
  editora = {涂, 来},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {Cross-linguistic TTS,Deep Neural Networks,Multi-style TTS,Text-to-Speech,多风格语音合成,深度神经网络,语音合成,跨语言语音合成},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\马_2019_多风格语音合成模型的研究.pdf}
}

@article{maiAnalyzingMultimodalSentiment2021,
  title = {Analyzing {{Multimodal Sentiment Via Acoustic-}} and {{Visual-LSTM With Channel-Aware Temporal Convolution Network}}},
  author = {Mai, Sijie and Xing, Songlong and Hu, Haifeng},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1424--1437},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3068598},
  abstract = {The emotion of human is always expressed in a multimodal perspective. Analyzing multimodal human sentiment remains challenging due to the difficulties of the interpretation in inter-modality dynamics. Mainstream multimodal learning architectures tend to design various fusion strategies to learn inter-modality interactions, which barely consider the fact that the language modality is far more important than the acoustic and visual modalities. In contrast, we learn inter-modality dynamics in a different perspective via acoustic- and visual-LSTMs where language features play dominant role. Specifically, inside each LSTM variant, a well-designed gating mechanism is introduced to enhance the language representation via the corresponding auxiliary modality. Furthermore, in the unimodal representation learning stage, instead of using RNNs, we introduce `channel-aware' temporal convolution network to extract high-level representations for each modality to explore both temporal and channel-wise interdependencies. Extensive experiments demonstrate that our approach achieves very competitive performance compared to the state-of-the-art methods on three widely-used benchmarks for multimodal sentiment analysis and emotion recognition.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustic-LSTM,Acoustics,channel-aware temporal convolution,Convolution,Feature extraction,multimodal sentiment analysis,Sentiment analysis,Task analysis,Videos,visual -LSTM,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mai et al_2021_Analyzing Multimodal Sentiment Via Acoustic- and Visual-LSTM With Channel-Aware.pdf}
}

@article{manderyUnifyingRepresentationsLargeScale2016,
  title = {Unifying {{Representations}} and {{Large-Scale Whole-Body Motion Databases}} for {{Studying Human Motion}}},
  author = {Mandery, Christian and Terlemez, Omer and Do, Martin and Vahrenkamp, Nikolaus and Asfour, Tamim},
  date = {2016-08},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {32},
  number = {4},
  pages = {796--809},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2016.2572685},
  url = {http://ieeexplore.ieee.org/document/7506114/},
  urldate = {2021-09-10},
  abstract = {Large-scale human motion databases are key for research questions ranging from human motion analysis and synthesis, biomechanics of human motion, data-driven learning of motion primitives, and rehabilitation robotics to the design of humanoid robots and wearable robots such as exoskeletons. In this paper we present a large-scale database of whole-body human motion with methods and tools, which allows a unifying representation of captured human motion, and efficient search in the database, as well as the transfer of subject-specific motions to robots with different embodiments. To this end, captured subject-specific motion is normalized regarding the subject's height and weight by using a reference kinematics and dynamics model of the human body, the master motor map (MMM). In contrast with previous approaches and human motion databases, the motion data in our database consider not only the motions of the human subject but the position and motion of objects with which the subject is interacting as well. In addition to the description of the MMM reference model, we present procedures and techniques for the systematic recording, labeling, and organization of human motion capture data, object motions as well as the subject\textendash object relations. To allow efficient search for certain motion types in the database, motion recordings are manually annotated with motion description tags organized in a tree structure. We demonstrate the transfer of human motion to humanoid robots and provide several examples of motion analysis using the database.},
  issue = {4},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mandery et al_2016_Unifying Representations and Large-Scale Whole-Body Motion Databases for.pdf}
}

@inproceedings{mansouriLaughterSynthesisComparison2020,
  title = {Laughter Synthesis: {{A}} Comparison between {{Variational}} Autoencoder and {{Autoencoder}}},
  shorttitle = {Laughter Synthesis},
  booktitle = {2020 5th {{International Conference}} on {{Advanced Technologies}} for {{Signal}} and {{Image Processing}} ({{ATSIP}})},
  author = {Mansouri, Nadia and Lachiri, Zied},
  date = {2020-09},
  pages = {1--6},
  publisher = {{IEEE}},
  location = {{Sousse, Tunisia}},
  doi = {10.1109/ATSIP49331.2020.9231607},
  url = {https://ieeexplore.ieee.org/document/9231607/},
  urldate = {2021-09-10},
  abstract = {Laughter is one of the most famous non verbal sounds that human produce since birth, it conveys messages about our emotional state. These characteristics make it an important sound that should be studied in order to improve the human-machine interactions. In this paper we investigate the audio laughter generation process from its acoustic features. This suggested process is considered as an analysis-transformationsynthesis benchmark based on unsupervised dimensionality reduction techniques: The standard autoencoder (AE) and the variational autoencoder (VAE). Therefore, the laughter synthesis methodology consists of transforming the extracted highdimensional log magnitude spectrogram into a low-dimensional latent vector. This latent vector contains the most valuable information used to reconstruct a synthetic magnitude spectrogram that will be passed through a specific vocoder to generate the laughter waveform. We systematically, exploit the VAE to create new sound (speech-laugh) based on the interpolation process. To evaluate the performance of these models two evaluation metrics were conducted: objective and subjective evaluations.},
  eventtitle = {2020 5th {{International Conference}} on {{Advanced Technologies}} for {{Signal}} and {{Image Processing}} ({{ATSIP}})},
  isbn = {978-1-72817-513-3},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mansouri_Lachiri_2020_Laughter synthesis.pdf}
}

@article{margeSpokenLanguageInteraction2022,
  title = {Spoken Language Interaction with Robots: {{Recommendations}} for Future Research},
  shorttitle = {Spoken Language Interaction with Robots},
  author = {Marge, Matthew and Espy-Wilson, Carol and Ward, Nigel G. and Alwan, Abeer and Artzi, Yoav and Bansal, Mohit and Blankenship, Gil and Chai, Joyce and Daum\'e, Hal and Dey, Debadeepta and Harper, Mary and Howard, Thomas and Kennington, Casey and Kruijff-Korbayov\'a, Ivana and Manocha, Dinesh and Matuszek, Cynthia and Mead, Ross and Mooney, Raymond and Moore, Roger K. and Ostendorf, Mari and Pon-Barry, Heather and Rudnicky, Alexander I. and Scheutz, Matthias and Amant, Robert St. and Sun, Tong and Tellex, Stefanie and Traum, David and Yu, Zhou},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101255},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101255},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000620},
  urldate = {2021-09-28},
  abstract = {With robotics rapidly advancing, more effective human\textendash robot interaction is increasingly needed to realize the full potential of robots for society. While spoken language must be part of the solution, our ability to provide spoken language interaction capabilities is still very limited. In this article, based on the report of an interdisciplinary workshop convened by the National Science Foundation, we identify key scientific and engineering advances needed to enable effective spoken language interaction with robotics. We make 25 recommendations, involving eight general themes: putting human needs first, better modeling the social and interactive aspects of language, improving robustness, creating new methods for rapid adaptation, better integrating speech and language with other communication modalities, giving speech and language components access to rich representations of the robot's current knowledge and state, making all components operate in real time, and improving research infrastructure and resources. Research and development that prioritizes these topics will, we believe, provide a solid foundation for the creation of speech-capable robots that are easy and effective for humans to work with.},
  language = {en},
  keywords = {_Waiting for read,Challenges,Issues,Priorities,Research agenda,Users},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Marge et al_2022_Spoken language interaction with robots.pdf}
}

@article{mcduffLongitudinalObservationalEvidence2021,
  title = {Longitudinal {{Observational Evidence}} of the {{Impact}} of {{Emotion Regulation Strategies}} on {{Affective Expression}}},
  author = {McDuff, Daniel and Jun, Eunice and Rowan, Kael and Czerwinski, Mary},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {636--647},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2961912},
  abstract = {The ability to regulate our emotions plays an important role in our psychological and physical health. Regulating emotions influences how and when emotions are expressed. We performed a large scale, longitudinal observational study to investigate the effect of emotion regulation ability on expressed affect. We found that expression of negative affect increased throughout the day. For people who suppress emotion this increase is slower that for those who do not. For those with stronger cognitive reappraisal abilities, though not significant, there was a trend for higher positive affect and negative affect increased significantly less steeply, suggesting that they might experience more positive and less negative affect. These results reflect some of the first results based on large scale, continuous tracking of behavioral expression of emotion longitudinally. Our results demonstrate the need to carefully consider the time of day and emotion regulation ability, in addition to gender and age, when attempting to automatically infer affective states for facial behavior.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_Waiting for read,aging,Atmospheric measurements,Emotion regulation,facial expression,gender,large scale,longitudinal,Mood,Particle measurements,Regulation,sex,Software,Tools},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\McDuff et al_2021_Longitudinal Observational Evidence of the Impact of Emotion Regulation.pdf}
}

@article{medinaImpulsiveNoiseDetection2021,
  title = {Impulsive {{Noise Detection}} for {{Speech Enhancement}} in {{HHT Domain}}},
  author = {Medina, C. and Coelho, R. and Z\~ao, L.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2244--2253},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3093392},
  abstract = {This paper introduces a novel single channel speech enhancement method in the time domain to mitigate the effects of acoustic impulsive noises. The ensemble empirical mode decomposition is applied to analyze the noisy speech signal. The estimation and selection of noise components is based on the impulsiveness index of decomposition modes. An adaptive threshold is proposed to define the criterion to select the noise components. The proposed method is evaluated in speech enhancement experiments considering four acoustic noises with different impulsiveness indices and non-stationarity degrees under various signal-to-noise ratios. Four speech enhancement algorithms are adopted as baseline in the evaluation analysis considering spectral and time domains. Seven objective measures are adopted to compare the proposed and baseline approaches in terms of speech quality and intelligibility. Results show that the proposed solution outperforms the competing algorithms for most of the noisy scenarios. The novel method shows particularly interesting performance when speech signals are corrupted by highly impulsive acoustic noises.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustic noise,Estimation,Hilbert-Huang transform,impulsive noises,Indexes,Noise measurement,non-stationary acoustic noises,Signal to noise ratio,Speech enhancement,Time-domain analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Medina et al_2021_Impulsive Noise Detection for Speech Enhancement in HHT Domain.pdf}
}

@article{meftahSpeakerIdentificationDifferent2020,
  title = {Speaker {{Identification}} in {{Different Emotional States}} in {{Arabic}} and {{English}}},
  author = {Meftah, Ali Hamid and Mathkour, Hassan and Kerrache, Said and Alotaibi, Yousef Ajami},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {60070--60083},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2983029},
  url = {https://ieeexplore.ieee.org/document/9046009/},
  urldate = {2021-09-10},
  abstract = {Speaker recognition is an important application of digital speech processing. However, a major challenge degrading the robustness of speaker-recognition systems is variation in the emotional states of speakers, such as happiness, anger, sadness, or surprise. In this paper, we propose a speaker recognition system corresponding to three states, namely emotional, neutral, and with no consideration for a speaker's state (i.e., the speaker can be in an emotional state or neutral state), for two languages: Arabic and English. Additionally, cross-language speaker recognition was applied in emotional, neutral, and (emotional + neutral) states. Convolutional neural network and long short-term memory models were used to design a convolutional recurrent neural network (CRNN) main system. We also investigated the use of linearly spaced spectrograms as speech-feature inputs. The proposed system utilizes the KSUEmotions, emotional prosody speech and transcripts, WEST POINT, and TIMIT corpora. The CRNN system exhibited accuracies as high as 97.4\% and 97.18\% for Arabic and English emotional speech inputs, respectively, and 99.89\% and 99.4\% for Arabic and English neutral speech inputs, respectively. For the cross-language program, the overall CRNN system accuracy was as high as 91.83\%, 99.88\%, and 95.36\% for emotional, neutral, and (emotional + neutral) states, respectively.},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Meftah et al_2020_Speaker Identification in Different Emotional States in Arabic and English.pdf}
}

@article{mehmetberkehanakcaySpeechEmotionRecognition2020,
  title = {Speech Emotion Recognition: {{Emotional}} Models, Databases, Features, Preprocessing Methods, Supporting Modalities, and Classifiers},
  shorttitle = {Speech Emotion Recognition},
  author = {Ak\c{c}ay, Mehmet Berkehan and O\u{g}uz, Kaya},
  date = {2020-01-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {116},
  pages = {56--76},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2019.12.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639319302262},
  urldate = {2023-01-18},
  abstract = {Speech is the most natural way of expressing ourselves as humans. It is only natural then to extend this communication medium to computer applications. We define speech emotion recognition (SER) systems as a collection of methodologies that process and classify speech signals to detect the embedded emotions. SER is not a new field, it has been around for over two decades, and has regained attention thanks to the recent advancements. These novel studies make use of the advances in all fields of computing and technology, making it necessary to have an update on the current methodologies and techniques that make SER possible. We have identified and discussed distinct areas of SER, provided a detailed survey of current literature of each, and also listed the current challenges.},
  language = {en},
  keywords = {_review,Classification,Speech databases,Speech emotion recognition,Speech features,Survey},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Akçay_Oğuz_2020_Speech emotion recognition.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\2QPIG3B8\\S0167639319302262.html}
}

@article{mehrabianPleasurearousaldominanceGeneralFramework1996,
  title = {Pleasure-Arousal-Dominance: {{A}} General Framework for Describing and Measuring Individual Differences in {{Temperament}}},
  shorttitle = {Pleasure-Arousal-Dominance},
  author = {Mehrabian, A.},
  date = {1996},
  doi = {10.1007/BF02686918},
  abstract = {Evidence bearing on the Pleasure-Arousal-Dominance (PAD) Emotional State Model was reviewed and showed that its three nearly orthogonal dimensions provided a sufficiently comprehensive description of emotional states. Temperament was defined as average emotional state across a representative sample of life situations. The Pleasure-Arousability-Dominance (PAD) Temperament Model was described. Evidence relating the PAD Temperament Model to 59 individual difference measures was reviewed. Formulas were offered for use of P, A, and D temperament scores to compute and predict a variety of personality scores (e.g., Anxiety, Depression, Panic, Somatization, Empathy, Affiliation, Achievement, Extroversion, Arousal Seeking, Loneliness, Neuroticism, Suicide Proneness, Binge Eating, Substance Abuse, Emotional Stability, Dependency, Aggressiveness, and Fidgeting).},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mehrabian_1996_Pleasure-arousal-dominance.pdf}
}

@article{michelsantiOverviewDeepLearningBasedAudioVisual2021,
  title = {An {{Overview}} of {{Deep-Learning-Based Audio-Visual Speech Enhancement}} and {{Separation}}},
  author = {Michelsanti, Daniel and Tan, Zheng-Hua and Zhang, Shi-Xiong and Xu, Yong and Yu, Meng and Yu, Dong and Jensen, Jesper},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1368--1396},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3066303},
  abstract = {Speech enhancement and speech separation are two related tasks, whose purpose is to extract either one or more target speech signals, respectively, from a mixture of sounds generated by several sources. Traditionally, these tasks have been tackled using signal processing and machine learning techniques applied to the available acoustic signals. Since the visual aspect of speech is essentially unaffected by the acoustic environment, visual information from the target speakers, such as lip movements and facial expressions, has also been used for speech enhancement and speech separation systems. In order to efficiently fuse acoustic and visual information, researchers have exploited the flexibility of data-driven approaches, specifically deep learning, achieving strong performance. The ceaseless proposal of a large number of techniques to extract features and fuse multimodal information has highlighted the need for an overview that comprehensively describes and discusses audio-visual speech enhancement and separation based on deep learning. In this paper, we provide a systematic survey of this research topic, focusing on the main elements that characterise the systems in the literature: acoustic features; visual features; deep learning methods; fusion techniques; training targets and objective functions. In addition, we review deep-learning-based methods for speech reconstruction from silent videos and audio-visual sound source separation for non-speech signals, since these methods can be more or less directly applied to audio-visual speech enhancement and separation. Finally, we survey commonly employed audio-visual speech datasets, given their central role in the development of data-driven approaches, and evaluation methods, because they are generally used to compare different systems and determine their performance.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Audio-visual processing,deep learning,Deep learning,Microphones,sound source separation,speech enhancement,Speech enhancement,speech separation,speech synthesis,Task analysis,Videos,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Michelsanti et al_2021_An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and.pdf}
}

@inproceedings{mirheidariAutomaticDetectionExpressed2022,
  title = {Automatic {{Detection}} of {{Expressed Emotion}} from {{Five-Minute Speech Samples}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {Automatic {{Detection}} of {{Expressed Emotion}} from {{Five-Minute Speech Samples}}},
  booktitle = {Interspeech 2022},
  author = {Mirheidari, Bahman and Bittar, Andre and Cummins, Nicholas and Downs, Johnny and Fisher, Helen L. and Christensen, Heidi},
  date = {2022-09-18},
  pages = {2458--2462},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10188},
  url = {https://www.isca-speech.org/archive/interspeech_2022/mirheidari22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mirheidari et al_2022_Automatic Detection of Expressed Emotion from Five-Minute Speech Samples.pdf}
}

@inproceedings{mitraSpeechEmotionInvestigating2022,
  title = {Speech {{Emotion}}: {{Investigating Model Representations}}, {{Multi-Task Learning}} and {{Knowledge Distillation}}},
  shorttitle = {Speech {{Emotion}}},
  booktitle = {Interspeech 2022},
  author = {Mitra, Vikramjit and Chien, Hsiang-Yun Sherry and Kowtha, Vasudha and Cheng, Joseph Yitan and Azemi, Erdrin},
  date = {2022-09-18},
  pages = {4715--4719},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-957},
  url = {https://www.isca-speech.org/archive/interspeech_2022/mitra22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mitra et al_2022_Speech Emotion.pdf}
}

@article{mittagDeepLearningBased2020,
  title = {Deep {{Learning Based Assessment}} of {{Synthetic Speech Naturalness}}},
  author = {Mittag, Gabriel and M\"oller, Sebastian},
  date = {2020-10-25},
  journaltitle = {Interspeech 2020},
  eprint = {2104.11673},
  eprinttype = {arxiv},
  pages = {1748--1752},
  doi = {10.21437/Interspeech.2020-2382},
  url = {http://arxiv.org/abs/2104.11673},
  urldate = {2021-09-10},
  abstract = {In this paper, we present a new objective prediction model for synthetic speech naturalness. It can be used to evaluate Text-To-Speech or Voice Conversion systems and works language independently. The model is trained end-to-end and based on a CNN-LSTM network that previously showed to give good results for speech quality estimation. We trained and tested the model on 16 different datasets, such as from the Blizzard Challenge and the Voice Conversion Challenge. Further, we show that the reliability of deep learning-based naturalness prediction can be improved by transfer learning from speech quality prediction models that are trained on objective POLQA scores. The proposed model is made publicly available and can, for example, be used to evaluate different TTS system configurations.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mittag_Möller_2020_Deep Learning Based Assessment of Synthetic Speech Naturalness.pdf}
}

@article{mohantaAnalysisClassificationSpeech2021,
  title = {Analysis and Classification of Speech Sounds of Children with Autism Spectrum Disorder Using Acoustic Features},
  author = {Mohanta, Abhijit and Mittal, Vinay Kumar},
  date = {2021-09-14},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  pages = {101287},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101287},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000899},
  urldate = {2021-09-28},
  abstract = {Children affected with autism spectrum disorder (ASD) produce speech sounds different from that of Normal or non-ASD children. Hence, analyzing acoustic features can help characterizing the ASD speech signals. In this study, the distinguishing characteristics of speech production are examined for ASD affected children, with comparison to Normal children's speech. Acoustic features are analyzed first and then classification of ASD vs Normal speech is attempted using different machine learning techniques. Two speech sound databases are recorded for this study: the speech database of children affected with ASD and the speech database of Normal children. English speech utterances are recorded for children of Indian regional (Tamil and Telugu) nativity. The changes due to autism effect are examined in context of 5 English vowel sounds (/a/, /e/, /i/, /o/, and /u/). Changes in the speech production characteristics are examined using three sets of features. Firstly, changes in the excitation source features are examined using instantaneous fundamental frequency (F0) and strength of excitation (SoE). Secondly, changes in the vocal tract (VT) filter features are examined using dominant frequencies (FD1, FD2) and formant frequencies (F1 to F5). Thirdly, changes in the source-filter combined features are examined using zero-crossing rate (ZCR), signal energy (E), Mel-frequency cepstral coefficients (MFCC), and linear prediction cepstrum coefficients (LPCC). Then, different combinations of the acoustic features are classified using machine learning methods such as probabilistic neural network (PNN), multilayer perceptron (MLP), support vector machine (SVM), and K-nearest neighbors (KNN). Analyses of acoustic features indicate notable differences between the speech of children with ASD and the Normal children. Results up to 98.17\% accuracy are obtained for classification between acoustic features of the speech of children with ASD and the Normal children. The observations and results of this study may be useful as acoustic biomarkers to identify autism and its progression/cure among children. This study may also be helpful towards developing an automated system for ASD diagnosis from children's speech sounds, in the future.},
  language = {en},
  keywords = {\#nosource,ASD affected children’s speech sounds analyses,Autism spectrum disorder,Dominant frequencies,English vowels,Formant frequencies,Strength of excitation}
}

@inproceedings{moineSpeakerAttentiveSpeech2021,
  title = {Speaker {{Attentive Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Moine, Cl\'ement Le and Obin, Nicolas and Roebel, Axel},
  date = {2021-08-30},
  pages = {2866--2870},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-573},
  url = {https://www.isca-speech.org/archive/interspeech_2021/moine21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Moine et al_2021_Speaker Attentive Speech Emotion Recognition.pdf}
}

@inproceedings{moraisSpeechEmotionRecognition2022,
  title = {Speech {{Emotion Recognition Using Self-Supervised Features}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Morais, Edmilson and Hoory, Ron and Zhu, Weizhong and Gat, Itai and Damasceno, Matheus and Aronowitz, Hagai},
  date = {2022},
  pages = {6922--6926},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747870},
  abstract = {Self-supervised pre-trained features have consistently delivered state-of-art results in the field of natural language processing (NLP); however, their merits in the field of speech emotion recognition (SER) still need further investigation. In this paper we introduce a modular End-to-End (E2E) SER system based on an Upstream + Downstream architecture paradigm, which allows easy use/integration of a large variety of self-supervised features. Several SER experiments for predicting categorical emotion classes from the IEMOCAP dataset are performed. These experiments investigate interactions among fine-tuning of self-supervised feature models, aggregation of frame-level features into utterance-level features and back-end classification networks. The proposed monomodal speech-only based system not only achieves SOTA results, but also brings light to the possibility of powerful and well fine-tuned self-supervised acoustic features that reach results similar to the results achieved by SOTA multimodal systems using both Speech and Text modalities.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustics,Conferences,Emotion recognition,end-to-end systems,Natural language processing,self-supervised features,Signal processing,Speech emotion recognition,Speech processing,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Morais et al_2022_Speech Emotion Recognition Using Self-Supervised Features.pdf}
}

@unpublished{moritaniStarGANbasedEmotionalVoice2021,
  title = {{{StarGAN-based Emotional Voice Conversion}} for {{Japanese Phrases}}},
  author = {Moritani, Asuka and Ozaki, Ryo and Sakamoto, Shoki and Kameoka, Hirokazu and Taniguchi, Tadahiro},
  date = {2021-04-05},
  eprint = {2104.01807},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2104.01807},
  urldate = {2021-09-10},
  abstract = {This paper shows that StarGAN-VC, a spectral envelope transformation method for non-parallel many-to-many voice conversion (VC), is capable of emotional VC (EVC). Although StarGAN-VC has been shown to enable speaker identity conversion, its capability for EVC for Japanese phrases has not been clarified. In this paper, we describe the direct application of StarGAN-VC to an EVC task with minimal fundamental frequency and aperiodicity processing. Through subjective evaluation experiments, we evaluated the performance of our StarGAN-EVC system in terms of its ability to achieve EVC for Japanese phrases. The subjective evaluation is conducted in terms of subjective classification and mean opinion score of neutrality and similarity. In addition, the interdependence between the source and target emotional domains was investigated from the perspective of the quality of EVC.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Moritani et al_2021_StarGAN-based Emotional Voice Conversion for Japanese Phrases.pdf}
}

@inproceedings{morotoHumanEmotionRecognition2022,
  title = {Human {{Emotion Recognition Using Multi-Modal Biological Signals Based On Time Lag-Considered Correlation Maximization}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Moroto, Yuya and Maeda, Keisuke and Ogawa, Takahiro and Haseyama, Miki},
  date = {2022},
  pages = {4683--4687},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746128},
  abstract = {A human emotion recognition using multi-modal biological signals based on time lag-considered correlation maximization is presented in this paper. Various multi-modal emotion recognition methods for visual stimuli have been studied and they focus on gaze and brain activity data. The visual stimuli captured by human eyes are sent to the brain by neurotransmitters. Thus, there is a time lag between gaze data, which record where humans gaze at, and brain activity data. However, most of the previous methods only integrate features obtained from each data without considering such a time lag. The proposed method newly introduces the mechanism to consider the time lag into the canonical correlation analysis scheme by assuming that the influence of the visual stimuli on brain activity data follows the Poisson distribution. The contribution of this paper is the construction of a recognition method with considering the time lag for getting truly close to the realization of the occurrence mechanism of human emotions. Experimental results show the effectiveness of considering the time lag between gaze and brain activity data.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Brain,brain activity,Correlation,Emotion recognition,multi-modal human emotion recognition,Neurotransmitters,sequential data,Signal processing,Speech recognition,time lag,Visual attention,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Moroto et al_2022_Human Emotion Recognition Using Multi-Modal Biological Signals Based On Time.pdf}
}

@inproceedings{mukherjeeTextAwareEmotional2022,
  title = {Text Aware {{Emotional Text-to-speech}} with {{BERT}}},
  booktitle = {Interspeech 2022},
  author = {Mukherjee, Arijit and Bansal, Shubham and Satpal, Sandeepkumar and Mehta, Rupesh},
  date = {2022-09-18},
  pages = {4601--4605},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11293},
  url = {https://www.isca-speech.org/archive/interspeech_2022/mukherjee22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Mukherjee et al_2022_Text aware Emotional Text-to-speech with BERT.pdf}
}

@online{MultiModalEmotionRecognition,
  title = {Multi-{{Modal Emotion Recognition}} with {{Self-Guided Modality Calibration}}},
  url = {https://ieeexplore.ieee.org/document/9747859/},
  urldate = {2022-06-12},
  abstract = {Multi-modal emotion recognition aims to extract sentiment-related information from multiple sources and integrate different modal representations for sentiment analysis. Alignment is an effective strategy to achieve semantically consistent representations for multi-modal emotion recognition, while the current alignment models are jointly unable to maintain the dependence of word-to-sentence and independence of unimodal learning. In this paper, we propose a Self-guided Modality Calibration Network (SMCN) to realize multi-modal alignment which can capture the global connections without interfering with unimodal learning. While preserving unimodal learning without interference, our model leverages semantic sentiment-related features to guide modality-specific representation learning. On one hand, SMCN simulates human thinking by deriving a branch for acquiring knowledge of other modalities in unimodal learning. This branch aims to lean high-level semantic information of other modalities for realizing semantic alignment between modalities. On the other hand, we also provide an indirect interaction manner to integrate unimodal feature and calibrate features in different levels for avoiding unimodal features mixed with other clues. Experiments demonstrate that our approach outperforms the state-of-the-art methods on both IEMOCAP and MELD datasets.},
  language = {en},
  keywords = {\#nosource}
}

@inproceedings{muppidiSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition Using Quaternion Convolutional Neural Networks}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Muppidi, Aneesh and Radfar, Martin},
  date = {2021-06},
  pages = {6309--6313},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414248},
  abstract = {Although speech recognition has become a widespread technology, inferring emotion from speech signals remains a challenge. Our paper addresses this problem by proposing a quaternion convolutional neural network (QCNN) based speech emotion recognition (SER) model in which Mel-spectrogram features of speech signals are encoded in an RGB quaternion domain. We demonstrate that our QCNN based SER model outperforms other real-valued methods in the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS, 8-classes) dataset, achieving, to the best of our knowledge, state-of-the-art results. The QCNN model also achieves comparable results with state-of-the-art methods in the Interactive Emotional Dyadic Motion Capture (IEMOCAP 4-classes) and Berlin EMO-DB (7-classes) datasets. Specifically, the model achieves an accuracy of 77.87\%, 70.46\%, and 88.78\% for the RAVDESS, IEMOCAP, and EMO-DB datasets, respectively. Additionally, model size results reveal that the quaternion unit structure is significantly better able to encode internal dependencies than real-valued structures.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Computational modeling,Convolutional Neural Networks,Emotion recognition,Encoding,Feature extraction,Neural networks,Quaternion Deep Learning,Quaternions,Signal Processing,Speech Emotion Recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Muppidi_Radfar_2021_Speech Emotion Recognition Using Quaternion Convolutional Neural Networks.pdf}
}

@inproceedings{naganoImpactEmotionalState2021,
  title = {Impact of {{Emotional State}} on {{Estimation}} of {{Willingness}} to {{Buy}} from {{Advertising Speech}}},
  booktitle = {Interspeech 2021},
  author = {Nagano, Mizuki and Ijima, Yusuke and Hiroya, Sadao},
  date = {2021-08-30},
  pages = {2486--2490},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-827},
  url = {https://www.isca-speech.org/archive/interspeech_2021/nagano21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Nagano et al_2021_Impact of Emotional State on Estimation of Willingness to Buy from Advertising.pdf}
}

@article{nakashikaGammaBoltzmannMachine2021,
  title = {Gamma {{Boltzmann Machine}} for {{Audio Modeling}}},
  author = {Nakashika, Toru and Yatabe, Kohei},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2591--2605},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3095656},
  abstract = {This paper presents an energy-based probabilistic model that handles nonnegative data in consideration of both linear and logarithmic scales. In audio applications, magnitude of time-frequency representation, including spectrogram, is regarded as one of the most important features. Such magnitude-based features have been extensively utilized in learning-based audio processing. Since a logarithmic scale is important in terms of auditory perception, the features are usually computed with a logarithmic function. That is, a logarithmic function is applied within the computation of features so that a learning machine does not have to explicitly model the logarithmic scale. We think in a different way and propose a restricted Boltzmann machine (RBM) that simultaneously models linear- and log-magnitude spectra. RBM is a stochastic neural network that can discover data representations without supervision. To manage both linear and logarithmic scales, we define an energy function based on both scales. This energy function results in a conditional distribution (of the observable data, given hidden units) that is written as the gamma distribution, and hence the proposed RBM is termed gamma-Bernoulli RBM. The proposed gamma-Bernoulli RBM was compared to the ordinary Gaussian-Bernoulli RBM by speech representation experiments. Both objective and subjective evaluations illustrated the advantage of the proposed model.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Boltzmann machine,Computational modeling,gamma distribution,Gamma distribution,nonnegative data modeling,Probabilistic logic,speech parameterization,speech synthesis,Stacking,Stochastic processes,Time-frequency analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Nakashika_Yatabe_2021_Gamma Boltzmann Machine for Audio Modeling.pdf}
}

@article{narendraDetectionParkinsonDisease2021,
  title = {The {{Detection}} of {{Parkinson}}'s {{Disease From Speech Using Voice Source Information}}},
  author = {Narendra, N.P. and Schuller, Bj\"orn and Alku, Paavo},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1925--1936},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3078364},
  abstract = {Developing automatic methods to detect Parkinson's disease (PD) from speech has attracted increasing interest as these techniques can potentially be used in telemonitoring health applications. This article studies the utilization of voice source information in the detection of PD using two classifier architectures: traditional pipeline approach and end-to-end approach. The former consists of feature extraction and classifier stages. In feature extraction, the baseline acoustic features\textemdash consisting of articulation, phonation, and prosody features\textemdash were computed and voice source information was extracted using glottal features that were estimated by iterative adaptive inverse filtering (IAIF) and quasi-closed phase (QCP) glottal inverse filtering methods. Support vector machine classifiers were developed utilizing the baseline and glottal features extracted from every speech utterance and the corresponding healthy/PD labels. The end-to-end approach uses deep learning models which were trained using both raw speech waveforms and raw voice source waveforms. In the latter, two glottal inverse filtering methods (IAIF and QCP) and zero frequency filtering method were utilized. The deep learning architecture consists of a combination of convolutional layers followed by a multilayer perceptron. Experiments were performed using PC-GITA speech database. From the traditional pipeline systems, the highest classification accuracy (67.93\%) was given by combination of baseline and QCP-based glottal features. From the end-to-end-systems, the highest accuracy (68.56\%) was given by the system trained using QCP-based glottal flow signals. Even though classification accuracies were modest for all systems, the study is encouraging as the extraction of voice source information was found to be most effective in both approaches.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Deep learning,Diseases,end-to-end systems,Feature extraction,glottal features,glottal source estimation,Parkinson's disease,Pipelines,Speech processing,support vector machines,Support vector machines,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Narendra et al_2021_The Detection of Parkinson's Disease From Speech Using Voice Source Information.pdf}
}

@article{nassifSpeechRecognitionUsing2019,
  title = {Speech {{Recognition Using Deep Neural Networks}}: {{A Systematic Review}}},
  shorttitle = {Speech {{Recognition Using Deep Neural Networks}}},
  author = {Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {19143--19165},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2896880},
  abstract = {Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.},
  eventtitle = {{{IEEE Access}}},
  language = {en},
  keywords = {_review,Acoustics,Computer architecture,Deep learning,deep neural network,Feature extraction,Hidden Markov models,Neural networks,Speech recognition,systematic review},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Nassif et al_2019_Speech Recognition Using Deep Neural Networks.pdf}
}

@inproceedings{NEURIPS2018_a19744e2,
  title = {Co-Teaching: {{Robust}} Training of Deep Neural Networks with Extremely Noisy Labels},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf},
  language = {en},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{ngaiEmotionRecognitionBased2022,
  title = {Emotion Recognition Based on Convolutional Neural Networks and Heterogeneous Bio-Signal Data Sources},
  author = {Ngai, Wang Kay and Xie, Haoran and Zou, Di and Chou, Kee-Lee},
  date = {2022-01-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {77},
  pages = {107--117},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.07.007},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253521001457},
  urldate = {2021-09-28},
  abstract = {Emotion recognition is a crucial application in human\textendash computer interaction. It is usually conducted using facial expressions as the main modality, which might not be reliable. In this study, we proposed a multimodal approach that uses 2-channel electroencephalography (EEG) signals and eye modality in addition to the face modality to enhance the recognition performance. We also studied the use of facial images versus facial depth as the face modality and adapted the common arousal\textendash valence model of emotions and the convolutional neural network, which can model the spatiotemporal information from the modality data for emotion recognition. Extensive experiments were conducted on the modality and emotion data, the results of which showed that our system has high accuracies of 67.8\% and 77.0\% in valence recognition and arousal recognition, respectively. The proposed method outperformed most state-of-the-art systems that use similar but fewer modalities. Moreover, the use of facial depth has outperformed the use of facial images. The proposed method of emotion recognition has significant potential for integration into various educational applications.},
  language = {en},
  keywords = {_Waiting for read,3D convolutional neural network,Arousal–valence model of emotions,Electroencephalogram,Emotion recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ngai et al_2022_Emotion recognition based on convolutional neural networks and heterogeneous.pdf}
}

@article{niermannListeningEnhancementNoisy2021,
  title = {Listening {{Enhancement}} in {{Noisy Environments}}: {{Solutions}} in {{Time}} and {{Frequency Domain}}},
  shorttitle = {Listening {{Enhancement}} in {{Noisy Environments}}},
  author = {Niermann, Markus and Vary, Peter},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {699--709},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3047234},
  abstract = {The intelligibility of speech from a telephone or a public address system is often affected by acoustical background noise in the near-end listening environment. Speech intelligibility and listening effort can be improved by adaptive pre-processing of the loudspeaker signal. This is called Near-End Listening Enhancement (NELE). The speech spectrum is dynamically modified, taking the acoustical background noise at the near-end into account. In this paper, two opposite NELE strategies with either Noise-Masking-Proportional Shaping or Noise-Masking-Inverse Shaping are proposed which are appropriate for different noise characteristics. Both strategies are formulated in closed form in the frequency domain. They do not require to optimize an intelligibility measure but use explicitly the masking threshold. Motivated by the frequency domain approach, a simpler time domain solution is derived which is based on linear prediction techniques and does not need the masking calculations. The proposed NELE solutions outperform state-of-the-art in terms of computational complexity, memory requirement, continuous processor load, and latency.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Discrete Fourier transforms,Frequency-domain analysis,Loudspeakers,Masking threshold,Near-end listening enhancement,Noise measurement,Signal to noise ratio,Speech enhancement,speech intelligibility},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Niermann_Vary_2021_Listening Enhancement in Noisy Environments.pdf}
}

@article{ningReviewDeepLearning2019,
  title = {A {{Review}} of {{Deep Learning Based Speech Synthesis}}},
  author = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
  date = {2019-09-27},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {9},
  number = {19},
  pages = {4050},
  issn = {2076-3417},
  doi = {10.3390/app9194050},
  url = {https://www.mdpi.com/2076-3417/9/19/4050},
  urldate = {2021-09-10},
  abstract = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of {$<$}text, speech{$>$} pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
  issue = {19},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ning et al_2019_A Review of Deep Learning Based Speech Synthesis.pdf}
}

@thesis{NiXiJinPingWaiJiaoSiXiangLiLunChuangXinYanJiu2020,
  type = {硕士},
  title = {习近平外交思想理论创新研究},
  author = {倪, 响},
  date = {2020},
  institution = {{江苏大学}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020758689.nh&uniplatform=NZKPT&v=I9YePwGs6ErcizWKF1gA9Vave1%25mmd2BD3L4Ey6rNdULvtMdftJlvztLQxg7yJOJLXBKf},
  urldate = {2021-11-21},
  abstract = {在党的十九大报告中,习近平总书记指出,经过长期努力,中国特色社会主义进入了新时代,这是我国发展新的历史方位。\textasciitilde ((1))面对世界百年未有之大变局,以习近平同志为核心的党中央进行了一系列外交理论与实践创新,形成了习近平外交思想。新时代,习近平外交思想贯穿着马克思主义的立场、观点和方法,顺应时代发展潮流,继承和发展了马克思主义经典作家和中共历代领导人的外交思想,借鉴西方主要外交思想,立足实践不断进行理论创新,包含了构建新型国际关系、共商共建共享的全球治理观、构建人类命运共同体、``亲、诚、惠、容''周边外交理念、正确义利观、``一带一路''倡议等一系列外交新理念和新战略。习近平外交思想在推动构建人类命运共...},
  language = {zh-CN},
  keywords = {A community with a shared future for mankind,Building a new type of international relations,New era,Theoretical innovation,Xi Jinping Thought on Diplomacy,习近平外交思想,人类命运共同体,新时代,构建新型国际关系,理论创新},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\倪_2020_习近平外交思想理论创新研究.pdf}
}

@article{noeUnifiedAssessmentFramework2021,
  title = {Towards a Unified Assessment Framework of Speech Pseudonymisation},
  author = {No\'e, Paul-Gauthier and Nautsch, Andreas and Evans, Nicholas and Patino, Jose and Bonastre, Jean-Fran\c{c}ois and Tomashenko, Natalia and Matrouf, Driss},
  date = {2021-09-21},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  pages = {101299},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101299},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821001005},
  urldate = {2021-09-28},
  abstract = {Anonymisation and pseudonymisation are two similar concepts used in privacy preservation for speech data. With no established definitions for these tasks, nor standard approaches to assessment, this paper provides definitions and presents two complementary assessment frameworks. The first is based on voice similarity matrices which provide both an immediate visualisation of privacy protection performance at the speaker level and two objective measures in the form of de-identification and voice distinctiveness preservation. The approach readily highlights imbalances in system performance at the speaker level. The second, referred to as the zero evidence biometric recognition assessment (ZEBRA) framework, is based on information theory and measures the amount of private information disclosed in speech data. The paper presents also an extension to the original ZEBRA framework. It aims to reflect the robustness of the privacy safeguard when a privacy adversary adapts to the protected speech. We demonstrate the application of both frameworks to assess pseudonymisation performance on the two VoicePrivacy 2020 challenge baseline solutions plus a third one. The two frameworks were designed independently of each other. The ZEBRA framework is fully consistent with the Bayesian decision theory and the other framework focuses instead on speaker-wise visualisations of a system performance. Thus, while metrics derived from them bear similarities, they expose differences in safeguard behaviour. The assessment of pseudonymisation remains challenging and merits greater attention in the future.},
  language = {en},
  keywords = {Anonymisation,Assessment methods,Privacy preserving speech transformation,Pseudonymisation,Speaker verification,Voice conversion},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\4D4JNLP9\\S0885230821001005.html}
}

@article{oglicLearningWaveformBasedAcoustic2021,
  title = {Learning {{Waveform-Based Acoustic Models Using Deep Variational Convolutional Neural Networks}}},
  author = {Oglic, Dino and Cvetkovic, Zoran and Sollich, Peter},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2850--2863},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3104193},
  abstract = {We investigate the potential of stochastic neural networks for learning effective waveform-based acoustic models. The waveform-based setting, inherent to fully end-to-end speech recognition systems, is motivated by several comparative studies of automatic and human speech recognition that associate standard non-adaptive feature extraction techniques with information loss, which can adversely affect robustness. Stochastic neural networks, on the other hand, are a class of models capable of incorporating rich regularization mechanisms into the learning process. We consider a deep convolutional neural network that first decomposes speech into frequency sub-bands via an adaptive parametric convolutional block where filters are specified by cosine modulations of compactly supported windows. The network then employs standard non-parametric 1D convolutions to extract relevant spectro-temporal patterns while gradually compressing the structured high dimensional representation generated by the parametric block. We rely on a probabilistic parametrization of the proposed neural architecture and learn the model using stochastic variational inference. This requires evaluation of an analytically intractable integral defining the Kullback\textendash Leibler divergence term responsible for regularization, for which we propose an effective approximation based on the Gauss\textendash Hermite quadrature. Our empirical results demonstrate a superior performance of the proposed approach over comparable waveform-based baselines and indicate that it could lead to robustness. Moreover, the approach outperforms a recently proposed deep convolutional neural network for learning of robust acoustic models with standard FBANK features.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,Convolution,Convolutional neural networks,Feature extraction,Neural networks,parametric filters,Speech processing,Speech recognition,Stochastic processes,variational inference,waveform-based speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Oglic et al_2021_Learning Waveform-Based Acoustic Models Using Deep Variational Convolutional.pdf}
}

@article{ongModelingEmotionComplex2021,
  title = {Modeling {{Emotion}} in {{Complex Stories}}: {{The Stanford Emotional Narratives Dataset}}},
  shorttitle = {Modeling {{Emotion}} in {{Complex Stories}}},
  author = {Ong, Desmond C. and Wu, Zhengxuan and Tan, Zhi-Xuan and Reddan, Marianne and Kahhale, Isabella and Mattek, Alison and Zaki, Jamil},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {579--594},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2955949},
  abstract = {Human emotions unfold over time, and more affective computing research has to prioritize capturing this crucial component of real-world affect. Modeling dynamic emotional stimuli requires solving the twin challenges of time-series modeling and of collecting high-quality time-series datasets. We begin by assessing the state-of-the-art in time-series emotion recognition, and we review contemporary time-series approaches in affective computing, including discriminative and generative models. We then introduce the first version of the Stanford Emotional Narratives Dataset (SENDv1): a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models. We demonstrate several baseline and state-of-the-art modeling approaches on the SEND, including a Long Short-Term Memory model and a multimodal Variational Recurrent Neural Network, which perform comparably to the human-benchmark. We end by discussing the implications for future research in time-series affective computing.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_Waiting for read,affect sensing and analysis,Affective computing,Biological system modeling,Computational modeling,Data models,emotional corpora,Hidden Markov models,multi-modal recognition,Recurrent neural networks,Videos},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ong et al_2021_Modeling Emotion in Complex Stories.pdf}
}

@article{palMetaLearningLatentSpace2021,
  title = {Meta-{{Learning With Latent Space Clustering}} in {{Generative Adversarial Network}} for {{Speaker Diarization}}},
  author = {Pal, Monisankha and Kumar, Manoj and Peri, Raghuveer and Park, Tae Jin and Kim, So Hyun and Lord, Catherine and Bishop, Somer and Narayanan, Shrikanth},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1204--1219},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3061885},
  abstract = {The performance of most speaker diarization systems with x-vector embeddings is both vulnerable to noisy environments and lacks domain robustness. Earlier work on speaker diarization using generative adversarial network (GAN) with an encoder network (ClusterGAN) to project input x-vectors into a latent space has shown promising performance on meeting data. In this paper, we extend the ClusterGAN network to improve diarization robustness and enable rapid generalization across various challenging domains. To this end, we fetch the pre-trained encoder from the ClusterGAN and fine tune it by using prototypical loss (meta-ClusterGAN or MCGAN) under the meta-learning paradigm. Experiments are conducted on CALLHOME telephonic conversations, AMI meeting data, DIHARD-II (dev set) which includes challenging multi-domain corpus, and two child-clinician interaction corpora (ADOS, BOSCC) related to the autism spectrum disorder domain. Extensive analyses of the experimental data are done to investigate the effectiveness of the proposed ClusterGAN and MCGAN embeddings over x-vectors. The results show that the proposed embeddings with normalized maximum eigengap spectral clustering (NME-SC) back-end consistently outperform the Kaldi state-of-the-art x-vector diarization system. Finally, we employ embedding fusion with x-vectors to provide further improvement in diarization performance. We achieve a relative diarization error rate (DER) improvement of 6.67\% to 53.93\% on the aforementioned datasets using the proposed fused embeddings over x-vectors. Besides, the MCGAN embeddings provide better performance in the number of speakers estimation and short speech segment diarization compared to x-vectors and ClusterGAN on telephonic conversations.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {ClusterGAN,Clustering algorithms,Gallium nitride,Generative adversarial networks,MCGAN,Neural networks,NME-SC,Prototypes,speaker diarization,speaker embeddings,Speech processing,Task analysis,x-vector},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Pal et al_2021_Meta-Learning With Latent Space Clustering in Generative Adversarial Network.pdf}
}

@article{pandeyDenseCNNSelfAttention2021,
  title = {Dense {{CNN With Self-Attention}} for {{Time-Domain Speech Enhancement}}},
  author = {Pandey, Ashutosh and Wang, DeLiang},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1270--1279},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3064421},
  abstract = {Speech enhancement in the time domain is becoming increasingly popular in recent years, due to its capability to jointly enhance both the magnitude and the phase of speech. In this work, we propose a dense convolutional network (DCN) with self-attention for speech enhancement in the time domain. DCN is an encoder and decoder based architecture with skip connections. Each layer in the encoder and the decoder comprises a dense block and an attention module. Dense blocks and attention modules help in feature extraction using a combination of feature reuse, increased network depth, and maximum context aggregation. Furthermore, we reveal previously unknown problems with a loss based on the spectral magnitude of enhanced speech. To alleviate these problems, we propose a novel loss based on magnitudes of enhanced speech and a predicted noise. Even though the proposed loss is based on magnitudes only, a constraint imposed by noise prediction ensures that the loss enhances both magnitude and phase. Experimental results demonstrate that DCN trained with the proposed loss substantially outperforms other state-of-the-art approaches to causal and non-causal speech enhancement.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Convolution,dense convolutional network,Feature extraction,frequency-domain loss,Noise measurement,self-attention network,Signal to noise ratio,Speech enhancement,Time-domain analysis,time-domain enhancement,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Pandey_Wang_2021_Dense CNN With Self-Attention for Time-Domain Speech Enhancement.pdf}
}

@article{panzhangDeepLearningFramework2021,
  title = {A {{Deep Learning Framework}} of {{Autonomous Pilot Agent}} for {{Air Traffic Controller Training}}},
  author = {{Pan Zhang} and {Changyu Yin} and {Bo Yang} and {Jianwei Zhang} and {asda} and {asdasasdas} and {asdadada} and {asdsadfasdas}},
  date = {2021},
  journaltitle = {IEEE Transactions on Human-Machine Systems},
  volume = {51},
  number = {5},
  pages = {442--450},
  issn = {2168-2305},
  doi = {10.1109/thms.2021.3102827},
  abstract = {In this work, a deep learning-based framework is proposed to implement an autonomous pilot agent (APA), which serves as a human pseudo-pilot to assist air traffic controller (ATCO) training. A novel paradigm, including speech recognition, language understanding, pilot repetition generation (PRG), and text-to-speech (TTS), is designed to formulate the framework pipeline, which also incorporates a simulation system interface. We mainly focus on the PRG and TTS models to address the ATC specificities in this work. The neural architecture is proposed to generate the text repetition instruction by using a sequence-to-sequence text mapping. The Transformer block is improved to implement a high-efficient TTS model, in which the nonautoregressive mechanism is applied to achieve the parallel synthesis. A dedicated phoneme vocabulary is designed to cope with the multilingual issue in the ATC domain and address the out-of-vocabulary problem. With the APA framework, a virtual training mode is proposed to complete the training task without the limitation of time and location. Experimental results on a real-world dataset show that the proposed APA framework replaces the human pilot with considerable high confidence in a real-time manner during the simulation training. Most importantly, the APA framework and the virtual training system are able to cope with the dilemma of physical attendance (like COVID-19) and improve the equipment utilization capacity for the ATCO training.},
  eventtitle = {{{IEEE Transactions}} on {{Human-Machine Systems}}},
  issue = {5},
  language = {en},
  keywords = {\#nosource,Air traffic controller (ATCO) training,Aircraft,Atmospheric modeling,autonomous pilot agent (APA),Hidden Markov models,Neural networks,pilot repetition generation (PRG),Speech recognition,Task analysis,text-to-speech (TTS),Training,virtual training}
}

@inproceedings{pappagariCopyPasteAugmentationMethod2021,
  title = {{{CopyPaste}}: {{An Augmentation Method}} for {{Speech Emotion Recognition}}},
  shorttitle = {{{CopyPaste}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Pappagari, Raghavendra and Villalba, Jes\'us and \.Zelasko, Piotr and Moro-Velazquez, Laureano and Dehak, Najim},
  date = {2021-06},
  pages = {6324--6328},
  issn = {2379-190X},
  doi = {10/gmr2jq},
  abstract = {Data augmentation is a widely used strategy for training robust machine learning models. It partially alleviates the problem of limited data for tasks like speech emotion recognition (SER), where collecting data is expensive and challenging. This study proposes CopyPaste, a perceptually motivated novel augmentation procedure for SER. Assuming that the presence of emotions other than neutral dictates a speaker's overall perceived emotion in a recording, concatenation of an emotional (emotion E) and a neutral utterance can still be labeled with emotion E. We hypothesize that SER performance can be improved using these concatenated utterances in model training. To verify this, three CopyPaste schemes are tested on two deep learning models: one trained independently and another using transfer learning from an x-vector model, a speaker recognition model. We observed that all three CopyPaste schemes improve SER performance on all the three datasets considered: MSP-Podcast, Crema-D, and IEMOCAP. Additionally, CopyPaste performs better than noise augmentation and, using them together improves the SER performance further. Our experiments on noisy test sets suggested that CopyPaste is effective even in noisy test conditions.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,CopyPaste,data augmentation,emotion recognition,Emotion recognition,Noise measurement,Signal processing,Speaker recognition,Speech recognition,Training,transfer learning,Transfer learning,x-vector},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Pappagari et al_2021_CopyPaste.pdf}
}

@inproceedings{parrySpeechEmotionRecognition2022,
  title = {Speech {{Emotion Recognition}} in the {{Wild}} Using {{Multi-task}} and {{Adversarial Learning}}},
  booktitle = {Interspeech 2022},
  author = {Parry, Jack and DeMattos, Eric and Klementiev, Anita and Ind, Axel and Morse-Kopp, Daniela and Clarke, Georgia and Palaz, Dimitri},
  date = {2022-09-18},
  pages = {1158--1162},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10581},
  url = {https://www.isca-speech.org/archive/interspeech_2022/parry22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Parry et al_2022_Speech Emotion Recognition in the Wild using Multi-task and Adversarial Learning.pdf}
}

@article{patilEffectivenessEnergySeparationbased2022,
  title = {Effectiveness of Energy Separation-Based Instantaneous Frequency Estimation for Cochlear Cepstral Features for Synthetic and Voice-Converted Spoofed Speech Detection},
  author = {Patil, Ankur T. and Patil, Hemant A. and Khoria, Kuldeep},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101301},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101301},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821001017},
  urldate = {2021-09-28},
  abstract = {In this article, we propose Cochlear Filter Cepstral Coefficient-Instantaneous Frequency feature set using Energy Separation Algorithm (CFCCIF-ESA) feature set to detect the speech synthesis (SS) and voice conversion (VC)-based spoofing attacks. The SS- and VC-based spoof generation techniques predominantly uses the magnitude spectrum information, neglecting the phase information. Hence, SS and VC generated speech signal possess the distorted phase in time or frequency-domain. In this work, we exploit this anomaly in phase to efficiently detect the spoofing attack. Here, instantaneous frequency (IF) is utilized to represent the phase information as IF is nothing but the derivative of unwrapped instantaneous (analytic) phase. The experiments are performed on ASVSpoof-2015 challenge dataset, which is specifically designed to do Spoof Speech Detection (SSD) task for SS and VC. In ASVSpoof-2015 challenge during INTERSPEECH 2015, SSD system designed using Cochlear Filter Cepstral Coefficient-Instantaneous Frequency (CFCCIF) feature set was the relatively best performing system. The CFCCIF feature set composed of the information obtained from the magnitude envelope derived using cochlear filterbank and instantaneous frequency (IF) which is derived from Hilbert transform-based approach. However, Hilbert transform-based estimation requires a speech segment of 10\textendash 30 ms and thus, it limits time resolution of IF estimation and hence, defeats the key objective of IF estimation to be able to fit the frequency of a sinusoid (corresponding to a monocomponent signal) locally and almost instantaneously. Energy Separation Algorithm (ESA) is known to accurately estimate the modulation patterns due to their relatively low computational complexity, high time resolution, and instantaneously adapting nature. To that effect, we exploit the ESA instead of Hilbert transform to estimate the IFs of the subband filtered signal using cochlear filterbank. The significant improvement in performance, in particular, relative reduction of 51.21\% and 46.87\% in EER is observed on development and evaluation subsets, respectively, for CFCCIF-ESA feature set over its CFCCIF counterpart, using Gaussian Mixture Model (GMM)-based classifier. This improvement in the performance indicates that the IFs estimated using ESA-based approach are able to efficiently capture the artefacts produced in the instantaneous phase by the SS- and VC-based spoof signals. Furthermore, experiments are also performed with convolutional neural network (CNN) classifier, which further enhances the performance.},
  language = {en},
  keywords = {_gj,\#nosource,CFCCIF-ESA,Cochlear Filterbank,Energy separation algorithm (ESA),Hilbert transform,Instantaneous frequency (IF),Spoof Speech Detection}
}

@article{patilImprovingPotentialEnhanced2022,
  title = {Improving the Potential of {{Enhanced Teager Energy Cepstral Coefficients}} ({{ETECC}}) for Replay Attack Detection},
  author = {Patil, Ankur T. and Acharya, Rajul and Patil, Hemant A. and Guido, Rodrigo Capobianco},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101281},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101281},
  url = {https://www.sciencedirect.com/science/article/pii/S088523082100084X},
  urldate = {2021-09-28},
  abstract = {In the scope of voice biometrics, the term replay attack, (RA) refers to the dishonest attempt made by an impostor to spoof someone else's identity by replaying the subject's previously recorded speech close to the Automatic Speaker Verification (ASV) system under attack. State-of-the-art strategies for RA detection, such as the Enhanced Teager Energy Cepstral Coefficients (ETECC), have shown promising results due to their precision in measuring energy from high frequency components of speech, as a function of two recently defined concepts: signal mass and Enhanced Teager Energy Operator (ETEO). Nevertheless, since the replay mechanism prominently deteriorates the speech signal spectrum just in those spectral zones, we propose the association of ETEO with different strategies to further improve the previous results in getting effective countermeasures against RAs. Specifically, comprehensive evaluations which include a detailed mathematical analysis, a simulation on amplitude and frequency modulated (AM\textendash FM) signals, and a spectrographic inspection involving different filterbank structures, along with their experimental results, are provided in this paper. In addition, ETEO-derived features are contrasted to existing feature sets by using Paraconsistent Feature Engineering (PFE) for feature ranking, expanding our previously published results. Lastly, experiments are performed with ASVSpoof-2017 version 2.0 dataset, Realistic Replay Attack Microphone Array Speech Corpus (ReMASC), BTAS-2016, dataset, ASVSpoof-2019 challenge dataset, and ASVSpoof-2015 challenge dataset, considering Gaussian Mixture Models (GMMs), Convolutional Neural Networks (CNNs) and Light-CNN architectures as being the classifiers. The standalone ETECC-GMM system showed the best performance by producing equal error rates (EERs) of 5.55\% and 10.75\% on development and evaluation sets, respectively.},
  language = {en},
  keywords = {_gj,Automatic speaker verification (ASV),Enhanced Teager Energy Cepstral Coefficients (ETECCs),Enhanced Teager Energy Operator (ETEO),Handcrafted features,Paraconsistent Feature Engineering (PFE),Replay attacks (RAs)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Patil et al_2022_Improving the potential of Enhanced Teager Energy Cepstral Coefficients (ETECC).pdf}
}

@inproceedings{paulEnhancingSpeechIntelligibility2020,
  title = {Enhancing {{Speech Intelligibility}} in {{Text-To-Speech Synthesis Using Speaking Style Conversion}}},
  booktitle = {Interspeech 2020},
  author = {Paul, Dipjyoti and Shifas, Muhammed P.V. and Pantazis, Yannis and Stylianou, Yannis},
  date = {2020-10-25},
  pages = {1361--1365},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2020-2793},
  url = {https://www.isca-speech.org/archive/interspeech_2020/paul20b_interspeech.html},
  urldate = {2021-09-10},
  abstract = {The increased adoption of digital assistants makes textto-speech (TTS) synthesis systems an indispensable feature of modern mobile devices. It is hence desirable to build a system capable of generating highly intelligible speech in the presence of noise. Past studies have investigated style conversion in TTS synthesis, yet degraded synthesized quality often leads to worse intelligibility. To overcome such limitations, we proposed a novel transfer learning approach using Tacotron and WaveRNN based TTS synthesis. The proposed speech system exploits two modification strategies: (a) Lombard speaking style data and (b) Spectral Shaping and Dynamic Range Compression (SSDRC) which has been shown to provide high intelligibility gains by redistributing the signal energy on the time-frequency domain. We refer to this extension as Lombard-SSDRC TTS system. Intelligibility enhancement as quantified by the Intelligibility in Bits (SIIBGauss) measure shows that the proposed LombardSSDRC TTS system shows significant relative improvement between 110\% and 130\% in speech-shaped noise (SSN), and 47\% to 140\% in competing-speaker noise (CSN) against the state-ofthe-art TTS approach. Additional subjective evaluation shows that Lombard-SSDRC TTS successfully increases the speech intelligibility with relative improvement of 455\% for SSN and 104\% for CSN in median keyword correction rate compared to the baseline TTS method.},
  eventtitle = {Interspeech 2020},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Paul et al_2020_Enhancing Speech Intelligibility in Text-To-Speech Synthesis Using Speaking.pdf}
}

@inproceedings{pengEfficientSpeechEmotion2021,
  title = {Efficient {{Speech Emotion Recognition Using Multi-Scale CNN}} and {{Attention}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Peng, Zixuan and Lu, Yu and Pan, Shengfeng and Liu, Yunfeng},
  date = {2021-06},
  pages = {3020--3024},
  issn = {2379-190X},
  doi = {10/gmr2jx},
  abstract = {Emotion recognition from speech is a challenging task. Recent advances in deep learning have led bi-directional recurrent neural network (Bi-RNN) and attention mechanism as a standard method for speech emotion recognition, extracting and attending multi-modal features - audio and text, and then fusing them for downstream emotion classification tasks. In this paper, we propose a simple yet efficient neural network architecture to exploit both acoustic and lexical information from speech. The proposed framework using multi-scale convolutional layers (MSCNN) to obtain both audio and text hidden representations. Then, a statistical pooling unit (SPU) is used to further extract the features in each modality. Besides, an attention module can be built on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the performance. Extensive experiments show that the proposed model outperforms previous state-of-the-art methods on IEMOCAP dataset with four emotion categories (i.e., angry, happy, sad and neutral) in both weighted accuracy (WA) and unweighted accuracy (UA), with an improvement of 5.0\% and 5.2\% respectively under the ASR setting.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Acoustics,Deep Learning and Natural Language Processing,Emotion recognition,Feature extraction,Natural language processing,Recurrent neural networks,Robustness,Speech Emotion Recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Peng et al_2021_Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention.pdf}
}

@inproceedings{pengModelingIntentionEmotion2022,
  title = {Modeling {{Intention}}, {{Emotion}} and {{External World}} in {{Dialogue Systems}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Peng, Wei and Hu, Yue and Xing, Luxi and Xie, Yuqiang and Zhang, Xingsheng and Sun, Yajing},
  date = {2022},
  pages = {7042--7046},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747565},
  abstract = {Intention, emotion and action are important elements in human activities. Modeling the interaction process between individuals by analyzing the relationships between these elements is a challenging task. However, previous work mainly focused on modeling intention and emotion independently, and neglected of exploring the mutual relationships between intention and emotion. In this paper, we propose a RelAtion Interaction Network (RAIN), consisting of Intention Relation Module and Emotion Relation Module, to jointly model mutual relationships and explicitly integrate historical intention information. The experiments on the dataset show that our model can take full advantage of the intention, emotion and action between individuals and achieve a remarkable improvement over BERT-style baselines. Qualitative analysis verifies the importance of the mutual interaction between the intention and emotion.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustics,Analytical models,Conferences,Emotion Prediction,Human Interaction,Intention Recognition,Psychology,Rain,Signal processing,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Peng et al_2022_Modeling Intention, Emotion and External World in Dialogue Systems.pdf}
}

@thesis{PengYuYinQingGanShiBieJiShuYanJiu2016,
  type = {硕士},
  title = {语音情感识别技术研究},
  author = {彭, 杰},
  date = {2016},
  institution = {{电子科技大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201701&filename=1016176908.nh&uniplatform=NZKPT&v=CwpCpughQFMB%25mmd2FW6r1WWH9EV2gSCHMGLAug%25mmd2Fquw9hlm5%25mmd2F%25mmd2BwMrucPiv6BgY3vNUpyG},
  urldate = {2021-09-10},
  abstract = {伴随着信息通讯技术、移动互联网技术的高速发展,人们越来越希望能够以一种智能化、情感化、人性化的方式与计算机进行交流。语音是人类最直接的交流方式,同时也是人类情感的主要载体。语音情感识别技术研究作为现如今一个新兴的研究方向,不仅对人机交互有着至关重要的意义,更对人工智能有着重要的影响。本文在研究和分析现有的一些语音情感识别技术的基础上,首先从超音段特征、谱特征以及一种基于临界带的多分辨率分析的特征入手,对本文所采用的情感特征进行分析;最后首次尝试采用投影字典对学习（DPL）算法来解决语音情感识别问题。本文的主要研究工作如下:1.本文对情感特征的研究,包括以下两点:1)提取了超音段特征中的响度特征和基音频率特征、谱特征中的MEL频率子带能量系数（MFBECS）特征和线性谱频率（LSF）特征。主要对基音频率F0特征的提取方法进行了研究,针对SHS算法存在倍频错误的问题,对SHS算法进行改进。2)因为并不是所有的频谱部分都对人类的感知系统有影响,所以引入了一种基于临界带的多分辨率分析的GPWP特征来对语音情感进行识别,并对该特征中所采用的小波包基函数进行了研究,结果表明coif2基函数的识别性能最好。2.本文研究了基于稀疏表示分类（SRC）的语音情感识别,针对SRC在解决语音情感识别时存在的速度慢且识别效果不理想的问题,首次尝试采用DPL的方法来解决语音情感识别问题。3.本文使用Emodb、Polish、eNTERFACE'05三个语音情感数据库来进行实验。首先对GPWP特征的识别性能进行研究,结果表明该特征的识别效果优于其他四种特征;其次,将本文提取的情感特征组合与相关文献进行对比,结果表明本文的特征组合性能较好;最后,对DPL所采用的原子个数进行了研究,并从时间性能和识别性能两个角度出发,将DPL方法与SRC、SVM、JSLRR和CRC四种识别方法进行了比较,结果表明该方法不仅具有较好的识别性能,同时还具有良好的时间性能。},
  editora = {漆, 进},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,emotional features,projection dictionary pair learning,sparse representation classification,speech emotion recognition,情感特征,投影字典对学习,稀疏表示分类,语音情感识别},
  annotation = {2 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\彭_2016_语音情感识别技术研究.pdf}
}

@inproceedings{pepinoEmotionRecognitionSpeech2021,
  title = {Emotion {{Recognition}} from {{Speech Using}} Wav2vec 2.0 {{Embeddings}}},
  booktitle = {Interspeech 2021},
  author = {Pepino, Leonardo and Riera, Pablo and Ferrer, Luciana},
  date = {2021-08-30},
  pages = {3400--3404},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-703},
  url = {https://www.isca-speech.org/archive/interspeech_2021/pepino21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Pepino et al_2021_Emotion Recognition from Speech Using wav2vec 2.pdf}
}

@inproceedings{perezMindGapValue2022,
  title = {Mind the Gap: {{On}} the Value of Silence Representations to Lexical-Based Speech Emotion Recognition},
  shorttitle = {Mind the Gap},
  booktitle = {Interspeech 2022},
  author = {Perez, Matthew and Jaiswal, Mimansa and Niu, Minxue and Gorrostieta, Cristina and Roddy, Matthew and Taylor, Kye and Lotfian, Reza and Kane, John and Provost, Emily Mower},
  date = {2022-09-18},
  pages = {156--160},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10943},
  url = {https://www.isca-speech.org/archive/interspeech_2022/perez22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Perez et al_2022_Mind the gap.pdf}
}

@inproceedings{periDisentanglementAudioVisualEmotion2021,
  title = {Disentanglement for {{Audio-Visual Emotion Recognition Using Multitask Setup}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Peri, Raghuveer and Parthasarathy, Srinivas and Bradshaw, Charles and Sundaram, Shiva},
  date = {2021-06},
  pages = {6344--6348},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414705},
  abstract = {Deep learning models trained on audio-visual data have been successfully used to achieve state-of-the-art performance for emotion recognition. In particular, models trained with multitask learning have shown additional performance improvements. However, such multitask models entangle information between the tasks, encoding the mutual dependencies present in label distributions in the real world data used for training. This work explores the disentanglement of multimodal signal representations for the primary task of emotion recognition and a secondary person identification task. In particular, we developed a multitask framework to extract low-dimensional embeddings that aim to capture emotion specific information, while containing minimal information related to person identity. We evaluate three different techniques for disentanglement and report results of up to 13\% disentanglement while maintaining emotion recognition performance.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Data models,Deep learning,disentanglement,Emotion recognition,Encoding,multimodal learning,multitask learning,Signal processing,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Peri et al_2021_Disentanglement for Audio-Visual Emotion Recognition Using Multitask Setup.pdf}
}

@article{phanAutoencoderSemisupervisedMultiple2021,
  title = {Autoencoder for {{Semisupervised Multiple Emotion Detection}} of {{Conversation Transcripts}}},
  author = {Phan, Duc-Anh and Matsumoto, Yuji and Shindo, Hiroyuki},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {682--691},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2018.2885304},
  abstract = {Textual emotion detection is a challenge in computational linguistics and affective computing study as it involves the discovery of all associated emotions expressed within a given piece of text. It becomes an even more difficult problem when applied to conversation transcripts, as we need to model the spoken utterances between speakers, keeping in mind the context of the entire conversation. In this paper, we propose a semisupervised multilabel method of predicting emotions from conversation transcripts. The corpus contains conversational quotes extracted from movies. A small number of them are annotated, while the rest are used for unsupervised training. We use the word2vec word-embedding method to build an emotion lexicon from the corpus and to embed the utterances into vector representations. A deep-learning autoencoder is then used to discover the underlying structure of the unsupervised data. We fine-tune the learned model on labeled training data, and measure its performance on a test set. The experiment result suggests that the method is effective and is only slightly behind human annotators.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_Waiting for read,autoencoder,Context modeling,Correlation,Data models,Emotion recognition,Motion pictures,multilabel,Neural networks,semisupervised learning,Social network services,Training data,word2vec},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Phan et al_2021_Autoencoder for Semisupervised Multiple Emotion Detection of Conversation.pdf}
}

@article{piazzaButchersDeliWorkers2021,
  title = {Butchers' and Deli Workers' Psychological Adaptation to Meat},
  author = {Piazza, Jared and Hodson, Gordon and Oakley, Alexandra},
  date = {2021},
  journaltitle = {Emotion},
  volume = {21},
  number = {4},
  pages = {730--741},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1931-1516},
  doi = {10.1037/emo0000738},
  abstract = {In many societies today, the average consumer is largely removed from the earlier stages of meat production wherein meat, in many ways, resembles an animal. The present study examined the emotional and psychological consequences of recurrent meat handling. Fifty-six individuals with commercial experience handling meat (butchers and deli workers) were contrasted with 103 individuals without such experience. Participants were presented images of meat from 3 animals\textemdash cows, sheep, and fish\textemdash that were experimentally manipulated in their degree of animal resemblance. Participants rated the images on measures of disgust, empathy for the animal, and meat\textendash animal association. Broader beliefs and attitudes about meat and animals were also assessed. We used mixed-effect linear modeling to examine the role of time spent handling meat in participants' psychological adaptation to it. We observed significant reductions in disgust, empathy, and meat\textendash animal association within the first year or 2 of meat handling for all types of meat. Time spent handling meat also predicted the degree to which a person defended and rationalized meat consumption and production, independent of a participant's gender and age. The findings have implications for understanding how people adapt to potentially aversive contexts such as handling animal parts. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  issue = {4},
  language = {en},
  keywords = {Adaptation,Agricultural Workers,Animals,Attitudes,Blue Collar Workers,Disgust,Empathy},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Piazza et al_2021_Butchers’ and deli workers’ psychological adaptation to meat.pdf}
}

@article{politisOverviewEvaluationSound2021,
  title = {Overview and {{Evaluation}} of {{Sound Event Localization}} and {{Detection}} in {{DCASE}} 2019},
  author = {Politis, Archontis and Mesaros, Annamaria and Adavanne, Sharath and Heittola, Toni and Virtanen, Tuomas},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {684--698},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2020.3047233},
  abstract = {Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of the DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learning-based approaches, and for evaluation of the submissions in an unlabeled subset. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a reevaluation of submissions using these new metrics. The new analysis reveals submissions that performed better on the joint task of detecting the correct type of event close to its original location than some of the submissions that were ranked higher in the challenge. Consequently, ranking of submissions which performed strongly when evaluated separately on detection or localization, but not jointly on both, was affected negatively.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustic scene analysis,Acoustics,Azimuth,Hidden Markov models,Measurement,microphone arrays,sound event localization and detection,sound source localization,Speech processing,Task analysis,Two dimensional displays},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Politis et al_2021_Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019.pdf}
}

@inproceedings{prabhuEndToEndLabelUncertainty2022,
  title = {End-{{To-End Label Uncertainty Modeling}} for {{Speech-based Arousal Recognition Using Bayesian Neural Networks}}},
  booktitle = {Interspeech 2022},
  author = {Prabhu, Navin Raj and Carbajal, Guillaume and Lehmann-Willenbrock, Nale and Gerkmann, Timo},
  date = {2022-09-18},
  pages = {151--155},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10490},
  url = {https://www.isca-speech.org/archive/interspeech_2022/prabhu22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Prabhu et al_2022_End-To-End Label Uncertainty Modeling for Speech-based Arousal Recognition.pdf}
}

@article{principiEffectObservedSubject2021,
  title = {On the {{Effect}} of {{Observed Subject Biases}} in {{Apparent Personality Analysis From Audio-Visual Signals}}},
  author = {Principi, Ricardo Dar\'io P\'erez and Palmero, Cristina and Junior, Julio C. S. Jacques and Escalera, Sergio},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {607--621},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2956030},
  abstract = {Personality perception is implicitly biased due to many subjective factors, such as cultural, social, contextual, gender, and appearance. Approaches developed for automatic personality perception are not expected to predict the real personality of the target but the personality external observers attributed to it. Hence, they have to deal with human bias, inherently transferred to the training data. However, bias analysis in personality computing is an almost unexplored area. In this article, we study different possible sources of bias affecting personality perception, including emotions from facial expressions, attractiveness, age, gender, and ethnicity, as well as their influence on prediction ability for apparent personality estimation. To this end, we propose a multimodal deep neural network that combines raw audio and visual information alongside predictions of attribute-specific models to regress apparent personality. We also analyze spatio-temporal aggregation schemes and the effect of different time intervals on first impressions. We base our study on the ChaLearn first impressions dataset, consisting of one-person conversational videos. Our model shows state-of-the-art results regressing apparent personality based on the Big-Five model. Furthermore, given the interpretability nature of our network design, we provide an incremental analysis on the impact of each possible source of bias on final network predictions.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {audio-visual recordings,Automatic personality perception,big-five,Computational modeling,convolutional neural networks,Deep learning,Feature extraction,first impressions,multi-modal recognition,Observers,OCEAN,personality computing,subjective bias,Videos,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Principi et al_2021_On the Effect of Observed Subject Biases in Apparent Personality Analysis From.pdf}
}

@article{purushothamanDereverberationAutoregressiveEnvelopes2022,
  title = {Dereverberation of Autoregressive Envelopes for Far-Field Speech Recognition},
  author = {Purushothaman, Anurenjan and Sreeram, Anirudh and Kumar, Rohit and Ganapathy, Sriram},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101277},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101277},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000802},
  urldate = {2021-09-28},
  abstract = {The task of speech recognition in far-field environments is adversely affected by the reverberant artifacts that elicit as the temporal smearing of the sub-band envelopes. In this paper, we develop a neural model for speech dereverberation using the long-term sub-band envelopes of speech. The sub-band envelopes are derived using frequency domain linear prediction (FDLP) which performs an autoregressive estimation of the Hilbert envelopes. The neural dereverberation model estimates the envelope gain which when applied to reverberant signals suppresses the late reflection components in the far-field signal. The dereverberated envelopes are used for feature extraction in speech recognition. Further, the sequence of steps involved in envelope dereverberation, feature extraction and acoustic modeling for ASR can be implemented as a single neural processing pipeline which allows the joint learning of the dereverberation network and the acoustic model. Several experiments are performed on the REVERB challenge dataset, CHiME-3 dataset and VOiCES dataset. In these experiments, the joint learning of envelope dereverberation and acoustic model yields significant performance improvements over the baseline ASR system based on log-mel spectrogram as well as other past approaches for dereverberation (average relative improvements of 10\textendash 24\% over the baseline system). A detailed analysis on the choice of hyper-parameters and the cost function involved in envelope dereverberation is also provided.},
  language = {en},
  keywords = {Automatic speech recognition,Dereverberation,Frequency domain linear prediction (FDLP),Joint learning,Neural speech enhancement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Purushothaman et al_2022_Dereverberation of autoregressive envelopes for far-field speech recognition.pdf}
}

@article{puvianiMathematicalDescriptionEmotional2021,
  title = {A {{Mathematical Description}} of {{Emotional Processes}} and {{Its Potential Applications}} to {{Affective Computing}}},
  author = {Puviani, Luca and Rama, Sidita and Vitetta, Giorgio Matteo},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {692--706},
  issn = {1949-3045},
  doi = {10.1109/taffc.2018.2887385},
  abstract = {In affective computing, machines should detect and recognize human emotions, and adapt their behavior to them. Nowadays these objectives are mainly achieved through the exploitation of machine learning techniques. These techniques are employed to process different emotional-related features and to produce classification labels or coordinates in a valence-arousal space. This approach, however, ignores the neurophysiological processes governing implicit emotional states and, consequently, suffers from substantial limitations. Moreover, machine learning methods employed today do not benefit from the knowledge of emotional dynamics for properly adapting themselves. In this manuscript, starting from recent neuroscience and computational theories, we show how a simple mathematical description of processes governing implicit emotional dynamics can be developed. Moreover, we discuss how our mathematical models can be exploited to improve tracking, estimation and active modulation of human emotions.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {_Waiting for read,Affective computing,Bayesian inference,classical conditioning,emotion modelling,Emotion recognition,Emotional processing,Heuristic algorithms,implicit evaluation,Machine learning,Mathematical model,Neuroscience,UCS revaluation,unconscious},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Puviani et al_2021_A Mathematical Description of Emotional Processes and Its Potential.pdf}
}

@article{qianAutoVCZeroShotVoice,
  title = {{{AutoVC}}: {{Zero-Shot Voice Style Transfer}} with {{Only Autoencoder Loss}}},
  author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark},
  pages = {10},
  abstract = {Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain underexplored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distributionmatching style transfer by training only on a selfreconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-theart results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Qian et al_AutoVC.pdf}
}

@inproceedings{qianWordwiseSparseAttention2022,
  title = {Word-Wise {{Sparse Attention}} for {{Multimodal Sentiment Analysis}}},
  booktitle = {Interspeech 2022},
  author = {Qian, Fan and Song, Hongwei and Han, Jiqing},
  date = {2022-09-18},
  pages = {1973--1977},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-532},
  url = {https://www.isca-speech.org/archive/interspeech_2022/qian22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  keywords = {_Code},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Qian et al_2022_Word-wise Sparse Attention for Multimodal Sentiment Analysis.pdf}
}

@thesis{QiJiYuYuYinQingGanShiBieDeQingXuJianCeXiTongYanJiuYuShiXian2018,
  type = {硕士},
  title = {基于语音情感识别的情绪监测系统研究与实现},
  author = {齐, 柱柱},
  date = {2018},
  institution = {{南京邮电大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1018898758.nh&uniplatform=NZKPT&v=gv3QWiW%25mmd2B6gyDufm9PNPc%25mmd2B4R3gXXWasJtKJImwx4P0SJh62JsGZuXdoQgVPigZNZ9},
  urldate = {2021-09-10},
  abstract = {计算机技术和人工智能的高速发展影响着人们的日常生活,随处可见人工智能产品带来的便利,人们对人工智能产品的要求也日益增高。语言是人们日常生活中最常用的交流方式,语言的声学表现形式是语音,而语音中包含着大量的情感信息。因此分析语音信号中的情感信息,并将其应用到人工智能产品中,是语音情感识别中的一个研究热点。本文首先对待识别的语音信号进行预处理操作,然后提取有用的情感特征参数,这里选用基音频率、短时能量、共振峰、线性预测倒谱系数（LPCC）和梅尔频率倒谱系数（MFCC）五种参数,并做归一化处理作为识别阶段的输入。本文提出一种基于HMM/RBF混合模型的语音情感识别方法,该方法综合了HMM模型极强的动态时序建模能力、RBF模型较强的分类决策能力等优点,通过两种模型相结合,大大提高了语音情感识别率。另外,在RBF网络学习过程中,引入``动态最优学习率''的概念。传统的RBF网络的学习率是事先设置的固定值,不可改变,设置的过大或过小,都会影响网络的稳定性和学习效果。本文提出的设置动态最优学习率的方法,不仅可以提高网络的收敛速度,而且提高了运行效率。经Matlab仿真实验表明,HMM/RBF混合模型的识别率高于单独的HMM和RBF模型,且具有更快的收敛速度。最后本文设计并开发一个基于语音情感识别的情绪监测系统,并将建立好的情感模型应用其中,监测用户的情感状态,帮助用户调节情绪。},
  editora = {林, 巧民},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_Waiting for read,_中文,Artificial intelligence,Dynamic optimal learning rate,HMM,RBF,Speech emotion recognition,人工智能,动态最优学习率,语音情感识别},
  annotation = {5 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\齐_2018_基于语音情感识别的情绪监测系统研究与实现.pdf}
}

@article{r.NovelApproachUnsupervised2022,
  title = {A Novel Approach to Unsupervised Pattern Discovery in Speech Using {{Convolutional Neural Network}}},
  author = {R., Kishore Kumar and Sreenivasa Rao, K.},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101259},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101259},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000668},
  urldate = {2021-09-28},
  abstract = {In this paper, a novel approach to unsupervised pattern discovery for speech signals is proposed. Recently, we introduced an image processing method (IPM) that extracts the desired keywords present in a pair of speech utterances. This method performs well in detecting true positives but, at the same time it also detects higher number of false positives. Therefore, this paper aims to reduce the detection of false positives and improve the accuracy of the pattern discovery task. In the proposed work, we use the Convolutional Neural Network (CNN) as a binary classifier to detect the true and false keyword match candidates. A new frame histogram technique is introduced to generate sufficient training samples from IPM to train the CNN. The trained CNN model classifies the matched patterns into true and false classes and identifies the pairs of speech documents that contain the same keyword. The proposed method is evaluated on the Hindi as well as Bengali speech databases. The results are compared with state-of-the-art methods. The detected matched pairs of speech utterances are grouped into broader domain clusters using Newman's clustering algorithm. These clusters are useful for speech retrieval tasks.},
  language = {en},
  keywords = {_Waiting for read,\#nosource,Clustering of speech utterances,Convolutional neural network,Frame histogram,Gaussian posterior features,Speech processing,Unsupervised pattern discovery}
}

@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-07-01},
  pages = {8748--8763},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/radford21a.html},
  urldate = {2022-08-27},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\BJHICTKV\\Radford 等。 - 2021 - Learning Transferable Visual Models From Natural L.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\D6YKFFX6\\Radford 等。 - 2021 - Learning Transferable Visual Models From Natural L.pdf}
}

@misc{radfordLearningTransferableVisual2021a,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2022-08-29},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision5.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\ZEPSNSXC\\2103.html}
}

@inproceedings{rajamaniNovelAttentionBasedGated2021,
  title = {A {{Novel Attention-Based Gated Recurrent Unit}} and Its {{Efficacy}} in {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Rajamani, Srividya Tirunellai and Rajamani, Kumar T. and Mallol-Ragolta, Adria and Liu, Shuo and Schuller, Bj\"orn},
  date = {2021-06},
  pages = {6294--6298},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414489},
  abstract = {Notwithstanding the significant advancements in the field of deep learning, the basic long short-term memory (LSTM) or Gated Recurrent Unit (GRU) units have largely remained unchanged and unexplored. There are several possibilities in advancing the state-of-art by rightly adapting and enhancing the various elements of these units. Activation functions are one such key element. In this work, we explore using diverse activation functions within GRU and bi-directional GRU (BiGRU) cells in the context of speech emotion recognition (SER). We also propose a novel Attention ReLU GRU (AR-GRU) that employs attention-based Rectified Linear Unit (AReLU) activation within GRU and BiGRU cells. We demonstrate the effectiveness of AR-GRU on one exemplary application using the recently proposed network for SER namely Interaction-Aware Attention Network (IAAN). Our proposed method utilising AR-GRU within this network yields significant performance gain and achieves an unweighted accuracy of 68.3\% (2\% over the baseline) and weighted accuracy of 66.9 \% (2.2 \% absolute over the baseline) in four class emotion recognition on the IEMOCAP database.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {AReLU,attention mechanism,Bidirectional control,Conferences,Databases,Deep learning,Emotion recognition,gated recurrent unit,Logic gates,ReLU,speech emotion recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rajamani et al_2021_A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\TBPVYNJ2\\9414489.html}
}

@inproceedings{rajanCrossAttentionPreferableSelfAttention2022,
  title = {Is {{Cross-Attention Preferable}} to {{Self-Attention}} for {{Multi-Modal Emotion Recognition}}?},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Rajan, Vandana and Brutti, Alessio and Cavallaro, Andrea},
  date = {2022},
  pages = {4693--4697},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746924},
  abstract = {Humans express their emotions via facial expressions, voice intonation and word choices. To infer the nature of the underlying emotion, recognition models may use a single modality, such as vision, audio, and text, or a combination of modalities. Generally, models that fuse complementary information from multiple modalities outperform their uni-modal counterparts. However, a successful model that fuses modalities requires components that can effectively aggregate task-relevant information from each modality. As cross-modal attention is seen as an effective mechanism for multi-modal fusion, in this paper we quantify the gain that such a mechanism brings compared to the corresponding self-attention mechanism. To this end, we implement and compare a cross-attention and a self-attention model. In addition to attention, each model uses convolutional layers for local feature extraction and recurrent layers for global sequential modelling. We compare the models using different modality combinations for a 7-class emotion classification task using the IEMOCAP dataset. Experimental results indicate that albeit both models improve upon the state-of-the-art in terms of weighted and unweighted accuracy for tri- and bi-modal configurations, their performance is generally statistically comparable. The code to replicate the experiments is available at https://github.com/smartcameras/SelfCrossAttn},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,Aggregates,attention,Conferences,Convolution,Convolutional codes,emotion recognition,Emotion recognition,Fuses,Multi-modal,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rajan et al_2022_Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion.pdf}
}

@article{ranjbartabarFirstImpressionsCount2021,
  title = {First {{Impressions Count}}! {{The Role}} of the {{Human}}'s {{Emotional State}} on {{Rapport Established}} with an {{Empathic}} versus {{Neutral Virtual Therapist}}},
  author = {Ranjbartabar, Hedieh and Richards, Deborah and Bilgin, Ayse Aysin and Kutay, Cat},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {788--800},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2899305},
  abstract = {Intelligent virtual agents are being endowed with empathic behaviours to perform roles such as virtual therapists. Studies often evaluate the level of rapport established, but do not measure the therapeutic benefit and the relative advantage of empathic versus neutral behaviours. We have created two virtual (empathic/neutral) therapists. Our experiment with 63 participants consisted of one within-subjects (empathic/neutral) and one between-subjects (order) factors. Regardless of the virtual therapist used, improvements in baseline emotion were reported after the first interaction (time one) and further improvement after the second interaction (time two). Our study reveals that if the human initially expresses strong emotional feeling for a problem they are facing, rapport will be higher for the empathic therapist and the level of rapport established at the first meeting will persist regardless of whether the second encounter used empathic or neutral dialogue. Conversely, participants experiencing low emotional feeling reported greater rapport with the neutral therapist, and that level of rapport persisted in the second encounter with the alternative therapist. This study shows that an empathic agent will not necessarily build more rapport or deliver better emotional outcomes than a neutral agent. Further studies are needed to determine when tailoring and complex behaviours are justified.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {Atmospheric measurements,Emotional responses,empathic agents,Intelligent virtual agents,Medical treatment,Particle measurements,Robots,Videos,virtual therapist},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ranjbartabar et al_2021_First Impressions Count.pdf}
}

@article{raoEmotionRecognitionSpeech2013,
  title = {Emotion Recognition from Speech Using Global and Local Prosodic Features},
  author = {Rao, K. Sreenivasa and Koolagudi, Shashidhar G. and Vempada, Ramu Reddy},
  date = {2013-06-01},
  journaltitle = {International Journal of Speech Technology},
  shortjournal = {Int J Speech Technol},
  volume = {16},
  number = {2},
  pages = {143--160},
  issn = {1572-8110},
  doi = {10/gjgmvp},
  url = {https://doi.org/10.1007/s10772-012-9172-2},
  urldate = {2021-09-15},
  abstract = {In this paper, global and local prosodic features extracted from sentence, word and syllables are proposed for speech emotion or affect recognition. In this work, duration, pitch, and energy values are used to represent the prosodic information, for recognizing the emotions from speech. Global prosodic features represent the gross statistics such as mean, minimum, maximum, standard deviation, and slope of the prosodic contours. Local prosodic features represent the temporal dynamics in the prosody. In this work, global and local prosodic features are analyzed separately and in combination at different levels for the recognition of emotions. In this study, we have also explored the words and syllables at different positions (initial, middle, and final) separately, to analyze their contribution towards the recognition of emotions. In this paper, all the studies are carried out using simulated Telugu emotion speech corpus (IITKGP-SESC). These results are compared with the results of internationally known Berlin emotion speech corpus (Emo-DB). Support vector machines are used to develop the emotion recognition models. The results indicate that, the recognition performance using local prosodic features is better compared to the performance of global prosodic features. Words in the final position of the sentences, syllables in the final position of the words exhibit more emotion discriminative information compared to the words and syllables present in the other positions.},
  issue = {2},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rao et al_2013_Emotion recognition from speech using global and local prosodic features.pdf}
}

@article{rehrSNRBasedFeaturesDiverse2021,
  title = {{{SNR-Based Features}} and {{Diverse Training Data}} for {{Robust DNN-Based Speech Enhancement}}},
  author = {Rehr, Robert and Gerkmann, Timo},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1937--1949},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3082702},
  abstract = {In this paper, we address the generalization of deep neural network (DNN) based speech enhancement to unseen noise conditions for the case that training data is limited in size and diversity. To gain more insights, we analyze the generalization with respect to (1) the size and diversity of the training data, (2) different network architectures, and (3) the chosen features. To address (1), we train networks on the Hu noise corpus (limited size), the CHiME 3 noise corpus (limited diversity) and also propose a large and diverse dataset collected based on freely available sounds. To address (2), we compare a fully-connected feed-forward and a long short-term memory (LSTM) architecture. To address (3), we compare three input features, namely logarithmized noisy periodograms, noise aware training (NAT) and the proposed signal-to-noise ratio based noise aware training (SNR-NAT). We confirm that rich training data and improved network architectures help DNNs to generalize. Furthermore, we show via experimental results and an analysis using t-distributed stochastic neighbor embedding (t-SNE) that the proposed SNR-NAT features yield robust and level independent results in unseen noise even with simple network architectures and when trained on only small datasets, which is the key contribution of this paper.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Deep neural networks,generalization,input features,Neural networks,Noise measurement,noise reduction,Signal processing algorithms,Signal to noise ratio,speech enhancement,Speech enhancement,Training,Training data},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rehr_Gerkmann_2021_SNR-Based Features and Diverse Training Data for Robust DNN-Based Speech.pdf}
}

@thesis{RenJiYuDuoJiFenLeiDeYuYinQingGanShiBie2016,
  type = {硕士},
  title = {基于多级分类的语音情感识别},
  author = {任, 浩},
  date = {2016},
  institution = {{哈尔滨工业大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201701&filename=1016774577.nh&uniplatform=NZKPT&v=T1p5%25mmd2Fiv6SuRbxj03JGl%25mmd2BPLASsub3Qa6qZleQpfHbk9zAbPU7146e0q7LIELmrEVF},
  urldate = {2021-09-10},
  abstract = {在人工智能飞速发展的今天,机器若要真正实现智能化,则必须要能够理解人类的情感,所以对于人类语音情感识别的研究显得尤为重要。本文选用柏林语音情感库,在传统SVM用于语音情感识别的基础之上,提出了一种多级分类算法,平均识别率较传统SVM方法提高了5.42\%;然后本文又利用PCA来进行特征优选,将优选后的特征用于语音情感识别实验,所得七种情感的平均识别率也较传统SVM方法提高了5.24\%;最后本文将多级分类与PCA相结合,在每一级判决器上都采用PCA来进行特征优选,最终得到七种情感的平均识别率较传统SVM方法提高了7.85\%。本文研究了如何对语音文本进行端点检测,并研究了MFCC、基音频率、共振峰、Delta特征、短时过零率、短时能量等情感特征的提取方法。采用所提取到的情感特征并利用传统SVM方法进行语音情感识别获得了58.69\%的平均识别率。由传统方法所得出的混淆矩阵,本文引入了混淆度的概念,从而提出了一种多级分类构造算法,即将容易区分的情感先分开,然后再对易混淆的情感进行分类,逐级地判断出待识别语音的情感类型。与传统SVM进行语音情感识别相比,基于多级分类的语音情感识别平均识别率提高了5.42\%,从而证明了该多级分类算法确实能够提高语音情感的识别率。然而,由于所涉及语音特征较多,算法对运行速度和存储空间都有较高的要求。本文采用PCA来优选特征,将PCA与传统SVM方法相结合得到的平均识别率较传统基于SVM方法提升了5.24\%,从而证明了PCA用于特征降维并优选特征确实能够提高情感识别率。最终将多级分类与PCA特征降维相结合,从而得到七种情感的平均识别率为66.54\%,与传统基于SVM方法进行语音情感识别进行对比,该方法得到的平均识别率提高了7.85\%,从而证明了本文所提出的方法的正确性。},
  editora = {叶, 亮},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Feature Extraction,Multi-layer Classification,PCA,Speech Emotion Recognition,多级分类,特征提取,语音情感识别},
  annotation = {3 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\任_2016_基于多级分类的语音情感识别.pdf}
}

@article{rezazadeganSymbolicStatisticalLearning2021,
  title = {Symbolic and {{Statistical Learning Approaches}} to {{Speech Summarization}}: {{A Scoping Review}}},
  shorttitle = {Symbolic and {{Statistical Learning Approaches}} to {{Speech Summarization}}},
  author = {Rezazadegan, Dana and Berkovsky, Shlomo and Quiroz, Juan C. and Kocaballi, A. Baki and Wang, Ying and Laranjo, Liliana and Coiera, Enrico},
  date = {2021-09-29},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  pages = {101305},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101305},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821001042},
  urldate = {2021-10-02},
  abstract = {Speech summarization techniques take human speech as input and then output an abridged version as text or speech. Speech summarization has applications in many domains from information technology to health care, for example improving speech archives or reducing clinical documentation burden. This scoping review maps close to 2 decades of speech summarization literature, spanning from the early machine learning works up to ensemble models, with no restrictions on the language summarized, research method, or paper type. We reviewed a total of 110 papers out of a set of 188 found through a literature search and extracted speech features used, methods, scope, and training corpora. Most studies employ one of four speech summarization architectures: (1) Sentence extraction and compaction; (2) Feature extraction and classification or rank-based sentence selection; (3) Sentence compression and compression summarization; and (4) Language modelling. We also discuss the strengths and weaknesses of these different methods and speech features. Overall, supervised methods (e.g. Hidden Markov support vector machines, Ranking support vector machines, Conditional random fields) performed better than unsupervised methods. As supervised methods require manually annotated training data which can be costly, there was more interest in unsupervised methods. Recent research into unsupervised methods focusses on extending language modelling, for example by combining Uni-gram modelling with deep neural networks. This review does not include recent work in deep learning.},
  language = {en},
  keywords = {\#nosource,Abstractive summarization,Automatic speech recognition,Extractive summarization,Speech summarization,Spontaneous speech},
  annotation = {ECC: No Data (logprob: -152.854)}
}

@inproceedings{rizosStarganEmotionalSpeech2020,
  title = {Stargan for {{Emotional Speech Conversion}}: {{Validated}} by {{Data Augmentation}} of {{End-To-End Emotion Recognition}}},
  shorttitle = {Stargan for {{Emotional Speech Conversion}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Rizos, Georgios and Baird, Alice and Elliott, Max and Schuller, Bjorn},
  date = {2020-05},
  pages = {3502--3506},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9054579},
  url = {https://ieeexplore.ieee.org/document/9054579/},
  urldate = {2021-09-10},
  abstract = {In this paper, we propose an adversarial network implementation for speech emotion conversion as a data augmentation method, validated by a multi-class speech affect recognition task. In our setting, we do not assume the availability of parallel data, and we additionally make it a priority to exploit as much as possible the available training data by adopting a cycle-consistent, class-conditional generative adversarial network with an auxiliary domain classifier. Our generated samples are valuable for data augmentation, achieving a corresponding 2 \% and 6 \% absolute increase in Micro- and MacroF1 compared to the baseline in a 3-class classification paradigm using a deep, end-to-end network. We finally perform a human perception evaluation of the samples, through which we conclude that our samples are indicative of their target emotion, albeit showing a tendency for confusion in cases where the emotional attribute of valence and arousal are inconsistent.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5},
  language = {en},
  keywords = {_Code,_read,_tablet},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rizos et al_2020_Stargan for Emotional Speech Conversion.pdf}
}

@article{rodriguezIncorporatingWirelessCommunication2021,
  title = {Incorporating {{Wireless Communication Parameters Into}} the {{E-Model Algorithm}}},
  author = {Rodr\'iguez, Dem\'ostenes Z. and Carrillo, Dick and Ram\'irez, Miguel A. and Nardelli, Pedro H. J. and M\"oller, Sebastian},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {956--968},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3057955},
  abstract = {Telecommunication service providers have to guarantee acceptable speech quality during a phone call to avoid a negative impact on the users' quality of experience. Currently, there are different speech quality assessment methods. ITU-T Recommendation G.107 describes the E-model algorithm, which is a computational model developed for network planning purposes focused on narrowband (NB) networks. Later, ITU-T Recommendations G.107.1 and G.107.2 were developed for wideband (WB) and fullband (FB) networks. These algorithms use different impairment factors, each one related to different speech communication steps. However, the NB, WB, and FB E-model algorithms do not consider wireless techniques used in these networks, such as Multiple-Input-Multiple-Output (MIMO) systems, which are used to improve the communication system robustness in the presence of different types of wireless channel degradation. In this context, the main objective of this study is to propose a general methodology to incorporate wireless network parameters into the NB and WB E-model algorithms. To accomplish this goal, MIMO and wireless channel parameters are incorporated into the E-model algorithms, specifically into the \textsubscript{e,eff} and \textsubscript{e,eff,WB} impairment factors. For performance validation, subjective tests were carried out, and the proposed methodology reached a Pearson correlation coefficient (PCC) and a root mean square error (RMSE) of 0.9732 and 0.2351, respectively. It is noteworthy that our proposed methodology does not affect the rest of the E-model input parameters, and it intends to be useful for wireless network planning in speech communication services.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Degradation,E-model,MIMO communication,MIMO system,MIMO system E-model,Modulation,Oral communication,packet loss,Planning,Prediction algorithms,speech quality assessment,wireless communication,Wireless networks},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rodríguez et al_2021_Incorporating Wireless Communication Parameters Into the E-Model Algorithm.pdf}
}

@article{routrayPhaseSensitiveMaskingbased2022,
  title = {Phase Sensitive Masking-Based Single Channel Speech Enhancement Using Conditional Generative Adversarial Network},
  author = {Routray, Sidheswar and Mao, Qirong},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101270},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101270},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000759},
  urldate = {2021-09-28},
  abstract = {We propose PSMGAN, an efficient phase sensitive masking-based single-channel speech enhancement technique using a conditional generative adversarial network (cGAN). The time\textendash frequency (T-F) masking-based speech enhancement approaches through deep neural networks (DNNs) have shown large speech intelligibility improvements. However, these approaches fail to achieve better enhancement results at low signal-to-noise ratio (SNR) conditions since they ignore the phase information during reconstruction. Alternatively, GANs have been introduced effectively for speech enhancement and achieved improved performance due to the adversarial training. Motivated by the recent success of GAN, we introduce the phase sensitive masking (PSM) in a cGAN framework for speech enhancement task. The reason for choosing a conditional generative model is that the data generation process can be controlled with the use of additional temporal context information. In addition, we use gradient penalty regularization in the discriminator of the cGAN network to avoid vanishing gradients problem which in turn stabilizes the training of the cGAN network and increases the quality of the generated samples. The use of PSM is due to the fact that it involves both amplitude and phase information and produces an improved estimate of clean speech signal with higher SNR as compared to other T-F masks. Experimental results show the proposed PSM based cGAN architecture has shown significant improvements in performance measures compared to other baselines such as SEGAN, Deep Feature Loss, MetricGAN, AECNN, DNN-cIRM, and end-to-end approach with reference to quality and intelligibility.},
  language = {en},
  keywords = {Adversarial training,Conditional generative adversarial network (cGAN),Deep learning,Phase sensitive mask,Single channel speech enhancement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Routray_Mao_2022_Phase sensitive masking-based single channel speech enhancement using.pdf}
}

@inproceedings{ruanHierarchicalMultiViewDependency2022,
  title = {Hierarchical and {{Multi-View Dependency Modelling Network}} for {{Conversational Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ruan, Yu-Ping and Zheng, Shu-Kai and Li, Taihao and Wang, Fen and Pei, Guanxiong},
  date = {2022},
  pages = {7032--7036},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747123},
  abstract = {This paper proposes a new model, called hierarchical and multi-view dependency modelling network (HMVDM), for the task of emotion recognition in conversations (ERC). The modelling of conversational context plays an important role in ERC, especially for the multi-turn and multi-speaker conversations which hold complex dependency between different speakers. In our proposed HMVDM \textsuperscript{1}, we model the dependency between different speakers at both tokenlevel and utterance-level. Specifically, the HMVDM model has a hierarchical structure with two main modules: 1) token-level dependency modelling module (TDM), which aims to learn the long-range token-level dependency between different utterances in a speaker-aware manner and output the utterance representation; 2) utterance-level dependency modelling module (UDM), which accepts the utterance representation from TDM as inputs and aims to learn the utterance-level dependency from intra-, inter-, and global-speaker(s) view simultaneously. Extensive experiments are conducted on four ERC benchmark datasets with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of our proposed HMVDM model and confirm the importance of hierarchical and multi-view context dependency modelling for ERC.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Acoustics,Benchmark testing,Conferences,context modelling,conversation,dependency,emotion recognition,Emotion recognition,Signal processing,Speech recognition,Time division multiplexing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ruan et al_2022_Hierarchical and Multi-View Dependency Modelling Network for Conversational.pdf}
}

@article{russellFacialVocalExpressions2003,
  title = {Facial and {{Vocal Expressions}} of {{Emotion}}},
  author = {Russell, James A. and Bachorowski, Jo-Anne and Fern\'andez-Dols, Jos\'e-Miguel},
  date = {2003},
  journaltitle = {Annual Review of Psychology},
  volume = {54},
  number = {1},
  eprint = {12415074},
  eprinttype = {pmid},
  pages = {329--349},
  doi = {10.1146/annurev.psych.54.101601.145102},
  url = {https://doi.org/10.1146/annurev.psych.54.101601.145102},
  urldate = {2021-11-28},
  abstract = {A flurry of theoretical and empirical work concerning the production of and response to facial and vocal expressions has occurred in the past decade. That emotional expressions express emotions is a tautology but may not be a fact. Debates have centered on universality, the nature of emotion, and the link between emotions and expressions. Modern evolutionary theory is informing more models, emphasizing that expressions are directed at a receiver, that the interests of sender and receiver can conflict, that there are many determinants of sending an expression in addition to emotion, that expressions influence the receiver in a variety of ways, and that the receiver's response is more than simply decoding a message.},
  issue = {1},
  language = {en},
  keywords = {_reading,affect,communication,display rule,nonverbal,perception},
  annotation = {\_eprint: https://doi.org/10.1146/annurev.psych.54.101601.145102},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Russell et al_2003_Facial and Vocal Expressions of Emotion.pdf}
}

@inproceedings{ryuminaAnnotationConfidenceVs2021,
  title = {Annotation {{Confidence}} vs. {{Training Sample Size}}: {{Trade-Off Solution}} for {{Partially-Continuous Categorical Emotion Recognition}}},
  shorttitle = {Annotation {{Confidence}} vs. {{Training Sample Size}}},
  booktitle = {Interspeech 2021},
  author = {Ryumina, Elena and Verkholyak, Oxana and Karpov, Alexey},
  date = {2021-08-30},
  pages = {3690--3694},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1636},
  url = {https://www.isca-speech.org/archive/interspeech_2021/ryumina21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ryumina et al_2021_Annotation Confidence vs.pdf}
}

@article{saitoPerceptualSimilarityAwareDeepSpeaker2021,
  title = {Perceptual-{{Similarity-Aware Deep Speaker Representation Learning}} for {{Multi-Speaker Generative Modeling}}},
  author = {Saito, Yuki and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1033--1048},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3059114},
  abstract = {We propose novel deep speaker representation learning that considers perceptual similarity among speakers for multi-speaker generative modeling. Following its success in accurate discriminative modeling of speaker individuality, knowledge of deep speaker representation learning (i.e., speaker representation learning using deep neural networks) has been introduced to multi-speaker generative modeling. However, the conventional discriminative algorithm does not necessarily learn speaker embeddings suitable for such generative modeling, which may result in lower quality and less controllability of synthetic speech. We propose three representation learning algorithms that utilize a perceptual speaker similarity matrix obtained by large-scale perceptual scoring of speaker-pair similarity. The algorithms train a speaker encoder to learn speaker embeddings with three different representations of the matrix: a set of vectors, the Gram matrix, and a graph. Furthermore, we propose an active learning algorithm that iterates the perceptual scoring and speaker encoder training. To obtain accurate embeddings while reducing costs of scoring and training, the algorithm selects unscored speaker-pairs to be scored next on the basis of the sequentially-trained speaker encoder's similarity prediction results. Experimental evaluation results show that 1) the proposed representation learning algorithms learn speaker embeddings strongly correlated with perceptual speaker-pair similarity, 2) the embeddings improve synthetic speech quality in speech autoencoding tasks better than conventional d-vectors learned by discriminative modeling, 3) the proposed active learning algorithm achieves higher synthetic speech quality while reducing costs of scoring and training, and 4) among the proposed similarity vector, matrix, graph embedding algorithms, the first achieves the best speaker similarity for synthetic speech and the third gives the most improvement in the synthetic speech naturalness.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,active learning,Adaptation models,Controllability,Deep speaker representation learning,Feature extraction,multi-speaker generative modeling,perceptual speaker similarity,Prediction algorithms,speaker embedding,Speech synthesis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Saito et al_2021_Perceptual-Similarity-Aware Deep Speaker Representation Learning for.pdf}
}

@article{sanchez-gutierrezDiscriminativeNeuralNetwork2020,
  title = {Discriminative Neural Network Pruning in a Multiclass Environment: {{A}} Case Study in Spoken Emotion Recognition},
  shorttitle = {Discriminative Neural Network Pruning in a Multiclass Environment},
  author = {S\'anchez-Guti\'errez, M\'aximo E. and Gonz\'alez-P\'erez, Pedro P.},
  date = {2020-06},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {120},
  pages = {20--30},
  issn = {01676393},
  doi = {10.1016/j.specom.2020.03.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639319302122},
  urldate = {2021-09-10},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sánchez-Gutiérrez_González-Pérez_2020_Discriminative neural network pruning in a multiclass environment.pdf}
}

@article{santoroEncodingNaturalSounds2014,
  title = {Encoding of {{Natural Sounds}} at {{Multiple Spectral}} and {{Temporal Resolutions}} in the {{Human Auditory Cortex}}},
  author = {Santoro, Roberta and Moerel, Michelle and Martino, Federico De and Goebel, Rainer and Ugurbil, Kamil and Yacoub, Essa and Formisano, Elia},
  year = {2014年1月2日},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {10},
  number = {1},
  pages = {e1003412},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003412},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003412},
  urldate = {2022-08-02},
  abstract = {Functional neuroimaging research provides detailed observations of the response patterns that natural sounds (e.g. human voices and speech, animal cries, environmental sounds) evoke in the human brain. The computational and representational mechanisms underlying these observations, however, remain largely unknown. Here we combine high spatial resolution (3 and 7 Tesla) functional magnetic resonance imaging (fMRI) with computational modeling to reveal how natural sounds are represented in the human brain. We compare competing models of sound representations and select the model that most accurately predicts fMRI response patterns to natural sounds. Our results show that the cortical encoding of natural sounds entails the formation of multiple representations of sound spectrograms with different degrees of spectral and temporal resolution. The cortex derives these multi-resolution representations through frequency-specific neural processing channels and through the combined analysis of the spectral and temporal modulations in the spectrogram. Furthermore, our findings suggest that a spectral-temporal resolution trade-off may govern the modulation tuning of neuronal populations throughout the auditory cortex. Specifically, our fMRI results suggest that neuronal populations in posterior/dorsal auditory regions preferably encode coarse spectral information with high temporal precision. Vice-versa, neuronal populations in anterior/ventral auditory regions preferably encode fine-grained spectral information with low temporal precision. We propose that such a multi-resolution analysis may be crucially relevant for flexible and behaviorally-relevant sound processing and may constitute one of the computational underpinnings of functional specialization in auditory cortex.},
  language = {en},
  keywords = {Acoustics,Auditory cortex,Auditory system,Functional magnetic resonance imaging,Modulation,Neuronal tuning,Permutation,Speech},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Santoro et al_2014_Encoding of Natural Sounds at Multiple Spectral and Temporal Resolutions in the.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\D695YIM4\\article.html}
}

@inproceedings{santosoPerformanceImprovementSpeech2022,
  title = {Performance {{Improvement}} of {{Speech Emotion Recognition}} by {{Neutral Speech Detection Using Autoencoder}} and {{Intermediate Representation}}},
  booktitle = {Interspeech 2022},
  author = {Santoso, Jennifer and Yamada, Takeshi and Ishizuka, Kenkichi and Hashimoto, Taiichi and Makino, Shoji},
  date = {2022-09-18},
  pages = {4700--4704},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-584},
  url = {https://www.isca-speech.org/archive/interspeech_2022/santoso22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Santoso et al_2022_Performance Improvement of Speech Emotion Recognition by Neutral Speech.pdf}
}

@inproceedings{santosoSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition Based}} on {{Attention Weight Correction Using Word-Level Confidence Measure}}},
  booktitle = {Interspeech 2021},
  author = {Santoso, Jennifer and Yamada, Takeshi and Makino, Shoji and Ishizuka, Kenkichi and Hiramura, Takekatsu},
  date = {2021-08-30},
  pages = {1947--1951},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-411},
  url = {https://www.isca-speech.org/archive/interspeech_2021/santoso21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Santoso et al_2021_Speech Emotion Recognition Based on Attention Weight Correction Using.pdf}
}

@article{sariAuxiliaryNetworksJoint2021,
  title = {Auxiliary {{Networks}} for {{Joint Speaker Adaptation}} and {{Speaker Change Detection}}},
  author = {Sari, Leda and Hasegawa-Johnson, Mark and Thomas, Samuel},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {324--333},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3040626},
  abstract = {Speaker adaptation and speaker change detection have both been studied extensively to improve automatic speech recognition (ASR). In many cases, these two problems are investigated separately: speaker change detection is implemented first to obtain single-speaker regions, and speaker adaptation is then performed using the derived speaker segments for improved ASR. However, in an online setting, we want to achieve both goals in a single pass. In this study, we propose a neural network architecture that learns a speaker embedding from which it can perform both speaker adaptation for ASR and speaker change detection. The proposed speaker embedding is computed using self-attention based on an auxiliary network attached to a main ASR network. ASR adaptation is then performed by subtracting, from the main network activations, a segment dependent affine transformation of the learned speaker embedding. In experiments on a broadcast news dataset and the Switchboard conversational dataset, we test our system on utterances with a change point in them and show that the proposed method achieves significantly better performance as compared to the unadapted main network (10-14\% relative reduction in word error rate (WER)). The proposed architecture also outperforms three different speaker segmentation methods followed by ASR (around 10\% relative reduction in WER).},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Adaptation models,automatic speech recognition,Computational modeling,Feature extraction,Speaker adaptation,speaker change detection,speaker segmentation,Speech processing,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sari et al_2021_Auxiliary Networks for Joint Speaker Adaptation and Speaker Change Detection.pdf}
}

@inproceedings{satoCreationAnalysisEmotional2020,
  title = {Creation and {{Analysis}} of {{Emotional Speech Database}} for {{Multiple Emotions Recognition}}},
  booktitle = {2020 23rd {{Conference}} of the {{Oriental COCOSDA International Committee}} for the {{Co-ordination}} and {{Standardisation}} of {{Speech Databases}} and {{Assessment Techniques}} ({{O-COCOSDA}})},
  author = {Sato, Ryota and Sasaki, Ryohei and Suga, Norisato and Furukawa, Toshihiro},
  date = {2020-11-05},
  pages = {33--37},
  publisher = {{IEEE}},
  location = {{Yangon, Myanmar}},
  doi = {10.1109/O-COCOSDA50338.2020.9295041},
  url = {https://ieeexplore.ieee.org/document/9295041/},
  urldate = {2021-09-10},
  eventtitle = {2020 23rd {{Conference}} of the {{Oriental COCOSDA International Committee}} for the {{Co-ordination}} and {{Standardisation}} of {{Speech Databases}} and {{Assessment Techniques}} ({{O-COCOSDA}})},
  isbn = {978-1-72819-896-5},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sato et al_2020_Creation and Analysis of Emotional Speech Database for Multiple Emotions.pdf}
}

@inproceedings{scheidwasser-clowSERABMultiLingualBenchmark2022,
  title = {{{SERAB}}: {{A Multi-Lingual Benchmark}} for {{Speech Emotion Recognition}}},
  shorttitle = {{{SERAB}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Scheidwasser-Clow, Neil and Kegler, Mikolaj and Beckmann, Pierre and Cernak, Milos},
  date = {2022},
  pages = {7697--7701},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747348},
  abstract = {Recent developments in speech emotion recognition (SER) often leverage deep neural networks (DNNs). Comparing and benchmarking different DNN models can often be tedious due to the use of different datasets and evaluation protocols. To facilitate the process, here, we present the Speech Emotion Recognition Adaptation Benchmark (SERAB), a framework for evaluating the performance and generalization capacity of different approaches for utterance-level SER. The benchmark is composed of nine datasets for SER in six languages. Since the datasets have different sizes and numbers of emotional classes, the proposed setup is particularly suitable for estimating the generalization capacity of pre-trained DNN-based feature extractors. We used the proposed framework to evaluate a selection of standard hand-crafted feature sets and state-of-the-art DNN representations. The results highlight that using only a subset of the data included in SERAB can result in biased evaluation, while compliance with the proposed protocol can circumvent this issue.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Benchmark testing,computational paralinguistics,deep neural networks,emotion recognition,Emotion recognition,Feature extraction,Pipelines,Protocols,Signal processing,speech processing,Speech recognition,transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Scheidwasser-Clow et al_2022_SERAB.pdf}
}

@unpublished{schnellEmoCatLanguageagnosticEmotional2021,
  title = {{{EmoCat}}: {{Language-agnostic Emotional Voice Conversion}}},
  shorttitle = {{{EmoCat}}},
  author = {Schnell, Bastian and Huybrechts, Goeric and Perz, Bartek and Drugman, Thomas and Lorenzo-Trueba, Jaime},
  date = {2021-01-14},
  eprint = {2101.05695},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2101.05695},
  urldate = {2021-09-10},
  abstract = {Emotional voice conversion models adapt the emotion in speech without changing the speaker identity or linguistic content. They are less data hungry than text-to-speech models and allow to generate large amounts of emotional data for downstream tasks. In this work we propose EmoCat, a language-agnostic emotional voice conversion model. It achieves high-quality emotion conversion in German with less than 45 minutes of German emotional recordings by exploiting large amounts of emotional data in US English. EmoCat is an encoder-decoder model based on CopyCat, a voice conversion system which transfers prosody. We use adversarial training to remove emotion leakage from the encoder to the decoder. The adversarial training is improved by a novel contribution to gradient reversal to truly reverse gradients. This allows to remove only the leaking information and to converge to better optima with higher conversion performance. Evaluations show that Emocat can convert to different emotions but misses on emotion intensity compared to the recordings, especially for very expressive emotions. EmoCat is able to achieve audio quality on par with the recordings for five out of six tested emotion intensities.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Schnell et al_2021_EmoCat.pdf}
}

@inproceedings{schullerASCInclusionInteractiveEmotion2013,
  title = {{{ASC-Inclusion}}: {{Interactive Emotion Games}} for {{Social Inclusion}} of {{Children}} with {{Autism Spectrum Conditions}}},
  shorttitle = {{{ASC-Inclusion}}},
  author = {Schuller, Bj\"orn and Marchi, Erik and Baron-Cohen, Simon and O'Reilly, Helen and Robinson, Peter and Davies, Ian and Golan, Ofer and Fridenson-Hayo, Shimrit and Tal, Shahar and Newman, Shai and Meir, Noga and Shillo, Roi and Camurri, Antonio and Piana, Stefano and B\"olte, Sven and Lundqvist, Daniel and Berggren, Steve and Baranger, Aur\'elie and Sullings, Nikki},
  date = {2013-05-01},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Schuller et al_2013_ASC-Inclusion.pdf}
}

@inproceedings{schullerHiddenMarkovModelbased2003,
  title = {Hidden {{Markov}} Model-Based Speech Emotion Recognition},
  booktitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2003. {{Proceedings}}. ({{ICASSP}} '03).},
  author = {Schuller, B. and Rigoll, G. and Lang, M.},
  date = {2003-04},
  volume = {2},
  pages = {II-1},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2003.1202279},
  abstract = {We introduce speech emotion recognition by use of continuous hidden Markov models. Two methods are propagated and compared. In the first method, a global statistics framework of an utterance is classified by Gaussian mixture models using derived features of the raw pitch and energy contour of the speech signal. A second method introduces increased temporal complexity, applying continuous hidden Markov models considering several states using low-level instantaneous features instead of global statistics. The paper addresses the design of working recognition engines, and results are achieved with respect to the alluded alternatives. A speech corpus consisting of acted and spontaneous emotion samples in German and English is described in detail. Both engines have been tested and trained using this equivalent speech corpus. Results in recognition of seven discrete emotions exceeded 86\% recognition rate. In comparison, the judgment of human deciders classifying the same corpus at 79.8\% recognition rate was analyzed.},
  eventtitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2003. {{Proceedings}}. ({{ICASSP}} '03).},
  keywords = {Data mining,Emotion recognition,Engines,Hidden Markov models,Humans,Natural languages,Speech analysis,Speech processing,Statistics,Testing},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\QTC6VLR3\\1202279.html}
}

@article{schullerSpeechEmotionRecognition2018,
  title = {Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends},
  shorttitle = {Speech Emotion Recognition},
  author = {Schuller, Bj\"orn W.},
  date = {2018-04-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {61},
  number = {5},
  pages = {90--99},
  issn = {0001-0782},
  doi = {10.1145/3129340},
  url = {https://doi.org/10.1145/3129340},
  urldate = {2023-01-18},
  abstract = {Tracing 20 years of progress in making machines hear our emotions based on speech signal properties.},
  language = {en},
  keywords = {_review},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Schuller_2018_Speech emotion recognition.pdf}
}

@misc{senerMultiTaskLearningMultiObjective2019,
  title = {Multi-{{Task Learning}} as {{Multi-Objective Optimization}}},
  author = {Sener, Ozan and Koltun, Vladlen},
  date = {2019-01-11},
  number = {arXiv:1810.04650},
  eprint = {1810.04650},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04650},
  url = {http://arxiv.org/abs/1810.04650},
  urldate = {2022-11-16},
  abstract = {In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.},
  archiveprefix = {arXiv},
  language = {en},
  version = {2},
  keywords = {_Code,_readed,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sener_Koltun_2019_Multi-Task Learning as Multi-Objective Optimization.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\5MFPXGFP\\1810.html}
}

@article{shagiMachineLearningApproach2022,
  title = {A Machine Learning Approach for Gender Identification Using Statistical Features of Pitch in Speeches},
  author = {Shagi, G. U. and Aji, S.},
  date = {2022-01-01},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {185},
  pages = {108392},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2021.108392},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X21004862},
  urldate = {2021-10-02},
  abstract = {Speech processing and recognition are some of the imperative research areas, remarkably in user authentication and entertainment. There are assorted well-known approaches in the domain, and the properties of pitch are broadly used in most of the works. The objective of this work is to explore the capabilities of statistical features of the pitch in the gender identification problem. We proposed an effective combination of features, PFG-Pitch Feature for Gender, for gender identification with machine learning algorithms. The feature sets from three datasets \textendash{} TIMIT, CHAINS and SLR-63- were used in the experiments. We could attain the maximum output in some ideal situations with the classical learning methods- CNN, MLP, SVM and LR. It is renowned that the outstanding performance from most of the machine learning classifiers embassies the magnitude of statistical features in speech processing.},
  language = {en},
  keywords = {Gender identification,Machine learning,Pitch,Statistical measures},
  annotation = {ECC: No Data (logprob: -200.762)},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\GN4JN86C\\S0003682X21004862.html}
}

@article{shahnawazuddinRobustChildrenSpeech2022,
  title = {Robust Children's Speech Recognition in Zero Resource Condition},
  author = {Shahnawazuddin, S. and Kumar, Avinash and Kumar, Vinit and Kumar, Saurabh and Ahmad, Waquar},
  date = {2022-01-01},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {185},
  pages = {108382},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2021.108382},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X2100476X},
  urldate = {2021-09-28},
  abstract = {User applications such as voice-based web search, online learning, and video gaming require an effective speech recognition module to take user commands. Nowadays, even children are frequently using such tools, especially for online learning and gaming. This has increased the demand for developing a noise-robust automatic speech recognition (ASR) system that can effectively transcribe children's data under varied ambient conditions. However, automatic recognition of children's speech is extremely challenging due to the insufficiency of data from child speakers in the majority of the languages across the world. Consequently, in this zero-resource condition, we are forced to decode children's speech on systems trained using adults' data. However, the acoustic mismatch between adults' and children's speech, such as differences in pitch, formant frequencies, and speaking-rates, leads to highly degraded recognition performance. To enhance the recognition rate under zero-resource conditions, we have explored the role of formant and duration-modification-based out-of-domain data augmentation in this paper. For that purpose, the formant frequencies of the adults' speech data are upscaled using warping of linear predictive coding coefficients. On pooling original and formant modified adults' speech data into training, the mismatch in formant locations is reduced leading to better recognition performance. Further improvement in recognition rate can be achieved by simultaneously modifying the duration as well as the formant frequencies of the training data. This case of out-of-domain data augmentation has also been studied in this work and found to yield added gains. In addition to data augmentation, a noise- and pitch-robust front-end acoustic feature extraction approach exploiting higher-order spectral analysis (simple and cross-bispectrum) is also proposed in this paper. The proposed features are noise-robust due to the inherent immunity of the bispectrum towards additive noises. An added advantage of bispectrum is reduced pitch sensitivity as demonstrated in this work. This, in turn, helps alleviate the aforementioned pitch-induced acoustic mismatch. The experimental evaluations presented in this paper demonstrate that the use of proposed acoustic features, as well as the out-of-domain data augmentation techniques, are highly suited for zero-resource children's speech recognition tasks under clean and noisy conditions.},
  language = {en},
  keywords = {_Waiting for read,\#nosource,Data augmentation,Duration modification,Formant modification,Higher-order spectral analysis,Noise robustness,Pitch-robustness,Zero-resource children’s ASR}
}

@thesis{ShangJiYuYuYinQingGanShiBieDeKeFuZuoYeQingXuFenXiYuKongZhi2019,
  type = {硕士},
  title = {基于语音情感识别的客服作业情绪分析与控制},
  author = {尚, 雨琪},
  date = {2019},
  institution = {{西安科技大学}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019618418.nh&uniplatform=NZKPT&v=puTk%25mmd2FqBQDCjDGjZxjB9q8pXB9rpb1jYIxLwmCbEOjL43qYabthRW2qiRPnpUNtlS},
  urldate = {2021-09-28},
  abstract = {在产品不断更新换代速度加快,信息技术不断地进行着推进,互联网的应用范围日益扩大的大时代背景下,企业都在为能够在激烈的竞争中获取到更多的客户资源而努力。客服中心起初仅是作为企业远程受理客户需求的一种简单服务模式的存在,之后在各行各业中快速发展开来。目前建立起来的客服中心中,电子商务业、通讯业、银行业等行业的发展速度最为迅速。其中,电商企业的客户消费理念不断提升,企业除了要在技术、研发和生产上不断创新和推进,更重要地是要更好地服务好客户。因此,客服中心在提升企业竞争优势中扮演着极其重要的角色。通过高质量的客服服务工作吸引开发更多的客户资源,使客户有较高的服务体验以此实现企业的优势性经营目标。全球韵律演讲特征与基本情绪之间的关系的研究已经表明,语音信号中的韵律特征可以作为一种可靠的情感指示。语音情感识别技术可以对语音中的特征参数进行提取分析得到与之相对应的情感表达。本文以西安科技大学临潼校区京东校园实训室为研究对象,从实训中心客服工作人员的语音作业现状入手,结合真实的客服服务录音和客户满意度,通过语音情感识别技术分析当前客服行业普遍存在的人员情绪控制的问题。并为更深层次挖掘影响客服中心人员服务质量的多种原因,提出了解决方案和改善措施。从客服工作人员作业语音情感层面和精神层面入手,建立Matlab语音分析编程进行数据整理研究和语音警报系统对客服工作人员的语音作业进行监控以及对情绪波动给出警报。同时,结合对客服工作人员的问卷调查可以初步预见到客服工作人员的情绪倦怠和离职意向。进而结合客服中心的实际运营情况、客服中心各项管理制度和考核指标、客服中心的发展目标、企业内部各部门职责等方面,提出切实可行的降低客服工作人员情绪波动的策略和预见改善客服工作人员情绪倦怠的具体措施,并加以运行。运行结果表明:本研究成果产生了一定的正向作用,支撑了改善策略的可实施性和可效性,也为行业内其他企业在面对作业情绪控制问题时,提供了可借鉴之处和帮助。},
  editora = {李, 文琴 and 蔺, 平安},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {customer satisfaction,improvement strategy,Matlab programming,Matlab编程,service emotions,Speech emotion recognition,作业情绪,客户满意度,改善策略,语音情感识别},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\尚_2019_基于语音情感识别的客服作业情绪分析与控制.pdf}
}

@inproceedings{sharmaMultiLingualMultiTaskSpeech2022,
  title = {Multi-{{Lingual Multi-Task Speech Emotion Recognition Using}} Wav2vec 2.0},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sharma, Mayank},
  date = {2022},
  pages = {6907--6911},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747417},
  abstract = {Speech Emotion Recognition (SER) has several use cases for Digital Entertainment Content (DEC) in Over-the-top (OTT) services, emotive Text-to-Speech (TTS) engines and voice assistants. In this work, we present a Multi-Lingual (MLi) and Multi-Task Learning (MTL) audio only SER system based on the multi-lingual pre-trained wav2vec 2.0 model. The model is fine-tuned on 25 open source datasets in 13 locales across 7 emotion categories. We show that, a) Our wav2vec 2.0 single task based model outperforms Pre-trained Audio Neural Network (PANN) based single task pre-trained model by 7.2\% (relative), b) The best MTL model outperforms the PANN based and wav2vec 2.0 based single task models by 8.6\% and 1.7\% (relative) respectively, c) The MTL based system outperforms pre-trained single task wav2vec 2.0 model in 9 out of 13 locales in terms of weighted F1 scores, and d) The MTL-MLi wav2vec 2.0 outperforms the state-of-the-art for the languages contained in the pre-training corpora.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Emotion recognition,Entertainment industry,Multi-task Multi-lingual speech emotion recognition,Multitasking,Neural networks,Over-the-top media services,PANN,Pre-trained wav2vec 2.0,Signal processing,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sharma_2022_Multi-Lingual Multi-Task Speech Emotion Recognition Using wav2vec 2.pdf}
}

@inproceedings{shenAutomaticDepressionDetection2022,
  title = {Automatic {{Depression Detection}}: An {{Emotional Audio-Textual Corpus}} and {{A Gru}}/{{Bilstm-Based Model}}},
  shorttitle = {Automatic {{Depression Detection}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Shen, Ying and Yang, Huiyu and Lin, Lin},
  date = {2022},
  pages = {6247--6251},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746569},
  abstract = {Depression is a global mental health problem, the worst case of which can lead to suicide. An automatic depression detection system provides great help in facilitating depression self-assessment and improving diagnostic accuracy. In this work, we propose a novel depression detection approach utilizing speech characteristics and linguistic contents from participants' interviews. In addition, we establish an Emotional Audio-Textual Depression Corpus (EATD-Corpus) which contains audios and extracted transcripts of responses from depressed and non-depressed volunteers. To the best of our knowledge, EATD-Corpus is the first and only public depression dataset that contains audio and text data in Chinese. Evaluated on two depression datasets, the proposed method achieves the state-of-the-art performances. The outperforming results demonstrate the effectiveness and generalization ability of the proposed method. The source code and EATD-Corpus are available at https://github.com/speechandlanguageprocessing/ICASSP2022-Depression.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Codes,Computer science,Conferences,Depression,Depression detection,EATD-Corpus,Linguistics,Mental health,Multi-modal fusion,Signal processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Shen et al_2022_Automatic Depression Detection.pdf}
}

@unpublished{shenNaturalTTSSynthesis2018,
  title = {Natural {{TTS Synthesis}} by {{Conditioning WaveNet}} on {{Mel Spectrogram Predictions}}},
  author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, R. J. and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
  date = {2018-02-15},
  eprint = {1712.05884},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1712.05884},
  urldate = {2021-09-10},
  abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Shen et al_2018_Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.pdf}
}

@inproceedings{shinTextdrivenEmotionalStyle2022,
  title = {Text-Driven {{Emotional Style Control}} and {{Cross-speaker Style Transfer}} in {{Neural TTS}}},
  booktitle = {Interspeech 2022},
  author = {Shin, Yookyung and Lee, Younggun and Jo, Suhee and Hwang, Yeongtae and Kim, Taesu},
  date = {2022-09-18},
  pages = {2313--2317},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10131},
  url = {https://www.isca-speech.org/archive/interspeech_2022/shin22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Shin et al_2022_Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural.pdf}
}

@inproceedings{shirianCompactGraphArchitecture2021,
  title = {Compact {{Graph Architecture}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Shirian, Amir and Guha, Tanaya},
  date = {2021-06},
  pages = {6284--6288},
  issn = {2379-190X},
  doi = {10/gmr2jn},
  abstract = {We propose a deep graph approach to address the task of speech emotion recognition. A compact, efficient and scalable way to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a Graph Convolution Network (GCN)-based architecture that can perform an accurate graph convolution in contrast to the approximate convolution used in standard GCNs. We evaluated the performance of our model for speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves comparable performance to the state-of-the-art with significantly fewer learnable parameters ( 30K) indicating its applicability in resource-constrained devices. Our code is available at /github.com/AmirSh15/Compact\_SER.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,Conferences,Convolution,Convolutional codes,Databases,Emotion recognition,graph convolutional networks,graph signal processing,Performance evaluation,Speech emotion recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Shirian_Guha_2021_Compact Graph Architecture for Speech Emotion Recognition.pdf}
}

@article{simonsAffectiveDynamicsVeterans2021,
  title = {Affective Dynamics among Veterans: {{Associations}} with Distress Tolerance and Posttraumatic Stress Symptoms},
  shorttitle = {Affective Dynamics among Veterans},
  author = {Simons, Jeffrey S. and Simons, Raluca M. and Grimm, Kevin J. and Keith, Jessica A. and Stoltenberg, Scott F.},
  date = {2021},
  journaltitle = {Emotion},
  volume = {21},
  number = {4},
  pages = {757--771},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1931-1516},
  doi = {10.1037/emo0000745},
  abstract = {We tested a dynamic structural equation model (DSEM; Asparouhov, Hamaker, \& Muth\'en, 2018) of positive and negative affect in 254 veterans with approximately 1.5 years of experience sampling data. The analysis provided estimates of several aspects of veteran's emotional experience including ``trait'' positive and negative affect (i.e., mean levels), inertia (i.e., tendency for emotions to self-perpetuate), innovation variance (conceptualized as lability, reactivity, or exposure to stressors), and cross-lagged associations between positive and negative affect. Veterans with higher trait negative affect had more negative affect inertia and innovation variance. This suggests a pattern whereby the veteran has more negative reactions, and negative emotions, in turn, tend to maintain themselves, contributing to higher trait negative affect. In contrast, veterans with higher trait positive affect exhibited more positive affect innovation variance (e.g., positive reactivity). Although veterans showed some consistency in dynamics across emotions (e.g., positive and negative reactivity were positively correlated), trait positive and negative affect were not significantly associated. Veterans with higher posttraumatic stress symptoms (PTSS) at baseline exhibited higher reactivity to negative events, less positive affect, and more negative affect during the follow-up. Veterans with higher distress tolerance reported not only lower PTSS but also a more adaptive pattern of affective experience characterized by lower inertia and reactivity in negative affect and more positive lagged associations between negative affect and subsequent positive affect. The results demonstrated that distress tolerance and PTSS in veterans were associated with dynamics of positive and negative emotion over time, suggesting specific differences in affect regulation processes. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  issue = {4},
  language = {en},
  keywords = {\#nosource,Distress,Emotional Regulation,Emotional Responses,Military Veterans,Negative Emotions,Positive Emotions,Posttraumatic Stress,Symptoms,Test Construction,Tolerance}
}

@article{sismanOverviewVoiceConversion2021,
  title = {An {{Overview}} of {{Voice Conversion}} and {{Its Challenges}}: {{From Statistical Modeling}} to {{Deep Learning}}},
  shorttitle = {An {{Overview}} of {{Voice Conversion}} and {{Its Challenges}}},
  author = {Sisman, Berrak and Yamagishi, Junichi and King, Simon and Li, Haizhou},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {132--157},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3038524},
  abstract = {Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Deep learning,Pipelines,speaker characterization,speech analysis,Speech analysis,Speech synthesis,Training,Training data,Vocoders,vocoding,Voice conversion,voice conversion challenges,voice conversion evaluation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sisman et al_2021_An Overview of Voice Conversion and Its Challenges.pdf}
}

@inproceedings{sivaprasadEmotionalProsodyControl2021,
  title = {Emotional {{Prosody Control}} for {{Speech Generation}}},
  booktitle = {Interspeech 2021},
  author = {Sivaprasad, Sarath and Kosgi, Saiteja and Gandhi, Vineet},
  date = {2021-08-30},
  pages = {4653--4657},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-307},
  url = {https://www.isca-speech.org/archive/interspeech_2021/sivaprasad21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sivaprasad et al_2021_Emotional Prosody Control for Speech Generation.pdf}
}

@thesis{SongJiYuWenBenYuYinHeShiPinDeDuoMoTaiQingGanShiBieDeYanJiu2019,
  type = {硕士},
  title = {基于文本、语音和视频的多模态情感识别的研究},
  author = {宋, 绪靖},
  date = {2019},
  institution = {{山东大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201902&filename=1019068013.nh&uniplatform=NZKPT&v=ab7teB%25mmd2B3M4lZfZNUKUQ0WmTtodpySfKY4dBGF2wZEX8j66st0bA%25mmd2F9gwXLawhwSEc},
  urldate = {2021-09-10},
  abstract = {随着人工智能技术的不断发展,作为情感计算领域一个重要的分支,情感识别越来越成为一个研究热点。由于单模态情感识别存在识别率低、鲁棒性差的缺点,研究人员对情感识别也逐渐从单模态情感识别过渡到多模态情感识别。通过引入更多模态的信息,来捕捉模态之间的互补信息,从而提高最后的识别效果。如何将不同模态的信息进行有效的融合是多模态情感识别的关键,同时也是多模态情感识别的一大难点。本文主要基于文本、语音和视频三个模态的信息在特征层融合的基础上来进行多模态情感识别的研究,并针对多模态情感识别关键技术进行了探索和改进。本论文的主要研究内容为:（1）研究适用于单个模态上有效的特征提取方法。对于文本模态主要采用双向LSTM网络进行文本情感特征的提取,有效利用文本的前后语义与语序信息,使提取到的文本情感特征含有重要的时序信息;对于语音模态采用卷积神经网络进行语音特征的提取,并使用开源工具openSMILE来提取语音信号的基础特征,两者相融合作为最后的语音情感特征,使语音特征更加齐全;对于视频模态采用三维卷积神经网络模型提取视频情感特征,和普通卷积神经网络模型相比,引入了时间维度,从而使提取的情感特征含有丰富的前后时序信息,同时引入人脸关键点特征作为辅助特征,从而使提取的视频情感特征更加丰富有效。（2）研究多模态情感信息的融合方式。详细分析了当前普遍使用的直接级联的特征融合方式,提出其存在的问题,并针对直接级联融合方式中存在的缺点进行了改进,最后提出了基于注意力机制的特征层融合方式。该方法使单个模态的特征通过学习得到一个符合数据集分布的影响权重,然后在进行特征融合的时候采用加权融合,使融合得到的特征更加有效,从而提高识别效果。（3）在基于注意力机制的特征层融合方式的基础上进一步研究,提出引入残差思想的特征层融合方式。由直接去优化映射函数改为去优化残差,使引入残差后的映射对输出的变化更敏感,从而更好的优化网络结构,使网络结构的表达能力更强,进一步提升最后的情感识别效果。（4）将提出的基于注意力机制的融合方式和引入残差思想的融合方式应用到多模态情感识别任务上,并在公开数据集上进行实验验证,通过实验结果分析讨论,验证我们提出的特征融合算法的有效性。},
  editora = {李, 玉军},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,attention mechanism,feature layer fusion,multimodal emotion recognition,residual,多模态情感识别,残差,注意力机制,特征层融合},
  annotation = {4 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\宋_2019_基于文本、语音和视频的多模态情感识别的研究.pdf}
}

@article{songKnowledgeGraphEmbedding2021,
  title = {A {{Knowledge Graph Embedding Approach}} for {{Metaphor Processing}}},
  author = {Song, Wei and Guo, Jingjin and Fu, Ruiji and Liu, Ting and Liu, Lizhen},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {406--420},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3040507},
  abstract = {Metaphor is a figure of speech that describes one thing (a target) by mentioning another thing (a source) in a way that is not literally true. Metaphor understanding is an interesting but challenging problem in natural language processing. This paper presents a novel method for metaphor processing based on knowledge graph (KG) embedding. Conceptually, we abstract the structure of a metaphor as an attribute-dependent relation between the target and the source. Each specific metaphor can be represented as a metaphor triple (target, attribute, source). Therefore, we can model metaphor triples just like modeling fact triples in a KG and exploit KG embedding techniques to learn better representations of concepts, attributes and concept relations. In this way, metaphor interpretation and generation could be seen as KG completion, while metaphor detection could be viewed as a representation learning enhanced concept pair classification problem. Technically, we build a Chinese metaphor KG in the form of metaphor triples based on simile recognition, and also extract concept-attribute collocations to help describe concepts and measure concept relations. We extend the translation-based and the rotation-based KG embedding models to jointly optimize metaphor KG embedding and concept-attribute collocation embedding. Experimental results demonstrate the effectiveness of our method. Simile recognition is feasible for building the metaphor triple resource. The proposed models improve the performance on metaphor interpretation and generation, and the learned representations also benefit nominal metaphor detection compared with strong baselines.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Bit error rate,Feature extraction,Knowledge graph embedding,Manuals,Metaphor detection,Metaphor generation,Metaphor interpretation,Metaphor processing,Semantics,Speech processing,Target recognition,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Song et al_2021_A Knowledge Graph Embedding Approach for Metaphor Processing.pdf}
}

@inproceedings{songMultiStageGraphRepresentation2022,
  title = {Multi-{{Stage Graph Representation Learning}} for {{Dialogue-Level Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Song, Yaodong and Liu, Jiaxing and Wang, Longbiao and Yu, Ruiguo and Dang, Jianwu},
  date = {2022},
  pages = {6432--6436},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746237},
  abstract = {With the development of speech emotion recognition (SER), most of current research is utterance-level and cannot fit the need of actual scenarios. In this paper, we propose a novel strategy that focuses on capturing dialogue-level contextual information. On the basis of utterance-level representation learned by convolutional neural network (CNN) which is followed by the bidirectional long short-term memory network (BLSTM), the proposed dialogue-level method consists of two modules. The first module is Dialogue Multi-stage Graph Representation Learning Algorithm (DialogMSG). The multi-stage graph that modeling from different dialogue scope is introduced to capture more effective information. The other one is a double-constrained module. This module includes not only an utterance-level classifier but also a dialogue-level graph classifier which is named as Atmosphere. The results of extensive experiments show that the proposed method outperforms the current state of the art on the IEMOCAP benchmark dataset.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_wreading,Analytical models,atmosphere,Atmospheric modeling,dialogue-level contextual information,double-constrained,Emotion recognition,Representation learning,Signal processing,Signal processing algorithms,Speech emotion recognition,Speech recognition,utterance-level representation},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Song et al_2022_Multi-Stage Graph Representation Learning for Dialogue-Level Speech Emotion.pdf}
}

@article{srinivasanCrossCulturalCulturalSpecificProduction2021,
  title = {Cross-{{Cultural}} and {{Cultural-Specific Production}} and {{Perception}} of {{Facial Expressions}} of {{Emotion}} in the {{Wild}}},
  author = {Srinivasan, Ramprakash and Martinez, Aleix M.},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {707--721},
  issn = {1949-3045},
  doi = {10.1109/taffc.2018.2887267},
  abstract = {Automatic recognition of emotion from facial expressions is an intense area of research, with a potentially long list of important application. Yet, the study of emotion requires knowing which facial expressions are used within and across cultures in the wild, not in controlled lab conditions; but such studies do not exist. Which and how many cross-cultural and cultural-specific facial expressions do people commonly use? And, what affect variables does each expression communicate to observers? If we are to design technology that understands the emotion of users, we need answers to these two fundamental questions. In this paper, we present the first large-scale study of the production and visual perception of facial expressions of emotion in the wild. We find that of the 16,384 possible facial configurations that people can theoretically produce, only 35 are successfully used to transmit emotive information across cultures, and only 8 within a smaller number of cultures. Crucially, we find that visual analysis of cross-cultural expressions yields consistent perception of emotion categories and valence, but not arousal. In contrast, visual analysis of cultural-specific expressions yields consistent perception of valence and arousal, but not of emotion categories. Additionally, we find that the number of expressions used to communicate each emotion is also different, e.g., 17 expressions transmit happiness, but only 1 is used to convey disgust.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {Computer vision,Cultural differences,emotion perception,face perception,Face recognition,Gold,machine learning,non-verbal communication,Production,Reliability,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Srinivasan_Martinez_2021_Cross-Cultural and Cultural-Specific Production and Perception of Facial.pdf}
}

@inproceedings{srinivasanRepresentationLearningCrossModal2022,
  title = {Representation {{Learning Through Cross-Modal Conditional Teacher-Student Training For Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Srinivasan, Sundararajan and Huang, Zhaocheng and Kirchhoff, Katrin},
  date = {2022},
  pages = {6442--6446},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747754},
  abstract = {Generic pre-trained speech and text representations promise to reduce the need for large labeled datasets on specific speech and language tasks. However, it is not clear how to effectively adapt these representations for speech emotion recognition. Recent public benchmarks show the efficacy of several popular self-supervised speech representations for emotion classification. In this study, we show that the primary difference between the top-performing representations is in predicting valence while the differences in predicting activation and dominance dimensions are less pronounced. However, we show that even the best-performing HuBERT representation underperforms on valence prediction compared to a multimodal model that also incorporates text representation. We address this shortcoming by injecting lexical information into the speech representation using the multimodal model as a teacher. To improve the efficacy of our approach, we propose a novel estimate of the quality of the emotion predictions, to condition teacher-student training. We report new audio-only state-of-the-art concordance correlation coefficient (CCC) values of 0.757, 0.627, 0.671 for activation, valence and dominance predictions, respectively, on the MSP-Podcast corpus, and also state-of-the-art values of 0.667, 0.582, 0.545 on the IEMOCAP corpus.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Degradation,Emotion recognition,multi-modal,Predictive models,Representation learning,Semantics,speech emotion recognition,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Srinivasan et al_2022_Representation Learning Through Cross-Modal Conditional Teacher-Student.pdf}
}

@unpublished{stantonPredictingExpressiveSpeaking2018,
  title = {Predicting {{Expressive Speaking Style From Text In End-To-End Speech Synthesis}}},
  author = {Stanton, Daisy and Wang, Yuxuan and Skerry-Ryan, R. J.},
  date = {2018-08-03},
  eprint = {1808.01410},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1808.01410},
  urldate = {2021-09-10},
  abstract = {Global Style Tokens (GSTs) are a recently-proposed method to learn latent disentangled representations of high-dimensional data. GSTs can be used within Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to uncover expressive factors of variation in speaking style. In this work, we introduce the Text-Predicted Global Style Token (TP-GST) architecture, which treats GST combination weights or style embeddings as ``virtual'' speaking style labels within Tacotron. TP-GST learns to predict stylistic renderings from text alone, requiring neither explicit labels during training, nor auxiliary inputs for inference. We show that, when trained on a dataset of expressive speech, our system generates audio with more pitch and energy variation than two state-of-the-art baseline models. We further demonstrate that TP-GSTs can synthesize speech with background noise removed, and corroborate these analyses with positive results on human-rated listener preference audiobook tasks. Finally, we demonstrate that multi-speaker TP-GST models successfully factorize speaker identity and speaking style. We provide a website with audio samples 1 for each of our findings.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,eess.AS,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Stanton et al_2018_Predicting Expressive Speaking Style From Text In End-To-End Speech Synthesis.pdf}
}

@inproceedings{steinertAudioVisualRecognitionEmotional2021,
  title = {Audio-{{Visual Recognition}} of {{Emotional Engagement}} of {{People}} with {{Dementia}}},
  booktitle = {Interspeech 2021},
  author = {Steinert, Lars and Putze, Felix and K\"uster, Dennis and Schultz, Tanja},
  date = {2021-08-30},
  pages = {1024--1028},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-567},
  url = {https://www.isca-speech.org/archive/interspeech_2021/steinert21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Steinert et al_2021_Audio-Visual Recognition of Emotional Engagement of People with Dementia.pdf}
}

@article{sunDeepDanceMusictoDanceMotion2021,
  title = {{{DeepDance}}: {{Music-to-Dance Motion Choreography With Adversarial Learning}}},
  shorttitle = {{{DeepDance}}},
  author = {Sun, Guofei and Wong, Yongkang and Cheng, Zhiyong and Kankanhalli, Mohan S. and Geng, Weidong and Li, Xiangdong},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {497--509},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.2981989},
  abstract = {The creation of improvised dancing choreographies is an important research field of cross-modal analysis. A key point of this task is how to effectively create and correlate music and dance with a probabilistic one-to-many mapping, which is essential to create realistic dances of various genres. To address this issue, we propose a GAN-based cross-modal association framework, DeepDance, which correlates two different modalities (dance motion and music) together, aiming at creating the desired dance sequence in terms of the input music. Its generator is to predictively produce the dance movements best-fit to current music piece by learning from examples. In another hand, its discriminator acts as an external evaluation from the audience and judges the whole performance. The generated dance movements and the corresponding input music are considered to be well-matched if the discriminator cannot distinguish the generated movements from the training samples according to the estimated probability. By adding motion consistency constraints in our loss function, the proposed framework is able to create long realistic dance sequences. To alleviate the problem of expensive and inefficient data collection, we propose an effective approach to create a large-scale dataset, YouTube-Dance3D, from open data source. Extensive experiments on currently available music-dance datasets and our YouTube-Dance3D dataset demonstrate that our approach effectively captures the correlation between music and dance and can be used to choreograph appropriate dance sequences.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {adversarial learning,Correlation,cross-modal association,Deep learning,Feature extraction,Generators,Music,Music-driven dance choreography,Task analysis,Three-dimensional displays},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sun et al_2021_DeepDance.pdf}
}

@inproceedings{sunMultimodalCrossSelfAttention2021,
  title = {Multimodal {{Cross-}} and {{Self-Attention Network}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sun, Licai and Liu, Bin and Tao, Jianhua and Lian, Zheng},
  date = {2021-06},
  pages = {4275--4279},
  issn = {2379-190X},
  doi = {10/gmr2j5},
  abstract = {Speech Emotion Recognition (SER) requires a thorough understanding of both the linguistic content of an utterance (i.e., textual information) and how the speaker utters it (i.e., acoustic information). The one vital challenge in SER is how to effectively fuse these two kinds of information. In this paper, we propose a novel Multimodal Cross- and Self-Attention Network (MCSAN) to tackle this problem. The core of MCSAN is to employ the parallel cross- and self-attention modules to explicitly model both inter- and intra-modal interactions of audio and text. Specifically, the cross-attention module utilizes the cross-attention mechanism to guide one modality to attend to the other modality and update the features accordingly. Similarly, the self-attention module employs the self-attention mechanism to propagate information within each modality. We evaluate MCSAN on two benchmark datasets, IEMOCAP and MELD. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on both datasets.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Conferences,cross-attention,Emotion recognition,Fuses,Linguistics,multimodal fusion,self-attention,Signal processing,speech emotion recognition,Speech recognition,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sun et al_2021_Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition.pdf}
}

@article{suPROTOTYPETOSTYLEDialogueGeneration2021,
  title = {{{PROTOTYPE-TO-STYLE}}: {{Dialogue Generation With Style-Aware Editing}} on {{Retrieval Memory}}},
  shorttitle = {{{PROTOTYPE-TO-STYLE}}},
  author = {Su, Yixuan and Wang, Yan and Cai, Deng and Baker, Simon and Korhonen, Anna and Collier, Nigel},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2152--2161},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3087948},
  abstract = {The ability of dialogue systems to express pre-specified style during conversations has a direct, positive impact on their usability and user satisfaction. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of content quality. In this work, we introduce a prototype-to-style (PS) framework to tackle the challenge of stylistic dialogue generation. The proposed framework first exploits an Information Retrieval (IR) system and extracts a response prototype from the retrieved response. A stylistic response generator then takes the response prototype and the desired style as input to produce a high-quality and stylistic response. To effectively train the proposed model and imitate the real testing environment, we introduce a new style-aware learning objective and a denoising learning strategy. Results on three benchmark datasets (gender, emotion, and sentiment) from two languages demonstrate that the proposed approach significantly outperforms existing baselines both in terms of in-domain and cross-domain evaluations.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Adaptation models,Analytical models,Generators,Prototypes,retrieval memory,retrieval-augmented dialogue generation,Speech processing,Stylistic dialogue generation,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Su et al_2021_PROTOTYPE-TO-STYLE.pdf}
}

@inproceedings{suVaccinatingSERNeutralize2022,
  title = {Vaccinating {{SER}} to {{Neutralize Adversarial Attacks}} with {{Self-Supervised Augmentation Strategy}}},
  booktitle = {Interspeech 2022},
  author = {Su, Bo-Hao and Lee, Chi-Chun},
  date = {2022-09-18},
  pages = {1153--1157},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10453},
  url = {https://www.isca-speech.org/archive/interspeech_2022/su22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Su_Lee_2022_Vaccinating SER to Neutralize Adversarial Attacks with Self-Supervised.pdf}
}

@article{taghizadehCrosslingualTransferLearning2022,
  title = {Cross-Lingual Transfer Learning for Relation Extraction Using {{Universal Dependencies}}},
  author = {Taghizadeh, Nasrin and Faili, Heshaam},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101265},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101265},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000711},
  urldate = {2021-09-28},
  abstract = {This paper focuses on the task of cross-language relation extraction, which aims to identify the semantic relations holding between entities in the text. The goal of the task is to train classifiers for low-resource languages by means of the annotated data from high-resource languages. Related methods usually employ parallel data or Machine Translator (MT) to project annotated data from a source to a target language. However, the availability and the quality of parallel data and MT are big challenges for low-resource languages. In this paper, a novel transfer learning method is presented for this task. The key idea is to utilize a tree-based representation of data, which is highly informative for classifying semantic relations, and also shared among different languages. All the training and test data are shown using this representation. We propose to use the Universal Dependency (UD) parsing, which is a language-agnostic formalism for representation of syntactic structures. Equipping UD parse trees with multi-lingual word embeddings makes an ideal representation for the cross-language relation extraction task. We propose two deep networks to use this representation. The first one utilizes the Shortest Dependency Path of UD trees, while the second employs the UD-based positional embeddings. Experiments are performed using SemEval 2010-task 8 training data, whereas French and Farsi are the test languages. The results show 63.9\% and 56.2\% F1 scores, for French and Farsi test data, respectively, which are 14.4\% and 17.9\% higher than the baseline. This work can be considered a simple yet powerful baseline for further investigation into the cross-language tasks.},
  language = {en},
  keywords = {_Waiting for read,Dependency context,Relation extraction,Tree-based models,Universal Dependency Parsing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Taghizadeh_Faili_2022_Cross-lingual transfer learning for relation extraction using Universal.pdf}
}

@article{tahonCanWeGenerate2020,
  title = {Can {{We Generate Emotional Pronunciations}} for {{Expressive Speech Synthesis}}?},
  author = {Tahon, Marie and Lecorve, Gwenole and Lolive, Damien},
  date = {2020-10-01},
  journaltitle = {IEEE Transactions on Affective Computing},
  shortjournal = {IEEE Trans. Affective Comput.},
  volume = {11},
  number = {4},
  pages = {684--695},
  issn = {1949-3045, 2371-9850},
  doi = {10.1109/TAFFC.2018.2828429},
  url = {https://ieeexplore.ieee.org/document/8341805/},
  urldate = {2021-09-10},
  abstract = {In the field of expressive speech synthesis, a lot of work has been conducted on suprasegmental prosodic features while few has been done on pronunciation variants. However, prosody is highly related to the sequence of phonemes to be expressed. This article raises two issues in the generation of emotional pronunciations for TTS systems. The first issue consists in designing an automatic pronunciation generation method from text, while the second issue addresses the very existence of emotional pronunciations through experiments conducted on emotional speech. To do so, an innovative pronunciation adaptation method which automatically adapts canonical phonemes first to those labeled in the corpus used to create a synthetic voice, then to those labeled in an expressive corpus, is presented. This method consists in training conditional random fields pronunciation models with prosodic, linguistic, phonological and articulatory features. The analysis of emotional pronunciations reveals strong dependencies between prosody and phoneme assimilation or elisions. According to perceptual tests, the double adaptation allows to synthesize expressive speech samples of good quality, but emotion-specific pronunciations are too subtle to be perceived by testers.},
  issue = {4},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tahon et al_2020_Can We Generate Emotional Pronunciations for Expressive Speech Synthesis.pdf}
}

@inproceedings{takashimaInteractiveCoLearningCrossModal2022,
  title = {Interactive {{Co-Learning}} with {{Cross-Modal Transformer}} for {{Audio-Visual Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Takashima, Akihiko and Masumura, Ryo and Ando, Atsushi and Yamazaki, Yoshihiro and Uchida, Mihiro and Orihashi, Shota},
  date = {2022-09-18},
  pages = {4740--4744},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11307},
  url = {https://www.isca-speech.org/archive/interspeech_2022/takashima22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Takashima et al_2022_Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion.pdf}
}

@inproceedings{tammewarEmotionCarrierRecognition2021,
  title = {Emotion {{Carrier Recognition}} from {{Personal Narratives}}},
  booktitle = {Interspeech 2021},
  author = {Tammewar, Aniruddha and Cervone, Alessandra and Riccardi, Giuseppe},
  date = {2021-08-30},
  pages = {2501--2505},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1100},
  url = {https://www.isca-speech.org/archive/interspeech_2021/tammewar21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tammewar et al_2021_Emotion Carrier Recognition from Personal Narratives.pdf}
}

@report{tanColearningLearningNoisy2021,
  title = {Co-Learning: {{Learning}} from {{Noisy Labels}} with {{Self-supervision}}},
  shorttitle = {Co-Learning},
  author = {Tan, Cheng and Xia, Jun and Wu, Lirong and Li, Stan Z.},
  date = {2021-10-30},
  eprint = {2108.04063},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3474085.3475622},
  url = {http://arxiv.org/abs/2108.04063},
  urldate = {2022-05-30},
  abstract = {Noisy labels, resulting from mistakes in manual labeling or webly data collecting for supervised learning, can cause neural networks to overfit the misleading information and degrade the generalization performance. Self-supervised learning works in the absence of labels and thus eliminates the negative impact of noisy labels. Motivated by co-training with both supervised learning view and self-supervised learning view, we propose a simple yet effective method called Co-learning for learning with noisy labels. Co-learning performs supervised learning and self-supervised learning in a cooperative way. The constraints of intrinsic similarity with the self-supervised module and the structural similarity with the noisily-supervised module are imposed on a shared common feature encoder to regularize the network to maximize the agreement between the two constraints. Co-learning is compared with peer methods on corrupted data from benchmark datasets fairly, and extensive results are provided which demonstrate that Co-learning is superior to many state-of-the-art approaches.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tan et al_2021_Co-learning.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\WGHMRV8E\\2108.html}
}

@article{tanDeepLearningBased2021,
  title = {Deep {{Learning Based Real-Time Speech Enhancement}} for {{Dual-Microphone Mobile Phones}}},
  author = {Tan, Ke and Zhang, Xueliang and Wang, DeLiang},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1853--1863},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3082318},
  abstract = {In mobile speech communication, speech signals can be severely corrupted by background noise when the far-end talker is in a noisy acoustic environment. To suppress background noise, speech enhancement systems are typically integrated into mobile phones, in which one or more microphones are deployed. In this study, we propose a novel deep learning based approach to real-time speech enhancement for dual-microphone mobile phones. The proposed approach employs a new densely-connected convolutional recurrent network to perform dual-channel complex spectral mapping. We utilize a structured pruning technique to compress the model without significantly degrading the enhancement performance, which yields a low-latency and memory-efficient enhancement system for real-time processing. Experimental results suggest that the proposed approach consistently outperforms an earlier approach to dual-channel speech enhancement for mobile phone communication, as well as a deep learning based beamformer.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {complex spectral mapping,Convolution,Deep learning,densely-connected convolutional recurrent network,dual-microphone mobile phones,Logic gates,Microphones,Mobile handsets,Noise measurement,Real-time speech enhancement,Speech enhancement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tan et al_2021_Deep Learning Based Real-Time Speech Enhancement for Dual-Microphone Mobile.pdf}
}

@article{tangMultiGranularitySequenceAlignment2021,
  title = {Multi-{{Granularity Sequence Alignment Mapping}} for {{Encoder-Decoder Based End-to-End ASR}}},
  author = {Tang, Jian and Zhang, Jie and Song, Yan and McLoughlin, Ian and Dai, Li-Rong},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2816--2828},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3101921},
  abstract = {Encoder-decoder based automatic speech recognition (ASR) methods are increasingly popular due to their simplified processing stages and low reliance on prior knowledge. Conventional encoder-decoder based approaches usually learn a sequence-to-sequence mapping function from the source speech to target units (e.g., subwords, characters) in an end-to-end manner. However, it is still unclear how to choose the optimal target unit, or granularity of multiple units. In general, as increasing the information available for learning sequence-to-sequence mapping functions can improve modeling effectiveness, we therefore propose a multi-granularity sequence alignment (MGSA) approach. This aims to enhance cross-sequence interactions between different granularity units for both modeling and inference stages in the encoder-decoder based ASR. Specifically, a decoder module is designed to generate multi-granularity sequence predictions. We then exploit the latent alignment mapping among units having different levels of granularity, by utilizing the decoded multi-level sequences as input for model prediction. The cross-sequence interaction can also be employed to re-calibrate output probabilities in the proposed post-inference algorithm. Experimental results on both WSJ-80 hrs and Switchboard-300 hrs datasets show the superiority of the proposed method compared to traditional multi-task methods as well as to single granularity baseline systems.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {deep learning,encoder-decoder,end-to-end ASR,Multi-granularity,post-inference,sequence alignment,Speech processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tang et al_2021_Multi-Granularity Sequence Alignment Mapping for Encoder-Decoder Based.pdf}
}

@article{tanModelCompressionDeep2021,
  title = {Towards {{Model Compression}} for {{Deep Learning Based Speech Enhancement}}},
  author = {Tan, Ke and Wang, DeLiang},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1785--1794},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3082282},
  abstract = {The use of deep neural networks (DNNs) has dramatically elevated the performance of speech enhancement over the last decade. However, to achieve strong enhancement performance typically requires a large DNN, which is both memory and computation consuming, making it difficult to deploy such speech enhancement systems on devices with limited hardware resources or in applications with strict latency requirements. In this study, we propose two compression pipelines to reduce the model size for DNN-based speech enhancement, which incorporates three different techniques: sparse regularization, iterative pruning and clustering-based quantization. We systematically investigate these techniques and evaluate the proposed compression pipelines. Experimental results demonstrate that our approach reduces the sizes of four different models by large margins without significantly sacrificing their enhancement performance. In addition, we find that the proposed approach performs well on speaker separation, which further demonstrates the effectiveness of the approach for compressing speech separation models.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Image coding,Model compression,Pipelines,pruning,quantization,Quantization (signal),Sensitivity analysis,sparse regularization,speech enhancement,Speech enhancement,Tensors,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tan_Wang_2021_Towards Model Compression for Deep Learning Based Speech Enhancement.pdf}
}

@inproceedings{taoCharacterizingTherapistSpeaking2022,
  title = {Characterizing {{Therapist}}'s {{Speaking Style}} in {{Relation}} to {{Empathy}} in {{Psychotherapy}}},
  booktitle = {Interspeech 2022},
  author = {Tao, Dehua and Lee, Tan and Chui, Harold and Luk, Sarah},
  date = {2022-09-18},
  pages = {2003--2007},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10416},
  url = {https://www.isca-speech.org/archive/interspeech_2022/tao22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tao et al_2022_Characterizing Therapist's Speaking Style in Relation to Empathy in.pdf}
}

@article{taoEndtoEndAudiovisualSpeech2021,
  title = {End-to-{{End Audiovisual Speech Recognition System With Multitask Learning}}},
  author = {Tao, Fei and Busso, Carlos},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {1--11},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.2975922},
  abstract = {An automatic speech recognition (ASR) system is a key component in current speech-based systems. However, the surrounding acoustic noise can severely degrade the performance of an ASR system. An appealing solution to address this problem is to augment conventional audio-based ASR systems with visual features describing lip activity. This paper proposes a novel end-to-end, multitask learning (MTL), audiovisual ASR (AV-ASR) system. A key novelty of the approach is the use of MTL, where the primary task is AV-ASR, and the secondary task is audiovisual voice activity detection (AV-VAD). We obtain a robust and accurate audiovisual system that generalizes across conditions. By detecting segments with speech activity, the AV-ASR performance improves as its connectionist temporal classification (CTC) loss function can leverage from the AV-VAD alignment information. Furthermore, the end-to-end system learns from the raw audiovisual inputs a discriminative high-level representation for both speech tasks, providing the flexibility to mine information directly from the data. The proposed architecture considers the temporal dynamics within and across modalities, providing an appealing and practical fusion scheme. We evaluate the proposed approach on a large audiovisual corpus (over 60 hours), which contains different channel and environmental conditions, comparing the results with competitive single task learning (STL) and MTL baselines. Although our main goal is to improve the performance of our ASR task, the experimental results show that the proposed approach can achieve the best performance across all conditions for both speech tasks. In addition to state-of-the-art performance in AV-ASR, the proposed solution can also provide valuable information about speech activity, solving two of the most important tasks in speech-based applications.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,Audiovisual speech recognition,deep learning,end-to-end speech systems,Feature extraction,multitask learning,Robustness,Speech processing,Task analysis,Timing,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tao_Busso_2021_End-to-End Audiovisual Speech Recognition System With Multitask Learning.pdf}
}

@inproceedings{taoHierarchicalAttentionNetwork2022,
  title = {Hierarchical {{Attention Network}} for {{Evaluating Therapist Empathy}} in {{Counseling Session}}},
  booktitle = {Interspeech 2022},
  author = {Tao, Dehua and Lee, Tan and Chui, Harold and Luk, Sarah},
  date = {2022-09-18},
  pages = {2008--2012},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10550},
  url = {https://www.isca-speech.org/archive/interspeech_2022/tao22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tao et al_2022_Hierarchical Attention Network for Evaluating Therapist Empathy in Counseling.pdf}
}

@thesis{TaoJiYuPuTuTeZhengDeYuYinQingGanShiBieRuoGanWenTiDeYanJiu2017,
  type = {博士},
  title = {基于谱图特征的语音情感识别若干问题的研究},
  author = {陶, 华伟},
  date = {2017},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2018&filename=1018002851.nh&uniplatform=NZKPT&v=C4pDOKVWUC6PQqosvbR%25mmd2BJK3Ukx4uVJsvLY6o9f9G0%25mmd2FLnrSN%25mmd2Bq1WEoHQZjCmtXv%25mmd2Fd},
  urldate = {2021-09-10},
  abstract = {为了使人机交互系统更加自然和智能,针对语音情感识别的研究受到越来越多学者的关注。近几十年,针对语音情感识别的研究已经取得了较大的进步,语音情感识别系统的性能得到了巨大的提升。然而,在现有的语音情感识别系统中,仍然缺乏一些能够准确识别语音情感信息的特征,因此,针对语音情感识别特征提取的研究,仍是语音情感识别领域研究的重点。语音中的情感内容与频谱能量的分布有密切的关系,部分学者将一段语音的频域系数构建成图像\textemdash\textemdash 谱图,并采用图像描述子从谱图中提取语音情感相关特征,取得一定的成果。由于该方向在情感识别领域刚刚兴起,仍然存在许多问题需要研究。首先、谱图中的哪些信息与语音情感类型有关?其次,如何从谱图中有效地提取这些信息?针对上述问题,本文基于语音情感信息与谱图纹理及能量分布的密切相关性,开展了基于语音谱图特征的情感识别研究,相关研究工作如下:1、基于语音情感类型与语音谱图纹理分布的密切相关性。提出了一种基于Gabor灰度图像谱局部二值模式（GGSLBP）特征提取方法。GGSLBP首先构建语谱图灰度图像;然后采用Gabor小波放大语谱图灰度图像的局部纹理信息,得到Gabor灰度图像谱;最后采用局部二值模式（LBP）提取Gabor灰度图像谱的局部纹理信息,得到GGSLBP特征。仿真实验显示:与传统的声学特征相比,GGSLBP特征具有较好的识别性能。2、针对局部二值模式（LBP）忽略了谱图中幅度信息且GGSLBP特征复杂度较高的问题,提出了一种面向语音情感识别的改进可辨别完全局部二值模式（IDisCLBPSER）特征提取方法。首先生成语谱图灰度图像;其次采用完全局部二值模式（CLBP）计算图像的符号模式（CLBPS）、幅度模式（CLBPM）;再次,不同于传统DisCLBP算法,IDisCLBPSER取消了 CLBPS、CLBPM旋转不变映射处理,直接采用可辨别特征学习模型计算CLBPS、CLBPM的全局显著性模式集合;最后,采用全局显著性模式集合对CLBPS、CBPM特征处理,并将处理后特征级联,得到IDisCLBPSER特征。实验表明:所提特征与现有声学特征融合后可以提升语音情感识别系统的识别性能。3、为探讨图像描述子中的旋转不变性是否适用于Mel对数能量谱图特征,提出了一种基于局部归一化中心矩谱图特征（LNCMSIF）提取方法。LNCMSIF首先采用二阶归一化中心矩描述Mel对数能量谱的局部能量分布信息,得到归一化中心矩谱;然后,采用离散余弦变换消除归一化中心矩谱系数间相关性,得到归一化中心矩谱倒谱系数;最后,将归一化中心矩谱及其倒谱系数组合在一起构成LNCMSIF特征。旋转不变性测试实验表明:旋转不变性不完全适用于Mel对数能量谱图特征。识别实验表明所提方法可以取得较好的识别结果。4、考虑到图像特征描述子的表征能力有限,不能充分地描述Mel对数能量谱中的情感信息。提出了 2种基于Gabor谱局部能量分布信息的谱图特征提取方法,分别为基于Gabor谱局部Hu不变矩谱图特征（GSLHuM）和基于Gabor谱局部归一化中心矩谱图特征（GSLNCM）。GSLHuM首先采用Gabor小波对Mel对数能量谱进行处理,得到Gabor谱;然后,采用1阶Hu不变矩描述Gabor谱局部能量分布信息;最后,采用离散余弦变换消除相关性,得到GSLHuM特征。类似的,GSLNCM特征采用归一化中心矩从Gabor谱中提取局部能量分布信息。仿真实验验证了所提的GSLHuM特征和GSLNCM特征的有效性。此外,与Mel对数能量谱图特征相比,旋转不变性对Gabor谱图特征识别性能影响较弱。},
  editora = {赵, 力},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Discriminative Feature Learning Model,Gabor Wavelet,Gabor小波,Hu Invariant Moments,Hu不变矩,Local Binary Pattern,Normalized Center Moments,Spectrum Image Features,Speech Emotion Recognition,可辨别特征学习模型,局部二值模式,归一化中心矩,语音情感识别,谱图特征},
  annotation = {6 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\陶_2017_基于谱图特征的语音情感识别若干问题的研究.pdf}
}

@inproceedings{terashimaCrossSpeakerEmotionTransfer2022,
  title = {Cross-{{Speaker Emotion Transfer}} for {{Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion}} with {{Pitch-Shift Data Augmentation}}},
  booktitle = {Interspeech 2022},
  author = {Terashima, Ryo and Yamamoto, Ryuichi and Song, Eunwoo and Shirahata, Yuma and Yoon, Hyun-Wook and Kim, Jae-Min and Tachibana, Kentaro},
  date = {2022-09-18},
  pages = {3018--3022},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11278},
  url = {https://www.isca-speech.org/archive/interspeech_2022/terashima22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Terashima et al_2022_Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using.pdf}
}

@article{teschNonlinearSpatialFiltering2021,
  title = {Nonlinear {{Spatial Filtering}} in {{Multichannel Speech Enhancement}}},
  author = {Tesch, Kristina and Gerkmann, Timo},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1795--1805},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3076372},
  abstract = {The majority of multichannel speech enhancement algorithms are two-step procedures that first apply a linear spatial filter, a so-called beamformer, and combine it with a single-channel approach for postprocessing. However, the serial concatenation of a linear spatial filter and a postfilter is not generally optimal in the minimum mean square error (MMSE) sense for noise distributions other than a Gaussian distribution. Rather, the MMSE optimal filter is a joint spatial and spectral nonlinear function. While estimating the parameters of such a filter with traditional methods is challenging, modern neural networks may provide an efficient way to learn the nonlinear function directly from data. To see if further research in this direction is worthwhile, in this work we examine the potential performance benefit of replacing the common two-step procedure with a joint spatial and spectral nonlinear filter. We analyze three different forms of non-Gaussianity: First, we evaluate on super-Gaussian noise with a high kurtosis. Second, we evaluate on inhomogeneous noise fields created by five interfering sources using two microphones, and third, we evaluate on real-world recordings from the CHiME3 database. In all scenarios, considerable improvements may be obtained. Most prominently, our analyses show that a nonlinear spatial filter uses the available spatial information more effectively than a linear spatial filter as it is capable of suppressing more than \$D-1\$ directional interfering sources with a \$D\$-dimensional microphone array without spatial adaptation.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Discrete Fourier transforms,Gaussian distribution,Microphone arrays,Multichannel,neural networks,Noise measurement,nonlinear spatial filtering,Probability density function,Random variables,speech enhancement,Speech enhancement},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tesch_Gerkmann_2021_Nonlinear Spatial Filtering in Multichannel Speech Enhancement.pdf}
}

@article{thiamMultiModalPainIntensity2021,
  title = {Multi-{{Modal Pain Intensity Recognition Based}} on the {{SenseEmotion Database}}},
  author = {Thiam, Patrick and Kessler, Viktor and Amirian, Mohammadreza and Bellmann, Peter and Layher, Georg and Zhang, Yan and Velana, Maria and Gruss, Sascha and Walter, Steffen and Traue, Harald C. and Schork, Daniel and Kim, Jonghwa and Andr\'e, Elisabeth and Neumann, Heiko and Schwenker, Friedhelm},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {743--760},
  issn = {1949-3045},
  doi = {10.1109/taffc.2019.2892090},
  abstract = {The subjective nature of pain makes it a very challenging phenomenon to assess. Most of the current pain assessment approaches rely on an individual's ability to recognise and report an observed pain episode. However, pain perception and expression are affected by numerous factors ranging from personality traits to physical and psychological health state. Hence, several approaches have been proposed for the automatic recognition of pain intensity, based on measurable physiological and audiovisual parameters. In the current paper, an assessment of several fusion architectures for the development of a multi-modal pain intensity classification system is performed. The contribution of the presented work is two-fold: (1) 3 distinctive modalities consisting of audio, video and physiological channels are assessed and combined for the classification of several levels of pain elicitation. (2) An extensive assessment of several fusion strategies is carried out in order to design a classification architecture that improves the performance of the pain recognition system. The assessment is based on the SenseEmotion Database and experimental validation demonstrates the relevance of the multi-modal classification approach, which achieves classification rates of respectively \$83.39\%\$83.39\%, \$59.53\%\$59.53\% and \$43.89\%\$43.89\% in a 2-class, 3-class and 4-class pain intensity classification task.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {Computer architecture,Databases,Electromyography,Feature extraction,multi-modal information fusion,multiple classifier systems,Pain,Pain intensity recognition,Physiology,Reliability,signal processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Thiam et al_2021_Multi-Modal Pain Intensity Recognition Based on the SenseEmotion Database.pdf}
}

@article{torresF0PerturbationDue2021,
  title = {F0 {{Perturbation Due}} to {{Articulatory Movements}}: {{Filtering}}, {{Characterization}} and {{Applications}}},
  shorttitle = {F0 {{Perturbation Due}} to {{Articulatory Movements}}},
  author = {Torres, Humberto M. and G\"uemes, Mercedes and Gurlekian, Jorge A. and Evin, Diego A.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1977--1986},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3082671},
  abstract = {This paper addresses the issue of local disturbances in the fundamental frequency contour of speech, caused by the articulation of voiced/unvoiced consonant phonemes. Depending on the intended use of the F0 contour, these disturbances are usually eliminated by a filtering, smoothing or stylization procedure. These procedures that seek to preserve only the F0 points perceptually relevant, are generally applied roughly at a global level, which may not completely eliminate micro intonation in some cases or distort macro intonation in others. In this work we propose a local filtering algorithm based on a fine level analysis of the microprosodic morphologies. The performance of the algorithm is validated by a perceptual experiment. Assuming the algorithm allows partial/total disturbance elimination, we perform a statistical description of the perturbation morphologies. Statistics were collected from a corpus of 741 sentences designed to study Argentine Spanish prosody. The corpus was recorded by four professional announcers native speakers from Buenos Aires city. The results show that perturbation morphologies are affected by: consonant phoneme identity; global F0 contour shape; and speaker identity. As an application case, we use the proposed filtering algorithm as a pre-processing stage in our automatic prominent syllable detection system, with a statistically significant improvement in its performance.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Electronic mail,F0 filtering,micro-intonation,Morphology,Perturbation methods,Production,prominence detection,segmental F0,Smoothing methods,Speech processing,Speech prosody,Splines (mathematics)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Torres et al_2021_F0 Perturbation Due to Articulatory Movements.pdf}
}

@inproceedings{tranPreTrainedAudioVisualTransformer2022,
  title = {A {{Pre-Trained Audio-Visual Transformer}} for {{Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tran, Minh and Soleymani, Mohammad},
  date = {2022},
  pages = {4698--4702},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747278},
  abstract = {In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7\% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10\% of the original training set provided, finetuning the pre-trained model can lead to at least 10\% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Emotion recognition,Mental health,multiomdal fusion,Robustness,Signal processing,Speech recognition,Training,Transformer,Transformers},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tran_Soleymani_2022_A Pre-Trained Audio-Visual Transformer for Emotion Recognition.pdf}
}

@inproceedings{triantafyllopoulosProbingSpeechEmotion2022,
  title = {Probing Speech Emotion Recognition Transformers for Linguistic Knowledge},
  booktitle = {Interspeech 2022},
  author = {Triantafyllopoulos, Andreas and Wagner, Johannes and Wierstorf, Hagen and Schmitt, Maximilian and Reichel, Uwe and Eyben, Florian and Burkhardt, Felix and Schuller, Bj\"orn W.},
  date = {2022-09-18},
  pages = {146--150},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10371},
  url = {https://www.isca-speech.org/archive/interspeech_2022/triantafyllopoulos22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Triantafyllopoulos et al_2022_Probing speech emotion recognition transformers for linguistic knowledge.pdf}
}

@inproceedings{triantafyllopoulosRoleTaskAcoustic2021,
  title = {The {{Role}} of {{Task}} and {{Acoustic Similarity}} in {{Audio Transfer Learning}}: {{Insights}} from the {{Speech Emotion Recognition Case}}},
  shorttitle = {The {{Role}} of {{Task}} and {{Acoustic Similarity}} in {{Audio Transfer Learning}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Triantafyllopoulos, Andreas and Schuller, Bj\"orn W.},
  date = {2021-06},
  pages = {7268--7272},
  issn = {2379-190X},
  doi = {10/gmr2kd},
  abstract = {With the rise of deep learning, deep knowledge transfer has emerged as one of the most effective techniques for getting state-of-the-art performance using deep neural networks. A lot of recent research has focused on understanding the mechanisms of transfer learning in the image and language domains. We perform a similar investigation for the case of speech emotion recognition (SER), and conclude that transfer learning for SER is influenced both by the choice of pre-training task and by the differences in acoustic conditions between the upstream and downstream data sets, with the former having a bigger impact. The effect of each factor is isolated by first transferring knowledge between different tasks on the same data, and then from the original data to corrupted versions of it but for the same task. We also demonstrate that layers closer to the input see more adaptation than ones closer to the output in both cases, a finding which explains why previous works often found it necessary to fine-tune all layers during transfer learning.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Acoustics,Deep learning,Emotion recognition,Neural networks,representation learning,Signal processing,Speech emotion recognition,Speech recognition,transfer learning,Transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Triantafyllopoulos_Schuller_2021_The Role of Task and Acoustic Similarity in Audio Transfer Learning.pdf}
}

@article{trinhDirectlyComparingListening2021,
  title = {Directly {{Comparing}} the {{Listening Strategies}} of {{Humans}} and {{Machines}}},
  author = {Trinh, Viet Anh and Mandel, Michael},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {312--323},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3040545},
  abstract = {Automatic speech recognition (ASR) has reached human performance on many clean speech corpora, but it remains worse than human listeners in noisy environments. This paper investigates whether this difference in performance might be due to a difference in the time-frequency regions that each listener utilizes in making their decisions and how these ``important'' regions change for ASRs using different acoustic models (AMs) and language models (LMs). We define important regions as time-frequency points in a spectrogram that tend to be audible when the listener correctly recognizes that utterance in noise. The evidence from this study indicates that a neural network AM attends to regions that are more similar to those of humans (capturing certain high-energy regions) than those of a traditional Gaussian mixture model (GMM) AM. Our analysis also shows that the neural network AM has not yet captured all the cues that human listeners utilize, such as certain transitions between silence and high speech energy. We also find that differences in important time-frequency regions tend to track differences in accuracy on specific words in a test sentence, suggesting a connection. Because of this connection, adapting an ASR to attend to the same regions humans use might improve its generalization in noise.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Automatic speech recognition,noise,Noise measurement,sentence recognition,Signal to noise ratio,Spectrogram,speech perception,Speech recognition,Task analysis,Time-frequency analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Trinh_Mandel_2021_Directly Comparing the Listening Strategies of Humans and Machines.pdf}
}

@inproceedings{tsengExtendingMusicBased2021,
  title = {Extending {{Music Based On Emotion And Tonality Via Generative Adversarial Network}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tseng, Bo-Wei and Shen, Yih-Liang and Chi, Tai-Shih},
  date = {2021-06},
  pages = {86--90},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413365},
  abstract = {We propose a generative model for music extension in this paper. The model is composed of two classifiers, one for music emotion and one for music tonality, and a generative adversarial network (GAN). Therefore, it can generate symbolic music not only based on low level spectral and temporal characteristics, but also on high level emotion and tonality attributes of previously observed music pieces. The generative model works in a universal latent space constructed by the variational autoencoder (VAE) for representing music pieces. We conduct subjective listening tests and derive objective measures for performance evaluation. Experimental results show that the proposed model produces much smoother and more authentic music pieces than the baseline model in terms of all subjective and objective measures.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustic measurements,Conferences,Current measurement,generative adversarial network,Generative adversarial networks,Music,music emotion,Music generation,Performance evaluation,Signal processing,tonality,variational auto-encoder},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tseng et al_2021_Extending Music Based On Emotion And Tonality Via Generative Adversarial Network.pdf}
}

@inproceedings{tStochasticProcessRegression2021,
  title = {Stochastic {{Process Regression}} for {{Cross-Cultural Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {T, Mani Kumar and Sanchez, Enrique and Tzimiropoulos, Georgios and Giesbrecht, Timo and Valstar, Michel},
  date = {2021-08-30},
  pages = {3390--3394},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-610},
  url = {https://www.isca-speech.org/archive/interspeech_2021/t21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\T et al_2021_Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition.pdf}
}

@inproceedings{tzirakisSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition Using Semantic Information}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tzirakis, Panagiotis and Nguyen, Anh and Zafeiriou, Stefanos and Schuller, Bj\"orn W.},
  date = {2021-06},
  pages = {6279--6283},
  issn = {2379-190X},
  doi = {10/gmr2kb},
  abstract = {Speech emotion recognition is a crucial problem manifesting in a multitude of applications such as human computer interaction and education. Although several advancements have been made in the recent years, especially with the advent of Deep Neural Networks (DNN), most of the studies in the literature fail to consider the semantic information in the speech signal. In this paper, we propose a novel framework that can capture both the semantic and the paralinguistic information in the signal. In particular, our framework is comprised of a semantic feature extractor, that captures the semantic information, and a paralinguistic feature extractor, that captures the paralinguistic information. Both semantic and paraliguistic features are then combined to a unified representation using a novel attention mechanism. The unified feature vector is passed through a LSTM to capture the temporal dynamics in the signal, before the final prediction. To validate the effectiveness of our framework, we use the popular SEWA dataset of the AVEC challenge series and compare with the three winning papers. Our model provides state-of-the-art results in the valence and liking dimensions.1},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,audiotextual information,deep learning,emotion recognition,Emotion recognition,Human computer interaction,Neural networks,paralinguistic,semantic,Semantics,Signal processing,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tzirakis et al_2021_Speech Emotion Recognition Using Semantic Information.pdf}
}

@inproceedings{umEmotionalSpeechSynthesis2020,
  title = {Emotional {{Speech Synthesis}} with {{Rich}} and {{Granularized Control}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Um, Se-Yun and Oh, Sangshin and Byun, Kyungguen and Jang, Inseon and Ahn, ChungHyun and Kang, Hong-Goo},
  date = {2020-05},
  pages = {7254--7258},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9053732},
  url = {https://ieeexplore.ieee.org/document/9053732/},
  urldate = {2021-09-10},
  abstract = {This paper proposes an effective emotion control method for an endto-end text-to-speech (TTS) system. To flexibly control the distinct characteristic of a target emotion category, it is essential to determine embedding vectors representing the TTS input. We introduce an inter-to-intra emotional distance ratio algorithm to the embedding vectors that can minimize the distance to the target emotion category while maximizing its distance to the other emotion categories. To further enhance the expressiveness of a target speech, we also introduce an effective interpolation technique that enables the intensity of a target emotion to be gradually changed to that of neutral speech. Subjective evaluation results in terms of emotional expressiveness and controllability show the superiority of the proposed algorithm to the conventional methods.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Um et al_2020_Emotional Speech Synthesis with Rich and Granularized Control.pdf}
}

@inproceedings{uroojAcousticProsodicCorrelates2021,
  title = {Acoustic and {{Prosodic Correlates}} of {{Emotions}} in {{Urdu Speech}}},
  booktitle = {Interspeech 2021},
  author = {Urooj, Saba and Mumtaz, Benazir and Hussain, Sarmad and ul Haq, Ehsan},
  date = {2021-08-30},
  pages = {396--400},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-910},
  url = {https://www.isca-speech.org/archive/interspeech_2021/urooj21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Urooj et al_2021_Acoustic and Prosodic Correlates of Emotions in Urdu Speech.pdf}
}

@inproceedings{vaarasAnalysisSelfSupervisedLearning2022,
  title = {Analysis of {{Self-Supervised Learning}} and {{Dimensionality Reduction Methods}} in {{Clustering-Based Active Learning}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Vaaras, Einari and Airaksinen, Manu and R\"as\"anen, Okko},
  date = {2022-09-18},
  pages = {1143--1147},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-329},
  url = {https://www.isca-speech.org/archive/interspeech_2022/vaaras22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Vaaras et al_2022_Analysis of Self-Supervised Learning and Dimensionality Reduction Methods in.pdf}
}

@inproceedings{vaarasAutomaticAnalysisEmotional2021,
  title = {Automatic {{Analysis}} of the {{Emotional Content}} of {{Speech}} in {{Daylong Child-Centered Recordings}} from a {{Neonatal Intensive Care Unit}}},
  booktitle = {Interspeech 2021},
  author = {Vaaras, Einari and Ahlqvist-Bj\"orkroth, Sari and Drossos, Konstantinos and R\"as\"anen, Okko},
  date = {2021-08-30},
  pages = {3380--3384},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-303},
  url = {https://www.isca-speech.org/archive/interspeech_2021/vaaras21_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Vaaras et al_2021_Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered.pdf}
}

@unpublished{valleMellotronMultispeakerExpressive2019,
  title = {Mellotron: {{Multispeaker}} Expressive Voice Synthesis by Conditioning on Rhythm, Pitch and Global Style Tokens},
  shorttitle = {Mellotron},
  author = {Valle, Rafael and Li, Jason and Prenger, Ryan and Catanzaro, Bryan},
  date = {2019-10-26},
  eprint = {1910.11997},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1910.11997},
  urldate = {2021-09-10},
  abstract = {Mellotron is a multispeaker voice synthesis model based on Tacotron 2 GST that can make a voice emote and sing without emotive or singing training data. By explicitly conditioning on rhythm and continuous pitch contours from an audio signal or music score, Mellotron is able to generate speech in a variety of styles ranging from read speech to expressive speech, from slow drawls to rap and from monotonous voice to singing voice. Unlike other methods, we train Mellotron using only read speech data without alignments between text and audio. We evaluate our models using the LJSpeech and LibriTTS datasets. We provide F0 Frame Errors and synthesized samples that include style transfer from other speakers, singers and styles not seen during training, procedural manipulation of rhythm and pitch and choir synthesis.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Valle et al_2019_Mellotron.pdf}
}

@article{vanderwesthuizenFeatureLearningEfficient2022,
  title = {Feature Learning for Efficient {{ASR-free}} Keyword Spotting in Low-Resource Languages},
  author = {van der Westhuizen, Ewald and Kamper, Herman and Menon, Raghav and Quinn, John and Niesler, Thomas},
  options = {useprefix=true},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101275},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101275},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000784},
  urldate = {2021-09-28},
  abstract = {We consider feature learning for a computationally efficient method of keyword spotting that can be applied in severely under-resourced settings. The objective is to support humanitarian relief programmes by the United Nations (UN) in parts of Africa in which almost no language resources are available. To allow a keyword spotting system to be rapidly developed in such a language, we rely on a small and easily-compiled set of isolated keywords. Using the isolated keywords as templates, we apply dynamic time warping (DTW) to a much larger corpus of in-domain but untranscribed speech. The resulting DTW alignment scores are used to train a convolutional neural network (CNN) which is orders of magnitude more computationally efficient than DTW and therefore suitable for real-time application. We optimise this ASR-free neural network keyword spotting procedure by identifying acoustic features that provide robust performance in this almost zero-resource setting. First, we consider the benefits of incorporating information from well-resourced but unrelated languages by incorporating a multilingual bottleneck feature (BNF) extractor. Next, we consider using features extracted from an autoencoder (AE) trained on in-domain but untranscribed data. Finally, we consider features obtained from a correspondence autoencoder (CAE) which is initialised with the AE and subsequently fine-tuned on the small set of in-domain labelled data. Experiments in South African English and Luganda, a low-resource language, demonstrate that, on their own, both the BNF and CAE features can achieve a 5\% relative performance improvement over baseline MFCCs. However, by using BNFs as input to the CAE, even better performance is achieved, resulting in a more than 27\% relative improvement over MFCCs in ROC area-under-the-curve (AUC) and more than twice as many top-10 retrievals. We also show that, using these features, the CNN-DTW keyword spotter performs almost as well as the DTW keyword spotter while comfortably outperforming a baseline CNN trained only on the keyword templates. We conclude that a CNN-DTW keyword spotter using BNF-derived CAE features represents a computationally efficient approach with very competitive performance that is suited to rapid deployment in a severely under-resourced scenario.},
  language = {en},
  keywords = {Convolutional neural networks,Dynamic time warping,Keyword spotting,Low-resource languages,Representation learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\van der Westhuizen et al_2022_Feature learning for efficient ASR-free keyword spotting in low-resource.pdf}
}

@article{vanrijnExploringEmotionalPrototypes2021,
  title = {Exploring Emotional Prototypes in a High Dimensional {{TTS}} Latent Space},
  author = {van Rijn, Pol and Mertes, Silvan and Schiller, Dominik and Harrison, Peter M. C. and Larrouy-Maestri, Pauline and Andr\'e, Elisabeth and Jacoby, Nori},
  options = {useprefix=true},
  date = {2021-08-30},
  journaltitle = {Interspeech 2021},
  eprint = {2105.01891},
  eprinttype = {arxiv},
  pages = {3870--3874},
  doi = {10.21437/Interspeech.2021-1538},
  url = {http://arxiv.org/abs/2105.01891},
  urldate = {2021-09-10},
  abstract = {Recent TTS systems are able to generate prosodically varied and realistic speech. However, it is unclear how this prosodic variation contributes to the perception of speakers' emotional states. Here we use the recent psychological paradigm `Gibbs Sampling with People' to search the prosodic latent space in a trained GST Tacotron model to explore prototypes of emotional prosody. Participants are recruited online and collectively manipulate the latent space of the generative speech model in a sequentially adaptive way so that the stimulus presented to one group of participants is determined by the response of the previous groups. We demonstrate that (1) particular regions of the model's latent space are reliably associated with particular emotions, (2) the resulting emotional prototypes are wellrecognized by a separate group of human raters, and (3) these emotional prototypes can be effectively transferred to new sentences. Collectively, these experiments demonstrate a novel approach to the understanding of emotional speech by providing a tool to explore the relation between the latent space of generative models and human semantics.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\van Rijn et al_2021_Exploring emotional prototypes in a high dimensional TTS latent space.pdf}
}

@article{vargasImprovedTrainingCNN2021,
  title = {On {{Improved Training}} of {{CNN}} for {{Acoustic Source Localisation}}},
  author = {Vargas, Elizabeth and Hopgood, James R. and Brown, Keith and Subr, Kartic},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {720--732},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3049337},
  abstract = {Convolutional Neural Networks (CNNs) are a popular choice for estimating Direction of Arrival (DoA) without explicitly estimating delays between multiple microphones. The CNN method first optimises unknown filter weights (of a CNN) by using observations and ground-truth directional information. This trained CNN is then used to predict incident directions given test observations. Most existing methods train using spectrally-flat random signals and test using speech. In this paper, which focuses on single source DoA estimation, we find that training with speech or music signals produces a relative improvement in DoA accuracy for a variety of audio classes across 16 acoustic conditions and 9 DoAs, amounting to an average improvement of around 17\% and 19\% respectively when compared to training with spectrally flat random signals. This improvement is also observed in scenarios in which the speech and music signals are synthesised using, for example, a Generative Adversarial Network (GAN). When the acoustic environments during test and training are similar and reverberant, training a CNN with speech outperforms Generalized Cross Correlation (GCC) methods by about 125\%. When the test conditions are different, a CNN performs comparably. This paper takes a step towards answering open questions in the literature regarding the nature of the signals used during training, as well as the amount of data required for estimating DoA using CNNs.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,Artificial neural networks,convolutional neural network (CNN),Direction of arrival,Direction-of-arrival estimation,Estimation,generative adversarial network (GAN),microphone arrays,Microphone arrays,Multiple signal classification,neural networks,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Vargas et al_2021_On Improved Training of CNN for Acoustic Source Localisation.pdf}
}

@article{vekkotEmotionalVoiceConversion2020,
  title = {Emotional {{Voice Conversion Using}} a {{Hybrid Framework With Speaker-Adaptive DNN}} and {{Particle-Swarm-Optimized Neural Network}}},
  author = {Vekkot, Susmitha and Gupta, Deepa and Zakariah, Mohammed and Alotaibi, Yousef Ajami},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {74627--74647},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2988781},
  url = {https://ieeexplore.ieee.org/document/9072171/},
  urldate = {2021-09-10},
  abstract = {We propose a hybrid network-based learning framework for speaker-adaptive vocal emotion conversion, tested on three different datasets (languages), namely, EmoDB (German), IITKGP (Telugu), and SAVEE (English). The optimized learning model introduced is unique because of its ability to synthesize emotional speech with an acceptable perceptive quality while preserving speaker characteristics. The multilingual model is extremely beneficial in scenarios wherein emotional training data from a specific target speaker are sparsely available. The proposed model uses speaker-normalized mel-generalized cepstral coefficients for spectral training with data adaptation using the seed data from the target speaker. The fundamental frequency (F0) is transformed using a wavelet synchrosqueezed transform prior to mapping to obtain a sharpened time\textendash frequency representation. Moreover, a feedforward artificial neural network, together with particle swarm optimization, was used for F0 training. Additionally, static-intensity modification was also performed for each test utterance. Using the framework, we were able to capture the spectral and pitch contour variabilities of emotional expression better than with other state-of-the-art methods used in this study. Considering the overall performance scores across datasets, an average melcepstral distortion (MCD) of 4.98 and root mean square error (RMSE-F0) of 10.67 were obtained in objective evaluations, and an average comparative mean opinion score (CMOS) of 3.57 and speaker similarity score of 3.70 were obtained for the proposed framework. Particularly, the best MCD of 4.09 (EmoDB-happiness) and RMSE-F0 of 9.00 (EmoDB-anger) were obtained, along with the maximum CMOS of 3.7 and speaker similarity of 4.6, thereby highlighting the effectiveness of the hybrid network model.},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Vekkot et al_2020_Emotional Voice Conversion Using a Hybrid Framework With Speaker-Adaptive DNN.pdf}
}

@inproceedings{velichkoComplexParalinguisticAnalysis2022,
  title = {Complex {{Paralinguistic Analysis}} of {{Speech}}: {{Predicting Gender}}, {{Emotions}} and {{Deception}} in a {{Hierarchical Framework}}},
  shorttitle = {Complex {{Paralinguistic Analysis}} of {{Speech}}},
  booktitle = {Interspeech 2022},
  author = {Velichko, Alena and Markitantov, Maxim and Kaya, Heysem and Karpov, Alexey},
  date = {2022-09-18},
  pages = {4735--4739},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11294},
  url = {https://www.isca-speech.org/archive/interspeech_2022/velichko22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Velichko et al_2022_Complex Paralinguistic Analysis of Speech.pdf}
}

@unpublished{vijayakumarSoundWord2VecLearningWord2017,
  title = {Sound-{{Word2Vec}}: {{Learning Word Representations Grounded}} in {{Sounds}}},
  shorttitle = {Sound-{{Word2Vec}}},
  author = {Vijayakumar, Ashwin K. and Vedantam, Ramakrishna and Parikh, Devi},
  date = {2017-08-29},
  eprint = {1703.01720},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.01720},
  urldate = {2021-10-19},
  abstract = {To be able to interact better with humans, it is crucial for machines to understand sound - a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic textual similarity assessment. In this work, we treat sound as a first-class citizen, studying downstream textual tasks which require aural grounding. To this end, we propose sound-word2vec - a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {_reading,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Vijayakumar et al_2017_Sound-Word2Vec.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\FZUTJU6I\\1703.html}
}

@article{vukovicCognitiveLoadEstimation2021,
  title = {Cognitive {{Load Estimation From Speech Commands}} to {{Simulated Aircraft}}},
  author = {Vukovic, Maria and Stolar, Melissa and Lech, Margaret},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1011--1022},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3057492},
  abstract = {This paper investigates and compares methods for cognitive load (CL) estimation from speech. The majority of previous studies of CL estimation used speech collected in laboratory conditions and conventional speech classification methods. Traditionally laboratory speech contains balanced classes that are labeled by a third party after the speech has been collected. In contrast, the speech used in this research was recorded during an experiment focused on human-machine interaction - where spoken commands were used to control simulated aircraft. The speech was labeled using subjective assessments of CL during an experiment that manipulated workload. Current state-of-the-art Convolutional Neural Network (CNN) classification was used for cognitive load estimation and was compared with conventional Support Vector Machine (SVM) and k-Nearest Neighbor (k-NN) classification. Different speaker-dependence models were compared across 2 and 3 classes. In addition, class boundary selection was optimized to reflect the subjective human workload response sigmoidal curve and compared with linear class boundaries. Results for 3-class CL estimation showed that CNN classifiers trained using speech spectrograms for Partially Speaker Dependent (PSD) models using sigmoidal curve class boundaries provided up to 83.7\% accuracy. CNN classifiers outperformed baseline SVM and k-NN classifiers (that used acoustic features) on the same dataset by 13.2\% and 10.5\% respectively. These outcomes indicate that spectrogram-trained CNN classifiers are a worthy consideration in paralinguistic classification problems.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,CNN,cognitive load prediction,Estimation,Labeling,speech classification,Speech processing,Speech recognition,speech spectrograms,Task analysis,Training,transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Vukovic et al_2021_Cognitive Load Estimation From Speech Commands to Simulated Aircraft.pdf}
}

@article{waliGenerativeAdversarialNetworks2021,
  title = {Generative Adversarial Networks for Speech Processing: {{A}} Review},
  shorttitle = {Generative Adversarial Networks for Speech Processing},
  author = {Wali, Aamir and Alamgir, Zareen and Karim, Saira and Fawaz, Ather and Ali, Mubariz Barkat and Adan, Muhammad and Mujtaba, Malik},
  date = {2021-10-02},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  pages = {101308},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101308},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821001066},
  urldate = {2021-10-05},
  abstract = {Generative adversarial networks (GANs) have seen remarkable progress in recent years. They are used as generative models for all kinds of data such as text, images, audio, music, videos, and animations. This paper presents a comprehensive review of the novel and emerging GAN-based speech frameworks and algorithms that have revolutionized speech processing. We have categorized speech GANs based on application areas: speech synthesis, speech enhancement \& conversion, and data augmentation in automatic speech recognition and emotion speech recognition systems. This review also includes a summary of the data sets and evaluation metrics commonly used in speech GANs. We also suggest some interesting research directions for future work and highlight the issues faced by current state-of-the-art speech GANs.},
  language = {en},
  keywords = {Data augmentation,GANs,Speech enhancement,Speech GANs,Speech synthesis},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\K5XBCHGS\\S0885230821001066.html}
}

@article{wallaceCombiningBackgroundNoise2022,
  title = {Combining Background Noise and Artificial Masking to Achieve Privacy in Sound Zones},
  author = {Wallace, Daniel and Cheer, Jordan},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101285},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101285},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000875},
  urldate = {2021-09-28},
  abstract = {A private sound zone can be created by focusing a spoken message towards a target listener using a loudspeaker array. In practice, however, the reproduced speech cannot be completely contained within the target zone due to practical limits on the directivity of the array. Despite these limitations, the privacy of the message can be maintained if the leaked speech is sufficiently masked by noise. Two possible sources of this masking noise are considered in this article: the ambient noise in the reproduction environment, and an additional masking signal radiated by the loudspeaker array. The present article demonstrates that the process of designing a private audio system is significantly affected by the presence of ambient noise. A key complication is that temporal fluctuations and spatial non-uniformity in the ambient noise can reduce its effectiveness as a masker. These features also make it more difficult to estimate the corresponding reduction in the intelligibility of speech in each listening zone. To mitigate this spatial and temporal variance, it is proposed that systems should be designed to rely only on the masking provided by the diffuse, quasi-stationary background noise component of the environmental noise. It is shown that when systems utilise a combination of the background noise and an additional, artificial masker, a lower level of acoustic contrast is required from the system, compared to the case where the masking is supplied by the background noise exclusively.},
  language = {en},
  keywords = {Auditory masking,Signal processing,Sound zones,Speech privacy},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wallace_Cheer_2022_Combining background noise and artificial masking to achieve privacy in sound.pdf}
}

@article{WangCengCiYunLuTeZhengDuiYuYinQingGanZhuanHuanDeYingXiangFenXi2017,
  title = {层次韵律特征对语音情感转换的影响分析},
  author = {王, 泽勋},
  date = {2017-01-01},
  journaltitle = {信息通信},
  number = {10},
  pages = {29--30},
  issn = {1673-1131},
  doi = {10.3969/j.issn.1673-1131.2017.10.011},
  url = {https://d.wanfangdata.com.cn/periodical/hbydjs201710011},
  urldate = {2021-09-10},
  abstract = {针对传统以音节为单位的情感语音转换方法不能有效反映韵律的动态变化特征问题,分析音节、韵律词、语句三个层次的韵律特征在不同情感下的变换规律.根据情感语音的层次韵律变换特点,在音节层韵律转换的前提下,结合韵律词和语句级别的韵律变化特征对音节的韵律特征进行修正,实现语音的情感转换.实验结果表明,对于开心、生气和悲伤三种情感语音的转换,采用层次韵律转换的方法可以有效提高情感语音合成的质量.},
  issue = {10},
  language = {zh-CN},
  keywords = {_中文,_情感语音转换,层次韵律,高斯混合模型},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\王_2017_层次韵律特征对语音情感转换的影响分析.pdf}
}

@article{wangCrosslanguageArticleLinking2022,
  title = {Cross-Language Article Linking with Deep Neural Network Based Paragraph Encoding},
  author = {Wang, Yu-Chun and Chuang, Chia-Min and Wu, Chun-Kai and Pan, Chao-Lin and Tsai, Richard Tzong-Han},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101279},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101279},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000826},
  urldate = {2021-09-28},
  abstract = {Cross-language article linking (CLAL), the task of generating links between articles in different languages from different encyclopedias, is critical for facilitating sharing among online knowledge bases. Some previous CLAL research has been done on creating links among Wikipedia wikis, but much of this work depends heavily on simple language patterns and encyclopedia format or metadata. In this paper, we propose a new CLAL method based on deep learning paragraph embeddings to link English Wikipedia articles with articles in Baidu Baike, the most popular online encyclopedia in mainland China. To measure article similarity for link prediction, we employ several neural networks with attention mechanisms, such as CNN and LSTM, to train paragraph encoders that create vector representations of the articles' semantics based only on article text, rather than link structure, as input data. Using our ``Deep CLAL'' method, we compile a data set consisting of Baidu Baike entries and corresponding English Wikipedia entries. Our approach does not rely on linguistic or structural features and can be easily applied to other language pairs by using pre-trained word embeddings, regardless of whether the two languages are on the same encyclopedia platform.},
  language = {en},
  keywords = {Convolutional neural network,Cross-language article linking,Deep learning,Link discovery,Long short-term memory,Paragraph encoding},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2022_Cross-language article linking with deep neural network based paragraph encoding.pdf}
}

@thesis{WangDuoCengCiYunLuHeDuanShiPuTongBuBianHuanDeQingGanYuYinHeCheng2015,
  type = {硕士},
  title = {多层次韵律和短时谱同步变换的情感语音合成},
  author = {王, 泽勋},
  date = {2015},
  institution = {{苏州大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201601&filename=1015403941.nh&uniplatform=NZKPT&v=gBGkE9dKg4T0WmXc0qyNFyhkSRjPbvPwvzGosGVIhlTZxNB8ceWplKFijXrv1So4},
  urldate = {2021-09-10},
  abstract = {在日常生活中,声音包含了表示文本内容的语义信息,而且也会传递一些情感信息。对于同一句话,如果说话人说话方式不同,听者所获得的信息也会不同。语音的情感转换,就是在语义相同的情况下,实现声音在不同情感间的转换。因此,情感转换是具有表现力的语音合成的重要研究方向。为了能够合成出高质量的情感语音,本文使用了一种多层次韵律和短时谱同步变换的情感合成方法。通过多层次的方法对高兴、生气、悲伤和中性这4种情感语音建立相应的韵律模型。在此基础上,训练得到中性语音与情感语音之间的映射关系,完成韵律转换。然后,再结合短时谱的转换,运用合成工具（STRAIGHT）最后合成有明显情感倾向的情感语音。对转换语音做ABX和MOS测评,结果表明多层次的方法明显改善了情感转换效果。同时,对于合成的情感语音进行谱失真检测,检测结果表明,相对于只对音节进行转换的方法,本文对于高兴、愤怒和悲伤的转换结果分别提高了2\%、4\%和6\%。},
  editora = {俞, 一彪},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,emotional synthesis,GMM,multi-level prosody,spectrum conversion,多层次韵律,短时谱转换,高斯混合模型},
  annotation = {4 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\王_2015_多层次韵律和短时谱同步变换的情感语音合成.pdf}
}

@article{wangEmotionCorrelationMining2021,
  title = {Emotion {{Correlation Mining Through Deep Learning Models}} on {{Natural Language Text}}},
  author = {Wang, Xinzhi and Kou, Luyao and Sugumaran, Vijayan and Luo, Xiangfeng and Zhang, Hui},
  date = {2021-09},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {51},
  number = {9},
  pages = {4400--4413},
  issn = {2168-2275},
  doi = {10.1109/tcyb.2020.2987064},
  abstract = {Emotion analysis has been attracting researchers' attention. Most previous works in the artificial-intelligence field focus on recognizing emotion rather than mining the reason why emotions are not or wrongly recognized. The correlation among emotions contributes to the failure of emotion recognition. In this article, we try to fill the gap between emotion recognition and emotion correlation mining through natural language text from Web news. The correlation among emotions, expressed as the confusion and evolution of emotion, is primarily caused by human emotion cognitive bias. To mine emotion correlation from emotion recognition through text, three kinds of features and two deep neural-network models are presented. The emotion confusion law is extracted through an orthogonal basis. The emotion evolution law is evaluated from three perspectives: one-step shift, limited-step shifts, and shortest path transfer. The method is validated using three datasets: 1) the titles; 2) the bodies; and 3) the comments of news articles, covering both objective and subjective texts in varying lengths (long and short). The experimental results show that in subjective comments, emotions are easily mistaken as anger. Comments tend to arouse emotion circulations of love\textendash anger and sadness\textendash anger. In objective news, it is easy to recognize text emotion as love and cause fear\textendash joy circulation. These findings could provide insights for applications regarding affective interaction, such as network public sentiment, social media communication, and human\textendash computer interaction.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  issue = {9},
  language = {en},
  keywords = {_Waiting for read,Affective computing,Brain modeling,Correlation,Deep learning,deep neural networks,emotion correlation mining,emotion recognition,Emotion recognition,Feature extraction,natural language processing (NLP),Social networking (online),Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_Emotion Correlation Mining Through Deep Learning Models on Natural Language Text.pdf}
}

@article{wangGeneratingImagesSpoken2021,
  title = {Generating {{Images From Spoken Descriptions}}},
  author = {Wang, Xinsheng and Qiao, Tingting and Zhu, Jihua and Hanjalic, Alan and Scharenborg, Odette},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {850--865},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3053391},
  abstract = {Text-based technologies, such as text translation from one language to another, and image captioning, are gaining popularity. However, approximately half of the world's languages are estimated to be lacking a commonly used written form. Consequently, these languages cannot benefit from text-based technologies. This paper presents 1) a new speech technology task, i.e., a speech-to-image generation (S2IG) framework which translates speech descriptions to photo-realistic images 2) without using any text information, thus allowing unwritten languages to potentially benefit from this technology. The proposed speech-to-image framework, referred to as S2IGAN, consists of a speech embedding network and a relation-supervised densely-stacked generative model. The speech embedding network learns speech embeddings with the supervision of corresponding visual information from images. The relation-supervised densely-stacked generative model synthesizes images, conditioned on the speech embeddings produced by the speech embedding network, that are semantically consistent with the corresponding spoken descriptions. Extensive experiments are conducted on four public benchmark databases: two databases that are commonly used in text-to-image generation tasks, i.e., CUB-200 and Oxford-102 for which we created synthesized speech descriptions, and two databases with natural speech descriptions which are often used in the field of cross-modal learning of speech and images, i.e., Flickr8k and Places. Results on these databases demonstrate the effectiveness of the proposed S2IGAN on synthesizing high-quality and semantically-consistent images from the speech signal, yielding a good performance and a solid baseline for the S2IG task.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Adversarial learning,Birds,Databases,Electronic mail,Image synthesis,multimodal modelling,Semantics,speech embedding,Speech processing,speech-to-image generation,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_Generating Images From Spoken Descriptions.pdf}
}

@inproceedings{wangGenerativeDataAugmentation2022,
  title = {Generative {{Data Augmentation Guided}} by {{Triplet Loss}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Wang, Shijun and Hemati, Hamed and Gu\dh nason, J\'on and Borth, Damian},
  date = {2022-09-18},
  pages = {391--395},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10667},
  url = {https://www.isca-speech.org/archive/interspeech_2022/wang22w_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2022_Generative Data Augmentation Guided by Triplet Loss for Speech Emotion.pdf}
}

@article{wangInteractionGazeGesture2021,
  title = {Interaction {{With Gaze}}, {{Gesture}}, and {{Speech}} in a {{Flexibly Configurable Augmented Reality System}}},
  author = {Wang, Zhimin and Wang, Haofei and Yu, Huangyue and Lu, Feng},
  date = {2021},
  journaltitle = {IEEE Transactions on Human-Machine Systems},
  volume = {51},
  number = {5},
  pages = {524--534},
  issn = {2168-2305},
  doi = {10.1109/thms.2021.3097973},
  abstract = {Multimodal interaction has become a recent research focus since it offers better user experience in augmented reality (AR) systems. However, most existing works only combine two modalities at a time, e.g., gesture and speech. Multimodal interactive system integrating gaze cue has rarely been investigated. In this article, we propose a multimodal interactive system that integrates gaze, gesture, and speech in a flexibly configurable AR system. Our lightweight head-mounted device supports accurate gaze tracking, hand gesture recognition, and speech recognition simultaneously. The system can be easily configured into various modality combinations, which enables us to investigate the effects of different interaction techniques. We evaluate the efficiency of these modalities using two tasks: the lamp brightness adjustment task and the cube manipulation task. We also collect subjective feedback when using such systems. The experimental results demonstrate that the Gaze+Gesture+Speech modality is superior in terms of efficiency, and the Gesture+Speech modality is more preferred by users. Our system opens the pathway toward a multimodal interactive AR system that enables flexible configuration.},
  eventtitle = {{{IEEE Transactions}} on {{Human-Machine Systems}}},
  issue = {5},
  language = {en},
  keywords = {\#nosource,Augmented reality (AR),gaze,gesture,Gesture recognition,Glass,Hardware,Head,human–computer interaction,multimodal interaction,speech,Speech recognition,Task analysis,Transforms}
}

@thesis{WangJiYuShenDuXueXiDeYuYinQingGanShiBieYanJiu2020,
  type = {硕士},
  title = {基于深度学习的语音情感识别研究},
  author = {汪, 炳元},
  date = {2020},
  institution = {{哈尔滨工业大学}},
  doi = {10.27061/d.cnki.ghgdu.2020.002224},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020398564.nh&uniplatform=NZKPT&v=VVU0WWWKAeKlyVr2sH2hEaD9g6kDXEjTnNVa3UXZ2ahIbQNee7pgoiU35H35Bqfe},
  urldate = {2021-09-10},
  abstract = {随着人工智能领域的发展日新月异,语音这一最直接便捷的交流通道,正受到越来越多的研究学者们的关注。语音中包含语句文本的字面意思和人的主观情感,只有让机器理解人类的感情,才能获得完整的语音信息,以能够实现人机之间和谐的语音交流。当下,远程互动教育、人性化的机器客服、心理辅导机器人等丰富多彩的应用在不断催促着语音情感识别领域的发展。但目前语音情感识别仍然面临诸多问题,主要是如何选择、构造对情感分类有效的语音特征,如何建立高性能的识别模型。本文先介绍了一种梯度提升树算法和岭回归的混合模型,再搭建了一套深度学习神经网络模型进行实验。本文先构建的是基于Light GBM的混合模型,使用open SMILE提取了Inter Speech国际语音情感挑战赛使用的8个特征集,在这8个特征集上分别训练一个Light GBM模型,然后将这些模型与岭回归混合,混合模型能够综合从不同特征集上学习到的信息,并通过岭回归来防止过拟合,从而获得了良好的识别性能。本文搭建CNN、LSTM和注意力机制混合的深度学习模型,把音色谱和MFCC、滚降频率等基于谱的启发性特征,通过CNN在每帧上抽取高级表征的时间序列,然后使用LSTM分析,之后通过注意力机制来利用LSTM中全时刻的状态信息,并专注于更具情绪分辨力的部分,从而提高了识别性能,本文通过设置3个对照实验,验证了注意力机制的效果。之后本文使用双向LSTM替换单向LSTM,由于双向LSTM能学习到上下文信息,对识别效果有一定提升。},
  editora = {李, 昕},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,attention mechanism,CNN,LSTM,speech emotion recognition,卷积神经网络,注意力机制,语音情感识别,长短时记忆网络},
  annotation = {1 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\汪_2020_基于深度学习的语音情感识别研究.pdf}
}

@thesis{WangJiYuTeZhengShaiXuanHeJueCeShuSVMDeYuYinQingGanShiBieJiShuYanJiu2018,
  type = {硕士},
  title = {基于特征筛选和决策树SVM的语音情感识别技术研究},
  author = {王, 富},
  date = {2018},
  institution = {{南京邮电大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1018891849.nh&uniplatform=NZKPT&v=usO8%25mmd2FeaTQoeCZROXE9nJqLJXxKCiE5Gu6ecDdYeE6m9iaMzc6LaWcfG%25mmd2FgRBxly47},
  urldate = {2021-09-10},
  abstract = {语音作为人类日常生活交流的主要方式,承载着说话人丰富的情感信息。真正意义上的人工智能需要机器从情感层面充分理解人类的意图,因此语音情感识别在未来的人工智能领域具有广阔的应用前景。目前,在语音情感识别的基本框架下,寻找一种具有高区分度的语音情感特征和构建高效的识别模型是当今研究的热点问题,它们的好坏直接影响着整个系统的识别效果。本文针对多种情感识别的情况,从特征参数和模型两个方面出发,提出了基于特征筛选和决策树SVM的语音情感识别方法以及基于DNN-决策树SVM的语音情感识别方法。本文的主要研究工作如下:（1）通过对语音情感识别的研究背景以及研究现状的充分学习,熟练掌握了语音情感识别的基本框架以及常用的识别方法。本文介绍了语音情感识别领域中的常用语料库、语音预处理技术以及常用的特征参数,并分析了各类特征参数的特性。除此之外,本文还介绍了特征参数的统计变量计算方法以及归一化方法,为后续的研究工作打下了扎实的基础。（2）在多种情感的识别任务中,由于情感间的混淆度增大而导致系统的整体识别率降低。针对此问题,本文提出了Fisher特征筛选决策树支持向量机（SVM）模型的语音情感识别方法。该方法首先通过计算情感混淆度的方式建立决策树SVM框架,然后根据Fisher判别系数的特征筛选方法,从备选特征参数中分别为决策树中每个SVM筛选出高区分度的特征参数用于训练,最后将此模型用于语音情感识别。实验表明,该方法的平均识别率高于基于传统SVM的方法。（3）针对传统特征不能够挖掘语音信号更深层次的情感信息的问题,本文对比较热门的深度学习进行了研究,设计了一个用于提取瓶颈特征的深度神经网络,并构建了DNN-SVM的语音情感识别系统。接下来,由于相同的特征参数对于区分不同的情感类别的贡献度不同,本文针对不同的情感分类训练不同的DNN网络来提取瓶颈特征,再将瓶颈特征分别用于训练决策树中的每个SVM,提出了基于DNN-决策树SVM的语音情感识别系统。实验表明,在输入参数相同的情况下,基于DNN-决策树SVM的语音情感识别系统比基于SVM的语音情感识别系统的识别效果有明显提高。},
  editora = {孙, 林慧},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Bottleneck feature,decision tree SVM,Deep Neural Network,feature selection,Fisher criterion,Fisher系数,speech emotion recognition,决策树SVM,深度学习,特征筛选,瓶颈特征,语音情感识别},
  annotation = {7 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\王_2018_基于特征筛选和决策树SVM的语音情感识别技术研究.pdf}
}

@thesis{WangJiYuYuYinHeMianBuBiaoQingRongHeDeQingXuShiBieSuanFaYanJiuJiQiShiXian2018,
  type = {硕士},
  title = {基于语音和面部表情融合的情绪识别算法研究及其实现},
  author = {王, 喆},
  date = {2018},
  institution = {{北方工业大学}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201802&filename=1018180277.nh&uniplatform=NZKPT&v=0s2vpXUReGwuFjyGKTEaRaIzLzqbESnVNpsROPYgEmSnhzjAqC8BVOrOwiFIiAVx},
  urldate = {2021-09-28},
  abstract = {情感计算作为人工智能的重要一部分,吸引了很多学者做相关的研究,尤其是在情绪特征提取和自动情绪识别算法方面。本文对自动情绪识别进行了研究,研究内容分为三部分:基于面部表情的情绪识别、基于语音的情绪识别和基于面部表情及语音融合的多模态情绪识别。首先是关于人脸表情的情绪识别算法研究。利用样条插值系数来表达面部器官的几何特征,并和HOG特征共同组成面部表情特征向量,然后利用高斯核支持向量机和多层感知机学习算法,得到基于面部表情的情绪分类器,并用两个常用的面部表情库CK+库和JAFFE库进行分析验证。实验结果验证了这种特征的有效性,并同时表明神经网络的隐藏层和学习率的设置会影响识别准确率,东西方人脸表情的差异和库的大小以及多样性也会影响训练的结果。另外,两种融合方式得到的结果会由于使用的分类器和使用的数据库的不同有不一样的趋势。其次是关于语音情绪识别算法研究,应用OpenSMILE这一工具进行音质、韵律学和统计学等共1582维的语音特征的提取,并训练高斯核支持向量机作为分类器,用中科院发布的CHEAVD2.0数据库进行验证。由于CHEAVD2.0是视频场景库,而且数据比较庞杂,训练得到的SVM分类器准确率不是很高。最后是语音和面部表情融合的情绪识别算法研究。在这里应用了前述的面部表情特征,并和提取的语音情绪特征混合进行特征层融合,训练高斯核支持向量机作为分类器进行情绪识别,并与决策层融合结果进行比较。算法利用CHEAVD 2.0数据库分别进行训练和实验,最后对实验结果进行了分析。},
  editora = {许, 芬},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {Automatic expression recognition,Cubic spline interpolation coefficients,Emotion recognition based on voice features,Feature fusion,Multi-modal emotion recognition,三次样条插值系数,多模态情绪识别,特征融合,表情识别,语音情绪识别},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\王_2018_基于语音和面部表情融合的情绪识别算法研究及其实现.pdf}
}

@inproceedings{wangLearningMutualCorrelation2021,
  title = {Learning {{Mutual Correlation}} in {{Multimodal Transformer}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Wang, Yuhua and Shen, Guang and Xu, Yuezhu and Li, Jiahang and Zhao, Zhengdao},
  date = {2021-08-30},
  pages = {4518--4522},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-2004},
  url = {https://www.isca-speech.org/archive/interspeech_2021/wang21ga_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_Learning Mutual Correlation in Multimodal Transformer for Speech Emotion.pdf}
}

@article{wangMultimicrophoneComplexSpectral2021,
  title = {Multi-Microphone {{Complex Spectral Mapping}} for {{Utterance-wise}} and {{Continuous Speech Separation}}},
  author = {Wang, Zhong-Qiu and Wang, Peidong and Wang, DeLiang},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2001--2014},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3083405},
  abstract = {We propose multi-microphone complex spectral mapping, a simple way of applying deep learning for time-varying non-linear beamforming, for speaker separation in reverberant conditions. We aim at both speaker separation and dereverberation. Our study first investigates offline utterance-wise speaker separation and then extends to block-online continuous speech separation (CSS). Assuming a fixed array geometry between training and testing, we train deep neural networks (DNN) to predict the real and imaginary (RI) components of target speech at a reference microphone from the RI components of multiple microphones. We then integrate multi-microphone complex spectral mapping with minimum variance distortionless response (MVDR) beamforming and post-filtering to further improve separation, and combine it with frame-level speaker counting for block-online CSS. Although our system is trained on simulated room impulse responses (RIR) based on a fixed number of microphones arranged in a given geometry, it generalizes well to a real array with the same geometry. State-of-the-art separation performance is obtained on the simulated two-talker SMS-WSJ corpus and the real-recorded LibriCSS dataset.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Array signal processing,Complex spectral mapping,Covariance matrices,deep learning,Deep learning,Geometry,microphone array processing,Microphone arrays,speaker separation,Speech processing,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_Multi-microphone Complex Spectral Mapping for Utterance-wise and Continuous.pdf}
}

@inproceedings{wangNovelEndtoendSpeech2021,
  title = {A {{Novel}} End-to-End {{Speech Emotion Recognition Network}} with {{Stacked Transformer Layers}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Xianfeng and Wang, Min and Qi, Wenbo and Su, Wanqi and Wang, Xiangqian and Zhou, Huan},
  date = {2021-06},
  pages = {6289--6293},
  issn = {2379-190X},
  doi = {10/gmr2jm},
  abstract = {Speech emotion recognition (SER) aims to automatically recognize emotional category for a given speech utterance. The performance of a SER system heavily relies on the effectiveness of global representation expressed at utterance level. To effectively extract such a global feature, the mainstream of recent SER architectures adopts a pipeline with two key modules, feature extraction and aggregation. Although variant module designs have brought impressive progresses, SER is still a challenging task. In contrast with those previous works, herein we propose a novel strategy for global SER feature extraction by applying an additional enhancement module on top of the current SER pipeline. To verify its effect, an end-to-end SER architecture is proposed where stacked multiple transformer layers are explored to enhance the aggregated global feature. Such an architecture is evaluated on IEMO-CAP and results strongly substantiate the effectiveness of our proposal. In terms of weighted accuracy on four emotion categories, our proposed SER system outperforms the prior arts by a large margin of relatively 20\% improvement. Our codes and the pre-trained SER models are made publicly available.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,Art,Conferences,Emotion recognition,end-to-end,Feature extraction,Pipelines,Signal processing,Speech emotion recognition,Speech recognition,stacked transformer layers},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer.pdf}
}

@inproceedings{wangRealizingSignLanguage2021,
  title = {Towards {{Realizing Sign Language}} to {{Emotional Speech Conversion}} by {{Deep Learning}}},
  booktitle = {2021 12th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  author = {Wang, Weizhe and Yang, Hongwu},
  date = {2021-01-24},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Hong Kong}},
  doi = {10.1109/ISCSLP49672.2021.9362060},
  url = {https://ieeexplore.ieee.org/document/9362060/},
  urldate = {2021-09-10},
  abstract = {This paper proposes a framework of sign language to emotional speech conversion based on deep learning to solve communication disorders between people with language barriers and healthy people. We firstly trained a gesture recognition model and a facial expression recognition model by a deep convolutional generative adversarial network (DCGAN). Then we trained an emotional speech acoustic model with a hybrid long short-term memory (LSTM). We select the initials and the finals of Mandarin as the emotional speech synthesis units to train a speaker-independent average voice model (AVM). The speaker adaptation is applied to train a speaker-dependent hybrid LSTM model with one target speaker emotional corpus from AVM. Finally, we combine the gesture recognition model and facial expression recognition model with the emotional speech synthesis model to realize the sign language to emotional speech conversion. The experiments show that the recognition rate of gesture recognition is 93.96\%, and the recognition rate of facial expression recognition in the CK+ database is 96.01\%. The converted emotional speech not only has high quality but also can accurately express the facial expression.},
  eventtitle = {2021 12th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  isbn = {978-1-72816-994-1},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang_Yang_2021_Towards Realizing Sign Language to Emotional Speech Conversion by Deep Learning.pdf}
}

@article{wangRobustSpeechSuperResolution2021,
  title = {Towards {{Robust Speech Super-Resolution}}},
  author = {Wang, Heming and Wang, DeLiang},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2058--2066},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3054302},
  abstract = {Speech super-resolution (SR) aims to increase the sampling rate of a given speech signal by generating high-frequency components. This paper proposes a convolutional neural network (CNN) based SR model that takes advantage of information from both time and frequency domains. Specifically, the proposed CNN is a time-domain model that takes the raw waveform of low-resolution speech as the input, and outputs an estimate of the corresponding high-resolution waveform. During the training stage, we employ a cross-domain loss to optimize the network. We compare our model with several deep neural network (DNN) based SR models, and experiments show that our model outperforms existing models. Furthermore, the robustness of DNN-based models is investigated, in particular regarding microphone channels and downsampling schemes, which have a major impact on the performance of DNN-based SR models. By training with proper datasets and preprocessing, we improve the generalization capability for untrained microphone channels and unknown downsampling schemes.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {bandwidth extension,convolutional neural network,Convolutional neural networks,Frequency-domain analysis,Neural networks,robust speech super-resolution,Robustness,Speech processing,Speech super-resolution,Superresolution,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang_Wang_2021_Towards Robust Speech Super-Resolution.pdf}
}

@article{wangSpeakerSeparationUsing2021,
  title = {Speaker {{Separation Using Speaker Inventories}} and {{Estimated Speech}}},
  author = {Wang, Peidong and Chen, Zhuo and Wang, DeLiang and Li, Jinyu and Gong, Yifan},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {537--546},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3045556},
  abstract = {We propose speaker separation using speaker inventories and estimated speech (SSUSIES), a framework leveraging speaker profiles and estimated speech for speaker separation. SSUSIES contains two methods, speaker separation using speaker inventories (SSUSI) and speaker separation using estimated speech (SSUES). SSUSI performs speaker separation with the help of speaker inventory. By combining the advantages of permutation invariant training (PIT) and speech extraction, SSUSI significantly outperforms conventional approaches. SSUES is a widely applicable technique that can substantially improve speaker separation performance using the output of first-pass separation. We evaluate the models on both speaker separation and speech recognition metrics.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Computer science,Correlation,Estimated speech,Feature extraction,Mathematical model,Particle separators,speaker inventory,speaker separation,Speech processing,speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_Speaker Separation Using Speaker Inventories and Estimated Speech.pdf}
}

@misc{wangTDNTemporalDifference2021,
  title = {{{TDN}}: {{Temporal Difference Networks}} for {{Efficient Action Recognition}}},
  shorttitle = {{{TDN}}},
  author = {Wang, Limin and Tong, Zhan and Ji, Bin and Wu, Gangshan},
  date = {2021-03-31},
  number = {arXiv:2012.10071},
  eprint = {2012.10071},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.10071},
  url = {http://arxiv.org/abs/2012.10071},
  urldate = {2022-11-12},
  abstract = {Temporal modeling still remains challenging for action recognition in videos. To mitigate this issue, this paper presents a new video architecture, termed as Temporal Difference Network (TDN), with a focus on capturing multi-scale temporal information for efficient action recognition. The core of our TDN is to devise an efficient temporal module (TDM) by explicitly leveraging a temporal difference operator, and systematically assess its effect on short-term and long-term motion modeling. To fully capture temporal information over the entire video, our TDN is established with a two-level difference modeling paradigm. Specifically, for local motion modeling, temporal difference over consecutive frames is used to supply 2D CNNs with finer motion pattern, while for global motion modeling, temporal difference across segments is incorporated to capture long-range structure for motion feature excitation. TDN provides a simple and principled temporal modeling framework and could be instantiated with the existing CNNs at a small extra computational cost. Our TDN presents a new state of the art on the Something-Something V1 \& V2 datasets and is on par with the best performance on the Kinetics-400 dataset. In addition, we conduct in-depth ablation studies and plot the visualization results of our TDN, hopefully providing insightful analysis on temporal difference modeling. We release the code at https://github.com/MCG-NJU/TDN.},
  archiveprefix = {arXiv},
  language = {en},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_TDN.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\ERLRPUZY\\2012.html}
}

@misc{wangTemporalSegmentNetworks2016,
  title = {Temporal {{Segment Networks}}: {{Towards Good Practices}} for {{Deep Action Recognition}}},
  shorttitle = {Temporal {{Segment Networks}}},
  author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  date = {2016-08-02},
  number = {arXiv:1608.00859},
  eprint = {1608.00859},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1608.00859},
  url = {http://arxiv.org/abs/1608.00859},
  urldate = {2022-11-12},
  abstract = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( \$ 69.4\textbackslash\% \$) and UCF101 (\$ 94.2\textbackslash\% \$). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.},
  archiveprefix = {arXiv},
  language = {en},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2016_Temporal Segment Networks.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\TAK5MWXJ\\1608.html}
}

@inproceedings{wangUnsupervisedInstanceDiscriminative2022,
  title = {Unsupervised {{Instance Discriminative Learning}} for {{Depression Detection}} from {{Speech Signals}}},
  booktitle = {Interspeech 2022},
  author = {Wang, Jinhan and Ravi, Vijay and Flint, Jonathan and Alwan, Abeer},
  date = {2022-09-18},
  pages = {2018--2022},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10814},
  url = {https://www.isca-speech.org/archive/interspeech_2022/wang22z_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2022_Unsupervised Instance Discriminative Learning for Depression Detection from.pdf}
}

@thesis{WangYuYinQingGanShiBieSuanFaYanJiu2019,
  type = {硕士},
  title = {语音情感识别算法研究},
  author = {王, 思羽},
  date = {2019},
  institution = {{南京邮电大学}},
  doi = {10.27251/d.cnki.gnjdc.2019.000221},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019691787.nh&uniplatform=NZKPT&v=gBDd%25mmd2BOvi0YRrftLg4zZNZGig3KOA%25mmd2FALqr6jp6Qj3Ur4xtyVETv0MwlYbzEdkU%25mmd2BYL},
  urldate = {2021-09-10},
  abstract = {随着计算机科学的发展,语音信号处理被广泛应用于社会的各个方面。目前语音处理领域中的语音情感识别技术已经变成了人机交互系统的关键。为了让人机交互更加的便捷和人性化,研究人员们开始对语音的情感信号着手进行研究。智能人机交互系统通过对操作者的情感进行分析,可以更为主动、更为准确地去实现操作人员的要求,而且可以及时的去调节对话的形式,使得交流更加智能化。本文的主要工作如下:（1）针对语音情感特征参数进行了优化。将梅尔频率倒谱系数优化整合,并与韵律特征、音质特征联合作为语音情感识别的特征参数。实验结果表明,由MFCC、I-MFCC和Mid-MFCC混合得到新的MFCC系数在整个频率段上的识别能力有明显提升。（2）提出了基于F-MFCC参数的语音情感识别算法。利用Fisher比准则对MFCC及其衍生参数I-MFCC、Mid-MFCC进行组合生成F-MFCC,将生成的F-MFCC与其他特征参数进行混合后,通过利用不同的基于谱的特征参数进行语音情感识别。实验结果表明,以F-MFCC作为特征参数能够进一步提升识别模型的识别率,且在一定程度上降低了特征参数的维度。（3）提出了基于新的决策模型的语音情感识别方法。将BP神经网络、支持向量机和K近邻算法各自得到的识别结果通过一个投票器,将投票器的输出作为识别结果。实验结果表明,新的决策模型能够减少语音误判的概率,使得最终的语音情感的平均识别率得到进一步提高。},
  editora = {邓, 立新},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,decision model,F-MFCC,feature fusion,MFCC,speech emotion recognition,决策模型,特征融合,语音情感识别},
  annotation = {1 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\王_2019_语音情感识别算法研究.pdf}
}

@inproceedings{weiAudioVisualDomainAdaptation2022,
  title = {Audio-{{Visual Domain Adaptation Feature Fusion}} for {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Wei, Jie and Hu, Guanyu and Yang, Xinyu and Luu, Anh Tuan and Dong, Yizhuo},
  date = {2022-09-18},
  pages = {1988--1992},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-703},
  url = {https://www.isca-speech.org/archive/interspeech_2022/wei22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  keywords = {_Code},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wei et al_2022_Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition.pdf}
}

@thesis{WeiJiYuShiPinShangXiaWenXinXiDeJiQiRenYuYinQingGanShiBieYanJiu2019,
  type = {硕士},
  title = {基于时频上下文信息的机器人语音情感识别研究},
  author = {卫, 伟},
  date = {2019},
  institution = {{南京邮电大学}},
  doi = {10.27251/d.cnki.gnjdc.2019.000636},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019691369.nh&uniplatform=NZKPT&v=NsWWENOw9GVaFvJV2%25mmd2FH%25mmd2FHpY9%25mmd2Bv0d0qOhEZjdCbZRwX4BpxoGIWUzbjr%25mmd2FlGkusEjq},
  urldate = {2021-09-10},
  abstract = {语音情感识别一直以来都是计算机视觉和机器学习领域的研究热点,``情感计算''这一概念在近几年已经引起了国内外许多情感分析专家的关注。说话者的语音信号中往往包含了丰富的情感信息,来帮助他更好的传递信息。同一个人用不同的情感表达同一句话时,其传递的信息可能不太相同。为了使计算机更好地理解人的情感,就必须提高语音情感识别的准确率。如今,语音情感识别在人工客服,远程教育,医学辅助,和汽车驾驶等人机交互领域的应用越来越广泛。语音信号的不同语气可以传达不同的情感,让计算机准确的识别语音情感是一项极具意义的任务。许多公开的情感语音库识别算法在对非限制条件下采集到语音信号的识别效果不佳,这些情感识别算法距离实际应用仍有较大的差距。传统的情感识别算法通常只是用到语音的时间信息或者是频域信息,但是情感的变化是一个动态的过程,它在变化的过程中具有非常明显的动态变化特征,也就是情感上下文信息,该信息一般使用连续帧语音信息来表示,利用LSTM算法来获取,提取语音情感的上下文信息特征能够有效提高情感识别率。针对以上所述,本文采用基于时频上下文信息的方法来提高语音情感识别的正确率和鲁棒性。本文的研究内容如下:（1）调研了常用的语音情感特征提取算法和分类方法,并对经典的语音情感识别方法进行了介绍,对比了各种语音情感识别方法的识别正确率,并分析了这些识别方法的优劣;（2）提出了针对机器人语音情感识别的特征提取方法,由于近年来,卷积神经网络在识别领域取得了巨大的成功,本文将其引入到情感识别领域并取得了不错的效果;为了进一步提高情感识别的正确率,本文除了利用语音信息的时域上下文特征外,又介绍并引入了语音频域的上下文信息特征进行识别,提高了整个系统的识别正确率;（3）提出了融合时频上下文信息的方法,把提取的时间域和频率域上下文信息特征进行融合,在知名的语音情感数据集上进行了实验,得到了不错的识别效果。最后,将基于时频上下文信息的语音情感识别方法应用于机器人后端的语音情感识别模块,实现智能机器人的语音情感识别功能。},
  editora = {龚, 建荣},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,frequency domain context,speech emotion recognition,time domain context,时域上下文,语音情感识别,频域上下文},
  annotation = {1 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\卫_2019_基于时频上下文信息的机器人语音情感识别研究.pdf}
}

@article{weiLearningVisualizeMusic2021,
  title = {Learning to {{Visualize Music Through Shot Sequence}} for {{Automatic Concert Video Mashup}}},
  author = {Wei, Wen-Li and Lin, Jen-Chun and Liu, Tyng-Luh and Tyan, Hsiao-Rong and Wang, Hsin-Min and Liao, Hong-Yuan Mark},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {1731--1743},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.3003631},
  abstract = {An experienced director usually switches among different types of shots to make visual storytelling more touching. When filming a musical performance, appropriate switching shots can produce some special effects, such as enhancing the expression of emotion or heating up the atmosphere. However, while the visual storytelling technique is often used in making professional recordings of a live concert, amateur recordings of audiences often lack such storytelling concepts and skills when filming the same event. Thus a versatile system that can perform video mashup to create a refined high-quality video from such amateur clips is desirable. To this end, we aim at translating the music into an attractive shot (type) sequence by learning the relation between music and visual storytelling of shots. The resulting shot sequence can then be used to better portray the visual storytelling of a song and guide the concert video mashup process. To achieve the task, we first introduces a novel probabilistic-based fusion approach, named as multi-resolution fused recurrent neural networks (MF-RNNs) with film-language, which integrates multi-resolution fused RNNs and a film-language model for boosting the translation performance. We then distill the knowledge in MF-RNNs with film-language into a lightweight RNN, which is more efficient and easier to deploy. The results from objective and subjective experiments demonstrate that both MF-RNNs with film-language and lightweight RNN can generate attractive shot sequences for music, thereby enhancing the viewing and listening experience.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {Head,knowledge distillation,Knowledge engineering,live concert,Mashups,Music,recurrent neural networks,Recurrent neural networks,Task analysis,Types of shots,video mashup,visual storytelling,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wei et al_2021_Learning to Visualize Music Through Shot Sequence for Automatic Concert Video.pdf}
}

@article{WeiXiJinPingWaiJiaoSiXiangYinLingXinShiDaiZhongGuoTeSeDaGuoWaiJiaoKaiTuoChuangXin2021,
  title = {习近平外交思想引领新时代中国特色大国外交开拓创新},
  author = {卫, 灵},
  date = {2021},
  journaltitle = {思想理论教育导刊},
  number = {07},
  pages = {32--38},
  issn = {1009-2528},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2021&filename=GXSJ202107007&uniplatform=NZKPT&v=RIxGdBkH%25mmd2BDzQ6iFKYkphCGPPStq9C6JM%25mmd2BBtFrr6lBNVbkivjrzlRYVL69glUMfq%25mmd2F},
  urldate = {2021-11-21},
  abstract = {习近平外交思想将马克思主义理论与中国外交实践相结合,开创了中国特色大国外交新局面。党的十八大以来,我国外事工作以习近平外交思想为遵循,在外交工作的方针原则、外交理论和实践上,不断创新发展中国特色大国外交。习近平外交思想中关于构建人类命运共同体和新型国际关系等一系列富有原创性的核心理念,饱含时代精神和中国特色,发展了中国化马克思主义外交理论体系,扬弃和超越了西方传统的国际关系理论。在习近平外交思想指引下,新时代中国特色大国外交彰显了"人民至上"的价值取向,展现了负责任大国的风范与形象,凸显了主动作为、积极有为的外交风格。在当今国际社会,中国以自身开拓进取的成功经验,宣讲中国故事,传播中国外交理念...},
  issue = {07},
  language = {zh-CN},
  keywords = {⛔ No DOI found,中国特色大国外交,习近平外交思想,创新性发展,鲜明特色},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\卫_2021_习近平外交思想引领新时代中国特色大国外交开拓创新.pdf}
}

@article{wenStructureAwareMotionDeblurring2021,
  title = {Structure-{{Aware Motion Deblurring Using Multi-Adversarial Optimized CycleGAN}}},
  author = {Wen, Yang and Chen, Jie and Sheng, Bin and Chen, Zhihua and Li, Ping and Tan, Ping and Lee, Tong-Yee},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {6142--6155},
  issn = {1941-0042},
  doi = {10.1109/tip.2021.3092814},
  abstract = {Recently, Convolutional Neural Networks (CNNs) have achieved great improvements in blind image motion deblurring. However, most existing image deblurring methods require a large amount of paired training data and fail to maintain satisfactory structural information, which greatly limits their application scope. In this paper, we present an unsupervised image deblurring method based on a multi-adversarial optimized cycle-consistent generative adversarial network (CycleGAN). Although original CycleGAN can handle unpaired training data well, the generated high-resolution images are probable to lose content and structure information. To solve this problem, we utilize a multi-adversarial mechanism based on CycleGAN for blind motion deblurring to generate high-resolution images iteratively. In this multi-adversarial manner, the hidden layers of the generator are gradually supervised, and the implicit refinement is carried out to generate high-resolution images continuously. Meanwhile, we also introduce the structure-aware mechanism to enhance the structure and detail retention ability of the multi-adversarial network for deblurring by taking the edge map as guidance information and adding multi-scale edge constraint functions. Our approach not only avoids the strict need for paired training data and the errors caused by blur kernel estimation, but also maintains the structural information better with multi-adversarial learning and structure-aware mechanism. Comprehensive experiments on several benchmarks have shown that our approach prevails the state-of-the-art methods for blind image motion deblurring.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  language = {en},
  keywords = {\#nosource,Computer architecture,edge refinement,Estimation,Generative adversarial networks,Image edge detection,Image restoration,Kernel,multi-adversarial,structure-aware,Training data,Unsupervised image deblurring}
}

@unpublished{whitehillMultiReferenceNeuralTTS2019,
  title = {Multi-{{Reference Neural TTS Stylization}} with {{Adversarial Cycle Consistency}}},
  author = {Whitehill, Matt and Ma, Shuang and McDuff, Daniel and Song, Yale},
  date = {2019-10-25},
  eprint = {1910.11958},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1910.11958},
  urldate = {2021-09-10},
  abstract = {Current multi-reference style transfer models for Text-to-Speech (TTS) perform sub-optimally on disjoints datasets, where one dataset contains only a single style class for one of the style dimensions. These models generally fail to produce style transfer for the dimension that is underrepresented in the dataset. In this paper, we propose an adversarial cycle consistency training scheme with paired and unpaired triplets to ensure the use of information from all style dimensions. During training, we incorporate unpaired triplets with randomly selected reference audio samples and encourage the synthesized speech to preserve the appropriate styles using adversarial cycle consistency. We use this method to transfer emotion from a dataset containing four emotions to a dataset with only a single emotion. This results in a 78\% improvement in style transfer (based on emotion classification) with minimal reduction in fidelity and naturalness. In subjective evaluations our method was consistently rated as closer to the reference style than the baseline. Synthesized speech samples are available at: https://sites.google.com/view/adv-cycle-consistent-tts},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Whitehill et al_2019_Multi-Reference Neural TTS Stylization with Adversarial Cycle Consistency.pdf}
}

@inproceedings{winataAttentionBasedLSTMPsychological2018,
  title = {Attention-{{Based LSTM}} for {{Psychological Stress Detection}} from {{Spoken Language Using Distant Supervision}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Winata, Genta Indra and Kampman, Onno Pepijn and Fung, Pascale},
  date = {2018-04},
  pages = {6204--6208},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461990},
  abstract = {We propose a Long Short-Term Memory (LSTM) with attention mechanism to classify psychological stress from self-conducted interview transcriptions. We apply distant supervision by automatically labeling tweets based on their hash-tag content, which complements and expands the size of our corpus. This additional data is used to initialize the model parameters, and which it is fine-tuned using the interview data. This improves the model's robustness, especially by expanding the vocabulary size. The bidirectional LSTM model with attention is found to be the best model in terms of accuracy (74.1\%) and f-score (74.3\%). Furthermore, we show that distant supervision fine-tuning enhances the model's performance by 1.6\% accuracy and 2.1\% f-score. The attention mechanism helps the model to select informative words.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Attention Mechanism,Data models,Distant Supervision,Interviews,LSTM,Natural Language Processing,Psychological Stress Detection,Psychology,Stress,Tagging,Training,Twitter},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Winata et al_2018_Attention-Based LSTM for Psychological Stress Detection from Spoken Language.pdf}
}

@inproceedings{wuAudioSaliencyMaskingTransformer2022,
  title = {An {{Audio-Saliency Masking Transformer}} for {{Audio Emotion Classification}} in {{Movies}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Ya-Tse and Li, Jeng-Lin and Lee, Chi-Chun},
  date = {2022},
  pages = {4813--4817},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746403},
  abstract = {The process of perception to affective response of humans is gated by a bottom-up saliency mechanism at the sensory level. In specifics, auditory saliency emphasizes audio segments that need to be attended to cognitively appraise and experience emotion. In this work, inspired by this mechanism, we propose an end-to-end feature masking network for audio emotion recognition in movies. Our proposed Audio-Saliency Masking Transformer (ASTM) adjusts feature embedding using two learnable masks; one of them cross-refers to an auditory saliency map, and the other one is through self-reference. By joint training for front-end mask gating and the transformer as the back-end emotion classifier, we achieve three-class UARs improvement of 1.74\%, 1.27\%, 0.95\%, 0.82\% when comparing to the best of the other models on experienced arousal, experienced valence, intended arousal, and intended valence, respectively. We further analyze which acoustic feature categories that our saliency mask attends to the most.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {affective multimedia,auditory saliency,Conferences,emotion recognition,Emotion recognition,Logic gates,Signal processing,Speech recognition,Training,transformer,Transformers},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2022_An Audio-Saliency Masking Transformer for Audio Emotion Classification in Movies.pdf}
}

@inproceedings{wuClimateWeatherInspecting2022,
  title = {Climate and {{Weather}}: {{Inspecting Depression Detection}} via {{Emotion Recognition}}},
  shorttitle = {Climate and {{Weather}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Wen and Wu, Mengyue and Yu, Kai},
  date = {2022},
  pages = {6262--6266},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746634},
  abstract = {Automatic depression detection has attracted increasing amount of attention but remains a challenging task. Psychological research suggests that depressive mood is closely related with emotion expression and perception, which motivates the investigation of whether knowledge of emotion recognition can be transferred for depression detection. This paper uses pretrained features extracted from the emotion recognition model for depression detection, further fuses emotion modality with audio and text to form multimodal depression detection. The proposed emotion transfer improves depression detection performance on DAIC-WOZ as well as increases the training stability. The analysis of how the emotion expressed by de-pressed individuals is further perceived provides clues for further understanding of the relationship between depression and emotion.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Depression,depression detection,emotion,Emotion recognition,Feature extraction,Mood,Signal processing,Stability analysis,Training,transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2022_Climate and Weather.pdf}
}

@article{wuDeepGraphBasedCharacterLevel2021,
  title = {Deep {{Graph-Based Character-Level Chinese Dependency Parsing}}},
  author = {Wu, Linzhi and Zhang, Meishan},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1329--1339},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3067212},
  abstract = {Character-level Chinese dependency parsing has been a concern of several studies that naturally handle word segmentation, POS (Part of Speech) tagging and dependency parsing jointly in an end-to-end way. Previous work mostly concentrates on a transition-based framework for this task because of its easy adaption, which is extremely important when feature representation relies heavily on the decoding strategy, particularly under the traditional statistical setting. Recently, on the one hand, sophisticated deep neural networks and deep contextualized word representations have greatly weakened the dependence between feature representation and decoding. On the other hand, (first-order) graph-based models, especially the biaffine parsers, are straightforward for dependency parsing, and meanwhile they can yield competitive parsing performance. In this paper, we make a comprehensive investigation of the deep graph-based character-level dependency parsing for Chinese. We start from an extension of a standard graph-based biaffine parser, and then exploit Chinese BERT as well as our improved encoders based on transformers to enhance the character-level dependency parsing model. We conduct a series of experiments on the Chinese benchmark datasets, showing the performances of various graph-based character-level models and analyzing the advantages of the character-level dependency parsing under the deep neural setting.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Adaptation models,Bit error rate,Character-level chinese parsing,Decoding,deep neural networks,dependency parsing,graph-based model,Standards,Tagging,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu_Zhang_2021_Deep Graph-Based Character-Level Chinese Dependency Parsing.pdf}
}

@thesis{WuDiXinZaoBiHuanJingXiaShuoHuaRenShiBieYanJiu2016,
  type = {博士},
  title = {低信噪比环境下说话人识别研究},
  author = {吴, 迪},
  date = {2016},
  institution = {{苏州大学}},
  url = {https://x.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFD2017&filename=1017015084.nh},
  urldate = {2021-09-10},
  abstract = {说话人识别是一种利用语音信号来验证身份信息的生物识别技术。现实中,说话人识别系统受制于语音中的噪声,系统的识别性能降低。说话人识别朝着商业应用发展,并且在噪声较强环境下的说话人识别正成为研究热点。而现有的技术在强噪声的低信噪比环境下,性能大幅下降。这对说话人识别提出了新的要求。目前低信噪比环境下的说话人识别存在三个待解决的关键问题。1.现有的说话人识别特征参数,在低信噪比的环境下,其自身鲁棒性急剧下降,无法满足说话人识别对特征参数的要求。2.在低信噪比环境下,现有的特征补偿方法性能下降,无法有效地提高特征参数的鲁棒性。3.现有说话人识别模型在低信噪比环境下性能降低或只能针对某类特殊噪声鲁棒性较好。论文在对噪声环境下的说话人识别进行深入研究的基础上,对于以上三个待解决的关键问题,分别研究了低信噪比环境下的鲁棒特征参数、特征补偿和识别模型。主要研究内容和创新点如下。1.针对低信噪比噪声环境下,说话人识别的特征参数鲁棒性下降的问题,论文提出感知语谱规整耳蜗滤波倒谱系数的特征提取方法。方法先构建符合耳蜗基底膜行波冲激响应及非线性频率分布的耳蜗滤波器组,使特征参数的提取过程在频域中拟合心理声学实验结果从而提高特征参数的鲁棒性。再由人耳感知特性的语音增强以及一个时-频域的二维增强,通过二维数据的边界检测得到感知语谱规整参数。并把耳蜗滤波器组的输出进一步规整为时-频域鲁棒性更好的感知语谱规整耳蜗滤波倒谱系数。实验结果表明,在所有测试的信噪比条件下,论文提出的特征参数,在所有实验噪声中的平均识别率,分别比另外三种特征参数高出26.6\%,22.2\%以及18.5\%,而在-10d B到10d B的信噪比条件下,提出的特征参数在所有条件中获得了最好的识别率。在低信噪比条件下,提出的感知语谱规整耳蜗滤波倒谱系数对不同噪声都具有相对较好的鲁棒性。2.针对低信噪比说话人识别中特征补偿方法鲁棒性下降的问题,提出了一种采用感知听觉场景分析的特征补偿方法。先求取语音的缺失数据特征谱,并由语音的感知特性求出感知特性的语音含量。含噪语音经过感知特性的语音增强和对其语谱的二维增强后求解出语音的分布,联合感知特性语音含量和缺失强度参数提取出感知听觉因子。再结合缺失数据特征谱把特征补偿的过程分解为不同听觉场景进行区分地分析和处理,从而增强特征参数的鲁棒性能。实验结果表明,在-10d B到10d B的低信噪比环境下,对于四种不同的噪声,提出的方法比另外五种方法的鲁棒性均有提高,平均识别率分别提高26.0\%,19.6\%,12.7\%,4.6\%和6.5\%。论文提出的方法,是一种在时-频域中提高语音特征鲁棒性的方法,更适合于低信噪比环境下的说话人识别。3.针对低信噪比说话人识别中模型鲁棒性下降的问题,提出了一个混合条件噪声场模型。它通过分数阶离散转移函数,把White噪声作为基噪声,Pink噪声作为指导噪声,构建了一个从白噪声到棕色噪声渐变的一系列有色噪声。并且以不同的信噪比加入到训练语音中,然后构建一个拥有各种噪声条件和各种信噪比条件的混合条件噪声场。再对混合条件噪声场中的每个说话人语音都构建模型,形成混合条件噪声场模型。识别时先在混合条件噪声场模型的每个说话人模型中找到匹配的混合条件噪声场模型,再在所有说话人中识别出说话人。实验结果表明,在所有的四种实验噪声中,提出的模型在-10d B到10d B信噪比下的平均识别率,分别比基线模型和另外两种参考模型高出42.7\%,32.2\%以及21.1\%。提出的说话人识别模型更适用于低信噪比的环境。},
  editora = {赵, 鹤鸣},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,low-SNR environment,Mixture-Condition Noise Field Model,noise,Perception Auditory Scene Analysis,Perception Spectrogram Norm Cochlea Filter Cepstral Coefficient,speaker recognition,低信噪比环境,噪声,感知听觉场景分析,感知语谱规整耳蜗滤波倒谱系数,混合条件噪声场模型,说话人识别},
  annotation = {3 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\吴_2016_低信噪比环境下说话人识别研究.pdf}
}

@inproceedings{wuEmotionRecognitionFusing2021,
  title = {Emotion {{Recognition}} by {{Fusing Time Synchronous}} and {{Time Asynchronous Representations}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Wen and Zhang, Chao and Woodland, Philip C.},
  date = {2021-06},
  pages = {6269--6273},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414880},
  abstract = {In this paper, a novel two-branch neural network model structure is proposed for multimodal emotion recognition, which consists of a time synchronous branch (TSB) and a time asynchronous branch (TAB). To capture correlations between each word and its acoustic realisation, the TSB combines speech and text modalities at each input window frame and then uses pooling across time to form a single embedding vector. The TAB, by contrast, provides cross-utterance information by integrating sentence text embeddings from a number of context utterances into another embedding vector. The final emotion classification uses both the TSB and the TAB embeddings. Experimental results on the IEMOCAP dataset demonstrate that the two-branch structure achieves state-of-the-art results in 4-way classification with all common test setups. When using automatic speech recognition (ASR) output instead of manually transcribed reference text, it is shown that the cross-utterance information considerably improves robustness against ASR errors. Furthermore, by incorporating an extra class for all the other emotions, the final 5-way classification system with ASR hypotheses can be viewed as a prototype for more realistic emotion recognition systems.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Acoustics,Conferences,Correlation,Emotion recognition,Neural networks,Prototypes,Signal processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2021_Emotion Recognition by Fusing Time Synchronous and Time Asynchronous.pdf}
}

@article{wuExemplarBasedEmotiveSpeech2021,
  title = {Exemplar-{{Based Emotive Speech Synthesis}}},
  author = {Wu, Xixin and Cao, Yuewen and Lu, Hui and Liu, Songxiang and Kang, Shiyin and Wu, Zhiyong and Liu, Xunying and Meng, Helen},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {874--886},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3052688},
  abstract = {Expressive text-to-speech (E-TTS) synthesis is important for enhancing user experience in communication with machines using the speech modality. However, one of the challenges in E-TTS is the lack of a precise description of emotions. Previous categorical specifications may be insufficient for describing complex emotions. The dimensional specifications face the difficulty of ambiguity in annotation. This work advocates a new approach of describing emotive speech acoustics using spoken exemplars. We investigate methods to extract emotion descriptions from the input exemplar of emotive speech. The measures are combined to form two descriptors, based on capsule network (CapNet) and residual error network (RENet). The first is designed to consider the spatial information in the input exemplary spectrogram, and the latter is to capture the contrastive information between emotive acoustic expressions. Two different approaches are applied for conversion from the variable-length feature sequence to fixed-size description vector: (1) dynamic routing groups similar capsules to the output description; and (2) recurrent neural network's hidden states store the temporal information for the description. The two descriptors are integrated to a state-of-the-art sequence-to-sequence architecture to obtain an end-to-end architecture that is optimized as a whole towards the same goal of generating correct emotive speech. Experimental results on a public audiobook dataset demonstrate that the two exemplar-based approaches achieve significant performance improvement over the baseline system in both emotion similarity and speech quality.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_reading,Acoustics,capsule,exemplary emotion descriptor,Expressive speech synthesis,Hidden Markov models,residual error,Spectrogram,speech emotion recognition,Speech enhancement,Speech recognition,Speech synthesis,Virtual assistants},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2021_Exemplar-Based Emotive Speech Synthesis.pdf}
}

@article{wuImprovingLowresourceMachine2022,
  title = {Improving Low-Resource Machine Transliteration by Using 3-Way Transfer Learning},
  author = {Wu, Chun-Kai and Shih, Chao-Chuang and Wang, Yu-Chun and Tsai, Richard Tzong-Han},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101283},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101283},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000863},
  urldate = {2021-09-28},
  abstract = {Transfer learning brings improvement to machine translation by using a resource-rich language pair to pretrain the model and then adapting it to the desired language pair. However, to date, there have been few attempts to tackle machine transliteration with transfer learning. In this article, we propose a method of using source\textendash pivot and pivot\textendash target datasets to improve source\textendash target machine transliteration. Our approach first bridges the source\textendash pivot and pivot\textendash target datasets by reducing the distance between source and pivot embeddings. Then, our model learns to translate from the pivot language to the target language. Finally, the source\textendash target dataset is used to fine tune the model. Our experiments show that our method is superior to the transfer learning method. When implemented with a state-of-the-art source\textendash target translation model from NEWS'18, our transfer learning method can improve the accuracy by 1.1\%.},
  language = {en},
  keywords = {Low resource,Machine transliteration,Transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2022_Improving low-resource machine transliteration by using 3-way transfer learning.pdf}
}

@thesis{WuJiYuShenDuXueXiDeYuYinQingXuShiBieSuanFaYanJiuYuYingYong2020,
  type = {硕士},
  title = {基于深度学习的语音情绪识别算法研究与应用},
  author = {吴, 思凡},
  date = {2020},
  institution = {{南京邮电大学}},
  doi = {10.27251/d.cnki.gnjdc.2020.000032},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020427192.nh&uniplatform=NZKPT&v=Me4SOlRRSkAEpCjTnO5q%25mmd2BhNzId4hw4SZGZDQwX49jLgFoqgv9Dlx5j3diXDs8qC1},
  urldate = {2021-09-28},
  abstract = {近年来,人机交互系统正逐渐走进我们的生活。语音情绪识别技术作为人机交互系统中的关键技术之一,可以准确识别情绪并帮助机器更好地了解使用者的意图,提升人机交互的质量,受到了国内外研究人员的广泛关注。随着深度学习在图像识别、语音识别等领域的成功应用,学者们开始尝试将其使用在语音情绪识别上,提出了很多基于深度学习的语音情绪识别算法。本文对这些算法进行了深入的研究,发现这些算法存在特征提取方式过于简单,对人为设计的特征利用率低、模型复杂度高,识别特定情绪准确率低等问题。针对这些问题,本文从特征提取算法和模型结构两个角度对语音情绪识别算法进行了改进,改进之后的算法不仅提高了语音情绪识别的准确率,还有效降低了情绪识别系统的复杂度。本文的主要研究内容如下:对基于深度学习的语音情绪识别算法进行研究。重点研究了基于深度神经网络的变长语音情绪识别算法,介绍了算法的模型结构和其中的关键技术。在标准语音情绪识别数据集\textemdash\textemdash 交互式情绪二元运动捕捉数据集（Interactive Emotional Dyadic Motion Capture,IEMOCAP）上对算法进行了性能测试。将定长语音情绪识别算法与变长语音情绪识别算法的性能进行对比分析,证明了变长语音情绪识别算法的优越性。最后分析了变长语音情绪识别算法存在的问题。针对变长语音情绪识别算法存在的对人为设计的特征利用率低、特征提取方式过于简单等问题,本文从特征提取算法和模型结构两个方面提出改进,提出基于加权特征融合算法和双向长短期记忆网络的变长语音情绪识别算法。在IEMOCAP数据集上对改进后的算法进行了性能测试,实验结果表明,改进之后的算法比原算法的准确率提升了超过5\%。针对变长语音情绪识别算法模型复杂度高和识别特定情绪准确率低的问题,本文使用轻量级卷积算法和多任务学习算法进行改进,提出了基于轻量深度神经网络多任务学习的变长语音情绪识别算法。在IEMOCAP数据集上对改进后的算法进行了性能测试,实验结果表明,改进之后的算法在提升了超过8\%的识别准确率的同时降低了模型70\%的复杂度。},
  editora = {李, 飞},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {Bidirectional Long Short-Term Memory,Mini Convolutional Network,Mini Deep neural network,Multi-task Learning,Speech Emotion Recognition,Weighted Feature Fusion,加权特征融合,双向长短期记忆网络,多任务学习,语音情绪识别,轻量级卷积,轻量级深度神经网络},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\吴_2020_基于深度学习的语音情绪识别算法研究与应用.pdf}
}

@inproceedings{wuNeuralArchitectureSearch2022,
  title = {Neural {{Architecture Search}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Xixin and Hu, Shoukang and Wu, Zhiyong and Liu, Xunying and Meng, Helen},
  date = {2022},
  pages = {6902--6906},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746155},
  abstract = {Deep neural networks have brought significant advancements to speech emotion recognition (SER). However, the architecture design in SER is mainly based on expert knowledge and empirical (trial-and-error) evaluations, which is time-consuming and resource intensive. In this paper, we propose to apply neural architecture search (NAS) techniques to automatically configure the SER models. To accelerate the candidate architecture optimization, we propose a uniform path dropout strategy to encourage all candidate architecture operations to be equally optimized. Experimental results of two different neural structures on IEMOCAP show that NAS can improve SER performance (54.89\% to 56.28\%) while maintaining model parameter sizes. The proposed dropout strategy also shows superiority over the previous approaches.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Conferences,Deep learning,Emotion recognition,neural architecture search,Neural networks,path dropout,Signal processing,Speech emotion recognition,Speech recognition,Training,uniform sampling},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2022_Neural Architecture Search for Speech Emotion Recognition.pdf}
}

@inproceedings{wuNovelSequentialMonte2022,
  title = {A {{Novel Sequential Monte Carlo Framework}} for {{Predicting Ambiguous Emotion States}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Jingyao and Dang, Ting and Sethu, Vidhyasaharan and Ambikairajah, Eliathamby},
  date = {2022},
  pages = {8567--8571},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746350},
  abstract = {When continuous emotion labelling of natural (non-acted) data is desired, it is typically collected from multiple annotators. However, most automatic emotion recognition systems trained on such data ignore disagreement between annotators and only models the average rating, despite the observation that the degree of disagreement would reflect the ambiguity and subtlety in every expression of emotions. In this paper, we propose a novel Sequential Monte Carlo framework that models the perceived emotion as time-varying distributions that allows for ambiguity to be incorporated. Additionally, we present alternative measures that consider both the similarity of prediction to the multiple labels, as well as whether the degree of ambiguity in the prediction and labels. The proposed system was validated on the publicly available RECOLA dataset.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Annotations,Conferences,Continuous emotion prediction,Emotion recognition,Gaussian Mixture Model,Gaussian Process,inter-rater variability,Mixture models,Monte Carlo methods,Predictive models,Sequential Monte Carlo,Signal processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2022_A Novel Sequential Monte Carlo Framework for Predicting Ambiguous Emotion States.pdf}
}

@article{wuQuasiPeriodicParallelWaveGAN2021,
  title = {Quasi-{{Periodic Parallel WaveGAN}}: {{A Non-Autoregressive Raw Waveform Generative Model With Pitch-Dependent Dilated Convolution Neural Network}}},
  shorttitle = {Quasi-{{Periodic Parallel WaveGAN}}},
  author = {Wu, Yi-Chiao and Hayashi, Tomoki and Okamoto, Takuma and Kawai, Hisashi and Toda, Tomoki},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {792--806},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3051765},
  abstract = {In this paper, we propose a quasi-periodic parallel WaveGAN (QPPWG) waveform generative model, which applies a quasi-periodic (QP) structure to a parallel WaveGAN (PWG) model using pitch-dependent dilated convolution networks (PDCNNs). PWG is a small-footprint GAN-based raw waveform generative model, whose generation time is much faster than real time because of its compact model and non-autoregressive (non-AR) and non-causal mechanisms. Although PWG achieves high-fidelity speech generation, the generic and simple network architecture lacks pitch controllability for an unseen auxiliary fundamental frequency (F0) feature such as a scaled F0. To improve the pitch controllability and speech modeling capability, we apply a QP structure with PDCNNs to PWG, which introduces pitch information to the network by dynamically changing the network architecture corresponding to the auxiliary F0 feature. Both objective and subjective experimental results show that QPPWG outperforms PWG when the auxiliary F0 feature is scaled. Moreover, analyses of the intermediate outputs of QPPWG also show better tractability and interpretability of QPPWG, which respectively models spectral and excitation-like signals using the cascaded fixed and adaptive blocks of the QP structure.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,Adaptation models,Artificial neural networks,Controllability,Gallium nitride,Generators,Neural vocoder,parallel WaveGAN,pitch-dependent dilated convolution,quasi-periodic WaveNet,Vocoders},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2021_Quasi-Periodic Parallel WaveGAN.pdf}
}

@article{wuQuasiPeriodicWaveNetAutoregressive2021,
  title = {Quasi-{{Periodic WaveNet}}: {{An Autoregressive Raw Waveform Generative Model With Pitch-Dependent Dilated Convolution Neural Network}}},
  shorttitle = {Quasi-{{Periodic WaveNet}}},
  author = {Wu, Yi-Chiao and Hayashi, Tomoki and Tobing, Patrick Lumban and Kobayashi, Kazuhiro and Toda, Tomoki},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1134--1148},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3061245},
  abstract = {In this paper, a pitch-adaptive waveform generative model named Quasi-Periodic WaveNet (QPNet) is proposed to improve the limited pitch controllability of vanilla WaveNet (WN) using pitch-dependent dilated convolution neural networks (PDCNNs). Specifically, as a probabilistic autoregressive generation model with stacked dilated convolution layers, WN achieves high-fidelity audio waveform generation. However, the pure-data-driven nature and the lack of prior knowledge of audio signals degrade the pitch controllability of WN. For instance, it is difficult for WN to precisely generate the periodic components of audio signals when the given auxiliary fundamental frequency (F0) features are outside the F0 range observed in the training data. To address this problem, QPNet with two novel designs is proposed. First, the PDCNN component is applied to dynamically change the network architecture of WN according to the given auxiliary F0 features. Second, a cascaded network structure is utilized to simultaneously model the long- and short-term dependencies of quasi-periodic signals such as speech. The performances of single-tone sinusoid and speech generations are evaluated. The experimental results show the effectiveness of the PDCNNs for unseen auxiliary F0 features and the effectiveness of the cascaded structure for speech generation.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Adaptation models,Controllability,Convolution,Feature extraction,Logic gates,Pitch controllability,pitch-dependent dilated convolution,Predictive models,quasi-periodic structure,vocoder,Vocoders,WaveNet},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2021_Quasi-Periodic WaveNet.pdf}
}

@article{wuSentimentTimeSeries2021,
  title = {Sentiment {{Time Series Calibration}} for {{Event Detection}}},
  author = {Wu, Jingyi and Shang, Lin and Gao, Xiaoying},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2407--2420},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3096653},
  abstract = {Event detection based on sentiment time series, which describe the trend of users' emotions or attitudes towards specific topics over time, has been widely applied in the analysis of social network or text mining. Most of the contributions directly generate time series sequences by classifiers. However, due to the missing corpus labels or the limited performance of the classifier, such generated sentiment time series may not correspond to the actual values, especially when the sentiment value changes drastically, called extreme value. We propose a new method to calibrate sentiment times series for event detection based on evaluation on a sampling dataset. Theoretical analysis of the calibration method is explicated, and it is proved that the sampling error of the performance indicators can be limited to a minimal range for extreme values, thus sentiment value error can be reduced. Experiments on simulated datasets and real-world datasets illustrate the effectiveness and robustness of our method.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Estimation,event detection,Event detection,Feature extraction,proportion estimation,Sentiment analysis,Social networking (online),Statistical analysis,Time series analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2021_Sentiment Time Series Calibration for Event Detection.pdf}
}

@article{wuTacklingPerceptionBias2021,
  title = {Tackling {{Perception Bias}} in {{Unsupervised Phoneme Discovery Using DPGMM-RNN Hybrid Model}} and {{Functional Load}}},
  author = {Wu, Bin and Sakti, Sakriani and Zhang, Jinsong and Nakamura, Satoshi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {348--362},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3042016},
  abstract = {The human perception of phonemes is biased against speech sounds. The lack of correspondence between perceptual phonemes and acoustic signals forms a big challenge in designing unsupervised algorithms to distinguish phonemes from sound. We propose the DPGMM-RNN hybrid model that improves phoneme categorization by relieving the fragmentation problem. We also merge segments with low functional load, which is the work done by segment contrasts to differentiate between utterances, just like humans who convert unambiguous segments into phonemes as units for immediate perception. Our results show that the DPGMM-RNN hybrid model relieves the fragmentation problem and improves phoneme discriminability. The minimal functional load merge compresses a segment system, preserves information and keeps phoneme discriminability.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Auditory system,Clustering algorithms,Context modeling,DPGMM,Ear,functional load,Load modeling,perception of phonemes,RNN,Unsupervised phoneme discovery,Visualization,zerospeech},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wu et al_2021_Tackling Perception Bias in Unsupervised Phoneme Discovery Using DPGMM-RNN.pdf}
}

@thesis{WuZaoShengLuBangDeYuYinQingGanShiBieYanJiu2017,
  type = {硕士},
  title = {噪声鲁棒的语音情感识别研究},
  author = {吴, 奥},
  date = {2017},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1018006076.nh&uniplatform=NZKPT&v=Dvvef8p7EI%25mmd2B8%25mmd2FQmFa83QwfFKWHa%25mmd2B55jnRqNR0FdCZAWxvaMwFESJz6dvX1BKCKTK},
  urldate = {2021-09-10},
  abstract = {随着电子信息技术的飞速发展,人们对人机交互体验的需求持续地增加,机器情感智能识别作为人机交互中的重要组成部分,其需求更是与日俱增。在语音情感技术的应用过程里面,总是伴随着各种环境噪声的影响。提取并选择出有效表征情感并且具有高的噪声鲁棒性的语音情感特征以及构建噪声鲁棒性的语音情感分类器,是本文的重点研究内容。本文简要叙述了噪声环境下语音情感识别技术方面的背景,并对噪声环境下语音情感识别的研究现状进行了概述。针对噪声条件下的语音情感识别任务,在最优小波包基的构建的基础上,利用短时帧分析与长时帧分析相结合的方法,并且利用具有较好噪声鲁棒性的子带频谱质心参数进行加权,本文提出了一种基于长时帧噪声补偿的小波包倒谱系数特征（Long time frame Analysis Weighted Wavelet Packet Cepstral Coefficient,LW-WPCC）提取算法。基于语音片段轨迹模型,给出了一种用于计算语音特征携带情感信息的量化准则函数,从而对高维的LW-WPCC特征进行特征选择。针对语音情感识别中的测试样本中普遍存在的噪声问题,通过计算重要性权重,对支持向量机（Support Vector Mach ion,SVM）分类器等效优化问题中的松弛变量进行加权,改进了传统的支持向量机的噪声鲁棒性。并通过混噪语音信号在不同信噪比水平下的情感识别实验,对本文中提取的LW-WPCC特征在噪声情况下语音情感识别能力进行分析评估,相比于传统的语音情感特征和支持向量机,改善后的情感识别方法拥有更出色的噪声鲁棒性和语音情感识别识别准确率。最后,给出了一种基于DBN网络的特征融合算法,将传统声学特征中的韵律特征,音质特征与WPCC以及LW-WPCC特征融合,通过对比实验对基于DBN网络的特征融合算法提取的融合特征在噪声情况下进行语音情感识别能力的分析评估。不同信噪比的混噪语音信号情感识别结果显示,改进的基于长时帧噪声补偿的小波包倒谱系数特征具有更好的噪声鲁棒性以及情感识别准确率。},
  editora = {章, 国宝 and 倪, 道宏},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,deep learning,feature selection,importance weight,noise robustness,Speech emotion recognition,support vector machine,噪声鲁棒性,支持向量机,深度学习,特征选择,语音情感识别,重要性权重},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\吴_2017_噪声鲁棒的语音情感识别研究.pdf}
}

@inproceedings{xiaTemporalContextSpeech2021,
  title = {Temporal {{Context}} in {{Speech Emotion Recognition}}},
  booktitle = {Interspeech 2021},
  author = {Xia, Yangyang and Chen, Li-Wei and Rudnicky, Alexander and Stern, Richard M.},
  date = {2021-08-30},
  pages = {3370--3374},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1840},
  url = {https://www.isca-speech.org/archive/interspeech_2021/xia21b_interspeech.html},
  urldate = {2022-09-30},
  eventtitle = {Interspeech 2021},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xia et al_2021_Temporal Context in Speech Emotion Recognition.pdf}
}

@inproceedings{xieAggregatedResidualTransformations2017,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xie, Saining and Girshick, Ross and Doll\'ar, Piotr and Tu, Zhuowen and He, Kaiming},
  date = {2017-07},
  pages = {5987--5995},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.634},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  language = {en},
  keywords = {Complexity theory,Computer architecture,Network topology,Neural networks,Neurons,Topology},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xie et al_2017_Aggregated Residual Transformations for Deep Neural Networks.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\N4IABRKY\\Xie 等。 - 2017 - Aggregated Residual Transformations for Deep Neura.pdf}
}

@article{xieBayesianLearningDeep2021,
  title = {Bayesian {{Learning}} for {{Deep Neural Network Adaptation}}},
  author = {Xie, Xurong and Liu, Xunying and Lee, Tan and Wang, Lan},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2096--2110},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3084072},
  abstract = {A key task for speech recognition systems is to reduce the mismatch between training and evaluation data that is often attributable to speaker differences. Speaker adaptation techniques play a vital role to reduce the mismatch. Model-based speaker adaptation approaches often require sufficient amounts of target speaker data to ensure robustness. When the amount of speaker level data is limited, speaker adaptation is prone to overfitting and poor generalization. To address the issue, this paper proposes a full Bayesian learning based DNN speaker adaptation framework to model speaker-dependent (SD) parameter uncertainty given limited speaker specific adaptation data. This framework is investigated in three forms of model based DNN adaptation techniques: Bayesian learning of hidden unit contributions (BLHUC), Bayesian parameterized activation functions (BPAct), and Bayesian hidden unit bias vectors (BHUB). In the three methods, deterministic SD parameters are replaced by latent variable posterior distributions for each speaker, whose parameters are efficiently estimated using a variational inference based approach. Experiments conducted on 300-hour speed perturbed Switchboard corpus trained LF-MMI TDNN/CNN-TDNN systems suggest the proposed Bayesian adaptation approaches consistently outperform the deterministic adaptation on the NIST Hub5'00 and RT03 evaluation sets. When using only the first five utterances from each speaker as adaptation data, significant word error rate reductions up to 1.4\% absolute (7.2\% relative) were obtained on the CallHome subset. The efficacy of the proposed Bayesian adaptation techniques is further demonstrated in a comparison against the state-of-the-art performance obtained on the same task using the most recent systems reported in the literature.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Acoustics,adaptation,Adaptation models,Bayes methods,Bayesian learning,Data models,Hidden Markov models,LHUC,Speech recognition,Switchboard,TDNN,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xie et al_2021_Bayesian Learning for Deep Neural Network Adaptation.pdf}
}

@unpublished{xieMultispeakerMultistyleVoice2021,
  title = {The {{Multi-speaker Multi-style Voice Cloning Challenge}} 2021},
  author = {Xie, Qicong and Tian, Xiaohai and Liu, Guanghou and Song, Kun and Xie, Lei and Wu, Zhiyong and Li, Hai and Shi, Song and Li, Haizhou and Hong, Fen and Bu, Hui and Xu, Xin},
  date = {2021-04-05},
  eprint = {2104.01818},
  eprinttype = {arxiv},
  primaryclass = {eess},
  url = {http://arxiv.org/abs/2104.01818},
  urldate = {2021-09-10},
  abstract = {The Multi-speaker Multi-style Voice Cloning Challenge (M2VoC) aims to provide a common sizable dataset as well as a fair testbed for the benchmarking of the popular voice cloning task. Specifically, we formulate the challenge to adapt an average TTS model to the stylistic target voice with limited data from target speaker, evaluated by speaker identity and style similarity. The challenge consists of two tracks, namely few-shot track and one-shot track, where the participants are required to clone multiple target voices with 100 and 5 samples respectively. There are also two sub-tracks in each track. For sub-track a, to fairly compare different strategies, the participants are allowed to use only the training data provided by the organizer strictly. For sub-track b, the participants are allowed to use any data publicly available. In this paper, we present a detailed explanation on the tasks and data used in the challenge, followed by a summary of submitted systems and evaluation results.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xie et al_2021_The Multi-speaker Multi-style Voice Cloning Challenge 2021.pdf}
}

@article{xieSpeechEmotionClassification2019,
  title = {Speech {{Emotion Classification Using Attention-Based LSTM}}},
  author = {Xie, Yue and Liang, Ruiyu and Liang, Zhenlin and Huang, Chengwei and Zou, Cairong and Schuller, Bj\"orn},
  date = {2019},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {11},
  pages = {1675--1685},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2019.2925934},
  abstract = {Automatic speech emotion recognition has been a research hotspot in the field of human-computer interaction over the past decade. However, due to the lack of research on the inherent temporal relationship of the speech waveform, the current recognition accuracy needs improvement. To make full use of the difference of emotional saturation between time frames, a novel method is proposed for speech recognition using frame-level speech features combined with attention-based long short-term memory (LSTM) recurrent neural networks. Frame-level speech features were extracted from waveform to replace traditional statistical features, which could preserve the timing relations in the original speech through the sequence of frames. To distinguish emotional saturation in different frames, two improvement strategies are proposed for LSTM based on the attention mechanism: first, the algorithm reduces the computational complexity by modifying the forgetting gate of traditional LSTM without sacrificing performance and second, in the final output of the LSTM, an attention mechanism is applied to both the time and the feature dimension to obtain the information related to the task, rather than using the output from the last iteration of the traditional algorithm. Extensive experiments on the CASIA, eNTERFACE, and GEMEP emotion corpora demonstrate that the performance of the proposed approach is able to outperform the state-of-the-art algorithms reported to date.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_readed,attention mechanism,Emotion recognition,Feature extraction,frame-level features,Harmonic analysis,Logic gates,LSTM,Speech emotion,Speech processing,Speech recognition,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xie et al_2019_Speech Emotion Classification Using Attention-Based LSTM.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\GYS5QCAH\\Xie 等。 - 2019 - Speech Emotion Classification Using Attention-Base.pdf}
}

@inproceedings{xiFrontendAttributesDisentanglement2022,
  title = {Frontend {{Attributes Disentanglement}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Xi, Yu-Xuan and Song, Yan and Dai, Li-Rong and McLoughlin, Ian and Liu, Lin},
  date = {2022},
  pages = {7712--7716},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746691},
  abstract = {Speech emotion recognition (SER) with limited size dataset is a challenging task, since a spoken utterance contains various disturbing attributes besides emotion, including speaker, content, and language. However, due to a close relationship between speaker and emotion attributes, simply fine-tuning a linear model is enough to obtain a good SER performance on the utterance-level embeddings (i.e., i-vector and x-vectors) extracted from the pre-trained speaker recognition (SR) frontends. In this paper, we aim to perform frontend attributes disentanglement (AD) for SER task, using a pre-trained SR model. Specifically, the AD module consists of attribute normalization (AN) and attribute reconstruction (AR) phases. The AN filters out the variation information using instance normalization (IN), and AR reconstructs the emotion-relevant features from the residual space to ensure high emotion discrimination. For better disentanglement, a dual space loss is then designed to encourage the separability of emotion-relevant and emotion-irrelevant spaces. To introduce the long-range contextual information for emotion related reconstruction, a time-frequency (TF) attention is further proposed. Different from the style disentanglement of the extracted x-vectors, the proposed AD module can be applied on frontend feature extractor. Experiments on IEMOCAP benchmark demonstrate the effectiveness of the proposed method.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Benchmark testing,convolutional neural network,disentanglement,Emotion recognition,Feature extraction,Information filters,speech emotion recognition,Speech recognition,style transformation,Time-frequency analysis,Training data},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xi et al_2022_Frontend Attributes Disentanglement for Speech Emotion Recognition.pdf}
}

@article{XingXiJinPingWaiJiaoSiXiangYuZhouBianMingYunGongTongTiJianShe2021,
  title = {习近平外交思想与周边命运共同体建设},
  author = {邢, 广程},
  date = {2021},
  journaltitle = {当代世界},
  number = {08},
  pages = {10--15},
  issn = {1006-4206},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2021&filename=JSDD202108004&uniplatform=NZKPT&v=R5r4pkxln1%25mmd2FexALvMH09W%25mmd2FCCDPN8kw0HMRPtRZgnKt5d4A0A7heXRyHi5vBfmXYh},
  urldate = {2021-11-21},
  abstract = {\&lt;正\&gt;党的十八大以来,习近平总书记深刻把握新时代中国和世界发展大势,在对外战略、外交政策和外交工作等方面进行一系列重大理论和实践创新,形成了习近平外交思想。习近平外交思想是习近平新时代中国特色社会主义思想的重要组成部分,是以习近平同志为核心的党中央治国理政思想在外交领域的重要理论结晶,也是新时代中国对外工作的行动指南。习近平外交思想完整地揭示了中国和世界发展变化的大趋势,全面回答了新时代中国对外战略和政策的一系列重大理论和实践问题,},
  issue = {08},
  language = {zh-CN},
  keywords = {⛔ No DOI found,中国与俄罗斯,习近平外交思想,习近平总书记,人类命运共同体,周边命运共同体,重要组成部分},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\邢_2021_习近平外交思想与周边命运共同体建设.pdf}
}

@inproceedings{xuDifferentialTimefrequencyLogmel2022,
  title = {Differential {{Time-frequency Log-mel Spectrogram Features}} for {{Vision Transformer Based Infant Cry Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Xu, Hai-tao and Zhang, Jie and Dai, Li-rong},
  date = {2022-09-18},
  pages = {1963--1967},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-18},
  url = {https://www.isca-speech.org/archive/interspeech_2022/xu22_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xu et al_2022_Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer.pdf}
}

@article{xueExploringAttentionMechanisms2021,
  title = {Exploring Attention Mechanisms Based on Summary Information for End-to-End Automatic Speech Recognition},
  author = {Xue, Jiabin and Zheng, Tieran and Han, Jiqing},
  date = {2021-11-20},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {465},
  pages = {514--524},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.09.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221013758},
  urldate = {2021-10-02},
  abstract = {Recent studies have confirmed that attention mechanisms with location constraint strategy are helpful to reduce the misrecognition caused by incorrect alignments in attention-based end-to-end automatic speech recognition (E2E ASR) systems. The significant advantage of these mechanisms is that they consider the monotonicity of the alignment by employing a location constraint vector. This vector is directly obtained from historical attention scores for most such attention mechanisms. However, an unreasonable vector may become an additional interference when an inaccurate historical attention score occurs. Moreover, the subsequent process of attention scoring will be affected by the interference continuously. To address the problem, we obtain a reasonable location constraint vector from the matching relationship between the historical output information and the summary information, where the summary information includes content and temporal information about speech sequence. We further propose an enhanced location constrained attention mechanism, i.e., summary constrained (SC) attention mechanism, to generate the vector by a matching relationship-based neural network. We use a summary subspace embedding learned by a linear subspace projection to represent the summary information. Furthermore, considering the complementarity of the SC and typical location constrained attention mechanisms, a fused attention mechanism is used to generate a more reasonable vector by combining the two mechanisms. The SC and fused attention mechanisms-based E2E ASR systems were evaluated on a Switchboard conversational telephone speech recognition. The experimental results show that our mechanisms obtained the relative reductions of 10.6\% and 16.7\% in the word error rate compared with the baseline system.},
  language = {en},
  keywords = {End-to-end automatic speech recognition,Location constrained attention mechanism,Summary constrained attention mechanism,Summary information,Summary subspace embedding},
  annotation = {ECC: 0000000},
  file = {E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\B2NBU7E7\\S0925231221013758.html}
}

@article{xueSelfAdaptiveMutationNeural2021,
  title = {A {{Self-Adaptive Mutation Neural Architecture Search Algorithm Based}} on {{Blocks}}},
  author = {Xue, Yu and Wang, Yankang and Liang, Jiayu and Slowik, Adam},
  date = {2021},
  journaltitle = {IEEE Computational Intelligence Magazine},
  volume = {16},
  number = {3},
  pages = {67--78},
  issn = {1556-6048},
  doi = {10.1109/mci.2021.3084435},
  abstract = {Recently, convolutional neural networks (CNNs) have achieved great success in the field of artificial intelligence, including speech recognition, image recognition, and natural language processing. CNN architecture plays a key role in CNNs' performance. Most previous CNN architectures are hand-crafted, which requires designers to have rich expert domain knowledge. The trial-and-error process consumes a lot of time and computing resources. To solve this problem, researchers proposed the neural architecture search, which searches CNN architecture automatically, to satisfy different requirements. However, the blindness of the search strategy causes a 'loss of experience' in the early stage of the search process, and ultimately affects the results of the later stage. In this paper, we propose a self-adaptive mutation neural architecture search algorithm based on ResNet blocks and DenseNet blocks. The self-adaptive mutation strategy makes the algorithm adaptively adjust the mutation strategies during the evolution process to achieve better exploration. In addition, the whole search process is fully automatic, and users do not need expert knowledge about CNNs architecture design. In this paper, the proposed algorithm is compared with 17 state-of-the-art algorithms, including manually designed CNN and automatic search algorithms on CIFAR10 and CIFAR100. The results indicate that the proposed algorithm outperforms the competitors in terms of classification performance and consumes fewer computing resources.},
  eventtitle = {{{IEEE Computational Intelligence Magazine}}},
  issue = {3},
  language = {en},
  keywords = {_Waiting for read,\#nosource,Artificial intelligence,Classification algorithms,Computer architecture,Image recognition,Natural language processing,Search problems,Speech recognition}
}

@article{xueSpeechEnhancementBased2021,
  title = {Speech {{Enhancement Based}} on {{Modulation-Domain Parametric Multichannel Kalman Filtering}}},
  author = {Xue, Wei and Moore, Alastair H. and Brookes, Mike and Naylor, Patrick A.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {393--405},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3040850},
  abstract = {Recently we presented a modulation-domain multichannel Kalman filtering (MKF) algorithm for speech enhancement, which jointly exploits the inter-frame modulation-domain temporal evolution of speech and the inter-channel spatial correlation to estimate the clean speech signal. The goal of speech enhancement is to suppress noise while keeping the speech undistorted, and a key problem is to achieve the best trade-off between speech distortion and noise reduction. In this paper, we extend the MKF by presenting a modulation-domain parametric MKF (PMKF) which includes a parameter that enables flexible control of the speech enhancement behaviour in each time-frequency (TF) bin. Based on the decomposition of the MKF cost function, a new cost function for PMKF is proposed, which uses the controlling parameter to weight the noise reduction and speech distortion terms. An optimal PMKF gain is derived using a minimum mean squared error (MMSE) criterion. We analyse the performance of the proposed MKF, and show its relationship to the speech distortion weighted multichannel Wiener filter (SDW-MWF). To evaluate the impact of the controlling parameter on speech enhancement performance, we further propose PMKF speech enhancement systems in which the controlling parameter is adaptively chosen in each TF bin. Experiments on a publicly available head-related impulse response (HRIR) database in different noisy and reverberant conditions demonstrate the effectiveness of the proposed method.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Correlation,Distortion,Kalman filtering,Kalman filters,microphone arrays,modulation domain,Noise measurement,Noise reduction,speech distortion,speech enhancement,Speech enhancement,Speech processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xue et al_2021_Speech Enhancement Based on Modulation-Domain Parametric Multichannel Kalman.pdf}
}

@article{xueVoiceConversionEmotional2018,
  title = {Voice Conversion for Emotional Speech: {{Rule-based}} Synthesis with Degree of Emotion Controllable in Dimensional Space},
  shorttitle = {Voice Conversion for Emotional Speech},
  author = {Xue, Yawen and Hamada, Yasuhiro and Akagi, Masato},
  date = {2018-09},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {102},
  pages = {54--67},
  issn = {01676393},
  doi = {10.1016/j.specom.2018.06.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639317303187},
  urldate = {2021-09-10},
  abstract = {This paper proposes a rule-based voice conversion system for emotion which is capable of converting neutral speech to emotional speech using dimensional space (arousal and valence) to control the degree of emotion on a continuous scale. We propose an inverse three-layered model with acoustic features as output at the top layer, semantic primitives at the middle layer and emotion dimension as input at the bottom layer; an adaptive-based fuzzy inference system acts as connectors to extract the non-linear rules among the three layers. The rules are applied by modifying the acoustic features of neutral speech to create the different types of emotional speech. The prosody-related acoustic features of F0 and power envelope are parameterized using the Fujisaki model and target prediction model separately. Perceptual evaluation results show that the degree of emotion can be perceived well in the dimensional space of valence and arousal.},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xue et al_2018_Voice conversion for emotional speech.pdf}
}

@thesis{XuJiYuQingGanTeZhengXinXiZengQiangDeYuYinQingGanShiBieYanJiu2017,
  type = {博士},
  title = {基于情感特征信息增强的语音情感识别研究},
  author = {徐, 新洲},
  date = {2017},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2018&filename=1018002870.nh&uniplatform=NZKPT&v=C4pDOKVWUC5hG49GB7P%25mmd2BttSVnkJYqf5UKfX%25mmd2Fpy3rKt18W56GPqi3swJLOBs3a%25mmd2BWV},
  urldate = {2021-09-10},
  abstract = {语音情感识别（SER;Speech Emotion Recognition）是当前研究的热点之一,旨在通过语音信号来估计情感状态的情况.语音情感识别主要的应用集中于人机交互（HCI;Human-Computer Interaction）、自闭症或抑郁症的初步诊断、极限环境下负面情绪的监测等.然而,由于适用于语音情感识别的常用特征集（paralinguistic特征）通常可能含有一些除情感识别外更适合其他任务的成分,所以在大部分原始特征集上,直接得到对于识别语音情感有效的特征较为困难.因此,本文将基于情感特征信息增强的语音情感识别作为关键问题进行研究.基于情感特征信息增强的方法通过综合考虑训练样本的信息（如,特征、标签等）,构建训练学习模型.接着,保持并强化该模型中的关键情感特性,并将其用于对样本进行特征重构,生成新的特征.这些经过训练学习的特征更适用于识别或估计等目标任务,从而使系统性能得到提升.但是,由于在特征集中常常存在着大量不利于识别情感成分的干扰因素,所以不同于特征信息增强中其他一些研究较多的应用课题（如,人脸识别、说话人识别、语音识别等）,解决语音信号中的情感识别问题有着较大的难度.因此,和特征信息增强相关的已有工作并不一定能够保证有效地识别语音中的情感成分,尤其在进一步实用化研究方面.本文中着重研究了作为特性保持思想细化研究课题之一的子空间学习方法,及其衍生出的各种结构,以强化语音中情感特征信息的表达.通过充分的实验验证,所提出的一系列算法能够较有效地解决语音情感识别问题.本文的主要贡献如下所述:（1）本文在多核子空间学习的结构下提出使用多尺度核,以有效地识别语音中的情感成分.算法在使用Fisher判别嵌入图的同时,针对语音情感识别提出多尺度Gaussian核,用于构建多核学习（MKL;Multiple Kernel Learning）中Gram阵的最优化线性组合.为评价所提出的 MS-KFDA（Multiscale-Kernel Fisher Discriminant Analysis）算法的识别性能,本文在多个语音情感数据库上,使用openSMILE中不同公开特征集进行了大量实验验证.实验结果表明,所提出的方法相比于常用线性维数约简方法以及单核方法,具有更好的识别性能.（2）进而,本文提出了一个使用局部惩罚判别分析的多尺度核学习方法,即MS-KLPDA（Multiscale-Kernel Locally Penalised Discriminant Analysis）,并将所提出的算法用于识别语音中的情感成分.在提出的方法中,加入局部惩罚判别分析项来控制边界样本对的权重,同时使用多尺度核学习的结构.通过语音情感数据库上一系列实验证明,所提出的MS-KLPDA方法在识别语音中的情感成分时,识别性能高于MS-KFDA以及一些常用算法.（3）本文针对多核子空间学习提出了一个二维的统一框架结构,该结构在多核学习的基础上提供了多个不含有非负约束的线性组合,将多核学习和二维子空间学习相结合,这样在学习过程中保有了更多的信息.针对语音情感识别的应用背景,在此框架下使用判别嵌入图,提出了一个新算法,即广义多核判别分析（GMKDA;Generalised Multiple Kernel Discriminant Analysis）,算法中同样采用所提出框架中提出的附加多核线性组合映射方向.多个基本的语音情感数据库上的实验结果表明,本文所提出的方法与一些常用方法以及子空间学习方法相比,能够针对语音情感识别的应用需求,取得更好的识别性能.（4）本文联系极限学习机（ELM;Extreme Learning Machine）和子空间学习,提出了统一的广义谱回归（GSR;Generalised Spectral Regression）框架,涵盖了图嵌入（GE;Graph Em-bedding）框架下谱回归（SR;Spectral Regression）的子空间学习,以及 ELM 方法.本文所提出的框架包括三个模块:数据映射、图分解、回归.在数据映射阶段,可以使用不同的映射方式提供对于样本不同视角的观测;在图分解阶段,设计得到的嵌入图通过谱分解求得虚拟坐标,这样就提供了一种能够更好描述数据结构的途径;最后,在回归阶段,通过回归运算将虚拟坐标同数据映射联系起来,从而完成对低维特征的学习.由该框架,提出了几种新的维数约简方法用于解决computational paralinguistics问题（如语音情感识别等）.之后,本文将一些先进的算法同所提出的方法在多个语音情感数据库上进行了详细的对比实验.（5）本文提出了基于人脸局部表情的双模态情感识别方法,使用视频信号中的人脸局部表情信息进行情感特征信息增强,并结合语音信号中的paralinguistic特征,进行特征层融合来对情感状态进行分类.实验结果表明,提出的双模态情感识别方法较同条件下的单模态方法,在情感状态识别性能方面有着明显的提升.},
  editora = {赵, 力},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,bimodality,discriminant analysis,emotional feature enhancement,extreme learning machines,graph embedding,multiple kernel learning,paralinguistics,spectral regression,speech emotion recognition,subspace learning,判别分析,双模态,图嵌入,多核学习,子空间学习,情感特征信息增强,极限学习机,语音情感识别,谱回归},
  annotation = {2 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\徐_2017_基于情感特征信息增强的语音情感识别研究.pdf}
}

@inproceedings{xuSpeechEmotionRecognition2021,
  title = {Speech {{Emotion Recognition}} with {{Multiscale Area Attention}} and {{Data Augmentation}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Xu, Mingke and Zhang, Fan and Cui, Xiaodong and Zhang, Wei},
  date = {2021-06},
  pages = {6319--6323},
  issn = {2379-190X},
  doi = {10/gmr2kc},
  abstract = {In Speech Emotion Recognition (SER), emotional characteristics often appear in diverse forms of energy patterns in spectrograms. Typical attention neural network classifiers of SER are usually optimized on a fixed attention granularity. In this paper, we apply multiscale area attention in a deep convolutional neural network to attend emotional characteristics with varied granularities and therefore the classifier can benefit from an ensemble of attentions with different scales. To deal with data sparsity, we conduct data augmentation with vocal tract length perturbation (VTLP) to improve the generalization capability of the classifier. Experiments are carried out on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved 79.34\% weighted accuracy (WA) and 77.54\% unweighted accuracy (UA), which, to the best of our knowledge, is the state of the art on this dataset.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,Acoustics,attention mechanism,Conferences,convolutional neural network,Convolutional neural networks,data augmentation,Emotion recognition,Neural networks,Perturbation methods,speech emotion recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xu et al_2021_Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation.pdf}
}

@article{xuTargetSpeakerVerification2021,
  title = {Target {{Speaker Verification With Selective Auditory Attention}} for {{Single}} and {{Multi-Talker Speech}}},
  author = {Xu, Chenglin and Rao, Wei and Wu, Jibin and Li, Haizhou},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2696--2709},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3100682},
  abstract = {Speaker verification has been studied mostly under the single-talker condition. It is adversely affected in the presence of interference speakers. Inspired by the study on target speaker extraction, e.g., SpEx, we propose a unified speaker verification framework for both single- and multi-talker speech, that is able to pay selective auditory attention to the target speaker. This target speaker verification (tSV) framework jointly optimizes a speaker attention module and a speaker representation module via multi-task learning. We study four different target speaker embedding schemes under the tSV framework. The experimental results show that all four target speaker embedding schemes significantly outperform other competitive solutions for multi-talker speech. Notably, the best tSV speaker embedding scheme achieves 76.0\% and 55.3\% relative improvements over the baseline system on the WSJ0-2mix-extr and Libri2Mix corpora in terms of equal-error-rate for 2-talker speech, while the performance of tSV for single-talker speech is on par with that of traditional speaker verification system, that is trained and evaluated under the same single-talker condition.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Convolution,Decoding,single- and multi-talker speaker verification,speaker extraction,Speech enhancement,Target speaker verification,Task analysis,Time-domain analysis,Training,Voice activity detection},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xu et al_2021_Target Speaker Verification With Selective Auditory Attention for Single and.pdf}
}

@article{XuZhongGuoTeSeGuoJiGuanXiLiLunYuXiJinPingWaiJiaoSiXiangBiTan2021,
  title = {中国特色国际关系理论与习近平外交思想笔谈},
  author = {徐, 坚 and 权, 衡 and 周, 方银 and 王, 公龙 and 张, 颖},
  date = {2021},
  journaltitle = {国际展望},
  volume = {13},
  number = {05},
  pages = {1-24+153-154},
  issn = {1006-1568},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2021&filename=GJZW202105001&uniplatform=NZKPT&v=47LcLgXMtAL3IQCRMGvJOMw8XzZwsiXUSuEN4ecuBOMm799i%25mmd2F0Ak1hQ5auAz9mAP},
  urldate = {2021-11-21},
  abstract = {上海国际问题研究院与中联部世界政党研究所、上海市社联、上海市国际关系学会于2021年5月29日在锦江小礼堂联合举办"习近平外交思想与中国共产党百年对外工作理论创新研讨会",本文是部分与会专家的报告。人类命运共同体理念是加强中国特色国际关系理论建设的重要引领。推陈出新,守正创新,是这一理论建设的重要方法。习近平外交思想在方法论上具有强烈的实践性、深厚的历史观、鲜明的时代性,以具有高度内在一致性的价值观引领外交思想体系。人类命运共同体思想生成于世界大变局背景下,为引导世界大变局朝着人类命运共同体方向演进提供了科学理论指导。双循环新格局是中国与世界经济关系的新定位,加快构建新发展格局,可以实现中国与...},
  issue = {05},
  language = {zh-CN},
  keywords = {⛔ No DOI found,Chinese diplomacy,dual circulation,global governance,international relations theory with Chinese characteristics,methodology,中国外交,中国特色国际关系理论,全球治理,双循环},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\徐 et al_2021_中国特色国际关系理论与习近平外交思想笔谈.pdf}
}

@article{yangChangesFacialExpressions2022,
  title = {Changes in Facial Expressions in Patients with {{Parkinson}}'s Disease during the Phonation Test and Their Correlation with Disease Severity},
  author = {Yang, Liqiong and Chen, Xiangling and Guo, Quanhao and Zhang, Jing and Luo, Man and Chen, Xiaqing and Wen, Yanxia and Zou, Xianwei and Xu, Fan},
  date = {2022-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {72},
  pages = {101286},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101286},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000887},
  urldate = {2021-09-28},
  abstract = {Purpose Parkinson's disease (PD) is a serious neurodegenerative disease affecting more than one million people globally. A mask-like face is an important early motor symptom of PD; it develops gradually and is asymptomatic. However, the relationship between the severity of PD and changes in facial expressions remains unclear. We aimed to elucidate the inner linkage. Method From January to September 2019, we recruited 16 PD patients in the First Affiliated Hospital of Chengdu Medical College and 16 healthy individuals. Participants performed phonation tests, during which their facial expressions were recorded, and the changes were analyzed using Noldus FaceReader 7.0. Results Facial expressions of neutral feelings, happiness, surprise, valence, and arousal during single-, double- and multiple-syllable phonation tests were significantly lower in patients with PD, whereas those of sadness, anger, scared, and disgust were significantly higher compared to those in healthy subjects. There were significant correlations between the changes in facial expressions and the severity of PD. Conclusion Patients with PD displayed a decrease in positive facial expressions, whereas they presented an increase in negative expressions. This is helpful for further research on facial dyskinesia in PD.},
  language = {en},
  keywords = {Disease severity,Facial expressions,Mask face,Parkinson's disease,Phonation test},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yang et al_2022_Changes in facial expressions in patients with Parkinson's disease during the.pdf}
}

@inproceedings{yangDecentralizingFeatureExtraction2021,
  title = {Decentralizing {{Feature Extraction}} with {{Quantum Convolutional Neural Network}} for {{Automatic Speech Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yang, Chao-Han Huck and Qi, Jun and Chen, Samuel Yen-Chi and Chen, Pin-Yu and Siniscalchi, Sabato Marco and Ma, Xiaoli and Lee, Chin-Hui},
  date = {2021-06},
  pages = {6523--6527},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413453},
  abstract = {We propose a novel decentralized feature extraction approach in federated learning to address privacy-preservation issues for speech recognition. It is built upon a quantum convolutional neural network (QCNN) composed of a quantum circuit encoder for feature extraction, and a recurrent neural network (RNN) based end-to-end acoustic model (AM). To enhance model parameter protection in a decentralized architecture, an input speech is first up-streamed to a quantum computing server to extract Mel-spectrogram, and the corresponding convolutional features are encoded using a quantum circuit algorithm with random parameters. The encoded features are then down-streamed to the local RNN model for the final recognition. The proposed decentralized framework takes advantage of the quantum learning progress to secure models and to avoid privacy leakage attacks. Testing on the Google Speech Commands Dataset, the proposed QCNN encoder attains a competitive accuracy of 95.12\% in a decentralized model, which is better than the previous architectures using centralized RNN models with convolutional features. We conduct an in-depth study of different quantum circuit encoder architectures to provide insights into designing QCNN-based feature extractors. Neural saliency analyses demonstrate a high correlation between the proposed QCNN features, class activation maps, and the input Mel-spectrogram. We provide an implementation1 for future studies.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_reading,Acoustic Modeling,and Federated Learning,Automatic Speech Recognition,Collaborative work,Computational modeling,Computer architecture,Convolution,Feature extraction,Predictive models,Quantum Machine Learning,Recurrent neural networks},
  annotation = {ECC: 0000003},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yang et al_2021_Decentralizing Feature Extraction with Quantum Convolutional Neural Network for.pdf}
}

@inproceedings{yangExploitingFinetuningSelfsupervised2022,
  title = {Exploiting {{Fine-tuning}} of {{Self-supervised Learning Models}} for {{Improving Bi-modal Sentiment Analysis}} and {{Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Yang, Wei and Fukayama, Satoru and Heracleous, Panikos and Ogata, Jun},
  date = {2022-09-18},
  pages = {1998--2002},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10354},
  url = {https://www.isca-speech.org/archive/interspeech_2022/yang22q_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yang et al_2022_Exploiting Fine-tuning of Self-supervised Learning Models for Improving.pdf}
}

@article{yangModifiedMagnitudePhaseSpectrum2021,
  title = {Modified {{Magnitude-Phase Spectrum Information}} for {{Spoofing Detection}}},
  author = {Yang, Jichen and Wang, Hongji and Das, Rohan Kumar and Qian, Yanmin},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1065--1078},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3060810},
  abstract = {Most of the existing feature representations for spoofing countermeasures consider information either from the magnitude or phase spectrum. We hypothesize that both magnitude and phase spectra can be beneficial for spoofing detection (SD) when collectively used to capture the signal artifacts. In this work, we propose a novel feature referred to as modified magnitude-phase spectrum (MMPS) to capture both magnitude and phase information from the speech signal. The constant-Q transform is used to obtain the magnitude and phase information in terms of MMPS, which can be denoted as CQT-MMPS. We then use this information for the proposal of a handcrafted feature, namely, constant-Q modified octave coefficients (CQMOC). To evaluate the proposed CQT-MMPS and CQMOC features, three classic anti-spoofing models are adopted, including the Gaussian mixture model (GMM), the light CNN (LCNN) and the ResNet. Additionally, since there is usually no prior knowledge about the spoofing kind in real-world applications, two novel methods referred to as three-class classifiers with maximum spoofing-score (TCMS) and multi-task learning (MTL) are designed for unknown-kind SD (UKSD). The experimental results on ASVspoof 2019 corpus show that CQMOC outperforms most of the commonly-used handcrafted features, and the CQT-based MMPS performs better than the magnitude-phase spectrum and the commonly-used log power spectrum. Further, the MMPS-based systems can achieve comparable or even better performance when compared with the state-of-the-art systems. We find that the newly-designed TCMS and MTL methods outperform the combination-based method for UKSD and meanwhile, generalize much better than the respective-kind-based methods in cross-spoofing-kind evaluation scenarios.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Constant-Q modified octave coefficients,Delays,Discrete Fourier transforms,Feature extraction,Mel frequency cepstral coefficient,modified magnitude-phase spectrum,Speech synthesis,Task analysis,Transforms,unknown-kind spoofing detection},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yang et al_2021_Modified Magnitude-Phase Spectrum Information for Spoofing Detection.pdf}
}

@inproceedings{yangMultimodalEmotionRecognition2022,
  title = {Multimodal {{Emotion Recognition}} with {{Surgical}} and {{Fabric Masks}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yang, Ziqing and Nayan, Katherine and Fan, Zehao and Cao, Houwei},
  date = {2022},
  pages = {4678--4682},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746414},
  abstract = {In this study, we investigate how different types of masks affect automatic emotion classification in different channels of audio, visual, and multimodal. We train emotion classification models for each modality with the original data without mask and the re-generated data with mask respectively, and investigate how muffled speech and occluded facial expressions change the prediction of emotions. Moreover, we conduct the contribution analysis to study how muffled speech and occluded face interplay with each other and further investigate the individual contribution of audio, visual, and audio-visual modalities to the prediction of emotion with and without mask. Finally, we investigate the cross-corpus emotion recognition across clear speech and re-generated speech with different types of masks, and discuss the robustness of speech emotion recognition.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {cross-corpus evaluation,Emotion recognition,Face recognition,mask,muffled speech,multimodal emotion classification,occluded facial expressions,Predictive models,Signal processing,Speech recognition,Surgery,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yang et al_2022_Multimodal Emotion Recognition with Surgical and Fabric Masks.pdf}
}

@article{yangStimuliAwareVisualEmotion2021,
  title = {Stimuli-{{Aware Visual Emotion Analysis}}},
  author = {Yang, Jingyuan and Li, Jie and Wang, Xiumei and Ding, Yuxuan and Gao, Xinbo},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {7432--7445},
  issn = {1941-0042},
  doi = {10.1109/tip.2021.3106813},
  abstract = {Visual emotion analysis (VEA) has attracted great attention recently, due to the increasing tendency of expressing and understanding emotions through images on social networks. Different from traditional vision tasks, VEA is inherently more challenging since it involves a much higher level of complexity and ambiguity in human cognitive process. Most of the existing methods adopt deep learning techniques to extract general features from the whole image, disregarding the specific features evoked by various emotional stimuli. Inspired by the Stimuli-Organism-Response (S-O-R) emotion model in psychological theory, we proposed a stimuli-aware VEA method consisting of three stages, namely stimuli selection (S), feature extraction (O) and emotion prediction (R). First, specific emotional stimuli (i. e., color, object, face) are selected from images by employing the off-the-shelf tools. To the best of our knowledge, it is the first time to introduce stimuli selection process into VEA in an end-to-end network. Then, we design three specific networks, i. e., Global-Net, Semantic-Net and Expression-Net, to extract distinct emotional features from different stimuli simultaneously. Finally, benefiting from the inherent structure of Mikel's wheel, we design a novel hierarchical cross-entropy loss to distinguish hard false examples from easy ones in an emotion-specific manner. Experiments demonstrate that the proposed method consistently outperforms the state-of-the-art approaches on four public visual emotion datasets. Ablation study and visualizations further prove the validity and interpretability of our method.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  language = {en},
  keywords = {_Waiting for read,convolutional neural networks,Deep learning,emotion classification,emotional stimuli,Face recognition,Feature extraction,hierarchical cross-entropy loss,Image color analysis,Psychology,Task analysis,Visual emotion analysis,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yang et al_2021_Stimuli-Aware Visual Emotion Analysis.pdf}
}

@thesis{YanJiYuJuanJiXunHuanShenJingWangLuoDeYuYinQingGanShiBieFangFaYanJiu2020,
  type = {硕士},
  title = {基于卷积循环神经网络的语音情感识别方法研究},
  author = {闫, 振兴},
  date = {2020},
  institution = {{山东大学}},
  doi = {10.27272/d.cnki.gshdu.2020.005356},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020136411.nh&v=},
  urldate = {2021-09-10},
  abstract = {近年来,随着语音识别技术的不断发展,人与机器的语音交互变得越来越自然。然而基于语音技术的人机交互,只是让机器简单的理解语义信息,无法实现让机器理解人类语音背后存在的情感信息。特别是人工智能与大数据技术在各行业的落地应用,将促使机器人越来越多的参与到我们的生活与工作中,如何让机器人与人的交流更加自然、更加和谐将变得尤为重要。语音情感识别作为人机交互的重要一环,将成为人工智能领域研究的一个新热点,让机器人能像人类一样理解和处理情感信息,将成为未来发展的必然趋势。本课题研究了基于卷积循环神经网络的语音情感识别方法,具体研究内容如下:基于卷积双向长短时记忆神经网络CNN-BiLSTM的语音情感识别方法。首先介绍了卷积神经网络和双向长短时记忆网络的基本理论,搭建了面向语音情感识别的CNN-BiLSTM深度学习模型,输入网络模型的语音情感特征是最常用的低维声学特征,围绕影响网络识别性能的因素包括局部特征学习模块个数、初始化学习率以及迭代次数进行相关的实验。针对CNN-BiLSTM网络模型训练收敛速度慢与识别准确率低的问题,提出了改进的语音情感识别方法。介绍了门控循环单元基本理论,门控循环单元神经网络（Gate Recurrent Unit,GRU）是递归神经网络的又一种变体,相比于LSTM网络,门控循环单元神经网络参数相对较少,训练过程更容易收敛,学习效率更高。对注意力机制进行了简单介绍,一段情感语音中存在情感语音帧和非情感语音帧,在语音情感识别中加入注意力机制,能让模型更多的关注语音帧而忽略非情感语音帧。实现了基于注意力机制和CNN-BiGRU网络的语音情感识别。最后,在德国柏林情感数据库Emo-DB上进行与说话者相关的语音情感实验,在七种情绪的平均识别率达到97.8\%。相比改进之前的模型,改进后的模型识别准确率明显提高。本课题的研究成果可以应用到人机交互领域,比如智能辅导系统、测谎、车载驾驶系统、机器人等,从而提高人机交互效率,使机器人在与人类交流时变得更加亲近和自然,促进机器人与人类的和谐共处。},
  editora = {宋, 勇},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Attentional mechanism,Convolutional neural network,Recurrent neural network,Speech emotion recognition,卷积神经网络,循环神经网络,注意力机制,语音情感识别},
  annotation = {👍3[2022-12-4]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\闫_2020_基于卷积循环神经网络的语音情感识别方法研究.pdf}
}

@article{yaoAdaptiveDeepMetric2021,
  title = {Adaptive {{Deep Metric Learning}} for {{Affective Image Retrieval}} and {{Classification}}},
  author = {Yao, Xingxu and She, Dongyu and Zhang, Haiwei and Yang, Jufeng and Cheng, Ming-Ming and Wang, Liang},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {1640--1653},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.3001527},
  abstract = {An image is worth a thousand words. Many researchers have conducted extensive studies to understand visual emotions since an increasing number of users express emotions via images and videos online. However, most existing methods based on convolutional neural networks aim to retrieve and classify affective images in a discrete label space while ignoring both the hierarchical and complex nature of emotions. On the one hand, different from concrete and isolated object concepts (e.g., cat and dog), a hierarchical relationship exists among emotions. On the other hand, most widely used deep methods depend on the representation from fully connected layers, which lacks the essential texture information for recognizing emotions. In this work, we address the above problems via adaptive deep metric learning. Specifically, we design an adaptive sentiment similarity loss, which is able to embed affective images considering the emotion polarity and adaptively adjust the margin between different image pairs. To effectively distinguish affective images, we further propose the sentiment vector that captures the texture information extracted from multiple convolutional layers. Finally, we develop a unified multi-task deep framework to simultaneously optimize both retrieval and classification goals. Extensive and thorough evaluations on four benchmark datasets demonstrate that the proposed framework performs favorably against the state-of-the-art methods.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {_Waiting for read,Affective image retrieval,convolutional neural network,deep metric learning,Feature extraction,Image analysis,Image retrieval,Measurement,Semantics,Task analysis,visual sentiment analysis,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yao et al_2021_Adaptive Deep Metric Learning for Affective Image Retrieval and Classification.pdf}
}

@article{yatabeDeterminedBSSBased2021,
  title = {Determined {{BSS Based}} on {{Time-Frequency Masking}} and {{Its Application}} to {{Harmonic Vector Analysis}}},
  author = {Yatabe, Kohei and Kitamura, Daichi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1609--1625},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3073863},
  abstract = {This paper proposes harmonic vector analysis (HVA) based on a general algorithmic framework of audio blind source separation (BSS) that is also presented in this paper. BSS for a convolutive audio mixture is usually performed by multichannel linear filtering when the numbers of microphones and sources are equal (determined situation). This paper addresses such determined BSS based on batch processing. To estimate the demixing filters, effective modeling of the source signals is important. One successful example is independent vector analysis (IVA) that models the signals via co-occurrence among the frequency components in each source. To give more freedom to the source modeling, a general framework of determined BSS is presented in this paper. It is based on the plug-and-play scheme using a primal-dual splitting algorithm and enables us to model the source signals implicitly through a time-frequency mask. By using the proposed framework, determined BSS algorithms can be developed by designing masks that enhance the source signals. As an example of its application, we propose HVA by defining a time-frequency mask that enhances the harmonic structure of audio signals via sparsity of cepstrum. The experiments showed that HVA outperforms IVA and independent low-rank matrix analysis (ILRMA) for both speech and music signals. A MATLAB code is provided along with the paper for a reference.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Blind source separation (BSS),Cepstrum,cepstrum analysis,Harmonic analysis,independent component analysis (ICA),Linear programming,plug-and-play scheme,proximal splitting algorithm,Signal processing algorithms,Spectrogram,Speech processing,Time-frequency analysis,Wiener-like mask},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yatabe_Kitamura_2021_Determined BSS Based on Time-Frequency Masking and Its Application to Harmonic.pdf}
}

@inproceedings{yehInteractionawareAttentionNetwork2019,
  title = {An {{Interaction-aware Attention Network}} for {{Speech Emotion Recognition}} in {{Spoken Dialogs}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yeh, Sung-Lin and Lin, Yun-Shao and Lee, Chi-Chun},
  date = {2019-05},
  pages = {6685--6689},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8683293},
  abstract = {Obtaining robust speech emotion recognition (SER) in scenarios of spoken interactions is critical to the developments of next generation human-machine interface. Previous research has largely focused on performing SER by modeling each utterance of the dialog in isolation without considering the transactional and dependent nature of the human-human conversation. In this work, we propose an interaction-aware attention network (IAAN) that incorporate contextual information in the learned vocal representation through a novel attention mechanism. Our proposed method achieves 66.3\% accuracy (7.9\% over baseline methods) in four class emotion recognition and is also the current state-of-art recognition rates obtained on the benchmark database.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Acoustics,attention mechanism,Benchmark testing,Context modeling,Emotion recognition,Feature extraction,interaction,Logic gates,speech emotion recognition,Speech recognition,spoken dialogs},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yeh et al_2019_An Interaction-aware Attention Network for Speech Emotion Recognition in Spoken.pdf}
}

@inproceedings{yinProgressiveCoTeachingAmbiguous2021,
  title = {Progressive {{Co-Teaching}} for {{Ambiguous Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yin, Yifei and Gu, Yu and Yao, Longshan and Zhou, Ying and Liang, Xuefeng and Zhang, He},
  date = {2021-06},
  pages = {6264--6268},
  issn = {2379-190X},
  doi = {10.1109/icassp39728.2021.9414494},
  abstract = {Speech emotion recognition is a challenging task due to the ambiguity of emotion, which makes it difficult to learn the features of emotion data using machine learning algorithms. However, previous studies conventionally ignore the ambiguity of emotion and treat the emotion data as the same difficulty level, which results in low recognition accuracy. Motivated by human and animal learning studies, we propose a novel method named Progressive Co-teaching (PCT) to learn speech emotion features from simple to difficult. PCT method automatically identifies the difficulty level of data by itself using loss values, and then each network exchanges easy instances with small loss to peer network for early training. The rest instances with large loss are added gradually for later training. The experiment results demonstrate that our method achieves an improvement of 3.8\% and 1.27\% on MAS and IEMOCAP database than the state-of-the-arts, respectively.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Ambiguous Emotion,Conferences,Databases,Emotion recognition,Machine learning algorithms,Progressive Co-teaching,Signal processing,Speech Emotion Recognition,Speech recognition,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yin et al_2021_Progressive Co-Teaching for Ambiguous Speech Emotion Recognition.pdf}
}

@inproceedings{yoonAttentiveModalityHopping2020,
  title = {Attentive {{Modality Hopping Mechanism}} for {{Speech Emotion Recognition}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yoon, Seunghyun and Dey, Subhadeep and Lee, Hwanhee and Jung, Kyomin},
  date = {2020-05},
  pages = {3362--3366},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054229},
  abstract = {In this work, we explore the impact of visual modality in addition to speech and text for improving the accuracy of the emotion detection system. The traditional approaches tackle this task by independently fusing the knowledge from the various modalities for performing emotion classification. In contrast to these approaches, we tackle the problem by introducing an attention mechanism to combine the information. In this regard, we first apply a neural network to obtain hidden representations of the modalities. Then, the attention mechanism is defined to select and aggregate important parts of the video data by conditioning on the audio and text data. Furthermore, the attention mechanism is again applied to attend the essential parts of speech and textual data by considering other modalities. Experiments are performed on the standard IEMOCAP dataset using all three modalities (audio, text, and video). The achieved results show a significant improvement of 3.65\% in terms of weighted accuracy compared to the baseline system.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,_readed,Aggregates,computational paralinguistics,deep learning,Emotion recognition,natural language processing,speech emotion recognition,Speech processing,Speech recognition,Standards,Task analysis,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yoon et al_2020_Attentive Modality Hopping Mechanism for Speech Emotion Recognition.pdf}
}

@inproceedings{yoonLanguageModelBasedEmotion2022,
  title = {Language {{Model-Based Emotion Prediction Methods}} for {{Emotional Speech Synthesis Systems}}},
  booktitle = {Interspeech 2022},
  author = {Yoon, Hyun-Wook and Kwon, Ohsung and Lee, Hoyeon and Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min and Hwang, Min-Jae},
  date = {2022-09-18},
  pages = {4596--4600},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-11133},
  url = {https://www.isca-speech.org/archive/interspeech_2022/yoon22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yoon et al_2022_Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis.pdf}
}

@article{yoonTutorNetFlexibleKnowledge2021,
  title = {{{TutorNet}}: {{Towards Flexible Knowledge Distillation}} for {{End-to-End Speech Recognition}}},
  shorttitle = {{{TutorNet}}},
  author = {Yoon, Ji Won and Lee, Hyeonseung and Kim, Hyung Yong and Cho, Won Ik and Kim, Nam Soo},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1626--1638},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3071662},
  abstract = {In recent years, there has been a great deal of research in developing end-to-end speech recognition models, which enable simplifying the traditional pipeline and achieving promising results. Despite their remarkable performance improvements, end-to-end models typically require expensive computational cost to show successful performance. To reduce this computational burden, knowledge distillation (KD), which is a popular model compression method, has been used to transfer knowledge from a deep and complex model (teacher) to a shallower and simpler model (student). Previous KD approaches have commonly designed the architecture of the student by reducing the width per layer or the number of layers of the teacher. This structural reduction scheme might limit the flexibility of model selection since the student model structure should be similar to that of the given teacher. To cope with this limitation, we propose a KD method for end-to-end speech recognition, namely TutorNet, that applies KD techniques across different types of neural networks at the hidden representation-level as well as the output-level. For concrete realizations, we firstly apply representation-level knowledge distillation (RKD) during the initialization step, and then apply the softmax-level knowledge distillation (SKD) combined with the original task learning. When the student is trained with RKD, we make use of frame weighting that points out the frames to which the teacher pays more attention. Through a number of experiments, it is verified that TutorNet not only distills the knowledge between networks with different topologies but also significantly contributes to improving the performance of the distilled student.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Computational modeling,connectionist temporal classification,Hidden Markov models,knowledge distillation,Knowledge engineering,Speech processing,Speech recognition,Task analysis,teacher-student learning,Training,transfer learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yoon et al_2021_TutorNet.pdf}
}

@article{yousefiBlockBasedHighPerformance2021,
  title = {Block-{{Based High Performance CNN Architectures}} for {{Frame-Level Overlapping Speech Detection}}},
  author = {Yousefi, Midia and Hansen, John H. L.},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {28--40},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3036237},
  abstract = {Speech technology systems such as Automatic Speech Recognition (ASR), speaker diarization, speaker recognition, and speech synthesis have advanced significantly by the emergence of deep learning techniques. However, none of these voice-enabled systems perform well in natural environmental circumstances, specifically in situations where one or more potential interfering talkers are involved. Therefore, overlapping speech detection has become an important front-end triage step for speech technology applications. This is crucial for large-scale datasets where manual labeling in not possible. A block-based CNN architecture is proposed to address modeling overlapping speech in audio streams with frames as short as 25 ms. The proposed architecture is robust to both: (i) shifts in distribution of network activations due to the change in network parameters during training, (ii) local variations from the input features caused by feature extraction, environmental noise, or room interference. We also investigate the effect of alternate input features including spectral magnitude, MFCC, MFB, and pyknogram on both computational time and classification performance. Evaluation is performed on simulated overlapping speech signals based on the GRID corpus. The experimental results highlight the capability of the proposed system in detecting overlapping speech frames with 90.5\% accuracy, 93.5\% precision, 92.7\% recall, and 92.8\% Fscore on same gender overlapped speech. For opposite gender cases, the network scores exceed 95\% in all the classification metrics.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,1-D CNN,binary classifier,co-channel speech detection,cocktail party problem,Computer architecture,convolutional neural network,Feature extraction,Mel frequency cepstral coefficient,overlapping speech detection,residual learning,simultaneous speaker detection,speech modeling,Speech recognition,speech separation,Time-frequency analysis,Voice activity detection},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yousefi_Hansen_2021_Block-Based High Performance CNN Architectures for Frame-Level Overlapping.pdf}
}

@unpublished{yuanImprovingZeroshotVoice2021,
  title = {Improving {{Zero-shot Voice Style Transfer}} via {{Disentangled Representation Learning}}},
  author = {Yuan, Siyang and Cheng, Pengyu and Zhang, Ruiyi and Hao, Weituo and Gan, Zhe and Carin, Lawrence},
  date = {2021-03-16},
  eprint = {2103.09420},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.09420},
  urldate = {2021-09-10},
  abstract = {Voice style transfer, also called voice conversion, seeks to modify one speaker's voice to generate speech as if it came from another (target) speaker. Previous works have made progress on voice conversion with parallel training data and pre-known speakers. However, zero-shot voice style transfer, which learns from non-parallel data and generates voices for previously unseen speakers, remains a challenging problem. We propose a novel zero-shot voice transfer method via disentangled representation learning. The proposed method first encodes speaker-related style and voice content of each input voice into separated low-dimensional embedding spaces, and then transfers to a new voice by combining the source content embedding and target style embedding through a decoder. With information-theoretic guidance, the style and content embedding spaces are representative and (ideally) independent of each other. On real-world VCTK datasets, our method outperforms other baselines and obtains state-of-the-art results in terms of transfer accuracy and voice naturalness for voice style transfer experiments under both many-to-many and zero-shot setups.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yuan et al_2021_Improving Zero-shot Voice Style Transfer via Disentangled Representation.pdf}
}

@thesis{YuanJiYuANNHeGMMRongHeDeYuYinQingGanShiBieFangFaDeYanJiu2016,
  type = {硕士},
  title = {基于ANN和GMM融合的语音情感识别方法的研究},
  author = {袁, 杰},
  date = {2016},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201701&filename=1016325509.nh&uniplatform=NZKPT&v=PmHaiheILibMJkGzdTLn3PjWwijGCCXwuGxsmFa30AKsFPfxQtfCnAwO%25mmd2F4EzRD1j},
  urldate = {2021-09-10},
  abstract = {人机交互是人与计算机之间使用某种对话语言,以一定的交互方式,为完成确定任务的人与计算机之间的信息交换过程,是计算机智能的重要体现,同时也可以让计算机更好的为人类服务。语音情感识别对发展人机交互来说至关重要。目前,语音情感识别的研究是一门综合认知科学、生理学、心理学、语言学、计算机科学等多学科的热点研究课题,正越来越受到国内外科研机构和研究人员的重视。本文主要围绕人工神经网络和高斯混合模型展开语音情感识别的研究,在原有结构模型的基础上从算法层面入手提出改进的方法,以期提高相关模型的识别精度和识别效率,并在文章最后提出了一种高斯混合模型和神经网络混合的语音情感识别模型。本论文的主要研究内容和创新点如下：（1）阐述了语音情感识别的研究背景与意义,总结了当前国内外的研究现状,并对当前有待深入研究和亟待解决的理论和技术问题进行了说明。（2）概述了与情感相关的一些基础知识,包括情感的定义与情感的分类。设计并录制了汉语语音情感数据库,该库包含高兴、愤怒、惊讶、悲伤等四种基本情感,且全部语音样本都经过有效性检验以确保数据符合规范。完成了语音情感识别过程中需要进行的预处理工作,简述了本文所用到的情感特征参数的提取方式以及情感特征向量的归一化方法。（3）研究了基于Elman神经网络的语音情感识别,并运用万有引力搜索算法（Gravitational Search Algorithm, GSA）以Elman网络进行优化,算法的核心思想是运用万有引力定律通过位置寻优来不断更新网络的权值参数,最终实现网络的最优化。（4）介绍了高斯混合模型（GMM）的EM优化算法,并分析了传统EM算法的缺点。由此本文研究了一种基于改进的GMM算法的语音情感识别方法,该算法通过设定一个初始GMM模型,运用迭代方式不断修正M值和GMM网络的参数,直至得到最终的GMM模型。（5）研究了GMM和深度信念网络融合的语音情感识别方法。在受限玻尔兹曼机（Restricted Boltzmann Machines, RBM）模型基础上构建深度信念网络（Deep Belief Network, DBN）,最后提出了一种多维GMM输出与深度信念神经网络相融合的方法实现语音情感识别。},
  editora = {罗, 琳},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,deep belief network,Elman recurrent neural network,gaussian mixture model,Gravitational search algorithm,speech emotion recognition,万有引力算法,埃尔曼递归神经网络,深度信念网络,语音情感识别,高斯混合模型},
  annotation = {3 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\袁_2016_基于ANN和GMM融合的语音情感识别方法的研究.pdf}
}

@article{yuAttentionLSTMAttentionModelSpeech2020,
  title = {Attention-{{LSTM-Attention Model}} for {{Speech Emotion Recognition}} and {{Analysis}} of {{IEMOCAP Database}}},
  author = {Yu, Yeonguk and Kim, Yoon-Joong},
  date = {2020-05},
  journaltitle = {Electronics},
  volume = {9},
  number = {5},
  pages = {713},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics9050713},
  url = {https://www.mdpi.com/2079-9292/9/5/713},
  urldate = {2023-01-28},
  abstract = {We propose a speech-emotion recognition (SER) model with an ``attention-long Long Short-Term Memory (LSTM)-attention'' component to combine IS09, a commonly used feature for SER, and mel spectrogram, and we analyze the reliability problem of the interactive emotional dyadic motion capture (IEMOCAP) database. The attention mechanism of the model focuses on emotion-related elements of the IS09 and mel spectrogram feature and the emotion-related duration from the time of the feature. Thus, the model extracts emotion information from a given speech signal. The proposed model for the baseline study achieved a weighted accuracy (WA) of 68\% for the improvised dataset of IEMOCAP. However, the WA of the proposed model of the main study and modified models could not achieve more than 68\% in the improvised dataset. This is because of the reliability limit of the IEMOCAP dataset. A more reliable dataset is required for a more accurate evaluation of the model's performance. Therefore, in this study, we reconstructed a more reliable dataset based on the labeling results provided by IEMOCAP. The experimental results of the model for the more reliable dataset confirmed a WA of 73\%.},
  issue = {5},
  language = {en},
  keywords = {attention mechanism,LSTM,speech-emotion recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yu_Kim_2020_Attention-LSTM-Attention Model for Speech Emotion Recognition and Analysis of.pdf}
}

@article{yuAudioVisualMultiChannelIntegration2021,
  title = {Audio-{{Visual Multi-Channel Integration}} and {{Recognition}} of {{Overlapped Speech}}},
  author = {Yu, Jianwei and Zhang, Shi-Xiong and Wu, Bo and Liu, Shansong and Hu, Shoukang and Geng, Mengzhe and Liu, Xunying and Meng, Helen and Yu, Dong},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2067--2082},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3078883},
  abstract = {Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on TF masking, Filter\&Sum and mask-based MVDR neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04\% (31.68\% relative) and 22.86\% (58.51\% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60\%.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {audio-visual,Interpolation,jointly fine-tuning,Lips,Microphone arrays,multi-channel,Overlapped speech recognition,Speech processing,Speech recognition,speech separation,Target recognition,visual occlusion,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yu et al_2021_Audio-Visual Multi-Channel Integration and Recognition of Overlapped Speech.pdf}
}

@article{zahediMinimumProcessingBeamforming2021,
  title = {Minimum {{Processing Beamforming}}},
  author = {Zahedi, Adel and Pedersen, Michael Syskind and \O stergaard, Jan and Christiansen, Thomas Ulrich and Bramsl\o w, Lars and Jensen, Jesper},
  date = {2021-01-01},
  pages = {2710--2724},
  url = {http://ieeexplore.ieee.org/document/9332253},
  abstract = {Most of the well-known classic beamformers have resulted from optimization problems that minimize a cost function such as the mean-square error (MSE) between the noisy speech and a reference clean speech. The rationale behind these formulations involves a speech-versus-noise dichotomy, where anything branded as noise shall be suppressed as much as possible. While leading to simple closed-form solutions and reasonably practical beamformers, this rationale has its own limitations, for instance, when the ambient noise provides context and is therefore not entirely undesirable. In this article, we offer a new rationale, where the output of the beamformer is minimally processed with respect to a certain reference signal, as long as a given performance criterion is fulfilled. We provide a case study where the performance criterion is inspired by the Speech Intelligibility Index (SII), and the processing penalty is MSE. Regarding the reference signal, we consider two cases. In the first case, the reference signal is set to the unprocessed recording from a reference microphone, giving rise to a beamformer that limits the processing of the noisy signal to a minimum necessary for fulfilling the intelligibility requirement. For the second case, the reference signal is the output of an aggressive beamformer, yielding a beamformer that essentially eliminates the noise unless the concomitant distortion of the clean speech violates the intelligibility requirement. Through simulation studies, we demonstrate some of the benefits that each of the two cases offer in relevant contexts.},
  language = {en},
  keywords = {Array signal processing,Beamforming,Frequency estimation,Mean square error methods,multichannel wiener filter,MVDR beamformer,Noise measurement,optimization,speech intelligibility index,Time-frequency analysis,White noise,Wiener filters},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zahedi et al_2021_Minimum Processing Beamforming.pdf}
}

@misc{zeghidourLEAFLearnableFrontend2021,
  title = {{{LEAF}}: {{A Learnable Frontend}} for {{Audio Classification}}},
  shorttitle = {{{LEAF}}},
  author = {Zeghidour, Neil and Teboul, Olivier and Quitry, F\'elix de Chaumont and Tagliasacchi, Marco},
  date = {2021-01-21},
  number = {arXiv:2101.08596},
  eprint = {2101.08596},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.08596},
  url = {http://arxiv.org/abs/2101.08596},
  urldate = {2022-09-14},
  abstract = {Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.},
  archiveprefix = {arXiv},
  language = {en},
  version = {1},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zeghidour et al_2021_LEAF.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\S3V63GR6\\2101.html}
}

@article{zeghidourWavesplitEndtoEndSpeech2021,
  title = {Wavesplit: {{End-to-End Speech Separation}} by {{Speaker Clustering}}},
  shorttitle = {Wavesplit},
  author = {Zeghidour, Neil and Grangier, David},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2840--2849},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3099291},
  abstract = {We introduce Wavesplit, an end-to-end source separation system. From a single mixture, the model infers a representation for each source and then estimates each source signal given the inferred representations. The model is trained to jointly perform both tasks from the raw waveform. Wavesplit infers a set of source representations via clustering, which addresses the fundamental permutation problem of separation. For speech separation, our sequence-wide speaker representations provide a more robust separation of long, challenging recordings compared to prior work. Wavesplit redefines the state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2/3mix), as well as in noisy and reverberated settings (WHAM/WHAMR). We also set a new benchmark on the recent LibriMix dataset. Finally, we show that Wavesplit is also applicable to other domains, by separating fetal and maternal heart rates from a single abdominal electrocardiogram.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Artificial neural networks,Benchmark testing,Convolution,Noise measurement,source separation,Source separation,speech enhancement,Speech processing,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zeghidour_Grangier_2021_Wavesplit.pdf}
}

@inproceedings{zhangAttentionBasedFully2018,
  title = {Attention {{Based Fully Convolutional Network}} for {{Speech Emotion Recognition}}},
  booktitle = {2018 {{Asia-Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  author = {Zhang, Yuanyuan and Du, Jun and Wang, Zirui and Zhang, Jianshu and Tu, Yanhui},
  date = {2018-11},
  pages = {1771--1775},
  publisher = {{IEEE}},
  location = {{Honolulu, HI, USA}},
  doi = {10/ghr7nn},
  url = {https://ieeexplore.ieee.org/document/8659587/},
  urldate = {2021-09-14},
  eventtitle = {2018 {{Asia-Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  isbn = {978-988-14768-5-2},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2018_Attention Based Fully Convolutional Network for Speech Emotion Recognition.pdf}
}

@inproceedings{zhangCrossCorpusSpeechEmotion2021,
  title = {Cross-{{Corpus Speech Emotion Recognition Using Joint Distribution Adaptive Regression}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Jiacheng and Jiang, Lin and Zong, Yuan and Zheng, Wenming and Zhao, Li},
  date = {2021-06},
  pages = {3790--3794},
  issn = {2379-190X},
  doi = {10/gmr2jr},
  abstract = {In this paper, we focus on the research of cross-corpus speech emotion recognition (SER), in which the training and testing speech signals in cross-corpus SER belong to dierent speech corpus. Due to this fact, mismatched feature distributions may exist between the training and testing speech feature sets degrading the performance of most originally well-performing SER methods. To deal with cross-corpus SER, we propose a novel domain adaptation (DA) method called joint distribution adaptive regression (JDAR). The basic idea of JDAR is to learn a regression matrix by jointly considering the marginal and conditional probability distribution between the training and testing speech signals and hence their feature distribution dierence can be alleviated in the subspace spanned by the learned regression matrix. To evaluate the proposed JDAR, we conduct extensive cross-corpus SER experiments on EmoDB, eNTERFACE, and CASIA speech databases. Experimental results show that the proposed JDAR achieves satisfactory performance and outperforms most of state-of-the-art subspace learning based DA methods.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Conferences,Cross-corpus speech emotion recognition,Databases,domain adaptation,Emotion recognition,Probability distribution,Signal processing,speech emotion recognition,Speech recognition,Training,transfer learning.},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_Cross-Corpus Speech Emotion Recognition Using Joint Distribution Adaptive.pdf}
}

@unpublished{zhangDeepFusionAttention2019,
  title = {Deep {{Fusion}}: {{An Attention Guided Factorized Bilinear Pooling}} for {{Audio-video Emotion Recognition}}},
  shorttitle = {Deep {{Fusion}}},
  author = {Zhang, Yuanyuan and Wang, Zi-Rui and Du, Jun},
  date = {2019-01-15},
  eprint = {1901.04889},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.04889},
  urldate = {2021-09-14},
  abstract = {Automatic emotion recognition (AER) is a challenging task due to the abstract concept and multiple expressions of emotion. Although there is no consensus on a definition, human emotional states usually can be apperceived by auditory and visual systems. Inspired by this cognitive process in human beings, it's natural to simultaneously utilize audio and visual information in AER. However, most traditional fusion approaches only build a linear paradigm, such as feature concatenation and multi-system fusion, which hardly captures complex association between audio and video. In this paper, we introduce factorized bilinear pooling (FBP) to deeply integrate the features of audio and video. Specifically, the features are selected through the embedded attention mechanism from respective modalities to obtain the emotion-related regions. The whole pipeline can be completed in a neural network. Validated on the AFEW database of the audio-video sub-challenge in EmotiW2018, the proposed approach achieves an accuracy of 62.48\%, outperforming the state-of-the-art result.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2019_Deep Fusion.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\LL5TDMAC\\1901.html}
}

@article{zhangEmotionAttentionAwareCollaborative2021,
  title = {Emotion {{Attention-Aware Collaborative Deep Reinforcement Learning}} for {{Image Cropping}}},
  author = {Zhang, Xiaoyan and Li, Zhuopeng and Jiang, Jianmin},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {2545--2560},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.3013350},
  abstract = {This paper proposes a collaborative deep reinforcement learning model for automatic image cropping (called CDRL-IC). By modeling image cropping as a decision-making process of reinforcement learning, our model could generate optimal cropping result in a few moving and zooming steps. An image with good composition is a comprehensive result by considering the relative importance of objects and also the spatial organization of visual elements. Therefore, emotion attention information which indicates the relationship and importance between objects is applied together with contextual information of color image for image cropping. In order to sufficiently use the emotion attention map and the color image, they are processed by two collaborative agents. The two agents make their primary learning separately and then share information through an information interaction module for making joint action prediction. In order to efficiently evaluate the cropping quality in the reward function, weighted Intersection Over Union (WIoU) is designed by integrating emotion attention map in the traditional IoU. Our CDRL-IC model is tested on a variety of datasets for both image cropping and thumbnail generation. The experiments show that our CDRL-IC model outperforms state-of-the-art methods on these benchmark datasets.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {Collaboration,Color,emotion attention,Feature extraction,History,image cropping,Learning (artificial intelligence),Microsoft Windows,Photo composition,reinforcement learning,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_Emotion Attention-Aware Collaborative Deep Reinforcement Learning for Image.pdf}
}

@article{zhangEmotionRecognitionMultimodal2021,
  title = {Emotion {{Recognition From Multimodal Physiological Signals Using}} a {{Regularized Deep Fusion}} of {{Kernel Machine}}},
  author = {Zhang, Xiaowei and Liu, Jinyong and Shen, Jian and Li, Shaojie and Hou, Kechen and Hu, Bin and Gao, Jin and Zhang, Tong and Hu, Bin},
  date = {2021-09},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {51},
  number = {9},
  pages = {4386--4399},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2020.2987575},
  abstract = {These days, physiological signals have been studied more broadly for emotion recognition to realize emotional intelligence in human\textendash computer interaction. However, due to the complexity of emotions and individual differences in physiological responses, how to design reliable and effective models has become an important issue. In this article, we propose a regularized deep fusion framework for emotion recognition based on multimodal physiological signals. After extracting the effective features from different types of physiological signals, we construct ensemble dense embeddings of multimodal features using kernel matrices, and then utilize a deep network architecture to learn task-specific representations for each kind of physiological signal from these ensemble dense embeddings. Finally, a global fusion layer with a regularization term, which can efficiently explore the correlation and diversity among all of the representations in a synchronous optimization process, is designed to fuse generated representations. Experiments on two benchmark datasets show that this framework can improve the performance of subject-independent emotion recognition compared to single-modal classifiers or other fusion methods. Data visualization also demonstrates that the final fusion representation exhibits higher class-separability power for emotion recognition.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  issue = {9},
  language = {en},
  keywords = {_Waiting for read,Brain modeling,Deep neural network,emotion recognition,Emotion recognition,Feature extraction,Fuses,Kernel,kernel machine,multimodal fusion,Physiology,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_Emotion Recognition From Multimodal Physiological Signals Using a Regularized.pdf}
}

@article{zhangEmotionRecognitionUsing2020,
  title = {Emotion Recognition Using Multi-Modal Data and Machine Learning Techniques: {{A}} Tutorial and Review},
  shorttitle = {Emotion Recognition Using Multi-Modal Data and Machine Learning Techniques},
  author = {Zhang, Jianhua and Yin, Zhong and Chen, Peng and Nichele, Stefano},
  date = {2020-07-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {59},
  pages = {103--126},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2020.01.011},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253519302532},
  urldate = {2023-01-18},
  abstract = {In recent years, the rapid advances in machine learning (ML) and information fusion has made it possible to endow machines/computers with the ability of emotion understanding, recognition, and analysis. Emotion recognition has attracted increasingly intense interest from researchers from diverse fields. Human emotions can be recognized from facial expressions, speech, behavior (gesture/posture) or physiological signals. However, the first three methods can be ineffective since humans may involuntarily or deliberately conceal their real emotions (so-called social masking). The use of physiological signals can lead to more objective and reliable emotion recognition. Compared with peripheral neurophysiological signals, electroencephalogram (EEG) signals respond to fluctuations of affective states more sensitively and in real time and thus can provide useful features of emotional states. Therefore, various EEG-based emotion recognition techniques have been developed recently. In this paper, the emotion recognition methods based on multi-channel EEG signals as well as multi-modal physiological signals are reviewed. According to the standard pipeline for emotion recognition, we review different feature extraction (e.g., wavelet transform and nonlinear dynamics), feature reduction, and ML classifier design methods (e.g., k-nearest neighbor (KNN), naive Bayesian (NB), support vector machine (SVM) and random forest (RF)). Furthermore, the EEG rhythms that are highly correlated with emotions are analyzed and the correlation between different brain areas and emotions is discussed. Finally, we compare different ML and deep learning algorithms for emotion recognition and suggest several open problems and future research directions in this exciting and fast-growing area of AI.},
  language = {en},
  keywords = {_review,Affective computing,Data fusion,Deep learning,Emotion recognition,Feature dimensionality reduction,Machine learning,Physiological signals},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2020_Emotion recognition using multi-modal data and machine learning techniques.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\CGQSUEWC\\S1566253519302532.html}
}

@inproceedings{zhangEstimatingMutualInformation2021,
  title = {Estimating {{Mutual Information}} in {{Prosody Representation}} for {{Emotional Prosody Transfer}} in {{Speech Synthesis}}},
  booktitle = {2021 12th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  author = {Zhang, Guangyan and Qiu, Shirong and Qin, Ying and Lee, Tan},
  date = {2021-01-24},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Hong Kong}},
  doi = {10.1109/ISCSLP49672.2021.9362098},
  url = {https://ieeexplore.ieee.org/document/9362098/},
  urldate = {2021-09-10},
  abstract = {An end-to-end prosody transfer system aims to transfer the speech prosody from one speaker to another speaker. One major application is the generation of emotional speech with a new speaker's voice. The end-to-end system uses an intermediate representation of prosody, which encompasses both speaker and emotion related information. The present study tackles the problem of estimating the mutual information between emotion and speaker-related factors in the prosody representation. A mutual information neural estimator (MINE) which could measure the mutual information between high-dimensional continuous prosody embedding and discrete speaker/emotion label is applied. The experimental results show that: 1) the prosody representation generated by the end-to-end system indeed contains both emotion and speaker information; 2) The mutual information would be determined by the type of input acoustic features to the reference encoder; 3) normalization for the log F0 feature is very effective in increasing emotion-related information in the prosody representation; 4) adversarial learning can be applied to reduce speaker information in the prosody representation. These results are useful to the further development of an optimal and practical emotional prosody transfer systems.},
  eventtitle = {2021 12th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  isbn = {978-1-72816-994-1},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_Estimating Mutual Information in Prosody Representation for Emotional Prosody.pdf}
}

@article{zhangExtractingPredictingWordLevel2021,
  title = {Extracting and {{Predicting Word-Level Style Variations}} for {{Speech Synthesis}}},
  author = {Zhang, Ya-Jie and Ling, Zhen-Hua},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1582--1593},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3074757},
  abstract = {This paper proposes a speech synthesis method based on unsupervisedly-learned fine-grained style representations, named word-level style variations (WSVs), in order to improve the naturalness of synthetic speech. The whole model contains a WSV extractor and a WSV predictor. The WSV extractor is jointly trained with a sequence-to-sequence (Seq2seq) synthesizer and learns a WSV vector from the mel-spectrogram of each prosodic word in the training set by extending the global style token (GST) framework. In contrast to GST weights which describe the global styles of utterances, WSVs operate at word-level and are expected to describe local style properties, such as stresses. Besides, Gumbel softmax is adopted and the extracted WSVs are close to one-hot vectors which facilitate the subsequent prediction task. The WSV predictor is a deterministic model which generates the sequence of WSV vectors from input text using an autoregressive LSTM network. In addition to phonetic information, e.g., phoneme sequences, Bidirectional Encoder Representation from Transformers (BERT) model is employed by the predictor to obtain the semantic descriptions of input text for better predicting the latent speech representation, i.e., WSVs. The WSV predictor is trained by considering both the accuracy of WSV prediction and the distortion of mel-spectrograms recovered from the predicted WSVs. Experimental results show that our proposed method can achieve better naturalness of synthetic speech than baseline Tacotron2, text-predicted global style token (TP-GST) and BERT-Tacotron2 models.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {BERT,Bit error rate,Data mining,neural network,Predictive models,Semantics,sequence-to-sequence,Speech synthesis,Synthesizers,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang_Ling_2021_Extracting and Predicting Word-Level Style Variations for Speech Synthesis.pdf}
}

@inproceedings{zhangImpactBackgroundNoise2022,
  title = {Impact of {{Background Noise}} and {{Contribution}} of {{Visual Information}} in {{Emotion Identification}} by {{Native Mandarin Speakers}}},
  booktitle = {Interspeech 2022},
  author = {Zhang, Minyue and Ding, Hongwei},
  date = {2022-09-18},
  pages = {1993--1997},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10142},
  url = {https://www.isca-speech.org/archive/interspeech_2022/zhang22r_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang_Ding_2022_Impact of Background Noise and Contribution of Visual Information in Emotion.pdf}
}

@article{zhangIntegratingPartSpeech2021,
  title = {Integrating {{Part}} of {{Speech Guidance}} for {{Image Captioning}}},
  author = {Zhang, Ji and Mei, Kuizhi and Zheng, Yu and Fan, Jianping},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {92--104},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.2976552},
  abstract = {To generate an image caption, firstly, the content of the image should be fully understood; and then the semantic information contained in the image should be described using a phrase or statement that conforms to certain grammatical rules. Thus, it requires techniques from both computer vision and natural language processing to connect the two different media forms together, which is highly challenging. To adaptively adjust the effect of visual information and language information on the captioning process, in this paper, the part of speech information is proposed to novelly integrate with image captioning models based on the encoder-decoder framework. First, a part of speech prediction network is proposed to analyze and model the part of speech sequences for the words in natural language sentences; then, different mechanisms are proposed to integrate the part of speech guidance information with merge-based and inject-based image captioning models, respectively; finally, according to the integrated frameworks, a multi-task learning paradigm is proposed to facilitate model training. Experiments are conducted on two widely used image captioning datasets, Flickr30 k and COCO, and the results have validated that the image captions generated by the proposed method contain more accurate visual information and comply with language habits and grammar rules better.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {Computer vision,Feature extraction,image captioning,multi-task learning,Part of speech,Predictive models,Semantics,Speech processing,Task analysis,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_Integrating Part of Speech Guidance for Image Captioning.pdf}
}

@thesis{ZhangJiYuBiaoQingHeYuYinXinHaoDeQingGanShiBieYanJiu2020,
  type = {硕士},
  title = {基于表情和语音信号的情感识别研究},
  author = {张, 鹤鹏},
  date = {2020},
  institution = {{山东大学}},
  doi = {10.27272/d.cnki.gshdu.2020.004267},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020137798.nh&uniplatform=NZKPT&v=BnAGg54SVKUwZzpEfEL6QipYFSGYaiy0jFd61Gir7f5q2kP3%25mmd2BReFDyT0x0yN96Iu},
  urldate = {2021-09-10},
  abstract = {随着人口老龄化现象和空巢老人数量的增加,家庭服务机器人成为研究热点。机器人对人类情感的自主分析有助于为人类提供更好的服务。在日常生活中,由于我们对情感信息的获取多来自于表情和语音信号,因此面部表情识别（Facial Expression Recognition FER）和语音情感识别（Speech Expression Recognition SER）就成为了情感识别研究的重要组成部分。同时,随着AI技术和计算机视觉技术的不断发展,基于深度学习和图像处理的情感识别方法被广泛应用。在此基础上,本文针对如何进一步提高面部表情识别和语音情感识别的准确率问题进行了研究。面部表情识别通常可以分为静态面部表情图像识别和面部表情图像序列识别两种。针对静态面部表情图像识别,本文对图像背景信息影响表情识别准确率的问题,利用提取面部前景图像的方法来提高识别率,对仅用单一图像特征导致表情识别效果不好的问题,本文利用RGB图像通道和局部二值模式图像通道融合的方法,提出了双通道权值融合的卷积神经网络（Double-channel weighted mixture convolution neural networks WMCNN）。模型在公开表情数据集 CK+、JAFFE、Oulu 和 MMI 上的识别率分别为 99.07\%、92.38\%、86.034\%和 78.24\%,通过与现有方法对比发现,我们的模型进一步提高了表情识别的准确率。同时,通过与单通道识别网络的识别结果进行对比,可以发现通过增加LBP图像通道可以有效的提高表情识别的准确率。针对常见的公开面部表情数据集样本数量小导致网络泛化性能差的问题,本文利用生成对抗网络（Expression Gan ExGAN）来扩充现有的表情数据集,构建了 Our-DB表情数据集。通过实验验证了使用扩充数据集训练WMCNN模型能够更好的提高模型的泛化性能,提高模型的识别效果。另外,本文针对网络对难分类表情识别率较差的问题,通过增加注意力网络和注意力损失,在WMCNN网络模型的基础上提出了双通道权值融合的注意力卷积神经网络（Attention convolution neural network based on two channel weight mixture AWMCNN）。并在Our-DB数据集上验证了 AWMCNN模型比WMCNN模型有更好的识别效果,能更好的识别难分类表情样本。针对单帧图像容易导致表情误识别的问题,使用基于视频序列的面部表情识别方法,在上述静态表情图像识别方法的基础上,结合循环神经网络提出了双通道权值融合的卷积长短期记忆网络（WMCNN-LSTM）和双通道权值融合的注意力卷积双向长短期记忆网络（AWMCNN-BILSTM）用于提升表情识别的准确率。最终,本文对WMCNN-LSTM模型在CK+、Oulu和MMI数据集上进行了十折交叉验证实验,实验结果分别为98.75\%、87.91\%和87.14\%。与基于静态表情图像的模型相比,WMCNN-LSTM网络可以进一步提高面部表情识别的准确率。同时,为了说明AWMCNN-BILSTM网络模型的识别效果,本文对WMCNN-LSTM网络和AWMCNN-BILSTM网络在Our-DB数据集上进行实验,实验结果分别为90.438\%和91.825\%,通过对比可以发现AWMCNN-BILSTM网络与WMCNN-LSTM网络相比可以更好的识别面部表情图像序列。在语音情感识别中,针对使用单一语音特征导致表情识别准确率不高的问题,本文展现了语谱图和3-D Log-Mels特征图在语音情感信号上的表现能力,通过结合这两种语音特征提出了 AWMCNN-BILSTM网络。该模型使用语谱图和3-D Log-Mels特征图作为两个通道的输入特征,两个通道的输出在决策层按照加权融合方法得到最终的识别结果。模型在公开语音情感数据集IEMOCAP和EMO-DB上的未加权准确率分别为69.2\%和93.05\%,与其他现有方法相比,均取得了较高的识别率。},
  editora = {黄, 彬},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,CNN,Facial expression recognition,GAN,RNN,Speech emotion recognition,卷积神经网络,循环神经网络,语音情感识别,面部表情识别},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\张_2020_基于表情和语音信号的情感识别研究.pdf}
}

@thesis{ZhangJiYuQianYiXueXiHeZiXueXiQingGanBiaoZhengDeQingGanYuYinHeCheng2019,
  type = {硕士},
  title = {基于迁移学习和自学习情感表征的情感语音合成},
  author = {张, 亚强},
  date = {2019},
  institution = {{北京邮电大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201902&filename=1019047370.nh&uniplatform=NZKPT&v=MBImbLqJgJUihS8DNUMJQbyhAMZk2KwF8%25mmd2BxJh6u7AZmzF00YJ06zsu7gDv6Rqgoq},
  urldate = {2021-09-13},
  abstract = {随着计算机科学和人工智能等领域的发展,作为人机交互核心技术的语音合成技术已经较为成熟,但目前语音合成技术主要针对中性语音的合成,情感语音合成技术仍然有待提高。情感作为一种重要的信息,会很大程度上改变语音所表达的内容,在缺少情感信息时会造成表达有歧义、人机沟通不顺畅等问题。本文针对情感语音合成中的情感表征问题进行分析,提出了一种自学习情感表征方法,并且基于自学习情感表征提出了一种情感语音合成方法,主要研究内容如下:1.针对现有情感表征对情感的描述力不够、不同人进行情感语音标注时存在差异以及人力标注代价过大等问题,提出了一种自学习情感表征方法,该方法通过一种自编码网络对语音中的情感信息建模,将语音中的情感信息进行量化,在训练时使用了对抗训练的方法保证在情感建模的过程中不受到说话人个体差异的影响。实验结果表示,自学习情感表征在不需要人工参与的情况下具有较好的情感描述能力,解决了标注代价大和个体标注差异性的问题。2.提出了一种基于迁移学习和自学习情感表征的情感语音合成方法。该方法将文本无关说话人验证任务中的说话人判别模型迁移至情感语音合成方法中,用来提取说话人的个性化信息;将个性化信息、自学习情感表征与文本内容输入到端到端情感语音合成器中,合成梅尔语谱图;通过WaveNet声码器将梅尔语谱图转化为情感语音。该方法在训练时不需要情感标注信息和说话人标签信息,相比于其他情感语音合成方法更加灵活,实验结果表示该方法可以合成自然度和情感度较高的情感语音。},
  editora = {谢, 东亮},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {adversarial training,emotion modeling,emotional speech synthesis,self-learning emotion representation,transfer learning,对抗训练,情感建模,情感语音合成,自学习情感表征,迁移学习},
  annotation = {2 citations(CNKI)[2021-9-13]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\张_2019_基于迁移学习和自学习情感表征的情感语音合成.pdf}
}

@thesis{ZhangJiYuYunFuWuDeQianRuShiRenJiYuYinJiaoHuXiTong2017,
  type = {硕士},
  title = {基于云服务的嵌入式人机语音交互系统},
  author = {张, 霞},
  date = {2017},
  institution = {{苏州大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1018034355.nh&uniplatform=NZKPT&v=RiWxBsnDApy9r6CDOaVN7RBBYU9YCRP1pYcuzXJ%25mmd2BtnYXZUPFu92Ggi7QroxyxeYV},
  urldate = {2021-09-10},
  abstract = {随着嵌入式技术的日益进步以及信息技术的快速发展,语音交互技术在嵌入式上的应用成为目前的研究热点,包括语音交互设备、语音识别、语音合成以及说话者识别等相关技术。同时深度学习和云计算技术的发展有效推动了语音交互技术的进展。为此,研究基于云服务的嵌入式人机语音交互系统的设计及其相关技术具有重要的理论意义及实用价值。本文在介绍语音信号处理基本技术的基础上,重点研究了基于麦克风阵列的声源定位技术,设计了平面四元十字方阵声源定位模型,推导了平面四元十字方阵基于时延参数的目标定位方程。在对广义互相关法时延估计算法分析基础上,引入了低通滤波和削波预处理以提升时延估计的性能,进而本文提出了基于线性预测残差的广义互相关算法,提升了时延估计性能。本文设计一个基于TMS320DM368数字媒体处理器的嵌入式人机语音交互平台。该平台采用模块化的设计方式,主要包括主处理器模块、电源模块、存储模块、网络模块、语音采集及输出模块等。论文同时在所设计的嵌入式平台上实现了Linux操作系统以及相关驱动移植。在搭建完成的系统平台上进行了上层应用开发,实现了多路麦克风语音信号的采集,完成了所设计的基于时延估计语音定位算法的移植,设计了微软云（Microsoft Azure）接口程序实现了基于云服务的语音识别与语音合成,并实现了合成语音的播放功能。本文在实验室环境下对所设计的嵌入式语音交互系统进行了测试,测试结果表明所设计的语音定位算法能实现方位角和俯仰角的估计精度在10度以内,能正确完成命令字的识别、合成及播放功能,完成了智能人机交互系统的框架搭建,为后续的研究提供了基础。},
  editora = {胡, 剑凌},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Cloud Service,Embedded System,Human-Machine Speech interaction,Sound Source Localization,云服务,人机语音交互,声源定位,嵌入式系统},
  annotation = {2 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\张_2017_基于云服务的嵌入式人机语音交互系统.pdf}
}

@thesis{ZhangKuaKuYuYinQingGanShiBieRuoGanGuanJianJiShuYanJiu2016,
  type = {博士},
  title = {跨库语音情感识别若干关键技术研究},
  author = {张, 昕然},
  date = {2016},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2017&filename=1017116637.nh&uniplatform=NZKPT&v=qlP%25mmd2BiSa9w0kSllJBVzMZVE8FuWFK5NBnKk6JTlglc8mO45hR4wqILtHNfofnzzDB},
  urldate = {2021-09-10},
  abstract = {语音情感识别（Speech Emotion Recognition,SER）是目前情感计算、模式识别、信号处理和人机交互领域的热门研究话题。SER的主要目的是对语音信号按照不同的情感进行分类,比如"生气"、"恐惧"、"厌恶"、"高兴"等。在过去的几年里,已经提出了许多有效的方法来应对SER中出现的问题。在各种研究方法中,大部分是集中在一个单一的语音数据库上进行的。然而,在许多实际应用情况下,用于训练的语料库与测试语料库存在非常大的差异,例如训练和测试数据库来自两种（或更多种）不同的语言、说话人、文化、分布方式、数据规模等。这就出现了一个重要的研究内容:跨数据库（Cross-corpus）的语音情感识别。由于SER的研究涉及特征提取、特征优选、分类器改进、特征融合等多个技术部分,因此本文根据其特点,针对跨数据库语音情感识别相关的关键技术进行研究。论文的主要研究内容如下:1.针对跨库语音情感特征优选分类,提出了带有无限成分数的t分布混合模型（iSMM）。它可以直接对多种语音情感样本进行有效的识别。与传统的高斯混合模型（GMM）相比,基于混合t分布的语音情感模型能有效处理样本特征空间中存在异常值的问题。首先,t分布混合模型对用于测试的非典型情感数据能够保持鲁棒性。其次,针对高维空间引起的数据高复杂度和训练样本不足的问题,本文将全局隐空间加入情感模型。这种方法使样本空间被划分的成分数量为无限,形成一个iSMM情感模型。此外,该模型可以自动确定最佳的成分数量,同时满足低复杂性,进而完成多种情感特征数据的分类。为验证所提出的iSMM模型对于不同情感特征分布空间的识别效果,本文在3个数据库上进行仿真实验,分别是:表演型语料库DES、EMO-DB和自发型语料库FAU。它们都是通用的语音情感数据库,且具有高维特征样本和不同的空间分布。在这种实验条件下,验证了各个模型对于特征异常值和高维数据的优选效果以及模型本身的泛化性。结果显示iSMM相比其它对比模型,保持了更稳定的识别性能。因此说明本文提出的基于无限t分布的情感模型,在处理不同来源的语音数据时具有较好的鲁棒性,且对带有离群值的高维情感特征具有良好的优选识别能力。2.结合K近邻、核学习方法、特征线重心法和LDA算法,提出了用于情感识别的LDA+kernel-KNNFLC方法。首先针对过大的先验样本特征数目造成的计算量庞大问题,采用重心准则学习样本距离,改进了核学习的K近邻方法;然后加入LDA对情感特征向量优化,在避免维度冗余的情况下,更好的保证了类间情感信息识别的稳定性。对于跨库领域的研究,关注了独立数据库中不同类别间边界拟合度过高导致的识别性能差异;通过对特征空间再学习,所提出的分类方法优化了情感特征向量的类间区分度,适合于不同语料来源的情感特征分类。在包含高维全局统计特征的两个语音情感数据库上进行了仿真实验。通过降维方案、情感分类器和维度参数进行多组实验对比分析,结果表明:LDA+kernel-KNNFLC方法在同条件下识别性能有显著提升,具有相对稳定的情感类别间分类能力。3.针对跨库条件下情感特征类别的改进（扩充）研究,提出了基于听觉注意模型的语谱图特征提取方法。模型模拟人耳听觉特性,能有效探测语谱图上变化的情感特征。同时,利用时频原子对模型进行改进,取得频率特性信号匹配的优势,从时域上提取情感信息。在语音情感识别技术中,由于噪声环境、说话方式和说话人特质等原因,会造成特征空间分布不匹配的情况。从语音学上分析,该问题多存在于跨数据库情感识别任务中。训练的声学模型和用于测试的语句样本之间的错位,会使语音情感识别性能急剧下降。语谱图的特征能从图像的角度对现有情感特征进行有效的补充。听觉注意机制使模型能提取跨语音数据库中的显著性特征,提高语音情感识别系统的情感辨识能力。仿真实验部分利用文章所提出的方法在跨库情感样本上进行特征提取,再通过典型的分类器进行识别。结果显示:与国际通用的标准方法相比,语谱图情感特征的识别性能提高了约9个百分点,从而验证了该方法对不同数据库具有更好的鲁棒性。4.利用深度学习领域的深度信念模型,提出了基于深度信念网络的特征层融合方法。将语音频谱图中隐含的情感信息作为图像特征,与传统声学情感特征融合。研究解决了跨数据库语音情感识别中,将不同尺度上提取的情感特征相结合的技术难点。利用STB/Itti模型对语谱图进行分析,从颜色、亮度、方向三个角度出发提取语谱图特征;然后研究改进了 DBN网络模型,并利用其对传统声学特征与语谱图特征进行了特征层融合,扩充了特征子集的尺度,提升了情感表征能力。通过在ABC数据库和多个中文数据库上的实验验证,特征融合后的新特征子集相比传统的语音情感特征,其跨数据库识别性能获得了明显提升。5.研究了由跨数据库条件下不同语言的使用和大量非特定说话人引起的SER模型特征自适应问题。根据前面章节所介绍的跨库语音情感识别的内容,对特征参数失真、语谱图特征构造、建模算法对比、在线优化等方面进行了自适应相关的研究,并对具体的实验性能进行了比较分析。首先,讨论了现有的语音情感识别自适应方法。然后,对于跨库的情况,进一步研究了自适应说话人加性特征失真的情况,并给出模型方案。接着,为研究多说话人自适应问题给SER系统带来的影响,对其过程进行建模,将高斯混合模型与学生t分布模型两种统计方法进行对比讨论。再分别利用各自适应方案来获取包括语谱图特征在内的特征函数集。此外,还使用了一些在线数据对特征函数进行了快速优化。最后,在四种不同语言的数据库上（包括:德语、英语、中文和越南语）验证了各自适应方案的有效性。实验结果表明:改进的自适应方案具有良好的说话人特征自适应效果,尤其在处理大量未知说话人的情况下显示了较好的模型参数迁移能力。此外,对于由跨数据库中不同语言对情感特性的影响,从特征自适应角度进行了实验分析和讨论。},
  editora = {赵, 力},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,cross-corpus,deep belief nets,feature adaptation,selective attention mechanism,spectrogram feature,speech emotion,Student's t-distribution,学生t分布,深度信念网络,特征自适应,语谱图特征,语音情感识别,跨数据库,选择注意机制},
  annotation = {5 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\张_2016_跨库语音情感识别若干关键技术研究.pdf}
}

@article{zhangPhaseDCNPhaseEnhancedDualPath2021,
  title = {{{PhaseDCN}}: {{A Phase-Enhanced Dual-Path Dilated Convolutional Network}} for {{Single-Channel Speech Enhancement}}},
  shorttitle = {{{PhaseDCN}}},
  author = {Zhang, Lu and Wang, Mingjiang and Zhang, Qiquan and Wang, Xinsheng and Liu, Ming},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2561--2574},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3092585},
  abstract = {Recent deep neural network (DNN) based single-channel speech enhancement methods have achieved remarkable results in the time-frequency (TF) magnitude domain. To further improve the quality and intelligibility of enhanced speech, the attention to phase enhancement is also increasing. In this paper, we propose a novel dilated convolutional network (DCN) model to simultaneously enhance the magnitude and phase of noisy speech. Unlike the direct complex spectral mapping methods, we take the complex spectrum of the signal as the main target and the ideal ratio mask (IRM) as the auxiliary target in a multi-target learning framework to achieve their complementary advantages. Firstly, a feature extraction module is introduced to achieve the fusion of local and long-term features. Two different targets are learned separately, but share the common feature extraction module, which is helpful to extract more general and suitable features. During the joint learning, the intermediate estimation of the IRM target in the auxiliary path, contributing as the attention gating factors, helps to distinguish the speech or non-speech components of the complex-valued signals in the main path. To leverage more fine-grained long-term contextual information, we introduce a multi-scale dilated convolution approach for feature encoding. Moreover, the proposed model is a causal system, which can fully meet the low latency requirements of real-time speech products. Experimental results show that, compared with other advanced systems, the proposed model not only has better speech denoising performance and phase estimation accuracy, but also generalizes better in the speaker, noise, and channel mismatch cases.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {complex spectrum,Convolution,dilated convolutional network,Feature extraction,Interference,multi-scale,multi-target learning,Neural networks,Noise measurement,Signal to noise ratio,Speech enhancement}
}

@inproceedings{zhangSelectiveMultiTaskLearning2022,
  title = {Selective {{Multi-Task Learning For Speech Emotion Recognition Using Corpora Of Different Styles}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Heran and Mimura, Masato and Kawahara, Tatsuya and Ishizuka, Kenkichi},
  date = {2022},
  pages = {7707--7711},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747466},
  abstract = {While speech emotion recognition (SER) has been actively studied, the amount and variations of training data are limited compared with speech recognition and speaker recognition tasks. Therefore, it is promising to combine multiple corpora to train a generalized SER model. However, the manner of emotion expression is different according to the settings, task domains, and languages. In particular, there is a mismatch between acted datasets and spontaneous datasets since the former includes much more rich and explicit emotion expressions than the latter. In this paper, we investigate effective combination methods based on multi-task learning (MTL) considering the style attribute. We also hypothesize the neutral expression, which has the largest number of samples, is not affected by the style, and thus propose a selective MTL method that applies MTL to emotion categories except for the neutral category. Experimental evaluations using the IEMOCAP database and a call center dataset confirm the effect of the combination of the two corpora, MTL, and the proposed selective MTL.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {Emotion recognition,multi-task learning,multiple corpora,Multitasking,self-attention mechanism,Signal processing,Speaker recognition,speech emotion recognition,Speech recognition,Training,Training data},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2022_Selective Multi-Task Learning For Speech Emotion Recognition Using Corpora Of.pdf}
}

@article{zhangSensorSelectionRelative2021,
  title = {Sensor {{Selection}} for {{Relative Acoustic Transfer Function Steered Linearly-Constrained Beamformers}}},
  author = {Zhang, Jie and Du, Jun and Dai, Li-Rong},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1220--1232},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3064399},
  abstract = {For multi-microphone speech enhancement, different microphones might have different contributions, assome are even marginal. This is more likely to happen in wireless acoustic sensor networks (WASNs), where somesensors might be distant. In this work, we therefore consider sensor selection for linearly-constrained beamformers. Theproposed sensor selection approach is formulated by minimizing the total output noise power and constraining thenumber of selected sensors. As the considered sensor selection problem requires the relative acoustic transfer function(RTF), the covariance whitening based RTF estimation or a direct-path RTF approximation is exploited. For a singletarget source, we can thus substitute the estimated RTF or the assumed RTF to the original problem formulation in orderto design a minimum variance distortionless response (MVDR) beamformer. Alternatively, we can integrate the two RTFsto design a linearly constrained minimum variance (LCMV) beamformer in order to alleviate the effects of RTFestimation/approximation errors. By leveraging the superiority of LCMV beamformers, the proposed approach can beapplied to the multi-source case. An evaluation using a simulated large-scale WASN demonstrates that the integration ofRTFs for the sensor selection based LCMV beamformer can be beneficial as opposed to relying on either of theindividual RTF steered sensor selection based MVDR beamformers. We conclude that the sensors that are close to thetarget source(s) and also some around the coherent interferers are more informative.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Array signal processing,Beamformers,convex optimization,covariance whitening,Microphones,relative acoustic transfer function,sensor selection,speech enhancement,Speech enhancement,Transfer functions,wireless acoustic sensor networks,Wireless communication,Wireless sensor networks},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_Sensor Selection for Relative Acoustic Transfer Function Steered.pdf}
}

@article{zhangStudyReferenceMicrophone2021,
  title = {A {{Study}} on {{Reference Microphone Selection}} for {{Multi-Microphone Speech Enhancement}}},
  author = {Zhang, Jie and Chen, Huawei and Dai, Li-Rong and Hendriks, Richard Christian},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {671--683},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3039930},
  abstract = {Multi-microphone speech enhancement methods typically require a reference position with respect to which the target signal is estimated. Often, this reference position is arbitrarily chosen as one of the reference microphones. However, it has been shown that the choice of the reference microphone can have a significant impact on the final noise reduction performance. In this paper, we therefore theoretically analyze the impact of selecting a reference on the noise reduction performance with near-end noise being taken into account. Following the generalized eigenvalue decomposition (GEVD) based optimal variable span filtering framework, we find that for any linear beamformer, the output signal-to-noise ratio (SNR) taking both the near-end and far-end noise into account is reference dependent. Only when the near-end noise is neglected, the output SNR of rank-1 beamformers does not depend on the reference position. However, in general for rank-r beamformers with r {$>$} 1 (e.g., the multichannel Wiener filter) the performance does depend on the reference position. Based on these, we propose an optimal algorithm for microphone reference selection that maximizes the output SNR. In addition, we propose a lower-complexity algorithm that is still optimal for rank-1 beamformers, but sub-optimal for the general r {$>$} 1 rank beamformers. Experiments using a simulated microphone array validate the effectiveness of both proposed methods and show that in terms of quality, several dB can be gained by selecting the proper reference microphone.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustic distortion,Array signal processing,low-rank approximation,Microphone arrays,Microphones,multi-channel beamform-ing,Noise reduction,reference microphone,relative acoustic transfer function,Signal to noise ratio,Speech enhancement,variable span linear filters},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_A Study on Reference Microphone Selection for Multi-Microphone Speech.pdf}
}

@article{zhangTransferLearningSpeech2021,
  title = {Transfer {{Learning From Speech Synthesis}} to {{Voice Conversion With Non-Parallel Training Data}}},
  author = {Zhang, Mingyang and Zhou, Yi and Zhao, Li and Li, Haizhou},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1290--1302},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3066047},
  abstract = {We present a novel voice conversion (VC) framework by learning from a text-to-speech (TTS) synthesis system, that is called TTS-VC transfer learning or TTL-VC for short. We first develop a multi-speaker speech synthesis system with sequence-to-sequence encoder-decoder architecture, where the encoder extracts the linguistic representations of input text, while the decoder, conditioned on target speaker embedding, takes the context vectors and the attention recurrent network cell output to generate target acoustic features. We take advantage of the fact that TTS system maps input text to speaker independent context vectors, thus re-purpose such a mapping to supervise the training of the latent representations of an encoder-decoder voice conversion system. In the voice conversion system, the encoder takes speech instead of text as the input, while the decoder is functionally similar to the TTS decoder. As we condition the decoder on a speaker embedding, the system can be trained on non-parallel data for any-to-any voice conversion. During voice conversion training, we present both text and speech to speech synthesis and voice conversion networks respectively. At run-time, the voice conversion network uses its own encoder-decoder architecture without the need of text input. Experiments show that the proposed TTL-VC system outperforms two competitive voice conversion baselines consistently, namely phonetic posteriorgram and AutoVC methods, in terms of speech quality, naturalness, and speaker similarity.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Autoencoder,context vector,Decoding,Encoding,Linguistics,non-parallel,Speech synthesis,text -to-speech (TTS),Training,Training data,transfer learning,Transfer learning,voice conversion (VC)},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang et al_2021_Transfer Learning From Speech Synthesis to Voice Conversion With Non-Parallel.pdf}
}

@article{zhangWeaklySupervisedEmotion2021,
  title = {Weakly {{Supervised Emotion Intensity Prediction}} for {{Recognition}} of {{Emotions}} in {{Images}}},
  author = {Zhang, Haimin and Xu, Min},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {23},
  pages = {2033--2044},
  issn = {1941-0077},
  doi = {10.1109/tmm.2020.3007352},
  abstract = {Recognition of emotions in images is attracting increasing research attention. Recent studies show that using local region information helps to improve the recognition performance. Intuitively, emotion intensity maps provide more detailed information than image regions. Inspired by this intuition, we propose an end-to-end deep neural network for image emotion recognition leveraging emotion intensity learning. The proposed network is composed of a first classification stream, an intensity prediction stream and a second classification stream. The intensity prediction stream is built on top of the feature pyramid network to extract multilevel features. The class activation mapping technique is used to generate pseudo intensity maps from the first classification stream to guide the proposed network for emotion intensity learning. The predicted intensity map is integrated into the second classification stream for final emotion recognition. The three streams are trained cooperatively to improve the performance. We evaluate the proposed network for both emotion recognition and sentiment classification on different benchmark datasets. The experimental results demonstrate that the proposed network achieves improved performance compared to previous state-of-the-art approaches.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  language = {en},
  keywords = {Annotations,deep neural networks,emotion intensity prediction,Emotion recognition,Feature extraction,Image emotion recognition,Neural networks,Semantics,Streaming media,Visualization},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhang_Xu_2021_Weakly Supervised Emotion Intensity Prediction for Recognition of Emotions in.pdf}
}

@thesis{ZhangXiJinPingDaGuoWaiJiaoSiXiangShiYuXiaDeZhongEGuanXiYanJiu2021,
  type = {硕士},
  title = {习近平大国外交思想视域下的中俄关系研究},
  author = {张, 鑫},
  date = {2021},
  institution = {{湖南工业大学}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFDTEMP&filename=1021108900.nh&uniplatform=NZKPT&v=po1BQW38ClPROBrNJ8OzGyUShPJu%25mmd2F4eVjGCM2DP0yuJxk%25mmd2FRpBWuDExjY1iXQrOn0},
  urldate = {2021-11-21},
  language = {zh-CN},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\张_2021_习近平大国外交思想视域下的中俄关系研究.pdf}
}

@thesis{ZhangXiJinPingWaiJiaoSiXiangDeLiLunYuShiJian2019,
  type = {硕士},
  title = {习近平外交思想的理论与实践},
  author = {张, 里男},
  date = {2019},
  institution = {{中共辽宁省委党校}},
  url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019177415.nh&uniplatform=NZKPT&v=4s7FOpMcrPTSmV7pjdRRlHt%25mmd2FObC2IJqmAs64OG3wBu%25mmd2F57suw%25mmd2Fy70ccyHHymIoJGu},
  urldate = {2021-11-21},
  abstract = {自党的十八大以来,在以习近平总书记为核心的党中央坚强领导下,我国外交工作取得了巨大的成就,并且提出了一系列新思想新战略。经过对多年来我国外交理论与实践的归纳总结,党中央在2018年中央外事工作会议上首次提出了习近平外交思想,同时确立为党和国家外交工作的指导思想。习近平外交思想以马克思列宁主义、毛泽东思想和中国特色社会主义理论体系为理论基础,全面统筹国内国外两个大局,服务于``两个一百年''奋斗目标和中华民族伟大复兴,具有丰富的思想内涵,是当前我国外交工作的理论指导和行动指南,必须不断坚持。本文首先是绪论部分,分别叙述了选题背景,选题目的和意义,国内外研究现状综述,相关概念界定,本文的研究方法和创新...},
  language = {zh-CN},
  keywords = {diplomatic thought,new era,xi jinping,习近平,外交思想,新时代},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\张_2019_习近平外交思想的理论与实践.pdf}
}

@article{zhaoConvertingForeignAccent2021,
  title = {Converting {{Foreign Accent Speech Without}} a {{Reference}}},
  author = {Zhao, Guanlong and Ding, Shaojin and Gutierrez-Osuna, Ricardo},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2367--2381},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3060813},
  abstract = {Foreign accent conversion (FAC) is the problem of generating a synthetic voice that has the voice identity of a second-language (L2) learner and the pronunciation patterns of a native (L1) speaker. This synthetic voice has been referred to as a ``golden-speaker'' in the pronunciation-training literature. FAC is generally achieved by building a voice-conversion model that maps utterances from a source (L1) speaker onto the target (L2) speaker. As such, FAC requires that a reference utterance from the L1 speaker be available at synthesis time. This greatly restricts the application scope of the FAC system. In this work, we propose a ``reference-free'' FAC system that eliminates the need for reference L1 utterances at synthesis time, and transforms L2 utterances directly. The system is trained in two steps. First, a conventional FAC procedure is used to create a golden-speaker using utterances from a reference L1 speaker (which are then discarded) and the L2 speaker. Second, a pronunciation-correction model is trained to convert L2 utterances to match the golden-speaker utterances obtained in the first step. At synthesis time, the pronunciation-correction model directly transforms a novel L2 utterance into its golden-speaker counterpart. Our results show that the system reduces foreign accents in novel L2 utterances, achieving a 20.5\% relative reduction in word-error-rate of an American English automatic speech recognizer and a 19\% reduction in perceptual ratings of foreign accentedness obtained through listening tests. Over 73\% of the listeners also rated golden-speaker utterances as having the same voice identity as the original L2 utterances.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Accent conversion,acoustic model,Acoustics,Computational modeling,Decoding,sequence-to-sequence voice conversion,speech modification,Speech processing,speech synthesis,Synthesizers,Training,Transforms},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhao et al_2021_Converting Foreign Accent Speech Without a Reference.pdf}
}

@inproceedings{zhaoDeepTransductiveTransfer2022,
  title = {Deep {{Transductive Transfer Regression Network}} for {{Cross-Corpus Speech Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Zhao, Yan and Wang, Jincen and Ye, Ru and Zong, Yuan and Zheng, Wenming and Zhao, Li},
  date = {2022-09-18},
  pages = {371--375},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-679},
  url = {https://www.isca-speech.org/archive/interspeech_2022/zhao22h_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhao et al_2022_Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion.pdf}
}

@article{zhaoDesign3DSteerable2021,
  title = {On the {{Design}} of {{3D Steerable Beamformers With Uniform Concentric Circular Microphone Arrays}}},
  author = {Zhao, Xudong and Huang, Gongping and Chen, Jingdong and Benesty, Jacob},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2764--2778},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3103129},
  abstract = {Circular microphone arrays (CMAs) and concentric CMAs (CCMAs) have been used in a wide range of applications such as smartspeakers and teleconferencing systems because of their flexible steering ability. Although many efforts have been devoted to beamforming with CCMAs, most existing methods consider only the 2-dimensional (2D) case and assume that the sound sources of interest are in the same plane as the sensor array (generally the horizontal plane), which often does not hold true in practical applications. This paper deals with the problem of beamforming with uniform CCMAs (UCCMAs) in the 3-dimensional (3D) space to control the steering of the spatial response and meanwhile form frequency-invariant beampatterns for processing broadband acoustic and speech signals. The major contributions of this work are summarized as follows: 1) it presents an analysis based on the spherical harmonics decomposition about the \$N\$th-order optimal and steerable directivity patterns; 2) a beamforming method is developed in which the beamformer's coefficients are identified by solving a linear system of equations formed by approximating the \$N\$th-order optimal target beampattern with the beamformer's beampattern while the resulting beampattern can be steered flexibly in the 3D space; 3) the sufficient and necessary condition on the array geometry and sensors' placement are given to ensure that the beamformer exists and is unique; and 4) the analytical forms of the directivity factor (DF) and white noise gain (WNG) of the resulting beamformer is given and discussion is presented on what conditions irregularities (deep nulls) in WNG and DF may occur. Simulations are provided to illustrate the property of the developed beamforming methods.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {\#nosource,Array signal processing,Beamforming,concentric circular arrays,directivity factor,Geometry,Harmonic analysis,microphone arrays,Microphone arrays,Sensor arrays,Speech processing,spherical harmonics,Three-dimensional displays,white noise gain}
}

@inproceedings{zhaoMemobertPreTrainingModel2022,
  title = {Memobert: {{Pre-Training Model}} with {{Prompt-Based Learning}} for {{Multimodal Emotion Recognition}}},
  shorttitle = {Memobert},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhao, Jinming and Li, Ruichen and Jin, Qin and Wang, Xinchao and Li, Haizhou},
  date = {2022},
  pages = {4703--4707},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746910},
  abstract = {Multimodal emotion recognition study is hindered by the lack of labelled corpora in terms of scale and diversity, due to the high annotation cost and label ambiguity. In this paper, we propose a multimodal pre-training model MEmoBERT for multimodal emotion recognition, which learns multimodal joint representations through self-supervised learning from a self-collected large-scale unlabeled video data that come in sheer volume. Furthermore, unlike the conventional "pre-train, finetune" paradigm, we propose a prompt-based method that reformulates the downstream emotion classification task as a masked text prediction one, bringing the downstream task closer to the pre-training. Extensive experiments on two benchmark datasets, IEMOCAP and MSP-IMPROV, show that our proposed MEmoBERT significantly enhances emotion recognition performance.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_wreading,Adaptation models,Emotion recognition,Emotion Recognition,Multimodal,Pre-training,Prompt,Robustness,Signal processing,Solid modeling,Speech recognition,Transformers},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhao et al_2022_Memobert.pdf}
}

@inproceedings{zhaoMultilevelFusionWav2vec2022,
  title = {Multi-Level {{Fusion}} of {{Wav2vec}} 2.0 and {{BERT}} for {{Multimodal Emotion Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Zhao, Zihan and Wang, Yanfeng and Wang, Yu},
  date = {2022-09-18},
  pages = {4725--4729},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10230},
  url = {https://www.isca-speech.org/archive/interspeech_2022/zhao22k_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhao et al_2022_Multi-level Fusion of Wav2vec 2.pdf}
}

@unpublished{zhaoNaturalBilingualCodeSwitched2020,
  title = {Towards {{Natural Bilingual}} and {{Code-Switched Speech Synthesis Based}} on {{Mix}} of {{Monolingual Recordings}} and {{Cross-Lingual Voice Conversion}}},
  author = {Zhao, Shengkui and Nguyen, Trung Hieu and Wang, Hao and Ma, Bin},
  date = {2020-10-15},
  eprint = {2010.08136},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2010.08136},
  urldate = {2021-09-10},
  abstract = {Recent state-of-the-art neural text-to-speech (TTS) synthesis models have dramatically improved intelligibility and naturalness of generated speech from text. However, building a good bilingual or code-switched TTS for a particular voice is still a challenge. The main reason is that it is not easy to obtain a bilingual corpus from a speaker who achieves native-level fluency in both languages. In this paper, we explore the use of Mandarin speech recordings from a Mandarin speaker, and English speech recordings from another English speaker to build high-quality bilingual and code-switched TTS for both speakers. A Tacotron2-based cross-lingual voice conversion system is employed to generate the Mandarin speaker's English speech and the English speaker's Mandarin speech, which show good naturalness and speaker similarity. The obtained bilingual data are then augmented with code-switched utterances synthesized using a Transformer model. With these data, three neural TTS models \textendash{} Tacotron2, Transformer and FastSpeech are applied for building bilingual and code-switched TTS. Subjective evaluation results show that all the three systems can produce (near)native-level speech in both languages for each of the speaker.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhao et al_2020_Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of.pdf}
}

@unpublished{zhaoNaturalControllableCrossLingual2021,
  title = {Towards {{Natural}} and {{Controllable Cross-Lingual Voice Conversion Based}} on {{Neural TTS Model}} and {{Phonetic Posteriorgram}}},
  author = {Zhao, Shengkui and Wang, Hao and Nguyen, Trung Hieu and Ma, Bin},
  date = {2021-02-03},
  eprint = {2102.01991},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2102.01991},
  urldate = {2021-09-10},
  abstract = {Cross-lingual voice conversion (VC) is an important and challenging problem due to significant mismatches of the phonetic set and the speech prosody of different languages. In this paper, we build upon the neural text-to-speech (TTS) model, i.e., FastSpeech, and LPCNet neural vocoder to design a new cross-lingual VC framework named FastSpeech-VC. We address the mismatches of the phonetic set and the speech prosody by applying Phonetic PosteriorGrams (PPGs), which have been proved to bridge across speaker and language boundaries. Moreover, we add normalized logarithm-scale fundamental frequency (Log-F0) to further compensate for the prosodic mismatches and significantly improve naturalness. Our experiments on English and Mandarin languages demonstrate that with only mono-lingual corpus, the proposed FastSpeech-VC can achieve high quality converted speech with mean opinion score (MOS) close to the professional records while maintaining good speaker similarity. Compared to the baselines using Tacotron2 and Transformer TTS models, the FastSpeech-VC can achieve controllable converted speech rate and much faster inference speed. More importantly, the FastSpeech-VC can easily be adapted to a speaker with limited training utterances.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhao et al_2021_Towards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural.pdf}
}

@inproceedings{zhengExploringMultitaskLearning2022,
  title = {Exploring {{Multi-task Learning Based Gender Recognition}} and {{Age Estimation}} for {{Class-imbalanced Data}}},
  booktitle = {Interspeech 2022},
  author = {Zheng, Weiqiao and Yang, Ping and Lai, Rongfeng and Zhu, Kongyang and Zhang, Tao and Zhang, Junpeng and Fu, Hongcheng},
  date = {2022-09-18},
  pages = {1983--1987},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-682},
  url = {https://www.isca-speech.org/archive/interspeech_2022/zheng22b_interspeech.html},
  urldate = {2022-10-01},
  eventtitle = {Interspeech 2022},
  language = {en},
  keywords = {_Code},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zheng et al_2022_Exploring Multi-task Learning Based Gender Recognition and Age Estimation for.pdf}
}

@article{zhengMultiFeatureBasedNetwork2021,
  title = {Multi-{{Feature Based Network Revealing}} the {{Structural Abnormalities}} in {{Autism Spectrum Disorder}}},
  author = {Zheng, Weihao and Eilam-Stock, Tehila and Wu, Tingting and Spagna, Alfredo and Chen, Chao and Hu, Bin and Fan, Jin},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Affective Computing},
  volume = {12},
  number = {3},
  pages = {732--742},
  issn = {1949-3045},
  doi = {10.1109/taffc.2018.2890597},
  abstract = {Autism spectrum disorder (ASD) is accompanied with impaired social-emotional functioning, such as emotional regulation and recognition, communication, and related behavior. Study of the alternations of the brain networks in ASD may not only help us in understanding this disorder but also inform us the mechanisms of affective computing in the brain. Although morphological features have been used in the diagnosis of a variety of neurological and psychiatric disorders, these features did not show significant discriminative value in identifying patients with ASD, possibly due to the omission of the information related to the changes in structural similarities among cortical regions. In this study, structural images from 66 high-functioning adults with ASD and 66 matched typically-developing controls (TDC) were used to test the hypothesis of cortico-cortical relationships are abnormal in ASD. Seven morphological features of each of the 360 brain regions were extracted and elastic network was used to quantify the similarities between each target region and all other regions. The similarities were then used to construct multi-feature-based networks (MFN), which were then submitted to a support vector machine classifier to classify the individuals of the two groups. Results showed that the classifier with features of MFN significantly improved the accuracy of discriminating patients with ASD from TDCs (78.63 percent) compared to using morphological features only ({$<$} 65 percent). The combination of MFN features with morphological features and other high-level MFN properties did not further enhance the classification performance. Our findings demonstrate that the variations in cortico-cortical similarities are important in the etiology of ASD and can be used as biomarkers in the diagnostic process.},
  eventtitle = {{{IEEE Transactions}} on {{Affective Computing}}},
  issue = {3},
  language = {en},
  keywords = {Autism,Autism spectrum disorder (ASD),diagnostic biomarker,Feature extraction,Medical diagnostic imaging,multi-feature-based network (MFN),social-emotional functioning,Support vector machines,Surface morphology,Surface reconstruction},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zheng et al_2021_Multi-Feature Based Network Revealing the Structural Abnormalities in Autism.pdf}
}

@thesis{ZhiJiYuShenDuShenJingWangLuoDeQingGanYuYinHeChengDeYanJiu2018,
  type = {硕士},
  title = {基于深度神经网络的情感语音合成的研究},
  author = {智, 鹏鹏},
  date = {2018},
  institution = {{西北师范大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1018167958.nh&uniplatform=NZKPT&v=zxN%25mmd2FWwN%25mmd2FRqorOF6MmsDmh%25mmd2BD4%25mmd2BHCCJQNMt3kQH9VcAa9YWRwZoTtgn4xF7yiXnctM},
  urldate = {2021-09-10},
  abstract = {随着计算机技术和信号处理技术的发展,语音合成的质量得到了较大程度的提升。但目前语音合成研究大都以合成中性语音为主,缺少对情感语音合成的研究。人类语言中包含了丰富的情感信息,这些信息仅靠中性的文本或语音无法充分传达。随着深度学习与人工智能的普及,富有情感的人机交流正在逐渐成为需求,情感语音合成的研究具有了越来越重要的意义。本文建立了一个包含11种典型情感的多个说话人的情感语料库,引入深度神经网络（deep neural network,DNN）模型,实现了基于DNN的情感语音合成与基于DNN的说话人自适应情感语音合成,在此基础上,进一步引入了PAD三维情感模型,实现了PAD情感修正的基于DNN的情感语音合成。论文的主要工作及创新如下:1.建立了多个说话人的典型情感语料库。使用专业录音设备,采用情感诱发方法获取了9位女性说话人的11种情感（放松、惊奇、温顺、喜悦、愤怒、焦虑、厌恶、轻蔑、恐惧、悲伤等）的语音。录音文件以16KHz采样率、16位量化、单声道格式保存为WAV文件。录音人对每种情感录制300句情感语音,情感语料库的建立为情感语音合成奠定了基础。2.提出了基于DNN的情感语音合成与基于DNN的说话人自适应情感语音合成方法。通过DNN的结构选择,建立了基于DNN的情感语音合成模型,采用深度学习方法实现了基于DNN的情感语音合成。在此基础上,采用多个说话人的多种情感语音进行DNN平均音模型训练,并采用目标情感说话人的语音,利用说话人自适应,在平均音模型上获得目标情感语音模型,从而实现目标情感语音的合成。评测结果表明,论文提出的基于DNN的说话人自适应情感语音合成方法合成的情感语音优于其他方法。3.提出了一种基于PAD三维情感模型实现连续情感的情感语音合成方法。采用PAD模型对情感语料库中的语料进行情感标注,通过计算标注点到已知情感点的距离判断语料的情感状态,获得情感的PAD参数。利用基于DNN的情感语音合成方法进行语音参数生成,并将合成的情感语音参数映射到PAD模型,通过对映射点与已知情感的距离计算修正情感参数,最终合成目标情感语音。实验结果表明,该方法合成的情感语音比其他方法具有更高的情感偏好度和主观得分。},
  editora = {杨, 鸿武},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_readed,_中文,_情感语音合成,deep neural network,emotional speech synthesis,Hidden Markov Model,PAD emotion model,PAD情感模型,speaker adaptive,statistical parameter speech synthesis,深度神经网络,统计参数语音合成,说话人自适应,隐马尔可夫模型},
  annotation = {3 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\智_2018_基于深度神经网络的情感语音合成的研究.pdf}
}

@article{ZhiLiYongShuoHuaRenZiGuaYingShiXianJiYuDNNDeQingGanYuYinHeCheng2018,
  title = {利用说话人自适应实现基于DNN的情感语音合成},
  author = {智, 鹏鹏 and 杨, 鸿武 and 宋, 南},
  date = {2018},
  journaltitle = {重庆邮电大学学报(自然科学版)},
  volume = {30},
  number = {05},
  pages = {673--679},
  issn = {1673-825X},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2018&filename=CASH201805013&uniplatform=NZKPT&v=ZU%25mmd2BfANVDHua2zN%25mmd2F8tLsYPl%25mmd2Bdw2upW6fiJnM%25mmd2BOWf2bncJy6Lis7vFTTnQnA%25mmd2FAjhab},
  urldate = {2021-09-10},
  abstract = {为了提高情感语音合成的质量,提出一种采用多个说话人的情感训练语料,利用说话人自适应实现基于深度神经网络的情感语音合成方法。该方法应用文本分析获得语音对应的文本上下文相关标注,并采用WORLD声码器提取情感语音的声学特征;采用文本的上下文相关标注和语音的声学特征训练获得与说话人无关的深度神经网络平均音模型,用目标说话人的目标情感的训练语音和说话人自适应变换获得与目标情感的说话人相关的深度神经网络模型,利用该模型合成目标情感语音。主观评测表明,与传统的基于隐马尔科夫模型的方法比较,该方法合成的情感语音的主观评分更高。客观实验表明,合成的情感语音频谱更接近原始语音。所以,该方法能够提高合成情感语音的自然度和情感度。},
  issue = {05},
  language = {zh-CN},
  keywords = {_中文,_情感语音合成,deep neural network,emotional speech synthesis,hidden Markov model,speak adaptive training,WORLD vocoder,WORLD声码器,深度神经网络,说话人自适应训练,隐马尔可夫模型},
  annotation = {5 citations(CNKI)[2021-9-10]{$<$}北大核心, CSCD{$>$}},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\智 et al_2018_利用说话人自适应实现基于DNN的情感语音合成.pdf}
}

@unpublished{zhouConvertingAnyoneEmotion2020,
  title = {Converting {{Anyone}}'s {{Emotion}}: {{Towards Speaker-Independent Emotional Voice Conversion}}},
  shorttitle = {Converting {{Anyone}}'s {{Emotion}}},
  author = {Zhou, Kun and Sisman, Berrak and Zhang, Mingyang and Li, Haizhou},
  date = {2020-10-13},
  eprint = {2005.07025},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2005.07025},
  urldate = {2021-09-10},
  abstract = {Emotional voice conversion aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. The prior studies on emotional voice conversion are mostly carried out under the assumption that emotion is speaker-dependent. We consider that there is a common code between speakers for emotional expression in a spoken language, therefore, a speaker-independent mapping between emotional states is possible. In this paper, we propose a speaker-independent emotional voice conversion framework, that can convert anyone's emotion without the need for parallel data. We propose a VAW-GAN based encoder-decoder structure to learn the spectrum and prosody mapping. We perform prosody conversion by using continuous wavelet transform (CWT) to model the temporal dependencies. We also investigate the use of F0 as an additional input to the decoder to improve emotion conversion performance. Experiments show that the proposed speaker-independent framework achieves competitive results for both seen and unseen speakers.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhou et al_2020_Converting Anyone's Emotion.pdf}
}

@unpublished{zhouEmotionalVoiceConversion2021,
  title = {Emotional {{Voice Conversion}}: {{Theory}}, {{Databases}} and {{ESD}}},
  shorttitle = {Emotional {{Voice Conversion}}},
  author = {Zhou, Kun and Sisman, Berrak and Liu, Rui and Li, Haizhou},
  date = {2021-05-31},
  eprint = {2105.14762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.14762},
  urldate = {2021-09-10},
  abstract = {In this paper, we first provide a review of the state-of-the-art emotional voice conversion research, and the existing emotional speech databases. We then motivate the development of a novel emotional speech database (ESD) that addresses the increasing research need. With this paper, the ESD database1 is now made available to the research community. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies. As case studies, we implement several state-of-the-art emotional voice conversion systems on the ESD database. This paper provides a reference study on ESD in conjunction with its release.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhou et al_2021_Emotional Voice Conversion.pdf}
}

@article{zhouGeneratingResponsesGiven2021,
  title = {Generating {{Responses With}} a {{Given Syntactic Pattern}} in {{Chinese Dialogues}}},
  author = {Zhou, Yi and Zheng, Xiaoqing and Huang, Xuanjing},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2888--2898},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3110124},
  abstract = {Recently, many efforts have been devoted to generating responses expressing a specific emotion or relating to a given topic in a controlled manner. However, limited attention has been given to generating responses with a specified syntactic pattern, which makes it possible to imitate someone's way of speaking in dialogue. To fulfill this goal, we propose two models to generate syntax-aware responses: a gross-constraint and a specific-constraint model. The former controls the syntactic patterns of generated responses at sentence-level, while the latter works at smaller language units, such as words or phrases, being capable of manipulating the syntactic structures of responses in a more subtle manner. The extensive experimental results on two different datasets show that both the two models not only can generate meaningful responses with a specific and coherent structure but also improve on the diversity of generated responses, with similar gains in readability, relevance, and diversity as measured by human judges.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {_Waiting for read,Chatbots,Computer architecture,Controlled text generation,deep neural networks,Process control,Speech processing,stylized dialogue response generation,syntactic patterns,Syntactics,Task analysis,Training,variational autoencoder},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhou et al_2021_Generating Responses With a Given Syntactic Pattern in Chinese Dialogues.pdf}
}

@inproceedings{zhouSeenUnseenEmotional2021,
  title = {Seen and {{Unseen Emotional Style Transfer}} for {{Voice Conversion}} with {{A New Emotional Speech Dataset}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhou, Kun and Sisman, Berrak and Liu, Rui and Li, Haizhou},
  date = {2021-06},
  pages = {920--924},
  publisher = {{IEEE}},
  issn = {2379-190X},
  doi = {10/gj26ch},
  abstract = {Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. Such networks learn to remember a fixed set of emotional styles. In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_reading,Electrostatic discharges,Emotion recognition,emotional speech dataset,emotional voice conversion,Linguistics,Signal processing,speech emotion recognition (SER),Speech recognition,Training,Transforms},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhou et al_2021_Seen and Unseen Emotional Style Transfer for Voice Conversion with A New.pdf}
}

@article{zhouUnitNetSequencetoSequenceAcoustic2021,
  title = {{{UnitNet}}: {{A Sequence-to-Sequence Acoustic Model}} for {{Concatenative Speech Synthesis}}},
  shorttitle = {{{UnitNet}}},
  author = {Zhou, Xiao and Ling, Zhen-Hua and Dai, Li-Rong},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2643--2655},
  issn = {2329-9304},
  doi = {10.1109/taslp.2021.3093823},
  abstract = {This paper presents UnitNet, a sequence-to-sequence (Seq2Seq) acoustic model for concatenative speech synthesis. Comparing with the Tacotron2 model for Seq2Seq speech synthesis, UnitNet utilizes the phone boundaries of training data and its decoder contains autoregressive structures at both phone and frame levels. This hierarchical architecture can not only extract embedding vectors for representing phone-sized units in the corpus but also measure the dependency among consecutive units, which makes the UnitNet model capable of guiding the selection of phone-sized units for concatenative speech synthesis. A byproduct of this model is that it can also be applied to statistical parametric speech synthesis (SPSS) and improve the robustness of Seq2Seq acoustic feature prediction since it adopts interpretable transition probability prediction rather than attention mechanism for frame-level alignment. Experimental results show that our UnitNet-based concatenative speech synthesis method not only outperforms the unit selection methods using hidden Markov models and Tacotron-based unit embeddings, but also achieves better naturalness and faster inference speed than the SPSS method using FastSpeech and Parallel WaveGAN. Besides, the UnitNet-based SPSS method makes fewer synthesis errors than Tacotron2 and FastSpeech without naturalness degradation.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  language = {en},
  keywords = {Acoustics,Computational modeling,Decoding,Hidden Markov models,Linguistics,Predictive models,sequence-to-sequence,speech synthesis,Speech synthesis,Tacotron,text-to-speech,unit selection},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhou et al_2021_UnitNet.pdf}
}

@thesis{ZhuJiYuBiaoQingHeYuYinDeShuangMoTaiQingGanShiBieYanJiu2017,
  type = {硕士},
  title = {基于表情和语音的双模态情感识別研究},
  author = {朱, 娜},
  date = {2017},
  institution = {{南京邮电大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1017859597.nh&uniplatform=NZKPT&v=Oxf%25mmd2Bz2COtFtWPDnl%25mmd2FdSIll5QSEVjbNa1yCzeXUptT32Mh5oGueDTUvD%25mmd2FlG17qm%25mmd2Bq},
  urldate = {2021-09-10},
  abstract = {情感识别是近十年情感计算领域的研究重点,相关技术的迅速发展有助于实现人机之间高度和谐的交互体验,给信息时代生活的人们带来更多便利。双模态情感识别和多模态情感识别通过引入更多模态的情感特征,突破单模态情感识别中情感特征单一的限制,包含充分的且彼此之间互补的情感信息,使得识别结果更加精准,识别性能较之单模态有明显的优势。人们在表达自身情感时往往会结合多种方式,其中表情和语音是最为直接和明显的,本论文研究基于表情和语音的双模态情感识别,通过研究探讨特征提取、特征融合的相关方法,具体研究工作如下:1.针对本文所涉及的表情和语音这两种模态的情感特征提取,本文提出表情特征采用Gabor特征和分块LBP特征,针对Gabor特征维数过大的问题,采用下采样和提取相应特征点区域的Gabor特征进行初步处理;语音特征采用open SMILE工具箱进行提取,详细介绍了open SMILE的操作原理,选取适当的特征集提取语音特征;最后对所获得的情感特征进行降维。2.针对基于表情和语音的双模态情感识别使用的特征融合方法,本文首先提出采用一种基于核矩阵的特征层融合方法,将两种模态特征映射到核空间之后进行加权组合,然后针对这种方法过程中的不足,进而提出一种基于多核学习的特征层融合方法,给不同模态特征分配多个不同种类不同参数的核函数,采用多个核函数的线性组合代替支持向量机中的单核函数,核函数的权重在学习过程中自动获得。同时,对于决策层融合,本文提出了一种基于后验概率的加权求和以及求积进行运算将两种模态的单模态情感识别结果进行处理得到最终的情感识别分类结果。3.在e NTERFACE'05数据库上进行大量实验,单模态情感识别实验结果表明基于Gabor特征的表情识别效果远好于使用LBP特征,基于表情特征的情感识别的效果远好于语音特征;基于不同特征融合方法的双模态情感识别实验的结果显示采用基于多核学习的特征层融合方法时识别率最高达到84.23\%,与单模态实验结果相比高出近11\%。},
  editora = {卢, 官明},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,Bimodal emotion recognition,facial expression emotion recognition,feature fusion,multiple kernel learning,speech emotion recognition,support vector machine,双模态情感识别,多核学习,支持向量机,特征融合,表情情感识别,语音情感识别},
  annotation = {3 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\朱_2017_基于表情和语音的双模态情感识別研究.pdf}
}

@thesis{ZhuJiYuShenDuXueXiDeYuYinQingGanShiBieFangFaDeYanJiu2016,
  type = {硕士},
  title = {基于深度学习的语音情感识别方法的研究},
  author = {朱, 从贤},
  date = {2016},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201701&filename=1016325504.nh&uniplatform=NZKPT&v=PmHaiheILiaKe5zwbPiJ6nKpKc2134flx4H4GOjHo0tHjF9HffGVYyCcWAv%25mmd2BfcoI},
  urldate = {2021-09-10},
  abstract = {语音是人机交互最自然最理想的方式之一,承载着说话人丰富的情感内容。语音情感识别的终极目标是让机器能够像人类一样通过语音识别人类情感,实现更好的人机交流,这在科技应用场景中有着非常广阔的未来。本文主要研究了基于深度学习的语音情感识别,将深度学习引入语音情感识别算法中,并针对相应的算法提出若干改进应用于语音情感识别中。本论文的主要研究内容如下：（1）学习并了解了语音情感识别的研究背景、意义、历史以及研究现状,重点讨论了语音情感识别的四个研究对象,分别为情感描述模型、情感数据库、情感特征参数以及情感分类算法。（2）设计并录制汉语语音情感数据库,该库包含害怕、厌倦、开心、烦躁、忧虑、伤心和生气七种基本情感状态语音,并经过试听测试。对数据库中的语音信号进行预处理后,并提取出语音能量、过零率、基频、子带能量、MFCC参数以及频谱特征等参数组成情感特征矢量。此外,为了后续研究的需要,本章节还介绍了语谱图这一语音二维表示形式。（3）讨论了深度学习理论的基础知识,包括人工神经网络（ANN）、softmax以及它们的训练算法。这为下面的深度学习理论提供了基本构件。学习了SDA网络的基本原理,并将SDA用于语音情感特征向量的降维处理,研究了SDA提炼深层特征的能力。与传统降维算法对比,表明了SDA降维具有如下两点优势：1、维数控制能力强；2、降维后分类效果好。此外,为了合理运用样本数据的标签信息,运用标签信息进一步提炼语音情感相关特征（DD-AEF）,对比其它特征,证明了DD-AEF特征在语音情感分类能力上具有明显的优势。最后,提出了运用SDA网络提取SDACC谱特征的方法,并实验对比了其与HuWSF谱特征的语音情感分类能力,证明了SDACC克服了HuWSF特征的缺陷,展现了卓越的性能。（4）讨论了CNN网络的基本原理及优势,研究将语谱图用于CNN进行语音情感识别的可行性,为此讨论了语谱图四种分割方式,并得出分段预处理是更好的语谱图分割方式的结论。在此基础上意识到多卷积核在微观和宏观两种尺寸下对语音情感特征有着更好的描述,所以讨论了将两种卷积核用于语音情感识别的模型。接着,根据语音情感显著性特征提取的方式,提出了CNN瓶颈特征（CNN-BN）的提取,提取了与目标标签更为相关且维数更低的特征集。最后讨论了CNN-BN特征维数与情感识别率的关系。（5）讨论了DBN的基本原理以及训练方式。同SDA一样,首先研究了DBN作为降维方式相对于其它降维方式的优劣,通过实验对比了DBN降维后特征与其它方式降维后特征对情感识别率的影响。接着,本章节利用DBN作为频谱提炼的方式,提出了类似于SDACC的DBNCC特征提取方式,为了进一步挖掘谱特征,提出了在分割能量图时,频率轴重叠分割的方式来提取改进的DBNCC特征。最后,通过实验对比了SDACC.传统DBNCC以及本章节提出的改进DBNCC语音情感分类的效果,实验证明了改进DBNCC特征的优越性能。本文创新点如下：（1）基于HuWSF特征提出了SDACC特征、DBNCC特征以及改进的DBNCC特征提取算法;（2）基于双核CNN以及显著性特征提取方式,提出了CNN-BN特征的提取算法。},
  editora = {罗, 琳},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,convolutional neutral network (CNN),deep belief network (DBN),deep learning,speech emotion recognition,stacked denoising autoencoder (SDA),卷积神经网络,带噪自编码神经网络,深度学习,深度置信网络,语音情感识别},
  annotation = {20 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\朱_2016_基于深度学习的语音情感识别方法的研究.pdf}
}

@thesis{ZhuMianXiangYuYinQingGanShiBieDeShenDuXueXiSuanFaYanJiu2018,
  type = {硕士},
  title = {面向语音情感识别的深度学习算法研究},
  author = {朱, 芳枚},
  date = {2018},
  institution = {{东南大学}},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1019821132.nh&uniplatform=NZKPT&v=qFFyKAlrPMbFvOfkY4XW7pHPoV5LfdUy36UTp%25mmd2Fj%25mmd2FxBaMx3dEskrfojIo0Oxa8Q1D},
  urldate = {2021-09-10},
  abstract = {众所周知,语音是人机交互最自然和最理想的方式之一。目前,人机交互中机器虽已实现与人类进行交流的基本需求,但往往忽略了语音中承载的丰富的情感信息,还远无法像人与人交流那般自然和友好。未来,提升人机交互的体验需要语音情感识别的辅助。近年来,深度学习已经在各个领域中都取得了巨大的成功,本文主要研究了基于深度学习的语音情感识别,并提出了若干改进算法用于改善语音情感识别。本文主要工作和创新点如下:（1）学习了语音情感识别的研究背景和意义,并从语音情感领域的四大问题:语音情感描述模型、语音情感数据库、语音情感特征、语音情感识别算法入手总结了相关的研究历史和现状。（2）介绍了在语音情感识别领域的特征处理工作,包括语音信号的预处理;关键特征的提取,如短时能量、短时过零率、共振峰、梅尔倒谱系数等;介绍了语音情感特征参数的全局统计特性的提取;最后介绍了常用的特征降维算法,并详述了本文实验使用的主分成分析算法,对特征进行``白化''和降维,为后续实验提供数据支持。（3）介绍了模式识别、机器学习以及它们之间的联系,并详细研究了在语音情感识别领域常使用的机器学习算法,包括K近邻准则、softmax回归、支持向量机、稀疏表示、神经网络,为后续提出的算法提供算法对比支持。研究了深度学习在特征学习上的优势和一些主流的深度学习结构,为后续章节提供理论支持。（4）提出了一种改进的栈式自编码结构用于语音情感识别,该结构既利用了降噪自编码器的鲁棒性,也利用了稀疏自编码器稀疏性。该结构主要包括2层,第一层使用降噪自编码学习一个比输入特征维数大的隐藏特征;为提高算法性能,第二层采用稀疏自编码从大量神经元中学习稀疏性特征,并基于得到的特征进行训练学习,最终将特征输入分类器中,进行分类识别。算法首先采用逐层预训练的方法,达到网络参数全面初始化的目的,然后通过反向传播算法对整个网络进行微调,从而生成用于识别的栈式自编码网络。实验显示,相较于单独使用栈式降噪或稀疏自编码,该结构具有更好的识别效果。此外,基于CASIA子库的对比实验显示,该结构远远优于K近邻算法,识别率提高了53.7\%,与稀疏表示方法相比提高了29.8\%,比传统支持向量机提高14.28\%,比人工神经网络提高1.9\%。在自行录制语音库中该结构的识别率比人工神经网络提高了1.64\%。（5）提出了一种融合注意力机制的循环神经网络结构,该结构能结合循环神经网络在学习时序数据方面的优势以及注意力机制可以学习特征权重的特点,使用简单的手工特征就能学习到更优的深度加权特征。该结构主要包含4层网络,第一层使用双向循环神经网络学习输入的时间依赖关系;第二层使用单向循环神经网络对特征进行再一次的学习,得到深度特征;第三层使用注意力机制层学习特征的权重,并对特征进行加权融合,使学习到的特征更具表征能力;第四层使用全连层网络对加权后的特征进行学习,并将学习后的特征送入到分类器中进行分类。在CASIAA库的实验表明,该结构的平均识别率最优达到88.19\%,识别率比仅使用RNNs结构高出4\%5\%,并且该结构明显提高了高兴和愤怒这两种情感的识别率。在CASIAB库中的实验表明,该结构最优识别率达到89.21\%,比他人提出的使用深度自编码结构在平均识别率上提高了5.71\%,在不同情感类别上的识别率也均有提高。},
  editora = {赵, 力},
  editoratype = {collaborator},
  language = {zh-CN},
  keywords = {_中文,attention,recurrent neural network,speech emotion recognition,stacked auto-encoders,循环神经网络,栈式自编码,注意力机制,深度学习,语音情感识别},
  annotation = {1 citations(CNKI)[2021-9-10]},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\朱_2018_面向语音情感识别的深度学习算法研究.pdf}
}

@inproceedings{zhuSpeechEmotionRecognition2022,
  title = {Speech {{Emotion Recognition}} with {{Global-Aware Fusion}} on {{Multi-Scale Feature Representation}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhu, Wenjing and Li, Xiang},
  date = {2022},
  pages = {6437--6441},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747517},
  abstract = {Speech Emotion Recognition (SER) is a fundamental task to predict the emotion label from speech data. Recent works mostly focus on using convolutional neural networks (CNNs) to learn local attention map on fixed-scale feature representation by viewing time-varied spectral features as images. However, rich emotional feature at different scales and important global information are not able to be well captured due to the limits of existing CNNs for SER. In this paper, we propose a novel GLobal-Aware Multi-scale (GLAM) neural network\textsuperscript{1} to learn multi-scale feature representation with global-aware fusion module to attend emotional information. Specifically, GLAM iteratively utilizes multiple convolutional kernels with different scales to learn multiple feature representation. Then, instead of using attention-based methods, a simple but effective global-aware fusion module is applied to grab most important emotional information globally. Experiments on the benchmark corpus IEMOCAP over four emotions demonstrates the superiority of our proposed model with 2.5\% to 4.5\% improvements on four common metrics compared to previous state-of-the-art approaches.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_Code,Attention Mechanism,Benchmark testing,Conferences,Convolution,Convolutional neural networks,Emotion recognition,Feature Fusion,Measurement,Multi-scale Features,Speech Emotion Recognition,Speech recognition},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhu_Li_2022_Speech Emotion Recognition with Global-Aware Fusion on Multi-Scale Feature.pdf}
}

@article{zhuTerabitFasterThanNyquistPDM2018,
  title = {Terabit {{Faster-Than-Nyquist PDM}} 16-{{QAM WDM Transmission With}} a {{Net Spectral Efficiency}} of 7.96 b/s/{{Hz}}},
  author = {Zhu, Yixiao and Jiang, Mingxuan and Chen, Zeyu and Zhang, Fan},
  date = {2018-07},
  journaltitle = {Journal of Lightwave Technology},
  volume = {36},
  number = {14},
  pages = {2912--2919},
  issn = {1558-2213},
  doi = {10.1109/JLT.2018.2829341},
  abstract = {In this paper, we experimentally demonstrate a 1.28 Tb/s faster-than-Nyquist (FTN) wavelength division multiplexing (WDM) system with polarization division multiplexed (PDM) 16-ary quadrature amplitude modulation (16-QAM) signal and coherent detection. Note that intersymbol interference (ISI) and intercarrier interference (ICI) are two major problems in FTN-WDM systems. In our experiment, we use a digital brick-wall filter at the transmitter to reduce the signal bandwidth and fit the FTN-WDM channel spacing. In doing so, the ICI can be mostly suppressed when aggregating the WDM channels. The aggressive filtering induced ISI is compensated based on duobinary signal processing at the receiver. Through numerical simulation, we evaluate the robustness of the receiver-side duobinary signal processing against bandwidth truncation ratio of the digital brick-wall filter, the ICI from neighboring channels, and laser phase noise. The FTN WDM transmission is based on five-channel 32 Gbaud PDM 16-QAM signal with 29 GHz channel spacing. The gross data capacity is 1.28 Tb/s (5 \texttimes{} 256 Gb/s). After 80 km standard single-mode fiber transmission, the bit-error rates of the five WDM channels are all below than 7\% hard-decision forward error correction threshold of 4.5 \texttimes{} 10-3. The net bit rate is 1.15 Tb/s and the net optical spectral efficiency achieves a record of 7.96 b/s/Hz for PDM 16-QAM format. To our best knowledge, our work is the first report of Terabit FTN-WDM system with high-order modulation of PDM 16-QAM signal.},
  eventtitle = {Journal of {{Lightwave Technology}}},
  language = {en},
  keywords = {_readed,Adaptive optics,Digital signal processing,faster-than-Nyquist,Optical fibers,Optical filters,Optical receivers,Optical transmitters,wavelength division multiplexing,Wavelength division multiplexing},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhu et al_2018_Terabit Faster-Than-Nyquist PDM 16-QAM WDM Transmission With a Net Spectral.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\3K5SBBGX\\Zhu 等。 - 2018 - Terabit Faster-Than-Nyquist PDM 16-QAM WDM Transmi.pdf}
}

@inproceedings{zhuUnpairedImagetoImageTranslation2017,
  title = {Unpaired {{Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2017},
  pages = {2242--2251},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.244},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X \textrightarrow{} Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y \textrightarrow{} X and introduce a cycle consistency loss to push F(G(X)) {$\approx$} X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  language = {en},
  keywords = {Extraterrestrial measurements,Graphics,Painting,Semantics,Training,Training data},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zhu et al_2017_Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\9SFAAVDW\\zhu2017.pdf.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\TREZGS4Y\\zhu2017.pdf.pdf;E\:\\mypack\\人生规划\\3.进修\\2.升学\\04.硕士学习\\3.研究\\Zotero\\storage\\WLMQTN2E\\Zhu 等。 - 2017 - Unpaired Image-to-Image Translation Using Cycle-Co.pdf}
}

@article{zouAdversarialTrainingSolving2021,
  title = {Adversarial {{Training}} for {{Solving Inverse Problems}} in {{Image Processing}}},
  author = {Zou, Zhengxia and Shi, Tianyang and Shi, Zhenwei and Ye, Jieping},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {2513--2525},
  issn = {1941-0042},
  doi = {10.1109/tip.2021.3053398},
  abstract = {Inverse problems are a group of important mathematical problems that aim at estimating source data x and operation parameters z from inadequate observations y. In the image processing field, most recent deep learning-based methods simply deal with such problems under a pixel-wise regression framework (from y to x) while ignoring the physics behind. In this paper, we re-examine these problems under a different viewpoint and propose a novel framework for solving certain types of inverse problems in image processing. Instead of predicting x directly from y, we train a deep neural network to estimate the degradation parameters z under an adversarial training paradigm. We show that if the degradation behind satisfies some certain assumptions, the solution to the problem can be improved by introducing additional adversarial constraints to the parameter space and the training may not even require pair-wise supervision. In our experiment, we apply our method to a variety of real-world problems, including image denoising, image deraining, image shadow removal, non-uniform illumination correction, and underdetermined blind source separation of images or speech signals. The results on multiple tasks demonstrate the effectiveness of our method.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  language = {en},
  keywords = {bidirectional mapping,deep learning,Degradation,generative adversarial networks,Image denoising,Image processing,Inverse problem,Inverse problems,Linear programming,Task analysis,Training},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zou et al_2021_Adversarial Training for Solving Inverse Problems in Image Processing.pdf}
}

@inproceedings{zouSpeechEmotionRecognition2022,
  title = {Speech {{Emotion Recognition}} with {{Co-Attention Based Multi-Level Acoustic Information}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zou, Heqing and Si, Yuke and Chen, Chen and Rajan, Deepu and Chng, Eng Siong},
  date = {2022},
  pages = {7367--7371},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747095},
  abstract = {Speech Emotion Recognition (SER) aims to help the machine to understand human's subjective emotion from only audio in-formation. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the pro-posed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  language = {en},
  keywords = {_readed,Acoustics,Co-attention mechanism,Data mining,Emotion recognition,Feature extraction,Multi-level acoustic information,Multimodal fusion,Speech emotion recognition,Speech processing,Speech recognition,Task analysis},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zou et al_2022_Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic.pdf}
}

@article{zulfiqarSpectroTemporalProcessingTwoStream2019,
  title = {Spectro-{{Temporal Processing}} in a {{Two-Stream Computational Model}} of {{Auditory Cortex}}},
  author = {Zulfiqar, Isma and Moerel, Michelle and Formisano, Elia},
  date = {2019},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  volume = {13},
  eprint = {32038212},
  eprinttype = {pmid},
  pages = {95},
  issn = {1662-5188},
  doi = {10.3389/fncom.2019.00095},
  abstract = {Neural processing of sounds in the dorsal and ventral streams of the (human) auditory cortex is optimized for analyzing fine-grained temporal and spectral information, respectively. Here we use a Wilson and Cowan firing-rate modeling framework to simulate spectro-temporal processing of sounds in these auditory streams and to investigate the link between neural population activity and behavioral results of psychoacoustic experiments. The proposed model consisted of two core (A1 and R, representing primary areas) and two belt (Slow and Fast, representing rostral and caudal processing respectively) areas, differing in terms of their spectral and temporal response properties. First, we simulated the responses to amplitude modulated (AM) noise and tones. In agreement with electrophysiological results, we observed an area-dependent transition from a temporal (synchronization) to a rate code when moving from low to high modulation rates. Simulated neural responses in a task of amplitude modulation detection suggested that thresholds derived from population responses in core areas closely resembled those of psychoacoustic experiments in human listeners. For tones, simulated modulation threshold functions were found to be dependent on the carrier frequency. Second, we simulated the responses to complex tones with missing fundamental stimuli and found that synchronization of responses in the Fast area accurately encoded pitch, with the strength of synchronization depending on number and order of harmonic components. Finally, using speech stimuli, we showed that the spectral and temporal structure of the speech was reflected in parallel by the modeled areas. The analyses highlighted that the Slow stream coded with high spectral precision the aspects of the speech signal characterized by slow temporal changes (e.g., prosody), while the Fast stream encoded primarily the faster changes (e.g., phonemes, consonants, temporal pitch). Interestingly, the pitch of a speaker was encoded both spatially (i.e., tonotopically) in Slow area and temporally in Fast area. Overall, performed simulations showed that the model is valuable for generating hypotheses on how the different cortical areas/streams may contribute toward behaviorally relevant aspects of auditory processing. The model can be used in combination with physiological models of neurovascular coupling to generate predictions for human functional MRI experiments.},
  language = {en},
  pmcid = {PMC6987265},
  keywords = {auditory cortex,dynamic neuronal modeling,rate coding,sound processing,temporal coding},
  file = {C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zulfiqar et al_2019_Spectro-Temporal Processing in a Two-Stream Computational Model of Auditory.pdf}
}
