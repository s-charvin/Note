---
title: "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"
author: "石昌文"
tags: [""]
description: ""
categories: [""]
keywords:  [""]
type: "笔记"
draft: true
layout: 
data: 2022-08-15 21:40:42
lastmod: 2022-08-27 16:38:27
---

# 重点

- 开源代码

# 摘要

图像到图像的转换是一类视觉和图形问题，其目标是使用对齐图像对的训练集来学习输入图像和输出图像之间的映射关系。然而，对于许多任务，配对训练数据是不可用的。我们提出了一种学习将图像从源域X转换到目标域Y的方法，在没有配对例子的情况下。我们的目标是学习映射G: X→Y，这样G(X)的图像分布与Y的分布就无法区分了，因为使用了对抗性损失。因为这个映射是高度不受约束的，我们将它与一个逆映射F: Y→X结合，并引入一个循环一致性损失来强制F(G(X))≈X(反之亦然)。在没有配对训练数据的情况下，对采集风格转移、物体变形、季节转移、照片增强等任务进行了定性分析。与之前几种方法的定量比较证明了我们的方法的优越性。

如果克劳德•莫奈在1873 年的一个美好春日，把画架放在了 Argenteuil 附近的 Seine 河岸边，他会看到什么呢? 如果当时彩色照片已经被发明了出来，它可能会记录下晴朗的蓝天和映着蓝天的清澈河流。对于莫奈，他就可能会通过纤细的笔触和明亮的调色板，来表达他对这一场景的印象。

又或者，如果莫奈在一个凉爽的夏夜，突然出现在了 Cassis 的小港口，他又会生出怎样的灵感呢? 在莫奈的画廊里随意逛一逛，就能想象出他会如何描绘这个场景: 也许是用柔和的色调，骤然点画，还有一种扁平的动态范围。

尽管我们从来没有在莫奈展览画中看到过有这些场景，但是我们可以想象这一切。而且，如果我们对莫奈的画作和风景有所了解，就可以推断出它们之间的场景差异，从而想象到如果我们将一个场景转换到另一个场景会是什么样子。

在本文中，我们提出了一种方法，它可以学习做这样一件事情：在没有任何配对训练示例的情况下，捕获一个图像集的特殊特征，并弄清楚如何将这些特征转换到另一个图像集中。


这个问题可以更广泛地描述为图像到图像的转换[22]，将一个图像从给定场景的一种表示 x 转换为另一种表示 y。例如，将灰度图转换为颜色图，将图像转换为语义标签，边缘映射为图片。经过计算机视觉、图像处理和图形学等方面多年来的研究，已经可以在有监督的条件下构建强大的转换系统[11,19,22,23,28,33, 45,56,58,62 ]。然而，获得配对的训练数据是困难和昂贵的。例如，对于像语义分割[4]这样的任务，只有少数几个数据集存在，而且它们相对较小。而想要获得图形任务的图像对（像艺术风格化）更加困难，因为所需的数据非常复杂，通常需要艺术创作。对于许多任务，如物体变形(如，斑马↔马)，期望的输出甚至不是很容易被定义。

因此，我们想寻找一种算法，可以学习在没有配对集的域之间进行转换。我们假设不同域之间有一些潜在的关系——例如，它们是同一个潜在场景的两种不同的渲染结果——并试图学习这种关系。尽管我们缺乏配对集形式的监督，但我们可以利用集合层面的监督：我们在域 $X$ 中给出一组图像，在域 $Y$ 中给出不同的集合。我们可以训练一个映射 $G: X \rightarrow Y$ 使得输出 $\hat{y}=G(x), x \in X$ 与对手图像 $y \in Y$ 无法区分，而对手经过训练可以将 $\hat{y}$ 与 $y$ 分开。理论上，这个目标可以在 $\hat{y}$ 上产生与经验分布 $p_{Y}(y)$ 匹配的输出分布（通常，这要求 $G$ 是随机的）[14]，以使最优的模型 $G$ 将域 $X$ 转换为与 $Y$ 相同分布的域 $\hat{Y}$。然而，这样的转换并不能保证单独的输入和输出 $x$ 和 $y$ 以有意义的方式配对 （因为会有无限多的映射 $G$ 会在 $\hat{y}$ 上产生相同的分布）。此外，在实践中，我们发现很难孤立地优化敌对目标：标准程序经常导致众所周知的模式崩溃问题，即所有输入图像映射到相同的输出图像，优化无法取得进展[13]。



为了处理这些问题，我们需要完善已有网络结构。因此，我们利用了翻译中的“cycle consistent”的特性：如果我们把一个句子从英语翻译成法语，然后再从法语翻译成英语，我们应该回到原来的句子[3]。在数学上，如果我们有一个转换函数 $G$ : $X \rightarrow Y$  和另一个转换函数 $F: Y \rightarrow X$，那么 $G$ 和 $F$  应该是互逆的，并且两个映射都应该是双射（一一映射）。基于这一结构假设，我们通过同时训练映射 $G$ 和 $F$，并添加循环一致性损失（cycle consistency loss ）[60]鼓励 $F(G(x)) \approx x$ 和 $G(F(y)) \approx y$ 。将这一损失与域 $X$ 和 $Y$ 上的对抗损失结合在一起。这使得我们能够在没有配对集条件下，实现图像到图像转换方面的全部目标。

我们将我们的方法应用于广泛的应用，包括样式迁移、对象变形、属性迁移和照片增强。我们还与以前的方法进行了比较，这些方法要么依赖于风格和内容的手动定义分解，要么依赖于共享嵌入函数，并表明我们的方法优于这些基线。我们的代码可在 https://github.com/junyanz/CycleGAN 获得。在 https://arxiv.org/abs/1703.10593 查看论文的完整版本 。

相关工作

生成对抗网络 (GAN) [14, 58] 

在图像生成 [5, 35]、图像编辑 [61] 和表示学习 [35, 39, 33] 方面取得了令人瞩目的成果。最近的方法在条件图像生成应用程序中采用了相同的想法，例如 text2image [36]、图像修复 [34] 和未来预测 [32]，以及视频 [50] 和 3D 模型 [53] 等其他领域。 GANs 成功的关键是对抗性损失的想法，它迫使生成的图像在原则上与真实图像无法区分。这对于图像生成任务特别强大，因为这正是许多计算机图形旨在优化的目标。我们采用对抗性损失来学习映射，使得翻译后的图像无法与目标域中的图像区分开来。

图像到图像转换 

图像到图像转换的想法至少可以追溯到 Hertzmann 等人的 Image Analogies [17]，他们在单个输入-输出训练图像对上采用非参数纹理模型 [8] .最近的方法使用输入输出示例的数据集来学习使用 CNN 的参数翻译函数，例如[29]。我们的方法建立在 Isola 等人的“pix2pix”框架之上。 [20]，它使用条件生成对抗网络 [14] 来学习从输入到输出图像的映射。类似的想法已应用于各种任务，例如从草图 [40] 或属性和语义布局 [22] 生成照片。然而，与这些先前的工作不同，我们在没有配对训练示例的情况下学习映射。

Unpaired Image-to-Image Translation 

其他几种方法也解决了不成对设置，其目标是关联两个数据域 X 和 Y 。罗萨莱斯等人。 [37] 提出了一个贝叶斯框架，该框架包括基于从源图像计算的基于补丁的马尔可夫随机场的先验，以及从多个风格图像获得的似然项。最近，CoupledGANs [28] 和跨模态场景网络 [1] 使用权重共享策略来学习跨域的通用表示。与我们的方法同时，Liu 等人。 [27] 结合变分自动编码器 [23] 和生成对抗网络扩展了这个框架。另一行并发工作 [42, 45, 2] 鼓励输入和输出共享某些“内容”特征，即使它们在“风格”上可能不同。他们还使用对抗网络，通过附加条款强制输出接近预定义度量空间中的输入，例如类标签空间 [2]、图像像素空间 [42] 和图像特征空间 [45]。

与上述方法不同，我们的公式不依赖于输入和输出之间任何特定于任务的、预定义的相似性函数，也不假设输入和输出必须位于相同的低维嵌入空间中。这使得我们的方法成为许多视觉和图形任务的通用解决方案。我们直接与第 5.1 节中的几种先前方法进行比较。与我们的工作同时，在这些相同的程序中，Yi 等人。 [55] 受机器翻译中的对偶学习的启发，独立引入了类似的非配对图像到图像翻译目标 [15]。

循环一致性 

使用传递性作为规范结构化数据的一种方式的想法由来已久。在视觉跟踪中，强制执行简单的前后一致性是几十年来的标准技巧[44]。在语言领域，通过“反向翻译和协调”来验证和改进翻译是人工翻译[3]（幽默地包括马克吐温[47]）以及机器[15]使用的一种技术。最近，高阶循环一致性已被用于运动结构 [56]、3D 形状匹配 [19]、共分割 [51]、密集语义对齐 [59、60] 和深度估计 [12]。其中，周等人。 [60] 和戈达尔等人。 [12] 与我们的工作最相似，因为它们使用循环一致性损失作为使用传递性来监督 CNN 训练的一种方式。在这项工作中，我们引入了一个类似的损失来推动 G 和 F 彼此一致。

Neural Style Transfer [11, 21, 48, 10] 

是另一种执行图像到图像转换的方法，它通过将一张图像的内容与另一张图像（通常是绘画）的风格相结合，通过匹配预训练深度特征的 Gram 矩阵统计。另一方面，我们的主要关注点是通过尝试捕获更高级别的外观结构之间的对应关系来学习两个域之间的映射，而不是两个特定图像之间的映射。因此，我们的方法可以应用于其他任务，例如绘画→照片，物体变形等，其中单个样本传输方法表现不佳。我们在 5.2 节中比较了这两种方法。

公式

我们的目标是在给定训练样本 {xi} 和 {yj} 的情况下学习两个域 X 和 Y 之间的映射函数。我们将数据分布表示为pdata(x)和pdata(y)。如图3(a)所示，我们的模型包括两种映射G: X→Y和F: Y→X。此外，我们引入了两个对抗性鉴别器DX和DY，其中DX旨在区分图像{x}和转换后的图像{F(y)};以同样的方式，DY旨在区别{y}和{G(x)}。我们的目标包含两类术语:对抗性损失（adversarial losses）[16]，用于将生成图像的分布与目标域的数据分布进行匹配;循环一致性损失（cycle consistency losses），以防止学习到的映射 G 和 F 相互矛盾。

Adversarial Loss

我们将对抗性损失 [16] 应用于两个映射函数。 对于映射函数 G : X → Y 及其判别器 DY，我们将目标表示为：

其中 G 尝试生成看起来与来自域 Y 的图像相似的图像 G(x)，而 DY 旨在区分生成样本 G(x) 和真实样本 y。 G 旨在针对试图最大化它的对手 D 最小化这个目标，即 minG maxDY LGAN(G, DY , X, Y )。 我们为映射函数 F : Y → X 及其判别器 DX 引入了类似的对抗性损失：即 minF maxDX LGAN(F, DX, Y, X)。

Cycle Consistency Loss

理论上，对抗性训练可以学习映射 G 和 F，它们分别产生与目标域 Y 和 X 相同分布的输出（严格来说，这需要 G 和 F 是随机函数）[15]。然而，如果容量足够大，网络可以将相同的输入图像集映射到目标域中图像的任意随机排列，其中任何学习的映射都可以产生与目标分布匹配的输出分布。因此，仅对抗性损失不能保证学习函数可以将单个输入 xi 映射到所需的输出 yi。为了进一步减少可能的映射函数的空间，我们认为学习的映射函数应该是循环一致的：如图 3（b）所示，对于来自域 X 的每个图像 x，图像平移循环应该能够带来 x回到原始图像，即 x → G(x) → F(G(x)) ≈ x。我们称之为前向循环一致性。类似地，如图 3 (c) 所示，对于来自域 Y 的每个图像 y，G 和 F 也应该满足后向循环一致性：y → F(y) → G(F(y)) ≈ y。我们使用循环一致性损失来激励这种行为：

在初步实验中，我们还尝试用 F(G(x)) 和 x 之间以及 G(F(y)) 和 y 之间的对抗性损失替换这种损失中的 L1 范数，但没有观察到性能提高。 循环一致性损失引起的行为可以在图 4 中观察到：重建图像 F(G(x)) 最终与输入图像 x 紧密匹配。

我们的全部目标是：

其中 λ 控制两个目标的相对重要性。 我们旨在解决：

请注意，我们的模型可以被视为训练两个“自动编码器”[20]：我们学习一个自动编码器 F ◦ G : X → X 和另一个 G◦F : Y → Y 。然而，这些自动编码器都有特殊的内部结构：它们通过中间表示将图像映射到自身，中间表示是将图像转换到另一个域。这种设置也可以看作是“对抗性自动编码器”[34]的一种特殊情况，它使用对抗性损失来训练自动编码器的瓶颈层以匹配任意目标分布。在我们的例子中，X → X 自动编码器的目标分布是域 Y 的目标分布。在第 5.1.4 节中，我们将我们的方法与完整目标的消融进行了比较，包括单独的对抗性损失 LGAN 和单独的循环一致性损失 Lcyc，并凭经验表明这两个目标在获得高质量结果方面起着关键作用。我们还评估了我们的方法，仅在一个方向上进行循环损失，并表明单个循环不足以规范这个约束不足问题的训练。

网络架构我们采用 Johnson 等人的生成网络架构。 [23] 在神经风格迁移和超分辨率方面取得了令人印象深刻的结果。 该网络包含三个卷积、几个残差块 [18]、两个步长为 1 2 的小步长卷积和一个将特征映射到 RGB 的卷积。 我们对 128 × 128 图像使用 6 个块，对 256×256 和更高分辨率的训练图像使用 9 个块。 类似于约翰逊等人。 [23]，我们使用实例归一化 [53]。 对于鉴别器网络，我们使用 70 × 70 PatchGANs [22, 30, 29]，旨在对 70 × 70 重叠图像块是真还是假进行分类。 这种补丁级鉴别器架构比全图像鉴别器具有更少的参数，并且可以以完全卷积的方式处理任意大小的图像[22]。

训练细节我们应用了最近工作中的两种技术来稳定我们的模型训练过程。首先，对于 LGAN（等式 1），我们用最小二乘损失代替负对数似然目标 [35]。这种损失在训练期间更稳定，并产生更高质量的结果。特别是，对于 GAN 损失 LGAN(G, D, X, Y )，我们训练 G 最小化 Ex∼pdata(x) [(D(G(x)) − 1)2 ] 并训练 D 最小化Ey∼pdata(y) [(D(y) − 1)2 ] + Ex∼pdata(x) [D(G(x))2 ]。其次，为了减少模型振荡 [15]，我们遵循 Shrivastava 等人的策略 [46] 并使用生成图像的历史而不是最新生成器生成的图像来更新鉴别器。我们保留一个图像缓冲区来存储 50 个先前创建的图像。对于所有实验，我们在等式 3 中设置 λ = 10。我们使用批量大小为 1 的 Adam 求解器 [26]。所有网络都是从零开始训练的，学习率为 0.0002。我们在前 100 个 epoch 中保持相同的学习率，并在接下来的 100 个 epoch 中线性衰减到零。有关数据集、架构和训练过程的更多详细信息，请参阅附录（第 7 节）。

我们首先将我们的方法与最近在配对数据集上进行非配对图像到图像转换的方法进行比较，其中地面实况输入输出对可用于评估。 然后我们研究对抗性损失和循环一致性损失的重要性，并将我们的完整方法与几个变体进行比较。 最后，我们展示了我们的算法在不存在配对数据的广泛应用中的通用性。 为简洁起见，我们将我们的方法称为 CycleGAN。 PyTorch 和 Torch 代码、模型和完整结果可以在我们的网站上找到。

# 结果

# 词汇记录

# 精读

## 引文
