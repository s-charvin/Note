---
title: "PosionEmbedding(位置嵌入)"
author: "石昌文"
tags: [""]
description: ""
categories: [""]
keywords:  [""]
type: ""
draft: true
layout: 
data: 2022-03-19 14:44:48
lastmod: 2022-03-22 17:16:24
---

# *Position Embedding

原始的Attention结构，即使输入词序顺序被打乱（意味着使用相同矩阵 **I** 经 **W** 得到的 **K**，**V** 均按行打乱顺序），最终得到 Attention 结果值是不会变化的。

但是如果引入一个位置向量，将词向量与位置向量结合，这样Attention最终的结果就会因为词向量在的不同位置，而产生变化。

> 1. 需要体现同一单词在不同位置的区别。
> 2. 需要体现一定的先后次序，并且在一定范围内的编码差异不应该依赖于文本的长度，具有一定的不变性。
> 3. 需要有值域的范围限制。
>
> > Transformer：
> >
> > ![](PosionEmbedding(位置嵌入).assets/image-20220304012205.png)


> >
> >pos表示在序列中的位置；2i和2i+1，表示其选择的奇数维度还是偶数维度，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码；$d_{model}$,为向量维度大小。使得每一维度上都包含了一定的位置信息，而各个位置字符的位置编码又各不相同。
> >
> >其中间隔使用sin和cos的原因，大致是因为后期可以使用积化和差公式。
> >
> >缺点是不具备方向性，即 $PE_{t}^{T}PE_{t+k}$ （前向间隔k）的运算和$PE_{t}^{T}PE_{t-k}$（后向间隔k）的运算得到的值相等。
