---
title: "论文例句"
author: "石昌文"
tags: [""]
description: ""
categories: [""]
keywords:  [""]
type: ""
draft: true
layout: 
data: 2022-01-23 16:45:30
lastmod: 2022-03-22 17:23:24
---

# Progressive Co-Teaching

> Y. Yin, Y. Gu, L. Yao, Y. Zhou, X. Liang和H. Zhang, 《Progressive Co-Teaching for Ambiguous Speech Emotion Recognition》, 收入 *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 6月 2021, 页 6264–6268. doi: [10.1109/icassp39728.2021.9414494](https://doi.org/10.1109/icassp39728.2021.9414494).

# 摘要

有....问题。过去的方法缺点。基本思想....。方法评估。

Speech emotion recognition is a challenging task due to the ambiguity of emotion, which makes it difficult to learn the features of emotion data using machine learning algorithms.

由于情感的模糊性，语音情感识别是一项具有挑战性的任务，这使得使用机器学习算法学习情感数据的特征变得困难。

However, previous studies conventionally ignore the ambiguity of emotion and treat the emotion data as the same difficulty level, which results in low recognition accuracy.

以往的研究往往忽略了情绪的模糊性，将每种情绪视为同一难度水平，导致识别准确率较低。

# 引言

Emotions play an important role in human communication, and speech is considered to be one of the most conventional expressions of human emotion. Speech emotion recognition (SER) has attracted huge attention from researchers due to its ability to better understand human conversations. Thus, plenty of SER applications have been employed in many fields, such as human-computer interaction and robots [1, 2]. 

情感在人类交流中起着重要的作用，而语言被认为是人类情感最传统的表达方式之一。语音情感识别(SER)因其能够更好地理解人类对话而引起研究人员的极大关注。因此，大量的SER应用被应用于许多领域，如人机交互和机器人[1，2]。

- 提出问题，多方面证明问题存在（情感数据的模糊性影响）。

	实验某些数据表明....；人类现实生活中也可知.....；心理学研究也证明......。

- 传统方法为什么没有解决此问题或者说怎么简单解决的，有什么不好？

- 本文所提方法灵感来源，介绍相关方法。

- 本文使用方法简单介绍，以及有效性评估。

- 本文的主要贡献。

# Multi-Speaker Fine-Grained Prosody Modeling

> C. Lu, X. Wen, R. Liu和X. Chen, 《Multi-Speaker Emotional Speech Synthesis with Fine-Grained Prosody Modeling》, 收入 *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, Toronto, ON, Canada, 6月 2021, 页 5729–5733. doi: [10.1109/ICASSP39728.2021.9413398](https://doi.org/10.1109/ICASSP39728.2021.9413398).

# 摘要

We present an end-to-end system for multi-speaker emotional speech synthesis. In particular, our system learns emotion classes from just two speakers then generalizes these classes to other speakers from whom no emotional data was seen. We address the problem by integrating disentangled, fine-grained prosody features with global, sentence-level emotion embedding. These fine-grained features learn to represent local prosodic variations disentangled from speaker, tone and global emotion label. Compared to systems that model emotions at sentence level only, our method achieves higher ratings in naturalness and expressiveness, while retaining comparable speaker similarity ratings.

# 引言

Emotional speech synthesis (ESS) aims to generate natural and expressive speech with prescribed emotion, usually one chosen from several predefined emotion classes (happy, angry, etc.). Just like humans modulating voice emotions for effective communication, ESS has the potential for improving human-compute voice interface. This is desirable in voice-enabled applications like spoken dialogue systems, spoken language translation and content creation using textto-speech (TTS) synthesis. While recent state-of-the-art speech synthesizers, e.g. [1, 2], take advantage of large multi-speaker datasets, obtaining emotional speech on similar scale can be challenging and expensive. One particular difficulty is that consistently producing speech with prescribed emotion needs professional training, and most of our “corpus speakers” cannot do it. In this paper we consider a more practical setup in which we have a few “emotional speakers”, who are professionals providing emotional speech examples, and many “neutral speakers”, from whom we have only emotionally neutral examples. More concretely, we use two emotional speakers (1 male and 1 female), and 8 neutral speakers (4 and 4). We define four emotion classes: angry, happy, sad, neutral. Our goal is to produce emotional speech in the voice of all 10 speakers. Most previous ESS systems [3–5] use global, sentence-level emotion representation for emotion control. While this is reasonable for large, balanced training data, the same cannot be taken for granted in our setup, where most speakers are never heard with emotion. To check this out we ran a set of preliminary studies using global emotion embedding, and observed neutral speakers’ synthesized emotional speech is much less natural and expressive than that of emotional speakers. In other words, the system had learned to associate emotions mostly with emotional speakers. This suggests that we look for a way to represent emotion that is detached from speaker identity. In this paper, we augment sentence-level emotion embedding with fine-grained prosody representation. The latter captures local speech variations not fully characterized by text, speaker ID and global emotion class. Concretely, we associate each spoken phoneme with a 3-dimension latent code learned within a conditional variational autoencoder (VAE) framework. This very tight 3-wide information bottleneck encourages learning acoustically important features not already present among the conditioning inputs, therefore detaches the latent code from speaker identity. By transferring this latent code from emotional speakers, we are able to produce emotional speech of neutral speakers. Experimental results show our method achieves consistent improvements in both naturalness and expressiveness.

情感语音合成(ESS)的目标是生成具有特定情感的自然且富有表现力的语音，通常是从几个预定义的情感类别(高兴、愤怒等)中选择一种。就像人类调节语音情绪以进行有效沟通一样，ESS具有改善人机语音交互界面的潜力。这在诸如口语对话系统、口语翻译和使用文本到语音(TTS)合成的内容创建的语音使能应用中是合乎需要的。虽然目前最先进的语音合成器(如[1，2])利用了大量的多说话人数据集，但获得类似规模的情感语音可能是具有挑战性和昂贵的。一个特别的困难是，持续地说出有特定情感的演讲需要专业训练，而我们的大多数“语料库演讲者”做不到这一点。在本文中，我们考虑了一种更实际的设置，在这种设置中，我们有几个“情绪化演讲者”，他们是提供情绪化演讲例子的专业人士，而许多“中立演讲者”，我们只有从他们那里得到的是情感中立的例子。更具体地说，我们使用了两个情绪化的说话者(1个男性和1个女性)，以及8个中性的说话者(4个和4个)。我们定义了四种情绪类别：愤怒、快乐、悲伤、中立。我们的目标是用所有10位演讲者的声音发出动人的演讲。大多数以前的ESS系统[3-5]使用全局的、句子级别的情绪表示来进行情绪控制。虽然这对于大型、平衡的训练数据来说是合理的，但在我们的设置中不能想当然地认为这是理所当然的，因为在我们的设置中，大多数演讲者从未听到过情绪激动的声音。为了验证这一点，我们进行了一系列使用全局情绪嵌入的初步研究，观察到中性说话者合成的情绪化语音比情绪化说话者要自然得多，表达能力也要差得多。换句话说，该系统已经学会了将情感主要与情绪化的说话者联系起来。这表明我们应该寻找一种与说话人身份无关的情感表达方式。在本文中，我们使用细粒度的韵律表示来增强语句级的情感嵌入。后者捕获不完全由文本、说话人ID和全局情感类别表征的局部语音变化。具体地说，我们将每个口语音素与在条件变分自动编码器(VAE)框架内学习的三维潜代码相关联。这一非常紧凑的3宽信息瓶颈鼓励学习条件输入中尚未出现的听觉上重要的特征，因此将潜在代码从说话人身份中分离出来。通过将这种潜在的密码从情绪化的说话者身上转移出来，我们就能够产生中性说话人的情绪化言语。实验结果表明，我们的方法在自然度和表现力方面都取得了一致的改善。

# Joint Distribution Adaptive Regression

> J. Zhang, L. Jiang, Y. Zong, W. Zheng和L. Zhao, 《Cross-Corpus Speech Emotion Recognition Using Joint Distribution Adaptive Regression》, 收入 *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 6月 2021, 页 3790–3794. doi: [10/gmr2jr](https://doi.org/10/gmr2jr).

# 摘要

因为....问题，提出...方法。方法基本思想....。方法评估。

**In this paper, we focus on the research of** cross-corpus speech emotion recognition (SER), **in which** the training and testing speech signals in cross-corpus SER belong to dierent speech corpus. Due to this fact, mismatched feature distributions may exist between the training and testing speech feature sets degrading the performance of most originally well-performing SER methods. **To deal with** cross-corpus SER, **we propose a novel** domain adaptation (DA) **method called** joint distribution adaptive regression (JDAR). The basic idea of JDAR is to learn a regression matrix by jointly considering the marginal and conditional probability distribution between the training and testing speech signals and hence their feature distribution dierence can be alleviated in the subspace spanned by the learned regression matrix. **To evaluate the proposed** JDAR, we conduct extensive cross-corpus SER experiments on EmoDB, eNTERFACE, and CASIA speech databases. **Experimental results show that the proposed** JDAR **achieves satisfactory performance and outperforms most of state-of-the-art** subspace learning based DA methods.

由于在训练和测试语音特征集之间可能存在不匹配的特征分布，从而降低了大多数最初表现良好的SER方法的性能。为了处理跨语料库的SER，我们提出了一种新的领域适应(DA)方法，称为联合分布自适应回归(JDAR)。JDAR的基本思想是通过联合考虑训练和测试语音信号之间的边缘概率分布和条件概率分布来学习回归矩阵，从而在学习的回归矩阵所跨越的子空间中缓解它们的特征分布差异。为了评估所提出的JDAR，我们在EmoDB、eNTERFACE和CASIA语音数据库上进行了广泛的跨语料库SER实验。实验结果表明，JDAR取得了令人满意的性能，并且优于大多数最先进的基于子空间学习的DA方法。

# 引言

The research of speech emotion recognition (SER) aims to enable the machine to automatically recognize human beings’ emotional states from the speech signals [1]. Undoubtedly, a robot that can detect emotional states from the speech signals makes the human-computer interaction (HCI) more friendly and fascinating. For this reason, speech emotion recognition (SER) has attracted much attention of researchers from affective computing, speech signal processing, and pattern recognition communities over the past decades and researchers have proposed a lot of well-performing SER methods [2, 3]. 

语音情感识别(SER)的研究目的是使机器能够从语音信号中自动识别人的情感状态[1]。毫无疑问，能够从语音信号中检测情感状态的机器人使人机交互变得更加友好和迷人。正因为如此，语音情感识别(SER)在过去的几十年里引起了情感计算、语音信号处理和模式识别界的研究人员的极大关注，研究者们提出了许多性能良好的SER方法[2，3]。

# Multi-Task Learning

> X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, “Speech Emotion Recognition with Multi-Task Learning,” Aug. 2021, pp. 4508–4512. doi: [10.21437/Interspeech.2021-1852](https://doi.org/10.21437/Interspeech.2021-1852).

# 摘要

Speech emotion recognition (SER) classifies speech into emotion categories such as: Happy, Angry, Sad and Neutral. Recently, deep learning has been applied to the SER task. This paper proposes a multi-task learning (MTL) framework to simultaneously perform speech-to-text recognition and emotion classification, with an end-to-end deep neural model based on wav2vec-2.0. Experiments on the IEMOCAP benchmark show that the proposed method achieves the state-of-the-art performance on the SER task. In addition, an ablation study establishes the effectiveness of the proposed MTL framework.

语音情绪识别（SER）将语音分为情绪类别，如：快乐，愤怒，悲伤和中性。最近，深度学习已应用于SER任务。本文提出了一种基于wav2vec-2.0的端到端深度神经模型，同时执行语音到文本识别和情绪分类的多任务学习（MTL）框架。在IEMOCAP基准上的实验表明，所提出的方法实现了SER任务的最新性能。此外，消融研究确定了拟议的MTL框架的有效性。

# 引言

Emotions such as Happy, Angry, Sad and Neutral, play an important role in human communication process. Emotion has been described as an “implicit channel” that is transmitted in addition to the explicit messages [1]. Participants in a conversation can communicate more effectively if they can recognize each others’ emotion states. Although it may not be very hard for humans to perceive others’ emotions, it remains a challenging task for computers. Considerable efforts have been devoted into emotion recognition (ER) in the human-computer interaction field since decades ago. People express emotions in many ways including body language, facial expressions, choice of words, tone of voice and more. Therefore, a variety of ER systems based on different types of input signals are proposed in the past, e.g. face emotion recognition [2]. Emotions are even correlated with human’s electrochemical characteristics such as EEG signals [3], suggesting electrochemical probes can be used to capture emotions. In this paper, we focus on the speech emotion recognition (SER) task that takes audio speech as input, and outputs emotion classes such as: Happy, Angry, Sad, Neutral. SER systems typically consist of several major cascading components: feature extraction, feature selection and classification [4].

快乐、愤怒、悲伤、中性等情绪在人类交流过程中起着重要的作用。情感被描述为一种“隐含的渠道”，除了显性的信息外，它还会被传递[1]。如果对话中的参与者能够识别彼此的情绪状态，他们就能更有效地沟通。虽然人类感知他人的情绪可能不是很难，但对计算机来说，这仍然是一项具有挑战性的任务。自几十年前以来，情感识别(ER)在人机交互领域已经投入了大量的努力。人们表达情感的方式有很多，包括肢体语言、面部表情、用词方式、语调等等。因此，过去提出了多种基于不同类型输入信号的ER系统，例如人脸情感识别[2]。情绪甚至与人类的电化学特征(如脑电信号)相关[3]，这表明电化学探针可以用来捕捉情绪。本文主要研究语音情感识别(SER)任务，该任务以音频语音为输入，输出快乐、愤怒、悲伤、中性等情感类别。SER系统通常由几个主要的级联组件组成：特征提取、特征选择和分类[4]。

# Semantic Information

> P. Tzirakis, A. Nguyen, S. Zafeiriou, and B. W. Schuller, “Speech Emotion Recognition Using Semantic Information,” in *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, Jun. 2021, pp. 6279–6283. doi: [10/gmr2kb](https://doi.org/10/gmr2kb).

# 摘要

**Speech emotion recognition is a crucial problem manifesting in a multitude of applications such as human computer interaction and education.** **Although several advancements have been made in the recent years, especially with the advent of Deep Neural Networks (DNN), most of the studies in the literature fail to consider** the semantic information in the speech signal. **In this paper, we propose a novel framework that can** capture both the semantic and the paralinguistic information in the signal. In particular, our framework is comprised of a semantic feature extractor, that captures the semantic information, and a paralinguistic feature extractor, that captures the paralinguistic information. Both semantic and paraliguistic features are then combined to a unified representation using a novel attention mechanism. The unified feature vector is passed through a LSTM to capture the temporal dynamics in the signal, before the final prediction. To validate the effectiveness of our framework, we use the popular SEWA dataset of the AVEC challenge series and compare with the three winning papers. Our model provides state-of-the-art results in the valence and liking dimensions.

语音情感识别是人机交互、教育等众多应用中的关键问题。尽管近年来取得了一些进展，特别是随着深度神经网络(DNN)的出现，但文献中的大多数研究都没有考虑语音信号中的语义信息。在本文中，我们提出了一种新的框架，可以同时捕捉信号中的语义信息和副语言信息。特别地，我们的框架由捕获语义信息的语义特征提取器和捕获副语言信息的副语言特征提取器组成。然后，使用一种新的注意机制将语义特征和并列特征组合成统一的表示。在最终预测之前，统一的特征向量通过LSTM来捕获信号中的时间动态。为了验证我们的框架的有效性，我们使用了AVEC挑战系列中流行的SEWA数据集，并与三篇获奖论文进行了比较。我们的模型在价位和喜好维度上提供了最先进的结果。

# 引言

Automatic affect recognition is a vital component in human-tohuman communication affecting our social interaction, perception among others [1]. In order to accomplish a natural interaction between human and machine, intelligent systems need to recognise the emotional state of individuals. However, the task is challenging, as human emotions lack of temporal boundaries and different individuals express emotions in different ways [2]. In addition, emotions are expressed through multiple modalities. Over the past two decades, a plethora of systems have been proposed that utilise several modalities such as physiological signals, facial expression, speech, and text [3, 4, 5, ?, 6]. To achieve an accurate emotion recognition system, it is important to consider multiple modalities, as complementary information exists among them [3]. Current studies exploit Deep Neural Networks (DNNs) to model affect using multiple modalities [7, 8, 9, 10]. Two modalities that have been extensively used for the emotion recognition task are speech and text [11, 12]. Whereas the speech signal provides lowlevel characteristics of the emotions (e. g., prosody), text provides high-level (semantic) information (e. g., the words “love” and “like” carry strong emotional content). To this end, several systems have shown that integrating both modalities, strong performance gains can be obtained [11]. However, one may argue that the textual information is redundant, as it is already included in the speech signal, and as such semantic information can be captured using only the speech modality. To this end, we propose an audiotextual training framework, where the text modality is used during training, but discarded during evaluation. In particular, we train Word2Vec [13] and Speech2Vec [14] models, and align their two embedding spaces such that Speech2Vec features are as close as possible with the Word2Vec ones [15]. In addition to the semantic information, we capture low-level characteristics of the speech signal by training a convolution recurrent neural network. The semantic and paralinguistic features are combined to a unified representation and passed through a long short-term memory (LSTM) module that captures the temporal dynamics in the signal, before the final prediction. To test the effectiveness of our model, we utilise the Sentiment Analysis in the Wild (SEWA) dataset, which was used in the Audio/Visual Emotion Challenge (AVEC) since 2017 [16]. The dataset provides three continuous affect dimensions: arousal, valence, and likability. Although the arousal and valence dimensions are easily integrated in a single network during the training phase of the models, the likability dimension can cause convergence and generalisation difficulties [16, 17]. To this end, we propose to use a novel ‘disentangled‘ attention mechanism to fuse the semantic and paralinguistic features such that the information required per affect dimension is disentangled. Our approach provides training stability, and, at the same time, increases the generalisability of the network during evaluation. We compare our framework with the three best performing papers of the competition [18, 19, 17] in terms of concordance correlation coefficient (ρc) [20, 21], and show that our method provides state-of-the-art results for the valence and likability dimensions. In summary, the main contributions of the paper are the following: (a) propose to use the acoustic speech signal to capture semantic information that exists in the text modality, (b) show how to disentangle the information in the network per affect dimensions for stable training and generalisability during the evaluation phase, and (c) produce state-of-the-art results in the valence and likability dimensions using the SEWA dataset.

自动情感识别是人与人之间交流中的一个重要组成部分，影响着我们的社会互动、感知等[1]。为了实现人与机器之间的自然交互，智能系统需要识别个体的情绪状态。然而，这项任务是具有挑战性的，因为人类的情感缺乏时间界限，不同的人表达情感的方式也不同[2]。此外，情感也是通过多种方式表达的。在过去的二十年里，已经提出了大量的系统，这些系统利用了几种模式，例如生理信号、面部表情、语音和文本[3，4，5，？，6]。为了实现准确的情感识别系统，重要的是要考虑多个模态，因为它们之间存在互补信息[3]。目前的研究利用深度神经网络(DNNs)使用多种模态对情感进行建模[7，8，9，10]。被广泛用于情感识别任务的两种模态是语音和文本[11，12]。语音信号提供了情感的低级特征(例如，韵律)，而文本提供了高级(语义)信息(例如，单词“爱”和“喜欢”承载了强烈的情感内容)。为此，几个系统已经表明，将这两种模式集成在一起，可以获得强劲的性能增益[11]。然而，有人可能会争辩说，文本信息是冗余的，因为它已经包括在语音信号中，并且这样的语义信息可以仅使用语音模态来捕获。为此，我们提出了一个听觉语篇训练框架，该框架在训练过程中使用文本通道，但在评估过程中将其丢弃。特别是，我们训练word2vec[13]和Speech2Vec[14]模型，并将它们的两个嵌入空间对齐，以便Speech2Vec特征尽可能接近word2vec[15]。除了语义信息外，我们还通过训练卷积递归神经网络来捕捉语音信号的低层特征。语义和副语言特征被组合成统一的表示，并在最终预测之前通过捕获信号中的时间动态的长短期记忆(LSTM)模块。为了测试我们模型的有效性，我们使用了野生(SEWA)数据集的情感分析，该数据集自2017年以来一直用于音频/视觉情感挑战(AVEC)[16]。数据集提供了三个连续的情感维度：唤醒、价格和可爱性。虽然在模型的训练阶段，唤醒维度和价态维度很容易整合到一个单一的网络中，但亲和度维度可能会导致收敛和推广困难[16，17]。为此，我们建议使用一种新颖的“解缠”注意机制来融合语义和副语言特征，从而使每个情感维度所需的信息被解缠。我们的方法提供了训练的稳定性，同时在评估过程中增加了网络的泛化能力。我们将我们的框架与竞争中表现最好的三篇论文[18，19，17]在和谐相关系数(ρc)[20，21]方面进行了比较，结果表明，我们的方法提供了价态和可爱维度的最新结果。综上所述，本文的主要贡献如下：(A)提出使用声学语音信号来捕捉文本模态中存在的语义信息；(B)展示如何将网络中的信息按影响维度进行分离，以在评估阶段实现稳定的训练和泛化；以及(C)使用SEWA数据集在价度和喜好维度上产生最先进的结果。

# Listener Adaptive Models

> A. Ando *et al.*, “Speech Emotion Recognition Based on Listener Adaptive Models,” in *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, Jun. 2021, pp. 6274–6278. doi: [10/gmr2j8](https://doi.org/10/gmr2j8).

# 摘要

本文提出了一个模型，可以干嘛。介绍模型提出的背景原因、实现思路、优点。总结。

**This paper presents a novel speech emotion recognition scheme that can deal with** the individuality of emotion perception. Most conventional methods directly model the majority decision of multiple listener’s perceived emotions. However, emotion perception varies with the listener, which means the conventional methods can mismatch the recognition results to human perception. **In order to mitigate this problem, we propose a** Listener Adaptive (LA) **model that** reflects emotion recognition criteria of each listener. One-hot listener codes with several adaptation layers are employed in the LA model. The LA model yields the posterior probabilities of the listener-specific perceived emotions. Majority-voted emotion can be also estimated by averaging, in the LA model, the posterior probabilities for all listeners. **Experiments on** two emotional speech datasets **demonstrate that the proposed approach offers improved listener-wise perceived emotion recognition performance in natural speech.**

本文提出了一种新的语音情感识别方案，可以处理情绪感知的个性。大多数的传统方法直接使用由多个听众投票，其中得票数最多的情绪。然而，情绪感知因听者而异，这意味着传统方法的识别结果可能会与人类感知不匹配。为了缓解这个问题，我们提出了一个反映每个听众的情感识别标准的听众自适应（LA）模型。在LA模型中采用了具有多个适应层的 One-hot 听众编码，产生特定听众感知情绪的后验概率，并且也可以通过平均所有听众的后验概率来估计传统模型中多数投票情绪。对两个情感语音数据集的实验表明，本文所提出的方法在自然语音中提供了改进的听众感知情感识别性能。

# 引言

Speech Emotion Recognition (SER) is an important technology for natural human-computer interaction. There are a lot of SER applications such as generating human-like responses in spoken dialog systems [1] and voice-of-customer analysis in contact center calls [2]. These applications require the emotion category that most human listeners would commonly perceive from a speech. The aim of this paper is to improve the performance of emotion categorization. A large number of SER methods have been proposed. Recent studies have achieved remarkable performance through the use of Deep Neural Networks (DNNs) [3–8]. The main advantage of this framework is that complex cues of perceived emotions can be learned automatically by combining different types of layers. Convolutional Neural Networks (CNNs) and Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) have been used to capture the local and contextual characteristics in an utterance [3,4,8]. The attention mechanism has also been employed to focus on the local characteristics of an utterance for recognition [5]. Furthermore, DNN-based models can utilize low-level features, e.g. log power-spectrogram or raw waveforms, which have rich but complex information [6, 7]. These methods construct the classification model that estimates the majorityvoted emotion category perceived by multiple listeners.

语音情感识别(SER)是实现自然人机交互的一项重要技术。有很多SER应用，例如在口语对话系统中生成类似人类的答复[1]，在联系中心呼叫中进行客户语音分析[2]。这些程序通常需要多数听众从演讲中感知情感类别。本文的目的是提高情感分类的性能。人们已经提出了大量的SER方法。最近的研究已经通过使用深度神经网络(DNNs)取得了显著的性能[3-8]。这个框架的主要优点是，通过组合不同类型的层，可以自动学习感知到的复杂情绪线索。卷积神经网络(CNNs)和长短期记忆递归神经网络(LSTM-RNNs)被用来捕捉话语中的局部和语境特征[3，4，8]。注意机制也被用来关注话语的局部特征以进行识别[5]。此外，基于DNN的模型可以利用低级特征，例如对数功率谱图或原始波形，这些特征具有丰富但复杂的信息[6，7]。这些方法构建了一个分类模型，用来估计多个听者感知的多数投票情绪类别。

# Stacked Transformer Layers

> X. Wang, M. Wang, W. Qi, W. Su, X. Wang, and H. Zhou, “A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers,” in *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, Jun. 2021, pp. 6289–6293. doi: [10/gmr2jm](https://doi.org/10/gmr2jm).

# 摘要

方向介绍。主流方法框架。本文方法和优点。实验证明。

**Speech emotion recognition (SER) aims to automatically recognize emotional category for a given speech utterance. The performance of a SER system heavily relies on the effectiveness of global representation expressed at utterance level. To effectively extract such a global feature, the mainstream of recent SER architectures adopts a pipeline with two key modules, feature extraction and aggregation. Although variant module designs have brought impressive progresses, SER is still a challenging task.** **In contrast with those previous works,** **herein we propose a novel strategy for global SER feature extraction by** applying an additional enhancement module on top of the current SER pipeline. To verify its effect, an endto-end SER architecture is proposed where stacked multiple transformer layers are explored to enhance the aggregated global feature. **Such an architecture is evaluated on IEMOCAP and results strongly substantiate the effectiveness of our proposal. In terms of weighted accuracy on four emotion categories, our proposed SER system outperforms the prior arts by a large margin of relatively** 20% **improvement.** Our codes and the pre-trained SER models are made publicly available.

语音情感识别(SER)的目的是自动识别给定语音话语的情感类别。SER系统的性能在很大程度上依赖于话语水平表达下的全局特征的有效性。为了有效地提取这样的全局特征，目前主流的SER体系结构采用流水线结构，包括特征提取和聚合两个重要组成部分。尽管各种各样的模块设计已经带来了令人印象深刻的进步，但SER仍然是一项具有挑战性的任务。与前人的工作不同，本文提出了一种新的全局SER特征提取策略，通过在现有SER流水线的基础上增加一个增强模块来实现全局SER特征提取。为了验证其效果，提出了一种端到端的SER结构，其中利用堆叠的多个 transformer 层来增强聚合的全局特征。在IEMOCAP上对这种体系结构进行了评估，结果有力地证明了我们的建议的有效性。在四种情感类别的加权准确率方面，我们提出的SER系统比现有技术有20%的较大幅度提高。我们的代码和预先训练的SER模型是公开提供的。

# 引言

Speech emotion, as one kind of meta-information apart from text, plays an important role for understanding speakers’ psychology and response. The relevant research, called speech emotion recognition (SER), aims to automatically recognize emotional category for a given speech utterance. Since emotions are usually conveyed in a subtle and variable way, it have been challenging to identify emotion embeddings, as representation of an utterance, that can effectively classify emotion categories.

With recent advances in Deep Neural Networks (DNNs), emotional embeddings have evolved from prior knowledgebased hand-crafted acoustic features, e.g. low-level descriptors (LLDs), to DNN-based deep emotion features . Various DNN architectures, like convolutional neural network (CNN), long-short term memory (LSTM), time-delay neural network (TDNN), residual network (ResNet), Dilated Residual Network (DRN), themselves or in combinations, have been explored in most recent SER works [1–4]. 

A lot of research works have been afforded in the SER area and it is still challenging because of decoupling emotion from linguistic feature and pooling valid emotion features from a long span of utterance. To address these, it became prevalent for a modern SER system to adopt a pipeline with two-modules: 1) a feature extraction module to generate temporal acoustic features that are emotionally relevant; and 2) an aggregation module to pool those temporal features into a compact global contextual representation (aka, emotional embedding) at utterance level. 

This paper is organized as follows. In Section 2, vanilla Transformer is briefly introduced. In Section 3, we propose STL-enhanced SER architecture in details. Experiments are reported in Section 4. And section 5 concludes the study.

言语情感作为文本之外的一种信息元，对理解说话人的心理和反应起着重要的作用。与之相关的研究被称为语音情感识别(SER)，其目的是自动识别给定语音话语的情感类别。由于情感通常是以一种微妙和可变的方式传达的，因此找到能够用来有效地识别并对情感类别进行分类的表征话语情感的嵌入特征一直是一项挑战。

随着深度神经网络(DNNs)的最新进展，情感嵌入已经从以前基于经验的手工制作声学特征(例如低级描述符(LLD))演变为基于DNN获得的深度情感特征。在最近的SER工作中，已经探索了各种DNN结构，如卷积神经网络(CNN)、长短期记忆(LSTM)、时延神经网络(TDNN)、残差网络(ResNet)、扩张残差网络(DRN)，它们本身或组合在一起[1-4]。

在SER领域已经有了大量的研究成果，但由于情感与语言特征的分离和长时话语中有效情感特征的集中，这方面的研究仍然很有挑战性。为了解决这些问题，现代SER系统普遍采用由两个模块组成的流水线：1)特征提取模块，用于生成情感相关的时态声学特征；2)聚合模块，用于将这些时态特征在话语级别汇集到紧密全局上下文表征(也称情感嵌入)中。为了产生有效的情感嵌入特征，最近的SER工作集中在开发不同的模块架构上。

# Slow-Fast Auditory Streams

> E. Kazakos, A. Nagrani, A. Zisserman, and D. Damen, “Slow-Fast Auditory Streams for Audio Recognition,” in *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, Jun. 2021, pp. 855–859. doi: [10.1109/ICASSP39728.2021.9413376](https://doi.org/10.1109/ICASSP39728.2021.9413376).

# 摘要

**We propose a** two-stream convolutional **network for audio recognition, that operates on time-frequency spectrogram inputs.** Following similar success in visual recognition, we learn Slow-Fast auditory streams with separable convolutions and multi-level lateral connections. The Slow pathway has high channel capacity while the Fast pathway operates at a fine-grained temporal resolution. We showcase the importance of our two-stream proposal on two diverse datasets: VGG-Sound and EPIC-KITCHENS-100, and achieve stateof-the-art results on both.

我们提出了一个用于语音识别的 two-stream 卷积网络，它运行在时频频谱图输入上。在视觉识别方面取得了类似的成功之后，我们学习了具有可分离卷积和多层次横向连接的 Slow-Fast 听觉流。Slow 通道具有高通道容量，而 Fast 通道运行在一个 fine-grained 的时间分辨率上。我们在两个不同的数据集（vg - sound和epic - kits -100）上展示了我们的 two-stream 方案的重要性，并在这两个数据集上实现了最先进的结果。

# 引言

Recognising objects, interactions and activities from audio is distinct from prior efforts for scene audio recognition, due to the need for recognising sound-emitting objects (e.g. alarm clock, coffee-machine), sounds generated from interactions with objects (e.g. put down a glass, close drawer), and activities (e.g. wash, fry). This introduces challenges related to variable-length audio associated with these activities. Some can be momentary (e.g. close) while others are repetitive over a longer period (e.g. fry), and many exhibit intra-class variations (e.g. cut onion vs cut cheese). Background or irrelevant sounds are often captured with these activities. We focus on two activity-based datasets, VGG-Sound [1] and EPIC-KITCHENS [2], captured from YouTube and egocentric videos respectively, and target activity recognition solely from the audio signal associated with these videos. There is strong evidence in neuroscience for the existence of two streams in the human auditory system, the ventral stream for identifying sound-emitting objects and the dorsal streams for locating these objects. Studies [3, 4] suggest the ventral stream accordingly exhibits high spectral resolution for object identification, while the dorsal stream has a high temporal resolution and operates at a higher sampling rate. Using this evidence as the driving force for designing our architecture, and inspired by a similar vision-based architecture [5], we propose two streams for auditory recognition: a Slow and a Fast stream, that realise some of the properties of the ventral and dorsal auditory pathways respectively. Our streams are variants of residual networks and use 2D separable convolutions that operate on frequency and time independently. The streams are fused in multiple representation levels with lateral connections from the Fast to the Slow stream, and the final representation is obtained by concatenating the global average pooled representations for action recognition. The contributions of this paper are the following: i) we propose a novel two-stream architecture for auditory recognition that respects evidence in neuroscience; ii) we achieve state-of-the-art results on both EPIC-KITCHENS and VGGSound; and finally iii) we showcase the importance of fusing our specialised streams through an ablation analysis. Our pretrained models and code is available at https://github. com/ekazakos/auditory-slow-fast.

与之前的场景音频识别不同，从音频中识别物体、交互和活动需要识别发出声音的物体(如闹钟、咖啡机)、与物体交互产生的声音(如放下玻璃、关闭抽屉)和活动(如洗涤、油炸)。这就带来了与这些活动相关的可变长度音频的挑战。有些可能是短暂的(如靠近)，而另一些则在较长的时间内重复(如油炸)，此外还有许多表现出类内的变化(如切洋葱和切奶酪)。通常背景或无关的声音活动经常被捕捉到。我们使用了两个分别从YouTube和egocentric的视频中捕获的基于活动的数据集：VGG-Sound[1]和EPIC-KITCHENS[2]，，并仅从与这些视频相关的音频信号中识别目标活动。神经科学有强有力的证据表明，在人类听觉系统中存在两种听觉流，一种是用于识别发出声音的物体的 the ventral stream，另一种是用于定位这些物体的 the dorsal streams。研究[3,4]表明the ventral stream 相应地表现出高声谱分辨率，用于目标识别，而 the ventral stream 具有高时间分辨率以及更高的工作采样率。
这一证据是我们设计此结构的动力，并受到类似的基于视觉的结构[5]的启发，我们提出了两种听觉识别流: Slow and Fast stream，它们分别实现了ventral 和 dorsal 听觉通道的一些属性。我们的 stream 是残差网络的变体，并使用二维可分离卷积，它分别在频率和时间上独立运行。通过从快流到慢流的横向连接，将流融合到多个表示级别，并通过连接全局平均池化表示获得最终表示，以进行动作识别。本文的贡献如下:i)我们提出了一种新的基于神经科学证据的听觉识别双流结构;ii)我们在epic - kitchen和VGGSound上都取得了最先进的效果;最后iii)我们通过消融分析来展示融合我们的特殊流的重要性。

# Compact Graph Architecture

> A. Shirian and T. Guha, “Compact Graph Architecture for Speech Emotion Recognition,” in *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, Jun. 2021, pp. 6284–6288. doi: [10/gmr2jn](https://doi.org/10/gmr2jn).

# 摘要

We propose a deep graph approach to address the task of speech emotion recognition. ==A compact, efficient and scalable way== to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a Graph Convolution Network (GCN)-based architec Fast streams ture that can perform an accurate graph convolution in contrast to the approximate convolution used in standard GCNs. We evalu Fast streams ated the performance of our model for speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves comparable performance to the state-of-the-art with significantly fewer learnable parameters (∼30K) indicating its ap Fast streams plicability in resource-constrained devices. Our code is available at /github.com/AmirSh15/Compact SER.

我们提出了一种深度图方法来解决语音情感识别的问题。图是一种紧凑、高效和可扩展的数据表示方式。根据图信号处理理论，我们提出将语音信号建模为循环图或线形图，构建一个基于图卷积网络(GCN)的体系结构，此结构与标准GCNs中使用的approximate convolution 相比，它能够执行精确的图卷积。我们在流行的IEMOCAP和MSP-IMPROV数据库上评估了我们的语音情感识别模型的性能。结果显示，我们的模型优于标准的GCN和其他相关的深度图架构，这表明了我们方法的有效性。与现有的语音情感识别方法相比，我们的模型使用更少的可学习参数(∼30K)，获得了与最先进水平相当的性能，这表明它在资源受限的设备中具有可适应性。

# 引言

Machine recognition of emotional content in speech is crucial in many human-centric systems, such as behavioral health monitoring and empathetic conversational systems. Speech Emotion Recogni Fast streams tion (SER) in general is a challenging task due to the huge variabil Fast streams ity in emotion expression and perception across speakers, languages and cultures. Many SER approaches follow a two-stage framework, where a set of Low-Level Descriptors (LLDs) are first extracted from raw audio. The LLDs are then input to a deep learning model to gener Fast streams ate discrete (or continuous) emotion labels [1, 2, 3, 4]. While using hand-crafted acoustic features is still common in SER, lexical fea Fast streams tures [5, 6] and log Mel spectrograms are also used as inputs [7]. Spectrograms are often used with Convolutional Neural Networks (CNNs) [7] that does not explicitly model the speech dynamics. Ex Fast streams plicit modeling of the temporal dynamics is important in SER as it reflects the changes in emotion dynamics [8]. To capture the tem Fast streams poral dynamics of emotion, recurrent models, especially the Long Fast streams Short Term Memory networks (LSTMs), are popular [2, 3, 4]. The recurrent models, though predominant in SER, often lead to complex architecture with millions of trainable parameters. A compact, efficient and scalable way to represent data is in the form of graphs. Graph Convolutional Networks (GCNs) [9] have been successfully used to address various problems in computer vi Fast streams sion and natural language processing, such as action recognition [10], object tracking [11] and text classification [12]. In the con Fast streams text of audio analysis, we are aware of only one recent work that proposed an attention-based graph neural network architecture for few-shot audio classification [13].

Motivated by the recent success of GCNs, we propose to adopt a deep graph approach to SER. We base our work on spectral GCNs which have a strong foundation on graph signal processing [14]. Spectral GCNs perform convolution operation on the spectrum of the graph Laplacian considering the convolution kernel (a diagonal ma Fast streams trix) to be learnable [15]. This involves eigen decomposition of the graph Laplacian matrix, which is computationally expensive. To re Fast streams duce the computational cost, ChebNet approximates the convolution operation (including the learnable convolution kernel) in terms of Chebyshev polynomials [16]. The most popular form of GCN uses a first order approximation of the Chebyshev polynomial to further simplify the convolution operation to a linear projection [9]. Such GCN models are simple to implement, and have been successfully used for various node classification tasks in social media networks and citation networks [9]. In this paper, we cast SER as a graph classification problem. We model a speech signal as a simple graph, where each node corre Fast streams sponds to a short windowed segment of the signal. Each node is con Fast streams nected to only two adjacent nodes thus transforming the signal to a line graph or a cycle graph. Owing to this particular graph structure, we take advantage of certain results in graph signal processing [17] to perform accurate graph convolution (in contrast to the approxi Fast streams mations used in popular GCNs). This leads to a light-weight GCN architecture with superior emotion recognition performance on the IEMOCAP [18] and the MSP-IMPROV [19] databases. To summarize, the contributions of our work are as follows: (i) To the best of our knowledge, this is the first work that takes a graph classification approach to SER. (ii) Leveraging theories from graph signal processing, we propose a GCN-based graph classifi Fast streams cation approach that can efficiently perform accurate graph convo Fast streams lution. (iii) Our model has significantly fewer trainable parameters (∼30K only). Despite its smaller size, our model achieves superior performance on both IEMOCAP and MSP-IMPROV databases out Fast streams performing relevant and competitive baselines.

语音情感内容的机器识别在许多以人为核心的系统中至关重要，如行为健康监测和 empathetic conversational systems 。由于说话人、语言和文化在情感表达和感知上的巨大差异，语音情感识别(SER)总体上是一项具有挑战性的任务。

许多SER方法遵循两阶段框架，首先从原始音频中提取一组低级描述符(Low-Level descriptor, LLD)。然后将LLD输入到深度学习模型中，以生成离散(或连续)的情感标签[1,2,3,4]。虽然在SER中使用手工制作的声学特征仍然很常见，但 lexical 特征[5,6]和Log Mel声谱图也被用作了输入[7]。频谱图通常与卷积神经网络(cnns)一起使用[7]，但后者没有明确地模拟语音动态。快速流时间动态显式建模在SER中具有重要意义，因为它反映了情绪动态的变化。为了捕捉情绪的时间动态特性，递归模型，特别是长短时记忆网络(LSTMs)非常流行[2,3,4]。递归模型虽然在SER中占主导地位，但常常产生具有数百万可训练参数的复杂体系结构。

以图的形式表示数据，是一种紧凑、高效和可扩展的方式。图卷积网络(GCNS)[9]已经成功地用于解决计算机视觉和自然语言处理中的各种问题，如动作识别[10]、对象跟踪[11]和文本分类[12]。在音频分析方面，我们知道最近只有一项工作提出了一种基于注意力的图神经网络体系结构来进行few-shot 音频分类[13]。

受到GCNS最近的成功的激励，我们建议采用一种深度图的方法来进行SER。我们的工作基于spectral GCNS，它在图形信号处理方面有很好的基础[14]。考虑到卷积核(对角线矩阵)是可学习的，spectral GCNs 在图的拉普拉斯矩阵的谱上执行卷积运算[15]。这涉及到图的拉普拉斯矩阵的特征分解，这在计算上是昂贵的。为了减少计算量，ChebNet用Chebyshev多项式逼近卷积运算(包括可学习的卷积核)[16]。最流行的GCN形式使用切比雪夫多项式的一阶近似，将卷积运算进一步简化为线性投影[9]。这样的GCN模型易于实现，并已成功地用于社交媒体网络和引文网络中的各种节点分类任务[9]。

本文将SER问题归结为一个图分类问题。
我们将语音信号建模为一个简单的图，其中每个节点对应于信号的一个 short windowed segment。每个节点仅连接到两个相邻节点，从而将信号转换为折线图或循环图。由于这种特殊的图结构，我们利用图信号处理[17]中的某些结果来执行精确的图形卷积(与流行的GCN中使用的approximations 不同)，提出了一个轻量级的GCN架构，在IEMOCAP[18]和MSP-Improv[19]数据库上具有优异的情感识别性能。

综上所述，我们的工作贡献如下：(i)据我们所知，这是第一个将图分类方法应用于SER的工作。(ii)利用图信号处理的理论，提出了一种基于GCN的图分类方法，可以有效地进行精确的图卷积。(iii)我们的模型具有明显较少的可训练参数(仅限∼30K)。尽管我们的模型体积较小，但在IEMOCAP和MSP-Improv数据库上都取得了优异的性能，超过了相关的和有竞争力的基线。

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言

> 

# 摘要

# 引言
