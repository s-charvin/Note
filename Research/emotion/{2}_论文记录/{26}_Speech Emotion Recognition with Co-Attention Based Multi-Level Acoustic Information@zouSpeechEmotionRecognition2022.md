---
title: "Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information"
description: ""
citekey: zouSpeechEmotionRecognition2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information
>2. Author：Heqing Zou, Yuke Si, Chen Chen, Deepu Rajan, Eng Siong Chng
>3. Entry：[Zotero link](zotero://select/items/@zouSpeechEmotionRecognition2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zou et al_2022_Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***


## ⭐ 重点

- 

## 摘要

> [!abstract] Speech Emotion Recognition (SER) aims to help the machine to understand human's subjective emotion from only audio in-formation. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the pro-posed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub.

> 

## 预处理

## 概述

## 结果

## 精读

### 引文

## 摘录
