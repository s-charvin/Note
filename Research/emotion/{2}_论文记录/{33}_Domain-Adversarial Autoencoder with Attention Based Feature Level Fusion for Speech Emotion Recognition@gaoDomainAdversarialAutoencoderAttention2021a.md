---
title: "Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for Speech Emotion Recognition"
description: ""
citekey: gaoDomainAdversarialAutoencoderAttention2021a
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for Speech Emotion Recognition
>2. Author：Yuan Gao, JiaXing Liu, Longbiao Wang, Jianwu Dang
>3. Entry：[Zotero link](zotero://select/items/@gaoDomainAdversarialAutoencoderAttention2021a) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gao et al_2021_Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for2.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***


## ⭐ 重点

- 

## 摘要

> [!abstract] Over the past two decades, although speech emotion recognition (SER) has garnered considerable attention, the problem of insufficient training data has been unresolved. A potential solution for this problem is to pre-train a model and transfer knowledge from large amounts of audio data. However, the data used for pre-training and testing originate from different domains, resulting in the latent representations to contain non-affective information. In this paper, we propose a domain-adversarial autoencoder to extract discriminative representations for SER. Through domain-adversarial learning, we can reduce the mismatch between domains while retaining discriminative information for emotion recognition. We also introduce multi-head attention to capture emotion information from different subspaces of input utterances. Experiments on IEMOCAP show that the proposed model outperforms the state-of-the-art systems by improving the unweighted accuracy by 4.15%, thereby demonstrating the effectiveness of the proposed model.

> 

## 预处理

## 概述

## 结果

## 精读

### 引文

## 摘录
