---
title: "流形学习"
description: ""
author: ""
tags: [""]
categories: ""
keywords:  [""]
draft: true
layout: ""
date: 2023-02-20 15:58:13
lastmod: 2023-02-20 18:24:34
---



## 流形


流形是拓扑和微分几何中最为基本的概念，

如上图所示，一个流形 (manifold) 是一个拓扑空间 $S$ 。
被一族开集所覆盖 $S \subset \cup_\alpha U_\alpha$ ，对于每个开集 $U_\alpha$ 存在一个同胚映射 $\varphi_\alpha: U_\alpha \rightarrow \mathbb{R}^n ， \varphi_\alpha$ 被称为是坐标映射， $\mathbb{R}^n$ 被称为是参数域。
$\left(U_\alpha, \varphi_\alpha\right)$ 构成一个局部坐标卡 (local chart)，所有局部坐标卡构成流形的图册 (atlas) $\Lambda=\left\{\left(U_\alpha, \varphi_\alpha\right)\right\}$ 。
在交集 $U_\alpha \cap U_\beta$ 上，每个点可以有多个局部坐标，在局部坐标间存在变换 $\varphi_{\alpha \beta}=\varphi_\beta \varphi_\alpha^{-1}$ 。
从流形到坐标域的变换 $\varphi_\alpha: U_\alpha \rightarrow \mathbb{R}^n$ 被称为是参数化。其逆变换，从局部坐标到流形的变换 $\varphi_\alpha^{-1}: \varphi_\alpha\left(U_\alpha\right) \rightarrow U_\alpha$ 被称为是流形的局部参数表示。
如果流形 $S$ 嵌入到欧氏空间 $\mathbb{R}^d$ 中，则欧氏空间 $\mathbb{R}^d$ 被称为是背景空间。
在局部坐标间存在变换函数 $\varphi_{\alpha \beta}=\varphi_\beta \varphi_\alpha^{-1}$ 或者 $\varphi_{\beta \alpha}=\varphi_\alpha \varphi_\beta^{-1}$ 。
这些变化函数很重要，因为根据它们的可微性，它们定义了一类新的可微流形（表示为如果它们是 $k$ 次连续可微的)。最重要的是一个无限可微的转换图，我们称之为平滑流形。
我们的想法就是，一旦我们有了光滑的流形，我们就可以做一些很好的事情，比如微积分（分析）。我们很清楚，一旦我们平滑地映射到低维欧几里得空间，事情就会更容易分析。对嵌入在高维空间中的流形进行分析可能会让人头疼，但在低维欧几里得空间中进行分析 (相对而言) 很容易!

[浅谈流形学习](https://blog.pluskid.org/archives/533)



首先, manifold learning 的一个基本假设是，数据在 manifold 上，而 manifold 上足够小的区域近似于 tangent space（欧式空间）。
在流形学习中有一个假设，就是所处理的数据采样于一个潜在的流形上，或是说对于这组数据存在一个潜在的流形。

比如在 Laplacian Eigenmaps 中要假设这个流形是紧致黎曼流形等。对于描述流形上的点，我们要用坐标，而流形上本身是没有坐标的，所以为了表示流形上的点，必须把流形放入外围空间（ambient space）中，那末流形上的点就可以用外围空间的坐标来表示。比如 R^3 中的球面是个 2 维的曲面，因为球面上只有两个自由度，但是球面上的点一般是用外围 R^3 空间中的坐标表示的，所以我们看到的 R^3 中球面上的点有 3 个数来表示的。
当然球面还有柱坐标球坐标等表示。对于 R^3 中的球面来说，那末流形学习可以粗略的概括为给出 R^3 中的表示，在保持球面上点某些几何性质的条件下，找出找到一组对应的内蕴坐标（intrinsic coordinate）表示，显然这个表示应该是两维的，因为球面的维数是两维的。这个过程也叫参数化（parameterization）。直观上来说，就是把这个球面尽量好的展开在通过原点的平面上。在 PAMI 中，这样的低维表示也叫内蕴特征（intrinsic feature）。一般外围空间的维数也叫观察维数，其表示也叫自然坐标（外围空间是欧式空间）表示, 在统计中一般叫 observation。



问题是这个足够小实际中根本不存在。换句说没有那么稠密的数据使得你在有效的足够小的空间中存在足够的数据去近似 manifold。

了解 curse of dimensionality 的人应该知道，在高维空间中，任意两个 sample 的距离都差不多，欧式距离失效，这也是 KNN 在高维数据上失效的原因。也就是说基于欧氏距离的任何算法都失效。所以所有的基于距离，建 KNN 图来近似 manifold 的方法在高维空间都失效，基于 Gauss kernel 的算法也都失效（包括 kernel density estimation，也包括 spectral clustering)。

如果想满足 manifold 上足够小的区域近似于 tangent space 的假设，去用局部数据来近似 manifold 的话，就需要维度的几何级数的数据。所以基于局部数据建图的 manifold learning 方法，本身就受 curse of dimensionality 的限制。
