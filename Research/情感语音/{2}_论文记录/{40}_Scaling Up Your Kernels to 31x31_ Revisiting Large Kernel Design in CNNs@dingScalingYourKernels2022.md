---
title: "Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"
description: ""
citekey: dingScalingYourKernels2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:52
lastmod: 2023-04-11 13:01:58
---

> [!info] 论文信息
>1. Title：Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs
>2. Author：Xiaohan Ding, Xiangyu Zhang, Jungong Han, Guiguang Ding
>3. Entry：[Zotero link](zotero://select/items/@dingScalingYourKernels2022) [URL link](https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ding et al_2022_Scaling Up Your Kernels to 31x31.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\3FNAJGAC\\Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html>)
>4. Other：2022 -      -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] 

> 我们重新审视现代卷积神经网络 (CNN) 中的大卷积核设计。受 vision transformers (ViT) 最新进展的启发，在本文中，我们证明了使用一些大型卷积核而不是一堆小卷积核可能是一种更强大的方法。我们提出了五个指导方针，例如，应用重参数化( re-parameterized)的大深度卷积来设计高效的高性能大卷积核 CNN。根据指南，我们提出了 RepLKNet 网络，这是一种纯 CNN 架构，其内核大小高达 31×31，而不是常用的 3×3。 RepLKNet 极大地缩小了 CNN 和 ViT 之间的性能差距，例如，在 ImageNet 和一些典型的下游任务上实现了与 Swin Transformer 相当或更好的结果，并且延迟更低。 RepLKNet 还表现出对大数据和大型模型的良好可扩展性，在多个测试任务中都得到了效果提升.

## 预处理

## 概述

## 结果

## 精读

卷积神经网络 (CNN) [41, 54] 曾经是现代计算机视觉系统中视觉编码器的常见选择。然而，最近，CNNs [41, 54] 受到了视觉转换器 (ViTs) [34, 60, 85, 94] 的极大挑战，它们在许多视觉任务上都表现出了领先的表现——不仅是图像分类 [34, 105] 和表示学习 [4, 10, 16, 101]，还有许多下游任务，例如对象检测 [24, 60]，语义分割 [94, 99] 和图像恢复 [11, 55]。

为什么 ViT 超级强大？

1. 一些工作认为，ViTs 中的多头自我注意 (MHSA) 机制起着关键作用。他们提供了实证结果来证明，MHSA 更灵活 [51]，能够（更少的归纳偏差）[20]，对失真更稳健 [67, 99]，或者能够对长期依赖关系进行建模 [70, 89]。
2. 但是一些工作挑战了 MHSA [116] 的必要性，将 ViT 的高性能归因于适当的构建块 [33] 和/或动态稀疏权重 [39,111]。更多作品 [20,39,43,96,116] 从不同的角度解释了 ViT 的优越性。
3. 在这项工作中，我们专注于一个观点：建立大感受野的方式。在 ViT 中，MHSA 通常设计为全局 [34、76、94] 或局部但具有大内核 [60、71、88]，因此单个 MHSA 层的每个输出都能够从大区域收集信息。然而，大内核在 CNN 中并不普遍使用（第一层 [41] 除外）。相反，一种典型的方式是使用一堆小的空间卷积 1 [41, 45, 48, 69, 75, 80, 109]（例如，3×3）来扩大最先进的感受野 CNN。只有一些老式的网络，如 AlexNet [54]、Inceptions [77-79] 以及从神经架构搜索 [38,44,57,117] 衍生的少数架构采用大空间卷积（其大小大于 5）作为主要部分。

上面的观点自然会引出一个问题：如果我们对传统的 CNN 使用一些大的而不是许多小的内核会怎样？大内核还是构建大感受野的方式是缩小 CNN 和 ViT 之间性能差距的关键？为了回答这个问题，我们系统地探索了 CNN 的大内核设计。我们遵循一个非常简单的“哲学”：只需将大的深度卷积引入常规网络，其大小范围从 3×3 到 31×31，尽管还有其他替代方案可以通过单层或几层引入大的感受野，例如特征金字塔 [92]、扩张卷积 [14、102、103] 和可变形卷积 [23]。通过一系列实验，我们总结了有效使用大卷积的五个经验指南：

1. 虽然大核卷积计算成本更高,但是通过改进可以使得大核深度卷积计算更高效；

通常，大核卷积的计算量很大，因为网络结构的参数量和 FLOPs(每秒浮点运算次数)与卷积核尺寸的平方成正比。但是这一缺点可以通过应用深度卷积(DW: Depth-wise)方法来克服[17，44]。


例如在本文提出的 RepLKNet 网络架构中，虽然在不同阶段将卷积核尺寸从原始的[3，3，3，3]增加到了[31，29，27，13]，但是 Flops 和参数量仅增加了 18.6%的 10.4%，这在可接受范围内。实际上, 剩下的 1×1 卷积运算控制了大部分的计算复杂性。

![]({40}_Scaling%20 Up%20 Your%20 Kernels%20 to%2031 x 31_%20 Revisiting%20 Large%20 Kernel%20 Design%20 in%20 CNNs@dingScalingYourKernels 2022.assets/image-20220709112959.png)

有人可能会担心，DW 卷积在现代并行计算设备(如 GPU)上的计算效率可能非常低。对于传统的卷积核大小为 3×3 的 DW [44，75,109]卷积来说确实如此，这是因为 DW 卷积操作的存算比太低导致的, 即计算过程与存储器访问的比率[64]，这对现代计算体系结构不友好。然而，我们发现，当卷积核大小变大时，计算密度会增加, 因此存算比更高：例如，在核大小为 11×11 的 DW 卷积运算中，每次对特征映射进行计算时，它最多可参与 121 次乘法运算，而在 3×3 内核中，这一数字仅为 9 次。根据 Roofline 模型，当卷积核大小变大时，实际延迟应该不会随着 Flops 的增加而增加。

[大核卷积的实现优化背后原理](https://zhuanlan.zhihu.com/p/479182218)

此外，我们发现现有的深度学习工具(如 Pytorch)对大型 DW 卷积的支持很差，如表所示。

![]({40}_Scaling%20 Up%20 Your%20 Kernels%20 to%2031 x 31_%20 Revisiting%20 Large%20 Kernel%20 Design%20 in%20 CNNs@dingScalingYourKernels 2022.assets/image-20220709114851.png)

因此，本文尝试了几种方法来优化 CUDA 内核。理论上, 基于 FFT 的方法[65]对于加速大核卷积似乎是有效果的。然而，在实践中，我们发现 block-wise(inverse) implicit gemm 算法是一个更好的选择。该实现方案已经集成到开源框架 MegEngine[1]中。同时也发布了一个高效的 PyTorch 实现方案。上表显示，与 Pytorch 基线相比，本文的实现方案要高效得多。通过优化，RepLKNet 中 DW 卷积的推理延迟从 49.5%降低到 12.3%，这与 Flops 占用大致成正比。

2. 短路连接(恒等跳过连接)对大核卷积网络非常重要；

为了演示这一点，我们使用 MobileNet V 2[75]进行基准测试，因为它大量使用 DW 层，并且有两个已发布的变体(带或不带 shortcuts)。对于大核对应的模型，我们只需将所有的 DW 3×3 层替换为 13×13。所有的模型都在 ImageNet 上进行了 100 个历元的相同训练配置的训练(详见附录 A)。表 2 显示，大型内核通过 shortcuts 将 MobileNet V 2 的精确度提高了 0.77%。然而，在没有 shortcuts 的情况下，大核函数的准确率仅为53.98%。

该指南也适用于 VITS。最近的一项工作[33]发现，如果没有 identity shortcut，注意力的排名会随着深度的增加而成倍增加，从而导致过度平滑问题。虽然大核 CNN 可能会以不同于 VIT 的机制退化，但我们也观察到，如果没有 shortcut，网络很难捕获局部细节。从与[91]类似的角度来看，shortcut 使模型成为由具有不同感受场(RF)的众多模型组成的隐式集合，因此它可以从更大的最大 RF 中受益，同时不会失去捕捉小规模模式的能力。

3. 用小内核重参数化[30]有助于弥补优化问题(性能下降问题,小数据集优化问题)；

我们将 MobileNet V 2 的 3×3 层分别替换为 9×9 和 13×13，并可选地采用结构重新参数化[26，27，30]方法。具体地说，我们构造了一个与大的层平行的 3×3 层，然后在批归一化(BN)[49]层(图 2)后将它们的输出相加。训练后，我们将小核以及 BN 参数合并到大核中，这样得到的模型与训练模型等价，但不再有小核。表 3 显示了直接将内核大小从 9 增加到 13 会降低精度，而重新参数化解决了这个问题。然后，我们将 ImageNet 训练的模型转移到使用 DeepLabv 3+[15]对城市景观进行语义分割[21]。我们只更换主干，并保留彩信[19]提供的所有默认训练设置。观察结果与 ImageNet 上的类似：3×3 重新参数使 9×9 模型的 MIU 值提高了 0.19，13×13 模型的 MIU 值提高了 0.93。有了这样简单的重新参数化，将内核大小从 9 增加到 13 不再降低 ImageNet 和 Cityscape 上的性能。

众所周知，VITS 存在优化问题，特别是在小数据集上[34，57]。一种常见的解决方法是引入卷积优先，例如，向每个自我注意块[18，96]添加一个 DW 3×3 卷积，这与我们的类似。这些策略在网络之前引入了额外的翻译等价性和局部性，使得在不损失通用性的情况下更容易在小数据集上进行优化。类似于 VIT 的行为[34]，我们还发现，当预训练数据集增加到 7300 万个图像时(参见下一节中的 RepLKNet-XL)，可以在不降级的情况下省略重新参数化。

4. 大核卷积对下游任务的提升更加明显；

表 3(重新参数后)显示了将 MobileNet V 2 的核大小从 3×3 增加到 9×9，使 ImageNet 的准确率提高了 1.33%，而城市景观提高了 3.99%。表 5 显示了类似的趋势：当内核大小从[3，3，3，3]增加到[31，29，27，13]时，ImageNet 的精度只提高了 0.96%，而 ADE 20 K[114]上的 MIEU 提高了 3.12%。这种现象表明，ImageNet 得分相近的模型在下游任务中可能具有非常不同的能力(就像表 5 中排名最低的3个模型一样)。

是什么导致了这一现象？首先，大核设计显著增加了有效感受野(ERF)[63]。大量的研究已经证明，暗示大量 ERF 的“上下文”信息在许多下游任务中是至关重要的，例如对象检测和语义分割[61，67，93,101,102]。我们将在 SEC 讨论这个话题。5.第二，我们认为另一个原因可能是较大的内核设计会给网络带来更多的形状偏差。简而言之，ImageNet 图片可以根据纹理或形状进行正确分类，如[7，35]中所建议的。然而，人类识别物体主要是基于形状线索而不是纹理，因此形状偏好较强的模型可能会更好地转移到下游任务。最近的一项研究[88]指出，VITS 的形状偏见很强，这在一定程度上解释了 VITS 在转移任务中超级强大的原因。相比之下，在 ImageNet 上训练的传统 CNN 倾向于偏向纹理[7，35]。幸运的是，我们发现简单地增大 CNN 中的核大小可以有效地改善形状偏差。详情请参阅附录 C。

5. 即使在小特征图上，大核卷积也很有用。

为了验证这一点，我们将 MobileNet V 2 最后阶段的 DW 卷积扩大到 7×7 或 13×13，因此内核大小与特征映射大小（默认为 7×7）相当甚至更大。我们按照指南 3 的建议对大内核应用重新参数化。表 4 显示尽管最后阶段的卷积已经涉及非常大的感受野，但是进一步增加内核大小仍然会导致性能改进，特别是在诸如 Cityscapes 的下游任务上。

当核大小变大时，注意 CNN 的平移等价并不严格成立。如图 3 所示，相邻空间位置的两个输出仅共享一小部分核权重，即，通过不同的映射进行变换。该物业也符合 VITS 的“哲学”--在获得更多容量之前放松对称性。有趣的是，我们发现在变压器领域广泛使用的二维相对位置嵌入[5，76]也可以被视为大的深度方向的核，其大小为(2 H−1)×(2 W−1)，其中 H 和 W 分别为特征图的高度和宽度。大核函数不仅有助于学习概念之间的相对位置，而且由于填充效应还可以对绝对位置信息进行编码[51]。

基于上述指导方针，我们提出了一种名为 RepLKNet 的新架构，这是一种纯 CNN 架构网络，其中使用重参数化方法的大卷积来构建大的感受野。我们的网络总体上遵循 Swin Transformer [60] 的宏观架构，并进行了一些修改，同时用大的深度卷积替换了多头自注意力。

据我们所知，到目前为止，CNN 仍然主导着小模型[108,110]，而视觉转换器被认为在更复杂的预算下比 CNN 更好。因此，在本文中，我们主要关注相对较大的模型(其复杂度与 ResNet-152[40]或 Swin-B[59]相当或更大)，以验证较大的内核设计是否能够消除 CNN 和 VITS 之间的性能差距。

我们主要对中型和大型模型进行基准测试，因为过去人们认为 ViT 在大数据和模型上优于 CNN。在 ImageNet 分类上，我们的基线（与 Swin-B 相似的模型大小），其内核大小高达 31×31，仅在 ImageNet 1 K 数据集上训练的 top-1 准确率达到 84.8%，比 Swin-B 好 0.3%，但延迟效率更高。

更重要的是，我们发现大内核设计在下游任务上特别强大。例如，在相似的复杂度和参数预算下，我们的网络在 COCO 检测 [56] 上比 ResNeXt-101 [100] 或 ResNet 101 [41] 骨干网高出 4.4%，在 ADE 20 K 分割 [115] 上高出 6.1%，这也与甚至比对应的 Swin Transformers 更好，但推理速度更快。鉴于更多的预训练数据（例如 73 M 图像）和更多的计算预算，我们的最佳模型在具有相似模型大小的最先进技术中获得了非常有竞争力的结果，例如 ImageNet 上 87.8% 的 top-1 准确率和 ADE 20 K 上 56.0% 的准确率，这表明对大规模深度学习应用程序具有出色的可扩展性。我们认为 RepLKNet 的高性能主要是因为我们通过大内核构建的大有效感受野 (ERF) [64]，如图 1 所示。此外，我们的实验表明 RepLKNet 比传统的 CNN 利用更多的形状信息，部分符合人类的认知。我们希望我们的发现可以帮助理解 CNN 和 ViT 的内在机制。

Models with Large Kernels

除了少数像 inceptions[79-81]这样的老式模型外，大型内核模型在 VGG-Net 之后变得不再流行[77]。一个有代表性的工作是全局卷积网络(GCNS)[67]，它使用非常大的 1×K 和 K×1 的卷积来改进语义分割任务。然而，据报道，较大的内核会损害 ImageNet 的性能。

局部关系网络(Local Relationship Networks，LRNet)[45]提出了一种空间聚集算子(LRLayer)来代替标准卷积，后者可以被视为动态卷积。7×7 的核对 LR-Net 有好处，9×9 的 LR-Net 性能下降。当核的大小和特征图一样大时，TOP-1 的准确率从 75.7%降到 68.4%。

最近，Swin Transformers[59]提出了通过转移窗口注意力来捕捉空间模式，其窗口大小从 7 到 12，这也可以被视为大核的变体。后续的[32，58]使用了更大的窗口尺寸。

受这些局部变压器的成功启发，最近的一项工作[38]用静态或动态的 7×7 沿深度方向的卷积取代了 MHSA 层[59]，同时仍然保持了可比较的结果。虽然[38]提出的网络与我们的设计模式相似，但动机不同：[38]没有研究 ERF、大核与性能之间的关系；相反，它将视觉转换器的优越性能归因于稀疏连接、共享参数和动态机制。

另外三部代表作是 Global Filter Networks(GFNets)[72]、CKConv[74]和 FlexConv[73]。GFNet 优化了傅立叶域中的空间连接权值，相当于空间域中的圆形全局卷积。CKConv 将核函数表示为连续函数来处理序列数据，可以构造任意大小的核函数。FlexConv 为不同的层学习不同的内核大小，可以与功能图一样大。尽管它们使用非常大的内核，但它们并不打算回答我们想要的关键问题：为什么传统的 CNN 性能不如 VITS，以及如何在普通 CNN 中应用大的内核。此外，[38]和[72]都不在强基线上评估他们的模型，例如，大于 SwinL 的模型。因此，目前还不清楚大核 CNN 是否能像变压器一样扩大规模。

ConvMixer[87]使用高达 9×9 的卷积来替换 VITS[34]或 MLP[84，85]的“混合器”组件。MetaFormer[103]指出，集中注意力是自我关注的替代方式。ConvNeXt[60]采用 7×7 深度卷积来设计强结构，突破了 CNN 性能的极限。虽然这些作品表现出出色的性能，但它们并没有从更大的卷积(例如，31×31)中获得好处。

Model Scaling Techniques

给定一个小模型，为了获得更好的性能，通常的做法是将其放大，因此，缩放策略在最终的精度和效率之间的权衡中起着至关重要的作用。对于 CNN，现有的伸缩方法通常关注模型深度、宽度、输入分辨率[31，68，82]、瓶颈比和组宽度[31，68]。然而，内核大小经常被忽略。在证券交易委员会。3，我们将证明核大小也是 CNN 中一个重要的伸缩维，特别是对于下游任务。

Structural Re-parameterization

结构再参数化[26-30]是一种通过转换参数来等价转换模型结构的方法。例如，RepVGG 针对深度推理时类似 VGG(例如，无分支)的模型，并在训练过程中构建了平行于 3×3 层的额外 ResNet 样式的快捷方式。与难以训练的真实 VGG 模型相比，这种捷径帮助该模型达到了令人满意的性能。经过训练后，通过一系列的线性变换将捷径吸收到并行的 3×3 核中，从而使得到的模型成为类 VGG 模型。在本文中，我们使用这种方法将一个相对较小的(例如，3×3 或 5×5)核添加到一个非常大的核中。通过这种方式，我们使非常大的核能够捕获小规模的模式，从而提高了模型的性能。

![]({40}_Scaling%20 Up%20 Your%20 Kernels%20 to%2031 x 31_%20 Revisiting%20 Large%20 Kernel%20 Design%20 in%20 CNNs@dingScalingYourKernels 2022.assets/image-20220709111517.png)

Stem 指的是起始层。由于我们的目标是下游密度预测任务的高性能，我们希望在开始时通过几个卷积层捕获更多细节。在第一次 3×3 和 2×下采样之后，我们安排了一个 DW 3×3 层来捕获低电平图案，一个 1×1 卷积，以及另一个 DW 3×3 层用于下采样。

Stages 1-4 每个都包含几个 RepLK 块，它们使用快捷方式(准则 2)和 DW 大内核(准则 1)。我们通常在 DW 转换之前和之后使用 1×1 转换。请注意，每个 DW 大卷积使用一个 5×5 的核进行重新参数化(准则 3)，图 4 中没有显示。除了大卷积层提供了足够的接受场和聚集空间信息的能力外，模型的表示能力也与深度密切相关。为了提供更多的非线性和跨通道的信息通信，我们希望使用 1×1 层来增加深度。受广泛应用于变压器[34，59]和 MLP[26，84，85]的前馈网络(FFN)的启发，我们使用了类似于 CNN 式的块，它由快捷方式、BN、两个 1×1 层和 Gelu[41]组成，因此被称为 ConvFFN 块。与传统的 FFN 在完全连通层之前使用层归一化[3]相比，BN 的优点是可以融合到 Conv 中进行有效的推理。通常，ConvFFN 模块的内部通道数为 4×作为输入。只需在交织注意块和 FFN 块的 Vit 和 Swin 之后，我们在每个 RepLK 块之后放置一个 ConvFFN。

Transition Blocks 放置在 stages 之间，首先通过 1×1 卷积增加通道尺寸，然后以 DW 3×3 卷积进行 2×下采样。

总而言之，每个阶段具有三个体系结构超参数：RepLK 块的数量 B、通道维度 C 和内核大小


我们通过固定 B=[2，2，18，2]，C=[128,256,512,1024]，变化 K，并观察分类和语义分割的性能，来继续评估 RepLKNet 上的大核函数。在没有仔细调整超参数的情况下，我们随意地将核大小分别设置为[13，13，13，13]，[25，25，25，13]，[31，29，27，13]，并将这些模型称为 RepLKNet-13/25/31。我们还构造了两条小内核基线，其中内核大小都是 3 或 7(RepLKNet-3/7)。在 ImageNet 上，我们使用 AdamW[62]优化器、随机增强[22]、Mixup[106]、CutMix[105]、随机擦除[113]和随机深度[48]，在最近的工作[4，59，60，86]的基础上，训练了 120 个纪元。对于语义分割，我们使用的是 ADE 20 K[114]，这是一个广泛使用的大规模语义分割数据集，包含 150 个类别的 20 K 图像用于训练，2 K 用于验证。我们使用经过 ImageNet 训练的模型作为主干，采用 MMS eggation[19]在 80 K 迭代训练设置下实现的 UperNet[97]，并对单尺度 MEU 进行了测试。表 5 显示了不同内核大小下的结果。在 ImageNet 上，虽然将内核大小从 3 增加到 13 会提高精度，但将内核大小增加到更大并不会带来进一步的改进。然而，在 ADE 20 K 上，将内核从[13，13，13，13]扩展到[31，29，27，13]会带来 0.82 个更高的 MIU 值，仅增加 5.3%的参数和 3.5%的 FLOPS，这突显了大内核对于下游任务的重要性。在接下来的小节中，我们使用具有更强训练配置的 RepLKNet 31 来与 ImageNet 分类、城市景观/ADE 20 K 语义分割和 COCO[55]对象检测的最新技术进行比较。我们将上述模型称为 RepLKNet-31 B(B 表示基本)，将 C=[192,384,768,1536]的更广泛的模型称为 RepLKNet 31 L(大)。我们构造了另一个 RepLKNet-XL，C=[256,512,1024,2048]，在 RepLK 块中采用1.5×反向瓶颈设计(即 DW 大卷积层的沟道为1.5×作为输入)。


由于 RepLKNet 的整体架构类似于 Swin，我们希望首先进行比较。对于 ImageNet-1 K 上的 RepLKNet-31 B，我们将上述训练时间表扩展到 300 个纪元，以便进行公平的比较。然后对输入分辨率为 384×384 的 Swin-B 模型进行 30 个历元的精调，使总的训练成本大大低于 Swin-B 模型从零开始训练的 384×384。然后在 ImageNet-22 K 上对 RepLKNet-B/L 模型进行预训练，在 ImageNet-1 K 上对 Finetune 模型进行预训练。RepLKNetXL 在我们的私有半监督数据集 MegData 73 M 上进行了预训练，该数据集在附录中介绍。我们还给出了在相同的 2080 Ti GPU 上测试的批处理大小为 64 的吞吐量。附录中提供了培训配置。表 6 显示，尽管非常大的核不是用于 ImageNet 分类的，但我们的 RepLKNet 模型显示了精度和效率之间的良好折衷。值得注意的是，仅通过 ImageNet-1 K 训练，RepLKNet-31 B 的准确率达到 84.8%，比 Swin-B 高 0.3%，运行速度快 43%。尽管 RepLKNet-XL 比 Swin-L 有更高的 FLOPS，但它运行得更快，这突出了非常大的内核的效率。

4.4.。语义分割

我们然后使用预先训练的模型作为城市景观(表 7)和 ADE 20 K(表 8)的主干。具体地说，我们使用由 MMS egationation[19]实现的 UperNet[97]，对于城市景观具有 80 K 迭代训练时间表，对于 ADE 20 K 具有 160 K 迭代训练时间表。由于我们只希望评估主干，所以我们不使用任何高级技术、技巧或定制算法。在城市景观上，ImageNet-1 K 预先训练的 RepLKNet 31 B 的表现远远超过 Swin-B(单刻度 Miou 为 2.7)，甚至超过 ImageNet 22 K 预先训练的 Swin-L。即使配备了为视觉转换器定制的 DiversePatch[36]技术，22 K 预训练的 Swin-L 的单比例尺 Mou 仍然低于我们的 1 K 预训练的 RepLKNet-31 B，尽管前者有 2×参数。在 ADE 20 K 上，RepLKNet-31 B 在 1 K 和 22 K 的预训练中都优于 Swin-B，单尺度 Miou 的差距尤为显著。在我们的半监督数据集 MegData 73 M 的预训练下，RepLKNet-XL 达到了 56.0 的 MIU 值，这表明了 RepLKNet-XL 对于大规模视觉应用的可行可扩展性。

对于目标检测，我们使用 RepLKNets 作为 FCOS[83]和 Cascade MASK R-CNN[8，39]的主干，这两种方法分别是一阶段和两阶段检测方法的代表，以及 MMDetect 中的默认配置[12]。FCOS 模型使用 2 x(24 个纪元)的训练时间表进行训练，以便与来自相同代码库的 X 101(ResNeXt-101[99]的缩写)基线进行公平比较[19]，而使用级联掩码 R-CNN 的其他结果都使用 3 x(36 纪元)。同样，我们只是更换主干，不使用任何先进技术。表 9 显示，RepLKNet 的性能比 ResNeXt-101-64 x 4 d 高出 4.4 MAP，同时具有更少的参数和更低的 FLOPS。请注意，使用 HTC[11]、HTC++[59]、软 NMS[6]或 6 x(72 个纪元)时间表等高级技术可以进一步改进结果。与 Swin 相比，RepLKNet 以更少的参数和更低的 FLOPS 实现了更高或更接近的 MAP。值得注意的是，RepLKNet-XL 达到了55.5的 MAP，这再次证明了可伸缩性。

### 引文

## 摘录
