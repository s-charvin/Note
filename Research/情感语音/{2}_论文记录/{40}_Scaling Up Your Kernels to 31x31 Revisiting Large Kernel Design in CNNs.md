---
title: ""
author: "石昌文"
tags: [""]
description: ""
categories: [""]
keywords:  [""]
type: "笔记"
draft: true
layout: 
data: 2022-06-02 14:56:58
lastmod: 2022-07-09 16:29:50
---

# 重点

- 开源代码

# 摘要

我们重新审视现代卷积神经网络 (CNN) 中的大卷积核设计。受 vision transformers (ViT) 最新进展的启发，在本文中，我们证明了使用一些大型卷积核而不是一堆小卷积核可能是一种更强大的方法。我们提出了五个指导方针，例如，应用重参数化( re-parameterized)的大深度卷积来设计高效的高性能大卷积核 CNN。根据指南，我们提出了 RepLKNet 网络，这是一种纯 CNN 架构，其内核大小高达 31×31，而不是常用的 3×3。 RepLKNet 极大地缩小了 CNN 和 ViT 之间的性能差距，例如，在 ImageNet 和一些典型的下游任务上实现了与 Swin Transformer 相当或更好的结果，并且延迟更低。 RepLKNet 还表现出对大数据和大型模型的良好可扩展性，在多个测试任务中都得到了效果提升.

# 结果

# 词汇记录

# 精读

卷积神经网络 (CNN) [41, 54] 曾经是现代计算机视觉系统中视觉编码器的常见选择。然而，最近，CNNs [41, 54] 受到了视觉转换器 (ViTs) [34, 60, 85, 94] 的极大挑战，它们在许多视觉任务上都表现出了领先的表现——不仅是图像分类 [34, 105] 和表示学习 [4, 10, 16, 101]，还有许多下游任务，例如对象检测 [24, 60]，语义分割 [94, 99] 和图像恢复 [11, 55]。

为什么 ViT 超级强大？

1. 一些工作认为，ViTs 中的多头自我注意 (MHSA) 机制起着关键作用。他们提供了实证结果来证明，MHSA 更灵活 [51]，能够（更少的归纳偏差）[20]，对失真更稳健 [67, 99]，或者能够对长期依赖关系进行建模 [70, 89]。
2. 但是一些工作挑战了 MHSA [116] 的必要性，将 ViT 的高性能归因于适当的构建块 [33] 和/或动态稀疏权重 [39,111]。更多作品 [20,39,43,96,116] 从不同的角度解释了 ViT 的优越性。
3. 在这项工作中，我们专注于一个观点：建立大感受野的方式。在 ViT 中，MHSA 通常设计为全局 [34、76、94] 或局部但具有大内核 [60、71、88]，因此单个 MHSA 层的每个输出都能够从大区域收集信息。然而，大内核在 CNN 中并不普遍使用（第一层 [41] 除外）。相反，一种典型的方式是使用一堆小的空间卷积1 [41, 45, 48, 69, 75, 80, 109]（例如，3×3）来扩大最先进的感受野CNN。只有一些老式的网络，如 AlexNet [54]、Inceptions [77-79] 以及从神经架构搜索 [38,44,57,117] 衍生的少数架构采用大空间卷积（其大小大于 5）作为主要部分。

上面的观点自然会引出一个问题：如果我们对传统的 CNN 使用一些大的而不是许多小的内核会怎样？大内核还是构建大感受野的方式是缩小 CNN 和 ViT 之间性能差距的关键？为了回答这个问题，我们系统地探索了 CNN 的大内核设计。我们遵循一个非常简单的“哲学”：只需将大的深度卷积引入常规网络，其大小范围从 3×3 到 31×31，尽管还有其他替代方案可以通过单层或几层引入大的感受野，例如特征金字塔 [92]、扩张卷积 [14、102、103] 和可变形卷积 [23]。通过一系列实验，我们总结了有效使用大卷积的五个经验指南：

1. 虽然大核卷积计算成本更高,但是通过改进可以使得大核深度卷积计算更高效；

通常，大核卷积的计算量很大，因为网络结构的参数量和FLOPs(每秒浮点运算次数)与卷积核尺寸的平方成正比。但是这一缺点可以通过应用深度卷积(DW: Depth-wise)方法来克服[17，44]。


例如在本文提出的RepLKNet网络架构中，虽然在不同阶段将卷积核尺寸从原始的[3，3，3，3]增加到了[31，29，27，13]，但是Flops和参数量仅增加了18.6%的10.4%，这在可接受范围内。实际上, 剩下的1×1卷积运算控制了大部分的计算复杂性。

![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709112959.png)

有人可能会担心，DW卷积在现代并行计算设备(如GPU)上的计算效率可能非常低。对于传统的卷积核大小为3×3的DW [44，75,109]卷积来说确实如此，这是因为DW卷积操作的存算比太低导致的, 即计算过程与存储器访问的比率[64]，这对现代计算体系结构不友好。然而，我们发现，当卷积核大小变大时，计算密度会增加, 因此存算比更高：例如，在核大小为11×11的DW 卷积运算中，每次对特征映射进行计算时，它最多可参与121次乘法运算，而在3×3内核中，这一数字仅为9次。根据Roofline模型，当卷积核大小变大时，实际延迟应该不会随着Flops的增加而增加。

[大核卷积的实现优化背后原理](https://zhuanlan.zhihu.com/p/479182218)

此外，我们发现现有的深度学习工具(如Pytorch)对大型DW卷积的支持很差，如表所示。

![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709114851.png)

因此，本文尝试了几种方法来优化CUDA内核。理论上, 基于FFT的方法[65]对于加速大核卷积似乎是有效果的。然而，在实践中，我们发现 block-wise(inverse) implicit gemm 算法是一个更好的选择。该实现方案已经集成到开源框架MegEngine[1]中。同时也发布了一个高效的PyTorch实现方案。上表显示，与Pytorch基线相比，本文的实现方案要高效得多。通过优化，RepLKNet中DW卷积的推理延迟从49.5%降低到12.3%，这与Flops占用大致成正比。

2. 短路连接(恒等跳过连接)对大核卷积网络非常重要；

为了演示这一点，我们使用MobileNet V2[75]进行基准测试，因为它大量使用DW层，并且有两个已发布的变体(带或不带shortcuts)。对于大核对应的模型，我们只需将所有的DW 3×3层替换为13×13。所有的模型都在ImageNet上进行了100个历元的相同训练配置的训练(详见附录A)。表2显示，大型内核通过shortcuts将MobileNet V2的精确度提高了0.77%。然而，在没有shortcuts的情况下，大核函数的准确率仅为53.98%。

该指南也适用于VITS。最近的一项工作[33]发现，如果没有 identity shortcut，注意力的排名会随着深度的增加而成倍增加，从而导致过度平滑问题。虽然大核CNN可能会以不同于VIT的机制退化，但我们也观察到，如果没有 shortcut，网络很难捕获局部细节。从与[91]类似的角度来看，shortcut使模型成为由具有不同感受场(RF)的众多模型组成的隐式集合，因此它可以从更大的最大RF中受益，同时不会失去捕捉小规模模式的能力。

3. 用小内核重参数化[30]有助于弥补优化问题(性能下降问题,小数据集优化问题)；

我们将MobileNet V2的3×3层分别替换为9×9和13×13，并可选地采用结构重新参数化[26，27，30]方法。具体地说，我们构造了一个与大的层平行的3×3层，然后在批归一化(BN)[49]层(图2)后将它们的输出相加。训练后，我们将小核以及BN参数合并到大核中，这样得到的模型与训练模型等价，但不再有小核。表3显示了直接将内核大小从9增加到13会降低精度，而重新参数化解决了这个问题。然后，我们将ImageNet训练的模型转移到使用DeepLabv3+[15]对城市景观进行语义分割[21]。我们只更换主干，并保留彩信[19]提供的所有默认训练设置。观察结果与ImageNet上的类似：3×3重新参数使9×9模型的MIU值提高了0.19，13×13模型的MIU值提高了0.93。有了这样简单的重新参数化，将内核大小从9增加到13不再降低ImageNet和Cityscape上的性能。

众所周知，VITS存在优化问题，特别是在小数据集上[34，57]。一种常见的解决方法是引入卷积优先，例如，向每个自我注意块[18，96]添加一个DW 3×3卷积，这与我们的类似。这些策略在网络之前引入了额外的翻译等价性和局部性，使得在不损失通用性的情况下更容易在小数据集上进行优化。类似于VIT的行为[34]，我们还发现，当预训练数据集增加到7300万个图像时(参见下一节中的RepLKNet-XL)，可以在不降级的情况下省略重新参数化。

4. 大核卷积对下游任务的提升更加明显；

表3(重新参数后)显示了将MobileNet V2的核大小从3×3增加到9×9，使ImageNet的准确率提高了1.33%，而城市景观提高了3.99%。表5显示了类似的趋势：当内核大小从[3，3，3，3]增加到[31，29，27，13]时，ImageNet的精度只提高了0.96%，而ADE20K[114]上的MIEU提高了3.12%。这种现象表明，ImageNet得分相近的模型在下游任务中可能具有非常不同的能力(就像表5中排名最低的3个模型一样)。

是什么导致了这一现象？首先，大核设计显著增加了有效感受野(ERF)[63]。大量的研究已经证明，暗示大量ERF的“上下文”信息在许多下游任务中是至关重要的，例如对象检测和语义分割[61，67，93,101,102]。我们将在SEC讨论这个话题。5.第二，我们认为另一个原因可能是较大的内核设计会给网络带来更多的形状偏差。简而言之，ImageNet图片可以根据纹理或形状进行正确分类，如[7，35]中所建议的。然而，人类识别物体主要是基于形状线索而不是纹理，因此形状偏好较强的模型可能会更好地转移到下游任务。最近的一项研究[88]指出，VITS的形状偏见很强，这在一定程度上解释了VITS在转移任务中超级强大的原因。相比之下，在ImageNet上训练的传统CNN倾向于偏向纹理[7，35]。幸运的是，我们发现简单地增大CNN中的核大小可以有效地改善形状偏差。详情请参阅附录C。

5. 即使在小特征图上，大核卷积也很有用。

为了验证这一点，我们将MobileNet V2最后阶段的DW卷积扩大到7×7或13×13，因此内核大小与特征映射大小（默认为7×7）相当甚至更大。我们按照指南3的建议对大内核应用重新参数化。表4显示尽管最后阶段的卷积已经涉及非常大的感受野，但是进一步增加内核大小仍然会导致性能改进，特别是在诸如Cityscapes的下游任务上。

当核大小变大时，注意CNN的平移等价并不严格成立。如图3所示，相邻空间位置的两个输出仅共享一小部分核权重，即，通过不同的映射进行变换。该物业也符合VITS的“哲学”--在获得更多容量之前放松对称性。有趣的是，我们发现在变压器领域广泛使用的二维相对位置嵌入[5，76]也可以被视为大的深度方向的核，其大小为(2H−1)×(2W−1)，其中H和W分别为特征图的高度和宽度。大核函数不仅有助于学习概念之间的相对位置，而且由于填充效应还可以对绝对位置信息进行编码[51]。

基于上述指导方针，我们提出了一种名为 RepLKNet 的新架构，这是一种 纯 CNN架构网络，其中使用重参数化方法的大卷积来构建大的感受野。我们的网络总体上遵循 Swin Transformer [60] 的宏观架构，并进行了一些修改，同时用大的深度卷积替换了多头自注意力。

据我们所知，到目前为止，CNN仍然主导着小模型[108,110]，而视觉转换器被认为在更复杂的预算下比CNN更好。因此，在本文中，我们主要关注相对较大的模型(其复杂度与ResNet-152[40]或Swin-B[59]相当或更大)，以验证较大的内核设计是否能够消除CNN和VITS之间的性能差距。

我们主要对中型和大型模型进行基准测试，因为过去人们认为 ViT 在大数据和模型上优于 CNN。在 ImageNet 分类上，我们的基线（与 Swin-B 相似的模型大小），其内核大小高达 31×31，仅在 ImageNet1K 数据集上训练的 top-1 准确率达到 84.8%，比 Swin-B 好 0.3%，但延迟效率更高。

更重要的是，我们发现大内核设计在下游任务上特别强大。例如，在相似的复杂度和参数预算下，我们的网络在 COCO 检测 [56] 上比 ResNeXt-101 [100] 或 ResNet101 [41] 骨干网高出 4.4%，在 ADE20K 分割 [115] 上高出 6.1%，这也与甚至比对应的 Swin Transformers 更好，但推理速度更快。鉴于更多的预训练数据（例如 73M 图像）和更多的计算预算，我们的最佳模型在具有相似模型大小的最先进技术中获得了非常有竞争力的结果，例如ImageNet 上 87.8% 的 top-1 准确率和 ADE20K 上 56.0% 的准确率，这表明对大规模深度学习应用程序具有出色的可扩展性。我们认为 RepLKNet 的高性能主要是因为我们通过大内核构建的大有效感受野 (ERF) [64]，如图 1 所示。此外，我们的实验表明 RepLKNet 比传统的 CNN 利用更多的形状信息，部分符合人类的认知。我们希望我们的发现可以帮助理解 CNN 和 ViT 的内在机制。

Models with Large Kernels

除了少数像inceptions[79-81]这样的老式模型外，大型内核模型在VGG-Net之后变得不再流行[77]。一个有代表性的工作是全局卷积网络(GCNS)[67]，它使用非常大的1×K和K×1的卷积来改进语义分割任务。然而，据报道，较大的内核会损害ImageNet的性能。

局部关系网络(Local Relationship Networks，LRNet)[45]提出了一种空间聚集算子(LRLayer)来代替标准卷积，后者可以被视为动态卷积。7×7的核对LR-Net有好处，9×9的LR-Net性能下降。当核的大小和特征图一样大时，TOP-1的准确率从75.7%降到68.4%。

最近，Swin Transformers[59]提出了通过转移窗口注意力来捕捉空间模式，其窗口大小从7到12，这也可以被视为大核的变体。后续的[32，58]使用了更大的窗口尺寸。

受这些局部变压器的成功启发，最近的一项工作[38]用静态或动态的7×7沿深度方向的卷积取代了MHSA层[59]，同时仍然保持了可比较的结果。虽然[38]提出的网络与我们的设计模式相似，但动机不同：[38]没有研究ERF、大核与性能之间的关系；相反，它将视觉转换器的优越性能归因于稀疏连接、共享参数和动态机制。

另外三部代表作是Global Filter Networks(GFNets)[72]、CKConv[74]和FlexConv[73]。GFNet优化了傅立叶域中的空间连接权值，相当于空间域中的圆形全局卷积。CKConv将核函数表示为连续函数来处理序列数据，可以构造任意大小的核函数。FlexConv为不同的层学习不同的内核大小，可以与功能图一样大。尽管它们使用非常大的内核，但它们并不打算回答我们想要的关键问题：为什么传统的CNN性能不如VITS，以及如何在普通CNN中应用大的内核。此外，[38]和[72]都不在强基线上评估他们的模型，例如，大于SwinL的模型。因此，目前还不清楚大核CNN是否能像变压器一样扩大规模。

ConvMixer[87]使用高达9×9的卷积来替换VITS[34]或MLP[84，85]的“混合器”组件。MetaFormer[103]指出，集中注意力是自我关注的替代方式。ConvNeXt[60]采用7×7深度卷积来设计强结构，突破了CNN性能的极限。虽然这些作品表现出出色的性能，但它们并没有从更大的卷积(例如，31×31)中获得好处。

Model Scaling Techniques

给定一个小模型，为了获得更好的性能，通常的做法是将其放大，因此，缩放策略在最终的精度和效率之间的权衡中起着至关重要的作用。对于CNN，现有的伸缩方法通常关注模型深度、宽度、输入分辨率[31，68，82]、瓶颈比和组宽度[31，68]。然而，内核大小经常被忽略。在证券交易委员会。3，我们将证明核大小也是CNN中一个重要的伸缩维，特别是对于下游任务。

Structural Re-parameterization

结构再参数化[26-30]是一种通过转换参数来等价转换模型结构的方法。例如，RepVGG针对深度推理时类似VGG(例如，无分支)的模型，并在训练过程中构建了平行于3×3层的额外ResNet样式的快捷方式。与难以训练的真实VGG模型相比，这种捷径帮助该模型达到了令人满意的性能。经过训练后，通过一系列的线性变换将捷径吸收到并行的3×3核中，从而使得到的模型成为类VGG模型。在本文中，我们使用这种方法将一个相对较小的(例如，3×3或5×5)核添加到一个非常大的核中。通过这种方式，我们使非常大的核能够捕获小规模的模式，从而提高了模型的性能。

![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709111517.png)

Stem 指的是起始层。由于我们的目标是下游密度预测任务的高性能，我们希望在开始时通过几个卷积层捕获更多细节。在第一次3×3和2×下采样之后，我们安排了一个DW 3×3层来捕获低电平图案，一个1×1卷积，以及另一个DW 3×3层用于下采样。

Stages 1-4 每个都包含几个RepLK块，它们使用快捷方式(准则2)和DW大内核(准则1)。我们通常在DW转换之前和之后使用1×1转换。请注意，每个DW大卷积使用一个5×5的核进行重新参数化(准则3)，图4中没有显示。除了大卷积层提供了足够的接受场和聚集空间信息的能力外，模型的表示能力也与深度密切相关。为了提供更多的非线性和跨通道的信息通信，我们希望使用1×1层来增加深度。受广泛应用于变压器[34，59]和MLP[26，84，85]的前馈网络(FFN)的启发，我们使用了类似于CNN式的块，它由快捷方式、BN、两个1×1层和Gelu[41]组成，因此被称为ConvFFN块。与传统的FFN在完全连通层之前使用层归一化[3]相比，BN的优点是可以融合到Conv中进行有效的推理。通常，ConvFFN模块的内部通道数为4×作为输入。只需在交织注意块和FFN块的Vit和Swin之后，我们在每个RepLK块之后放置一个ConvFFN。

Transition Blocks 放置在stages之间，首先通过1×1卷积增加通道尺寸，然后以DW 3×3卷积进行2×下采样。

总而言之，每个阶段具有三个体系结构超参数：RepLK块的数量B、通道维度C和内核大小


我们通过固定B=[2，2，18，2]，C=[128,256,512,1024]，变化K，并观察分类和语义分割的性能，来继续评估RepLKNet上的大核函数。在没有仔细调整超参数的情况下，我们随意地将核大小分别设置为[13，13，13，13]，[25，25，25，13]，[31，29，27，13]，并将这些模型称为RepLKNet-13/25/31。我们还构造了两条小内核基线，其中内核大小都是3或7(RepLKNet-3/7)。在ImageNet上，我们使用AdamW[62]优化器、随机增强[22]、Mixup[106]、CutMix[105]、随机擦除[113]和随机深度[48]，在最近的工作[4，59，60，86]的基础上，训练了120个纪元。对于语义分割，我们使用的是ADE20K[114]，这是一个广泛使用的大规模语义分割数据集，包含150个类别的20K图像用于训练，2K用于验证。我们使用经过ImageNet训练的模型作为主干，采用MMS eggation[19]在80K迭代训练设置下实现的UperNet[97]，并对单尺度MEU进行了测试。表5显示了不同内核大小下的结果。在ImageNet上，虽然将内核大小从3增加到13会提高精度，但将内核大小增加到更大并不会带来进一步的改进。然而，在ADE20K上，将内核从[13，13，13，13]扩展到[31，29，27，13]会带来0.82个更高的MIU值，仅增加5.3%的参数和3.5%的FLOPS，这突显了大内核对于下游任务的重要性。在接下来的小节中，我们使用具有更强训练配置的RepLKNet31来与ImageNet分类、城市景观/ADE20K语义分割和COCO[55]对象检测的最新技术进行比较。我们将上述模型称为RepLKNet-31B(B表示基本)，将C=[192,384,768,1536]的更广泛的模型称为RepLKNet31L(大)。我们构造了另一个RepLKNet-XL，C=[256,512,1024,2048]，在RepLK块中采用1.5×反向瓶颈设计(即DW大卷积层的沟道为1.5×作为输入)。


由于RepLKNet的整体架构类似于Swin，我们希望首先进行比较。对于ImageNet-1K上的RepLKNet-31B，我们将上述训练时间表扩展到300个纪元，以便进行公平的比较。然后对输入分辨率为384×384的Swin-B模型进行30个历元的精调，使总的训练成本大大低于Swin-B模型从零开始训练的384×384。然后在ImageNet-22K上对RepLKNet-B/L模型进行预训练，在ImageNet-1K上对Finetune模型进行预训练。RepLKNetXL在我们的私有半监督数据集MegData73M上进行了预训练，该数据集在附录中介绍。我们还给出了在相同的2080Ti GPU上测试的批处理大小为64的吞吐量。附录中提供了培训配置。表6显示，尽管非常大的核不是用于ImageNet分类的，但我们的RepLKNet模型显示了精度和效率之间的良好折衷。值得注意的是，仅通过ImageNet-1K训练，RepLKNet-31B的准确率达到84.8%，比Swin-B高0.3%，运行速度快43%。尽管RepLKNet-XL比Swin-L有更高的FLOPS，但它运行得更快，这突出了非常大的内核的效率。

4.4.。语义分割

我们然后使用预先训练的模型作为城市景观(表7)和ADE20K(表8)的主干。具体地说，我们使用由MMS egationation[19]实现的UperNet[97]，对于城市景观具有80K迭代训练时间表，对于ADE20K具有160K迭代训练时间表。由于我们只希望评估主干，所以我们不使用任何高级技术、技巧或定制算法。在城市景观上，ImageNet-1K预先训练的RepLKNet31B的表现远远超过Swin-B(单刻度Miou为2.7)，甚至超过ImageNet22K预先训练的Swin-L。即使配备了为视觉转换器定制的DiversePatch[36]技术，22K预训练的Swin-L的单比例尺Mou仍然低于我们的1K预训练的RepLKNet-31B，尽管前者有2×参数。在ADE20K上，RepLKNet-31B在1K和22K的预训练中都优于Swin-B，单尺度Miou的差距尤为显著。在我们的半监督数据集MegData73M的预训练下，RepLKNet-XL达到了56.0的MIU值，这表明了RepLKNet-XL对于大规模视觉应用的可行可扩展性。

对于目标检测，我们使用RepLKNets作为FCOS[83]和Cascade MASK R-CNN[8，39]的主干，这两种方法分别是一阶段和两阶段检测方法的代表，以及MMDetect中的默认配置[12]。FCOS模型使用2x(24个纪元)的训练时间表进行训练，以便与来自相同代码库的X101(ResNeXt-101[99]的缩写)基线进行公平比较[19]，而使用级联掩码R-CNN的其他结果都使用3x(36纪元)。同样，我们只是更换主干，不使用任何先进技术。表9显示，RepLKNet的性能比ResNeXt-101-64x4d高出4.4 MAP，同时具有更少的参数和更低的FLOPS。请注意，使用HTC[11]、HTC++[59]、软NMS[6]或6x(72个纪元)时间表等高级技术可以进一步改进结果。与Swin相比，RepLKNet以更少的参数和更低的FLOPS实现了更高或更接近的MAP。值得注意的是，RepLKNet-XL达到了55.5的MAP，这再次证明了可伸缩性。

## 引文
