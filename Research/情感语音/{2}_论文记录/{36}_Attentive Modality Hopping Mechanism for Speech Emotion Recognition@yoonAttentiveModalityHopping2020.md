---
title: "Attentive Modality Hopping Mechanism for Speech Emotion Recognition"
description: ""
citekey: yoonAttentiveModalityHopping2020
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：Attentive Modality Hopping Mechanism for Speech Emotion Recognition
>2. Author：Seunghyun Yoon, Subhadeep Dey, Hwanhee Lee, Kyomin Jung
>3. Entry：[Zotero link](zotero://select/items/@yoonAttentiveModalityHopping2020) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yoon et al_2020_Attentive Modality Hopping Mechanism for Speech Emotion Recognition.pdf>)
>4. Other：2020 - ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***


## ⭐ 重点

- 

## 摘要

> [!abstract] In this work, we explore the impact of visual modality in addition to speech and text for improving the accuracy of the emotion detection system. The traditional approaches tackle this task by independently fusing the knowledge from the various modalities for performing emotion classification. In contrast to these approaches, we tackle the problem by introducing an attention mechanism to combine the information. In this regard, we first apply a neural network to obtain hidden representations of the modalities. Then, the attention mechanism is defined to select and aggregate important parts of the video data by conditioning on the audio and text data. Furthermore, the attention mechanism is again applied to attend the essential parts of speech and textual data by considering other modalities. Experiments are performed on the standard IEMOCAP dataset using all three modalities (audio, text, and video). The achieved results show a significant improvement of 3.65% in terms of weighted accuracy compared to the baseline system.

> 

## 预处理

## 概述

## 结果

## 精读

### 引文

## 摘录
