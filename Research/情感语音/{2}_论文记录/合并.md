---
title: "{{title}}"
description: ""
author: ""
tags: [""]
categories: ""
keywords:  [""]
draft: true
layout: ""
date: 2023-04-11 18:13:35
lastmod: 2023-04-11 18:13:44
---


---
title: "Speech Emotion Recognition with Multi-Task Learning"
description: ""
citekey: caiSpeechEmotionRecognition 2021 a
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:27:21
lastmod: 2023-04-11 10:58:46
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition with Multi-Task Learning
>2. Author：Xingyu Cai, Jiahong Yuan, Renjie Zheng, Liang Huang, Kenneth Church
>3. Entry：[Zotero link](zotero://select/items/@caiSpeechEmotionRecognition2021a) [URL link](https://www.isca-speech.org/archive/interspeech_2021/cai21b_interspeech.html) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cai et al_2021_Speech Emotion Recognition with Multi-Task Learning.pdf>)
>4. Other：2021 - Interspeech 2021  ISCA   -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 论文源文件
- [开源代码](https://github.com/TideDancer/interspeech21_emotion)
- 利用预先训练好的 Wave 2 vec-2.0 进行语音特征提取，通过情感分类(Ser)和语音识别(ASR)两个任务对 SER 数据进行微调。
- 改进了训练时的 loss 值，额外添加了文本识别损失（CTC，忽略文本和语音长度差异，有效反向传播梯度）
- 语音识别(ASR)可以作为副产品获得。
- 最终预测时，将 Softmax 算子替换成 argmax 算子。- 论文源文件
- [开源代码](https://github.com/TideDancer/interspeech21_emotion)
- 利用预先训练好的 Wave 2 vec-2.0 进行语音特征提取，通过情感分类(Ser)和语音识别(ASR)两个任务对 SER 数据进行微调。
- 改进了训练时的 loss 值，额外添加了文本识别损失（CTC，忽略文本和语音长度差异，有效反向传播梯度）
- 语音识别(ASR)可以作为副产品获得。
- 最终预测时，将 Softmax 算子替换成 argmax 算子。

## 摘要

> [!abstract] 
> Speech emotion recognition (SER) classifies speech into emotion categories such as: Happy, Angry, Sad and Neutral. Recently, deep learning has been applied to the SER task. This paper proposes a multi-task learning (MTL) framework to simultaneously perform speech-to-text recognition and emotion classification, with an end-to-end deep neural model based on wav 2 vec-2.0. Experiments on the IEMOCAP benchmark show that the proposed method achieves the state-of-the-art performance on the SER task. In addition, an ablation study establishes the effectiveness of the proposed MTL framework.

> 语音情感识别(SER)将语音分为快乐、愤怒、悲伤和中性等情感类别。最近，深度学习被应用到 SER 任务中。提出了一种同时进行语音到文本识别和情感分类的多任务学习(MTL)框架，并基于 Wav 2 vec-2.0 建立了端到端的深度神经模型。在 IEMOCAP 基准测试平台上的实验表明，该方法在 SER 任务上达到了最好的性能。此外，一项消融研究确定了拟议的 MTL 框架的有效性。

## 预处理

## 概述

## 结果

IEMOCAP

![]({4}_Speech%20 Emotion%20 Recognition%20 with%20 Multi-Task%20 Learning@caiSpeechEmotionRecognition 2021 a.assets/image-20220417160641.png)

## 精读

IEMOCAP

![]({4}_Speech%20 Emotion%20 Recognition%20 with%20 Multi-Task%20 Learning@caiSpeechEmotionRecognition 2021 a.assets/image-20220304005708.png)

许多识别模型使用的频谱特征，以及韵律特征的显式表示、音质特征和基于 Teager 能量算子的特征[5]。这些方法需要很强的领域知识和对语音的深入理解。

**Multi-task Learning**

多任务学习使用共享架构模型，同时优化不同任务的多个目标，其优势是辅助信息和不同任务之间的能够交叉正则化(隐含地，任务 a 可以是任务 b 目标的调节器 )。同时，联合优化带来了挑战[18]。

**Wav 2 Vec-2.0: Pretraining with Fine-Tuning for Speech**

预训练阶段通常在无监督的情况，使用一个大数据集训练一个模型，比如 bert，让模型学习数据的有意义表示，预训练完成后，该模型就可以通过使用相对较少的带有标签的受监督的训练数据对特定的下行任务数据进行微调训练。预训练阶段所获得的信息，可以帮助训练针对下行特定任务数据的模型且不容易过拟此类特定任务的数据。

wav 2 vec-2.0 ，使用类似于 bert 所采用的无监督方法，通过对大量的音频数据进行预先训练来学习语音表达，试图恢复编码音频特征的随机掩码部分。经过训练之后，可以针对不同的下游任务上进行了微调。

**Model Architecture**

我们提出了一种端到端模型，输入原始语音波形，并输出预测的情感标签。

我们将预先训练的 Wav2vec-2.0模型表示为 $f(·)$ ，假设输入波形为 $x$ ，其长度为 $L$ (样本数量： $L$ )，从 Wav2vec-2.0中最后一个隐藏层得到的输出作为特征 $z$ ，即 $z=f(X)$ ，对应灰色块部分。后面 ASR 路径（使用由 V=32个字符组成的词汇表，其中有26个英文字母和几个标点符号），以 $z$ 为输入，通过一个完全连接层(FC 块) $g$ ，获得对字符的预测： $y=g(f(X))$ 。后面的另一个的 SER 路径，以 $z$ 为输入，通过池化层 P 和全连接层 h，获得对情感类的预测： $C=h(ΣP(z_{i}))$ 。在两条路径的末端，我们对 $y$ 和 $c$ 应用 Softmax 算子，以将它们转换为概率向量，然后通过 CTC 和 CrossEntropy 分别求取 ASR 和 SER 的损失，并最终联合为最终损失值 $\mathcal{L}=\mathcal{L}_{\mathrm{CE}}+\alpha \mathcal{L}_{\mathrm{CTC}}$ 。最终预测时，将 Softmax 算子替换成 argmax 算子。

### 引文

- 在早期的工作[4]中，发现支持在向量机、LDA、QDA 和 HMM 中，支持向量机和隐马尔可夫模型的表现相对较好。人们经常发现组合方法更有效[9，10，11]，分类准确性更高。

- 作者在[12]中对 CNN 和 LSTM 结构进行了评估，发现 3 层卷积层加上一个双 LSTM 层的级联得到了最好的结果。

- 在[13]中，骨干卷积网络 ResNet-101 来提供更强的特征提取。

- 在文献[14]中，作者提出了一个由注意力滑动递归神经网络(ASRNN)组成的模型。

- 在文献[15]中，作者结合了编码的语言特征和声学特征，建立了一个多头自我注意模型来研究这两个特征对 SER 任务的影响。

- 在[16]中，作者利用了基于深度注意力的语言模型，并将停顿作为检测情绪的关键特征。

- 在[17]中，从几个方面对 CNN+注意和 biLSTM+注意两种模式进行了评估和比较。

- 在计算机视觉领域，[19]，同时在 12 个不同数据集上使用 MTL 模型，在其中的 11 个数据集上取得了最先进的结果。

- 在语音识别领域，[20,21]将 Connectionist Temporal Classification (CTC)[22]映射层与基于注意力的解码器结合，在一个 shared attention encoder（共享注意力编码器）上，执行端到端的语音识别(ASR 任务)。

- text-to-speech(TTS)模型 FastSpeech-2 [23]联合使用 mel 语谱图以及韵律：音高、持续时间和能量。

- 众所周知的深度语言模型 BERT [24]在训练前阶段采用了两个任务: 掩蔽令牌预测和下一句预测。

- 对于编码任务，[25]同时根据性别和情绪对输入话语进行分类; 这种方法表现得比只预测情绪的模型更好。

- 此外，MTL 与其他技术密切相关，例如迁移学习[26]和持续学习[27]。

- 深度语言模型 ERNIE-2.0 [28]在 MTL 框架中结合了持续学习框架并并在 GLUE 任务上取得了最佳结果。

---
title: "Speech Emotion Recognition Using Semantic Information"
description: ""
citekey: tzirakisSpeechEmotionRecognition2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:27:40
lastmod: 2023-04-11 10:59:51
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition Using Semantic Information
>2. Author：Panagiotis Tzirakis, Anh Nguyen, Stefanos Zafeiriou, Björn W. Schuller
>3. Entry：[Zotero link](zotero://select/items/@tzirakisSpeechEmotionRecognition2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tzirakis et al_2021_Speech Emotion Recognition Using Semantic Information.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] Speech emotion recognition is a crucial problem manifesting in a multitude of applications such as human computer interaction and education. Although several advancements have been made in the recent years, especially with the advent of Deep Neural Networks (DNN), most of the studies in the literature fail to consider the semantic information in the speech signal. In this paper, we propose a novel framework that can capture both the semantic and the paralinguistic information in the signal. In particular, our framework is comprised of a semantic feature extractor, that captures the semantic information, and a paralinguistic feature extractor, that captures the paralinguistic information. Both semantic and paraliguistic features are then combined to a unified representation using a novel attention mechanism. The unified feature vector is passed through a LSTM to capture the temporal dynamics in the signal, before the final prediction. To validate the effectiveness of our framework, we use the popular SEWA dataset of the AVEC challenge series and compare with the three winning papers. Our model provides state-of-the-art results in the valence and liking dimensions.1

> 语音情感识别是人机交互、教育等众多应用中的一个关键问题。尽管近年来取得了一些进展，特别是随着深度神经网络(DNN)的出现，但文献中的大多数研究都没有考虑语音信号中的语义信息。在本文中，我们提出了一种新的框架，可以同时捕捉信号中的语义信息和副语言信息。特别是，我们的框架由一个捕获语义信息的语义特征抽取器和一个捕获副语言信息的副语言特征抽取器组成。然后，使用一种新的注意机制将语义特征和并列特征组合成统一的表示。在最终预测之前，统一的特征向量通过 LSTM 来捕获信号中的时间动态。为了验证我们的框架的有效性，我们使用了 AVEC 挑战系列中流行的 SEWA 数据集，并与三篇获奖论文进行了比较。我们的模型在价位和喜好维度上提供了最先进的结果。

## 预处理

## 概述

## 结果

Speech2Vec

计算结果系数

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161416.png)

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161252.png)

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161340.png)

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161312.png)

## 精读

语音情感识别是人机交互和教育等众多应用中的一个关键问题。尽管近年来取得了一些进展，特别是随着深度神经网络（DNN）的出现，文献中的大多数研究都没有考虑语音信号中的语义信息。

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220304005759.png)

我们的跨模式框架可以利用语义(高级)信息(SEC。3.1)和语音信号中的副语言(低级)动态(Sec.。3.2)。使用一种新的注意力融合策略(SEC)将低级别和高级特征集融合在一起。3.3)在将它们馈送到单层LSTM模块之前，捕获信号中的时间动态，用于最终的帧级预测。

1. 第一步捕获语音信号中的 semantic information

	训练了 Speech2Vec 和 Word2Vec 模型（可以分别使用语音数据和文本信息获得含语义信息的 Speech embedding  spaces 和 Word embedding  spaces）得到特征矩阵，然后使用[15]的方法(将得到的 embedding  spaces 通过域对抗方法[^Adversarialtraining]得到对齐后的 embedding  spaces )。即通过领域对抗性训练来学习。其中鉴别器试图最小化$L_{D}\left(\theta_{D} \mid W\right)=-\frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}\left(\text { speech }=1 \mid W s_{i}\right)-\frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}\left(\text { speech }=0 \mid t_{i}\right)$，其中$θ$是鉴别器的参数，$P_{\theta_{D}}(speech=1/0|z)$是特征矩阵$z$源自语音特征矩阵或文本特征矩阵的概率，使其能准确识别特征矩阵$z$的来源。而生成器试图最小化$L_{G}\left(W \mid \theta_{D}\right)=-\frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}\left(\text { speech }=0 \mid W s_{i}\right) -\frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}\left(\text { speech }=1 \mid t_{i}\right)$，持续优化$W$来欺骗鉴别器，使$T'=WS$和$T$尽可能相似，最终使$W$将得到的特征向量矩阵对齐。

2. 上述公式平等对待所有嵌入向量，而频率较高的词在向量空间中的嵌入质量会比频率较低的词更好。

	为此，我们使用频繁词来创建自定字典，该字典指定哪些语音嵌入向量{$S_{k}$}对应于哪些文本嵌入向量{$T_{k}$}，继续改善映射矩阵$W$。最终优化出$W^{∗}= \mathop{argmin}\limits_{W}||WS_{k}−T_{r}||_{F}$，公式解可由$S_{r} T_{r}^{T}$的奇异值分解得到的，即$S V D\left(S_{r} T_{r}^{T}\right)=$ $U \Sigma V^{T}$。**并最终得到想要的 Speech Semantic Features：$X_{s}$** 

3. 第三步为了得到语音信号中的 paralinguistic information
	使用原始波形作为输入，通过一个三层的一维卷积神经网络得到 Paralinguistic Features $X_{p}$ ![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220304012843.png)

4. 融合两个特征（简单串联或注意力机制融合）

    - 将 $X_{s}$和 $X_{p}$  简单的串联得到$X_{fusion}=[X_{s},X_{p}]$。
    - 每个特征集点乘线性映射矩阵$W_{s},W_{p}$，使其位于相同的向量空间（相同纬度），第一层使用注意力机制融合这两种特征得到$\tilde{\mathbf{x}}_{sp}=\text {Attention}\left(\tilde{\mathbf{x}}_{s}, \tilde{\mathbf{x}}_{p}\right)$，通过三个具有相同维度的全连接层(FC)，得到三种信息流$a,v,l$，刚好对应arousal, valence, liking 情感维度。然后再通过两个注意力机制一步步融合三种特征得$X_{fusion}=\text {Attention}\left(\text {Attention}\left(a, l\right), v\right)$

5. 在最终预测之前，$X_{fusion}$**通过 ==LSTM== 来捕获信号中的时间动态。**

6. 模型设置

Adam优化方法[27]、固定学习速率为10−4、小batch 大小25，序列长度为300，dropout[28]率为0.5、LSTM网络的 gradient norm clipping为5.0、原始波形长10秒、采样率22050 Hz、目标损失函数Concordance Correlation Coefficient (ρc)

### 引文

- 已经广泛用于情绪识别任务的两种模式是语音和文本[11,12]。为此，几个系统已经表明，整合这两种模式，可以获得强大的性能收益[11]。

- Word2Vec [13]、 Speech2Vec [14]对齐它们的两个嵌入空间，使得Speech2Vec功能尽可能接近Word2Vec功能[15]

- 虽然在模型的训练阶段，唤醒和效价维度很容易整合到单个网络中，但可喜性维度可能会导致收敛和泛化困难[16,17]。

- 例如，Trigeorgis等[22]利用卷积神经网络捕获信号中的空间信息，并利用时间信息的递归神经网络。

- 在一项类似的研究中，Tzirakis等[23]表明，使用更深的架构和更长的输入窗口可以产生更好的结果。

- 在另一项研究中，Neumann等[25]提出了一种将CNN与注意力相结合的注意卷积神经网络（ACNN）。

- 特别是，Tzirakis等[8]使用音频和视觉信息进行连续情绪识别。虽然这项研究取得了良好的结果，但它利用这两种模式进行模型的培训和评估。在最近的一项研究中，Albanie等人。

- 将知识从视觉信息（面部表情）转移到语音模型。

---
title: ""
author: "石昌文"
tags: [""]
description: ""
categories: [""]
keywords:  [""]
type: ""
draft: true
layout: 
data: 2022-04-17 14:15:55
lastmod: 2022-06-06 19:31:43
---

# 重点

- 论文源文件
- 缓解传统方法没有考虑到不同听众的个性化情感感知。
- 最终结果可以转换为传统的情绪感知结果（求平均）
- 采用的方法是自适应方法（自适应全连接（AFC），自适应LSTM（ALSTM）和自适应CNN（ACNN）
- 采用了一个更大型的、持续更新的开源库MSP-Podcast。

思考：
- 把所有听众都考虑了，每个听众都是不同的，那么这些听众是否可以进一步分类，即不同类人有不同的情感判别特性，然后根据类别设计不同模型？
- 优点：计算次数变少，把人分类后，可扩展性也更好。

# 摘要

This paper presents a novel speech emotion recognition scheme that can deal with the individuality of emotion perception. Most conventional methods directly model the majority decision of multiple listener’s perceived emotions. However, emotion perception varies with the listener, which means the conventional methods can mismatch the recognition results to human perception. In order to mitigate this problem, we propose a Listener Adaptive (LA) model that reflects emotion recognition criteria of each listener. One-hot listener codes with several adaptation layers are employed in the LA model. The LA model yields the posterior probabilities of the listener-specific perceived emotions. Majority-voted emotion can be also estimated by averaging, in the LA model, the posterior probabilities for all listeners. Experiments on two emotional speech datasets demonstrate that the proposed approach offers improved listener-wise perceived emotion recognition performance in natural speech.

提出了一种新的能够处理情感感知个性化的语音情感识别方案。大多数传统方法直接对多个听者的感知情绪的多数决策进行建模。然而，情感感知随着听者的不同而不同，这意味着传统的识别方法可能会使识别结果与人的感知不匹配。为了缓解这一问题，我们提出了一种反映每个听者情感识别标准的听者自适应(LA)模型。LA模型采用具有多个适配层的One-HotLister编码。LA模型产生特定于听者的感知情绪的后验概率。在LA模型中，多数人投票的情绪也可以通过平均所有听众的后验概率来估计。在两个情感语音数据集上的实验表明，该方法在自然语音中具有更好的听者感知情感识别性能。

# 词汇记录

# 结果

IEMOCAP&&MSP-Podcast

![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220417161651.png)

# 精读

![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220304005823.png)

![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220304005857.png)

大多数传统情感识别方法，将声音特征输入模型后，经由解码器和编码器得到后验概率，最终从中选出最大概率对应的标签，即为情感预测标签。而在此方式中所使用的训练集中的真实标签，是由一组参与情感标签标注的人，对声音序列进行标注，并取出投票数最多的情感标签作为其真实标签得到的，即 the majority decision of multiple listener’s perceived emotions 方法。然而现实情况是情绪感知会因听者而异（听众依赖问题），这意味着传统方法可能会使识别结果与人类个体的个性化感知不匹配。为了缓解传统方法没有考虑到不同听众的个性化情感感知特征，本文提出了一个 Listener Adaptive模型 (LA，听众自适应模型) ，它可以反映每个听众的情绪感知标准。

本文所提出的方法假定每个注释训练集语音的听众都是已知的，并且包括在了训练数据中，然后用一个模型来学习每个听众的情绪感知标准，以此解决听众依赖性问题。

本文中的LA模型将 One-hot listener code **v (l)** 输入到  listener embedding layer中，得到 Listener embedding 向量 **e (l)**，然后将之作为辅助信息与声学特征 **x** 共同输入到多个adaptation layers （自适应全连接层（AFC），自适应LSTM层（ALSTM）和自适应CNN层（ACNN））中。通过切换 listener code ，用单一模型估计依赖于听众的情绪感知后验概率，最终产生每个特定听众的情绪感知的后验概率。对于传统的 Majority-voted 情绪感知结果可以通过在 LA 模型中对所有听众的后验概率进行平均来估计特定标签的概率。 

==声学特征：400维对数功率谱图，其中帧长和帧移分别为40ms 和10ms。Dft长度为1600(10hz 网格分辨率) ，使用0-4千赫兹频率范围。所有的功率谱都使用训练数据集的均值和方差进行了 z 标准化$^{[7, 19]}$。==

本文使用了两个情感语音数据集: Msp-podcast和Iemocap。Msp-podcast有非常自然的语音、大量听众以及语音标注，每个语音都至少有3个听众(平均每个语音有6-7个听众)标注。这个数据集有两种情绪标注，主要情绪和次要情绪，文中只使用了主要情绪构成训练集。 其中开放数据集中的60名发言者的8215个片段用于测试，44名发言者的4418个片段用于验证，其余的25332个片段用于训练。在训练集中，标注少于100条的听众被归类为“其他听众”，因为这些听众会使得学习依赖于听者的情绪感知特征变得困难。Iemocap 是一个被广泛使用的数据集，其中包含了一些听众标注的演讲。它包含10个专业演员(5个男性和5个女性)的视听数据。本文只使用了即兴发挥的语音，并将快乐和兴奋的标签组合成 hap 类。在这个语料库中共有6个学生听众，其中每个语音会得到3个标注，因此我们统一了每个听众的多个情绪标注，规则如下: 优先选择标注中情绪标签出现次数最多的，否则选择第一个标注。对于标注少于500条的听众被归类为“其他听众”，和 msp-podcast 一样。评估表现采用一个说话人缺失交叉验证进行比较，一个说话人用于测试，另一个用于验证，其他8个说话人用于训练。

为了阐明情绪感知中听者依赖性的存在，我们首先使用Cohen’s kappa 系数来研究听者注释的相似性。通过5级匹配((4 targets + Oth)计算相似度。我们选择 msp-podcast/iemocap 中的前10名/前3名的听众来评估两个听众对的相似性，在这些听众对中，两个听众都注释了超过20个相同的话语(结果中小于20个是“-”)。结果表明，MSP-Podcast中相当数量的听众对相似度较低。听众1与听众4,9,10表现出高度相似性，但与其他听众相似性低。另一方面，iemocap 中听众对的相似度均大于0.4(中度匹配) ，而听众1和听众3的相似度相对较低。这些表明情绪感知取决于听众，至少对于 MSP-Podcast。

基线是 majority-voted 情绪识别模型和那些受过 soft-target 目标训练的模型。在依赖听者的情绪识别任务中，基线模型的输出被看作是听者的个体估计结果。由于该方法统一了 la 模型的多个输出，因此在多数情感识别中也比较了不同初始参数情感模型的集成。听众人数为每个语音的平均听众人数，即 msp 播客和 iemocap 中的平均听众人数分别为7和3。在基线模型中，

输入 batch 的大小为8（iemocap）和16（MSPPodcast），结构包含3层 cnn（16,24,32+batch normalization+ReLu激活函数+2 × 2 max-pooling 层）、1层双向 lstm（128个隐藏单元+0.2 Dropout 率）、4-head 结构注意力层（0.2 Dropout 率）和2层 fc（64个隐藏单元+0.2 Dropout 率）。最终的损失优化方法使用的是Adam（学习率为0.0001）。

==在训练步骤中，以 inverse values of the class frequencies 作为 class weights ，以 mitigate the class imbalance problem（缓解类不平衡问题）。==采用 Speed perturbation 方法进行数据增强，其因子分别设置为0.9、0.95、1.05和1.1。SpecAugment（针对ASR的数据增强方法）也应用了两个 time and frequency masking（时频掩蔽方法）。这些方法是基于LA模型的。

单独使用 afc、 alstm 和 acnn 层，并将这些适应层组合作为 LA 模型进行评估。La 模型的结构和超参数与基线相同。在 MSP-Podcast和 iemocap 中，听者嵌入向量维数分别为32和8。在 ld 模型训练中，计算每个listener的类权重，并将 listener 权重与listener注释频率的反比值相乘，得到最终的损失权重。其他的训练条件与基线相符。

评价指标为加权准确度(wa：所有语音的分类准确度)和未加权准确度(ua：个体情绪类准确度的宏观平均值)。![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220304012859.png)

符号(ens.)是指模型的整体结果。

在 msp-podcast 中，在听者依赖任务中，具有 afc 层的 la 模型的宏观平均值显著高于软标签模型和具有comparable UAs(p  .05)的 majority-voted 模型(配对 t 检验 p < 0.05)。在多数人投票的情感识别中也出现了类似的结果;

具有相似 UA 的基线及其集合的 WA 高出 10% 以上。从这些结果和表3可以看出，在听者的情绪感知差异很 大的情况下，例如自然语言，所提出的LA模型是有效的。

在IEMOCAP中，基于AFC分层的LA模型在依赖于听者和多数投票的情绪任务中分别表现出略好于基线的表现和几乎相同的表现。这些都表明，在情感感知不太依赖于听者的情况下，所提出的方法不会恶化估计性能。最后，对LA模型中的适配层进行了比较。AFC适应层比ALSTM和ACNN有更好的效果。一种可能性是，个性化情绪感知的出现在对情绪线索的感知做出决定的编码器中，而不是从音频信号中提取情绪线索的解码器中。结果还表明，听众和说话人的个性应该在评估模型的不同部分进行建模；

## 引用

- 在口头对话系统中生成类似人类的响应[1]

- 联系中心呼叫中客户分析的声音[2]

- 最近的研究通过使用深度神经网络（DNN）取得了显着的效果[3-8]，该框架的主要优点是，通过组合不同类型的图层，可以自动学习感知情绪的复杂线索。

- 注意机制也被用来关注话语识别的局部特征[5]。

- 基于DNN的模型可以利用低级特征，例如对数功率谱图或原始波形，这些特征具有丰富但复杂的信息[6,7]。

- 卷积神经网络 (CNN) 和长短期记忆循环神经网络 (LSTM-RNN) 已被用于捕获话语中的局部和上下文特征 [3,4,8]。

- 情绪表达和情绪感知两个方面的研究：情绪呈现的方式取决于说话者，他们无意识地设定了话语的声学特征[9]。情绪的感知方式取决于听者的年龄，性别和文化[10,11]。

- 鉴于情绪表达方面的问题，说话者依赖性通常在SER中考虑。为了减轻对不同说话者的依赖性，提出了说话者适应[12]或通过多任务学习区分说话者性别[8,13]。

- 利用听众依赖的情绪感知做SER：一种方法是通过软标签[14]或多变量高斯[15]来模拟听众的情绪感知分布。但是，它不能区分听众。另一个结构个别听众模型来描述情绪感知的可变性[16]。这个框架的问题之一是模型大小；它需要与听众相同数量的隔离模型，这在大量听众的情况下是不切实际的。

- 受到语音处理中domain adaptation的启发[17,18]

- 多数票情绪识别模型直接评价多数票情绪的后验概率。该模型由编码器和解码器组成[8,19]。
- 基线是 majority-voted 情感识别模型和那些与软目标训练 [24] 。
---
title: "Hierarchical Network Based on the Fusion of Static and Dynamic Features for Speech Emotion Recognition"
description: ""
citekey: caoHierarchicalNetworkBased2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:11
lastmod: 2023-04-11 11:03:14
---

> [!info] 论文信息
>1. Title：Hierarchical Network Based on the Fusion of Static and Dynamic Features for Speech Emotion Recognition
>2. Author：Qi Cao, Mixiao Hou, Bingzhi Chen, Zheng Zhang, Guangming Lu
>3. Entry：[Zotero link](zotero://select/items/@caoHierarchicalNetworkBased2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cao et al_2021_Hierarchical Network Based on the Fusion of Static and Dynamic Features for.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 论文源文件
- 通过静态 Log-Mel Filter bank 特征（Mel滤波器组）和其动态一阶导数和二阶导数特征进行互补学习，逐步了解高层次的情绪表示。 
- 利用了一种 gate-based 的多特征融合单元，用于在帧级上有效地将不同特征整合在一起。
- 使用了 Bahdanau 可微分注意力模型计算情绪
- 使用 z-score 标准化消除说话人之间的差异- 论文源文件
- 通过静态 Log-Mel Filter bank 特征（Mel滤波器组）和其动态一阶导数和二阶导数特征进行互补学习，逐步了解高层次的情绪表示。 
- 利用了一种 gate-based 的多特征融合单元，用于在帧级上有效地将不同特征整合在一起。
- 使用了 Bahdanau 可微分注意力模型计算情绪
- 使用 z-score标准化 消除说话人之间的差异

## 摘要

> [!abstract] Many studies on automatic speech emotion recognition (SER) have been devoted to extracting meaningful emotional features for generating emotion-relevant representations. However, they generally ignore the complementary learning of static and dynamic features, leading to limited performances. In this paper, we propose a novel hierarchical network called HNSD that can efficiently integrate the static and dynamic features for SER. Specifically, the proposed HNSD framework consists of three different modules. To capture the discriminative features, an effective encoding module is firstly designed to simultaneously encode both static and dynamic features. By taking the obtained features as inputs, the Gated Multi-features Unit (GMU) is conducted to explicitly determine the emotional intermediate representations for frame-level features fusion, instead of directly fusing these acoustic features. In this way, the learned static and dynamic features can jointly and comprehensively generate the unified feature representations. Benefiting from a well-designed attention mechanism, the last classification module is applied to predict the emotional states at the utterance level. Extensive experiments on the IEMOCAP benchmark dataset demonstrate the superiority of our method in comparison with state-of-the-art baselines.

> 许多自动语音情感识别(SER)的研究致力于提取有意义的情感特征以生成与情感相关的表征。然而，它们普遍忽略了静态和动态特征的互补学习，导致了性能的限制。在本文中，我们提出了一种新的层次化网络HNSD，它能有效地集成SER的静态和动态特征。具体地说，拟议的HNSD框架由三个不同的模块组成。为了获取可区分的特征，首先设计了一个有效的编码模块，同时对静态和动态特征进行编码。将获得的特征作为输入，通过门控多特征单元(GMU)明确地确定帧特征融合的情感中间表征，而不是直接融合这些声学特征。这样，学习到的静态特征和动态特征可以联合、综合地生成统一的特征表示。得益于精心设计的注意机制，最后一个分类模块用于预测话语层面的情绪状态。在IEMOCAP基准数据集上的广泛实验表明，与最先进的基线相比，我们的方法具有优越性。

## 预处理

## 概述

## 结果

IEMOCAP

![]({8}_Hierarchical%20Network%20Based%20on%20the%20Fusion%20of%20Static%20and%20Dynamic%20Features%20for%20Speech%20Emotion%20Recognition@caoHierarchicalNetworkBased2021.assets/image-20220417161851.png)

![]({8}_Hierarchical%20Network%20Based%20on%20the%20Fusion%20of%20Static%20and%20Dynamic%20Features%20for%20Speech%20Emotion%20Recognition@caoHierarchicalNetworkBased2021.assets/image-20220417161903.png)

## 精读

![]({8}_Hierarchical%20Network%20Based%20on%20the%20Fusion%20of%20Static%20and%20Dynamic%20Features%20for%20Speech%20Emotion%20Recognition@caoHierarchicalNetworkBased2021.assets/image-20220304005951.png)

静态特征中包含了足够的频域信息。 

1. 使用汉明窗将语音信号分割成25ms 窗宽、10ms 移位的短帧。
2. 利用短时距傅里叶变换信号将信号从时域转换到频域。
3. 对 mel 滤波器组（26个）的能谱进行了对数运算。将Log-Mel 特征作为静态特征，由矩阵$X^{S}∈R^{T×N}$表示，其中 $T$ 是帧数，$N$ 是 Mel滤波器的数量。 $x^{S}_{i}∈ X^{S}$是含 $N$ 个滤波器的第 $i$ 帧（i∈[1，t]）。

动态特征描述了帧间的频谱变化，并反映了情绪的变化过程，可以使用一阶差分方程获得，并由$X^{d} = [Y, Z]$表示，$x^{d}_{i}∈ X^{d}$, i 为第 i 帧（i∈[1，t]）。

1. $y_{i} =\sum_{n=1}^{M} n\left(x_{i+n}^{s}-x_{i-n}^{s}\right) / 2 \sum_{n=1}^{M} n^{2}$
2. $z_{i} =\sum_{n=1}^{M} n\left(y_{i+n}-y_{i-n}\right) / 2 \sum_{n=1}^{M} n^{2}$

第一层是一个有效编码模块。为了防止不同的特征之间彼此干扰，通过使用长短期记忆单元(LSTM，隐藏单元512)，把$X^{S}$和$X^{d}$ 进行编码，用$f^{s}_{i}$ 和 $f^{d}_{i}$ 表示。在这个模块中可以分别学习静态特征和动态特征在时间尺度上的上下文信息，并将静态和动态特征嵌入到高维特征表示中。

第二层中引入门控多特征单元（GMU），将所获得的特征$f^{s}_{i}$ 和 $f^{d}_{i}$作为输入，通过以Tanh为激活函数的神经元$（h_{d/S}=\tanh \left(W_{d/s}\left(f^{d/s}\right)^{T}\right)）$得到 $h^{S}$和$h^{d}$ 。同时定义了一个门神经元控制$f^{s}$ 和 $f^{d}$的权重，表示为$z=\sigma\left(W_{c}\left[f^{s}, f^{d}\right]^{T}\right) ∈ R^{1×T}$，最终得到协同融合特征$h=h_{s}*z+h_{d}*(1-z)∈ R^{hs×T}$

第三层采用 Bahdanau 的注意力机制模型通过前馈神经网络计算权重，每一帧的情绪分数$s_{t}=V^{T} \tanh \left(W h_{t}+b\right)$，其反映了每个帧的情感贡献。其中 $h_{t}$ 表示 $h$ 第$t$帧的融合特征， $V∈R^{d×1}$（本文中d选用的16）和$W≥R^{d×Hs}$是权重矩阵，$b∈R^{d×1}$是偏置向量。然后将情绪分数标准化得到是 t-th 帧的$\alpha_{t}=\frac{\exp \left(s_{t}\right)}{\sum_{i=1}^{T} \exp \left(s_{i}\right)}$。而话语层次的特征就可以通过对每一帧按其贡献加权得到   $u=\sum_{t=1}^{T} \alpha_{t} h_{t}$

通过全连接层（神经元个数：128）和sorftmax层得到最终的情感识别结果。训练过程中使用Adam（学习率：0.0001，weight decay ：0.001）进行反馈优化参数

通过WA（（神经元个数：128））和UA对测试结果进行评估。

### 引文

- 许多研究已经采用了基于卷积神经网络（CNN）和经常性神经网络（RNN）的模型，以产生更多辨别性声学特征，以提高SER [2,3,4,5,6]的性能。

- “例如，李等人。 [4]设计了两个不同的卷积核，用于分别从频谱图分别捕获时间和频域特性。 李等人。''

- [5]提出了一种具有多头自注意的扩展神经网络，用于从 mel frequency 倒谱系数(mfccs)研究情绪相关特征。

- 近年来，注意机制在提高注意效果方面取得了很大进展[7,8,9]。

- [7]应用RNN以检测MFCC的时间上下文信息，并引入了可靠的关注机制，专注于语音信号的情绪相关区域。

- 陈等人。 [10]和Lee等人。 [11]考虑了语音的动态特征作为网络的输入，它们将连接的静态和动态特征转移到深层神经网络中。然而，这种策略忽略了静态和动态特征将相互干扰，这导致表现不令人满意[12]。 

- [13],给出了通过门控多特征单元(gmu)寻找情绪中间表示的灵感。

- Bahdanau注意机制[14,15]致力于在最后一个模块计算情绪突出框架。

- Bi-LSTM [11]：输入为32种特征的Bi-LSTM网络（包括F0，语音概率，过零率，12维MFCC的log energy系数，以及一阶时间导数）。  

- LSTM+CTC [9]：输入为40维的log Mel-filter bank 特征的 attention-based BI-LSTM网络，与连接主义时间分类（CTC）组合。   

- CNN+ Attention[4]：基于声谱图，使用含两组 filter的CNN提取 time-specific and frequency-specific 特征，然后将其连接起来输入到后续的卷积层中，使用 Attention pooling 方法学习  the final emotional representation.。  

- Transformer[20]：基于IS09特征集（统计功能集），结合 self-attention 研究情感语音识别。 

- CNN + RNN + Attention [10]：输入为一阶和二阶动态特征的基于attention的三维CNN 模型判别信息  

- CNN-GRU-SEQCAP [21]：考虑声谱中activities的spatial relationship的基于 capsule networks (CapsNets) 的架构，the spatial relationship of activities in spectrograms  

- Transformer + auxiliary learning [22]: 输入为log Mel-Filter Bank 特征的一种基于多任务学习的多头注意力机制的深度学习网络。

---
title: "A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion Recognition"
description: ""
citekey: rajamaniNovelAttentionBasedGated 2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:22
lastmod: 2023-04-11 17:54:18
---

> [!info] 论文信息
>1. Title：A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion Recognition
>2. Author：Srividya Tirunellai Rajamani, Kumar T. Rajamani, Adria Mallol-Ragolta, Shuo Liu, Björn Schuller
>3. Entry：[Zotero link](zotero://select/items/@rajamaniNovelAttentionBasedGated2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rajamani et al_2021_A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\TBPVYNJ 2\\9414489.html>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 论文源文件
- 提出了一种使用了增强版激活函数的方法：Attention ReLU GRU，即将 attention-based Rectified Linear Unit (AReLU) 作为激活函数的 GRU 和 BiGRU。
- openSMILE 工具包[16]提取特征
- 引出了一些重要的激活函数：SELU [18], EELU [19], Mish [20], and learnable activations such as Comb [21] and PAU [22].
- 论文源文件
- 提出了一种使用了增强版激活函数的方法：Attention ReLU GRU，即将 attention-based Rectified Linear Unit (AReLU) 作为激活函数的 GRU 和 BiGRU。
- openSMILE 工具包[16]提取特征
- 引出了一些重要的激活函数：SELU [18], EELU [19], Mish [20], and learnable activations such as Comb [21] and PAU [22].

## 摘要

> [!abstract] Notwithstanding the significant advancements in the field of deep learning, the basic long short-term memory (LSTM) or Gated Recurrent Unit (GRU) units have largely remained unchanged and unexplored. There are several possibilities in advancing the state-of-art by rightly adapting and enhancing the various elements of these units. Activation functions are one such key element. In this work, we explore using diverse activation functions within GRU and bi-directional GRU (BiGRU) cells in the context of speech emotion recognition (SER). We also propose a novel Attention ReLU GRU (AR-GRU) that employs attention-based Rectified Linear Unit (AReLU) activation within GRU and BiGRU cells. We demonstrate the effectiveness of AR-GRU on one exemplary application using the recently proposed network for SER namely Interaction-Aware Attention Network (IAAN). Our proposed method utilising AR-GRU within this network yields significant performance gain and achieves an unweighted accuracy of 68.3% (2% over the baseline) and weighted accuracy of 66.9 % (2.2 % absolute over the baseline) in four class emotion recognition on the IEMOCAP database.

> 尽管深度学习领域取得了重大进展，但基本的长短期记忆(LSTM)或门控递归单位(GRU)单位在很大程度上保持不变和未被探索。通过正确地调整和加强这些单位的各种要素，有几种可能性来推进最先进的技术。激活功能就是这样的关键要素之一。在这项工作中，我们探索在语音情感识别(SER)的背景下，在 GRU 和双向 GRU(BiGRU)细胞中使用不同的激活函数。我们还提出了一种新的注意力重现 GRU(AR-GRU)，它在 GRU 和 BiGRU 细胞中使用基于注意的整流线性单元(AReLU)的激活。我们使用最近提出的 SER 网络，即交互感知注意网络(IAN)，在一个示例性应用上演示了 AR-GRU 的有效性。我们提出的方法在这个网络中使用 AR-GRU 获得了显著的性能提升，在 IEMOCAP 数据库上的四类情感识别中，未加权准确率为 68.3%(超过基线 2%)，加权准确率为 66.9%(绝对准确率为 2.2%)。

## 预处理

## 概述

## 结果

IEMOCAP

![]({9}_A%20 Novel%20 Attention-Based%20 Gated%20 Recurrent%20 Unit%20 and%20 its%20 Efficacy%20 in%20 Speech%20 Emotion%20 Recognition@rajamaniNovelAttentionBasedGated 2021.assets/image-20220417162004.png)

## 精读

蕴含在人类声音中的副语言信息揭示了说话者的情感状态。这些信息在人与人之间的互动中至关重要，因为我们人类利用这些信息来调整，例如，我们的信息内容或者我们的声调，目的是使互动更加顺畅，并且使我们对自己的语言产生共鸣。

激活函数是改善 LSTM 或 GRU 等 Unit 的一个关键因素，因此，在本文中，我们探讨了语音情感识别中 GRU 和双向 GRU Unit 内不同的激活函数，并提出了一种新的 Attention ReLU GRU(AR-GRU)方法，即使用 AReLU （Attention-based Rectified Linear Units）作为激活函数的 GRU 和 BiGRU 。

AR-GRU 有助于在富含情绪的语音间捕捉特征间的长程交互（依赖），并在解决梯度消失问题的同时提高 SER 系统的性能。

AReLU 中是通过使用 Alpha 为 0.01 和 Beta 为-4，使 AReLU 类似于 RELU 的激活函数。因为 Alpha 是控制 negative values 的比例因子，而值 0.01 与 clamp function 相结合对 negative values 影响最小，β是控制 positive values 的比例因子，将其设置为-4 会使此比例因子失效。然后不断探索取值效果，最终得到β值为 1，Alpha 为 0.01 时，结果最好。

我们使用[13]作为基线模型，它采用了一个 BiGRU 来处理说话人的当前话语，以及两个 GRU 来处理前面的话语。基于 Emobase 2010 的配置，使用 openSMILE 工具包[16]，提取声学低级描述符（LLDs），包括梅尔-频谱系数（MFCCs）、音调和它们在语料的每个短帧中的统计数据。

使用 5 折交叉验证（5-fold Cross Validation）和留一法进行验证，通过观察每 100 个训练周期在验证集上的表现来提前停止。

![]({9}_A%20 Novel%20 Attention-Based%20 Gated%20 Recurrent%20 Unit%20 and%20 its%20 Efficacy%20 in%20 Speech%20 Emotion%20 Recognition@rajamaniNovelAttentionBasedGated 2021.assets/image-20220304010017.png)

## 扩展知识

**GRU**

门控递归单元(GRU)是递归神经网络(RNN)的一种，它使用门控机制(gating mechanisms )来控制和管理神经网络中神经元之间的信息流。GRU 的结构允许自适应地从大型数据序列捕获相关性，同时确保不会丢弃来自序列较早部分的信息。这是通过选通机制实现的，该选通机制规定了在每个时间步要保留或丢弃的信息。与 LSTM 相比，GRU 能够克服消失梯度问题，并且由于需要优化的参数数量较少，因此训练速度更快。

传统 GRU 中的经典激活函数是双曲正切(TANH)。尽管使用 tanh 函数有其固有的优点，但由于密集的激活计算，它具有很高的计算复杂度，并且容易受到消失梯度问题的影响。

[**AReLU**](https://zhuanlan.zhihu.com/p/158389615)

AReLU 一种基于注意力机制的可动态学习的线性整流单元（激活函数）[14]，可以表示为 element-wise sign-based attention 机制（基于符号的注意力机制：ELSA）和标准 Rectified Linear Unit（ReLU） 的组合，其对梯度消失的抵抗力更强，只有两个额外的可学习参数，可以在较小的学习率下促进快速的网络训练。

$\mathcal{F}\left(x_{i}, \alpha, \beta\right)=\mathcal{R}\left(x_{i}\right)+\mathcal{L}\left(x_{i}, \alpha, \beta\right)\\ \begin{cases}C(\alpha) x_{i} & , x_{i}<0 \\ (1+\sigma(\beta)) x_{i} & , x_{i}=0\end{cases}$

$X = {x_{i}}$ ,是激活层的输入， ${α, β} ∈ R^{2}$ 是可学习参数，C 是剪裁操作，作用是将 alpha 的值限制在 $[0.01, 0.99]$ 之间，阻止其变为 0， $σ$ 是 sigmoid 函数。理解：对于大于零的数据梯度进行放大，小于零的则缩小。

### 引文

- 隐马尔可夫模型(hmms)或支持向量机(svms)[3,4]。

- 包括卷积神经网络[5] 

- 回归神经网络[6,7,8] 

- cnns 和 rnns [9]

- 例如长短期记忆(lstm)[10]和门控循环单元(gru)[11] ，可以捕捉序列数据的时间动态，能够捕获的时间依赖性的声学特征。

- 注意力机制可以用来帮助 rnns 关注最突出的情感信息[6,7,8]。

- 此外，上下文信息也可以用来改善服务器系统的性能，如最近的工作[12,13]所示。

- 我们使用交互感知的注意力网络（IAAN）[13]作为基线模型，Yeh et al. [13]通过交互感知注意网络(iaan)成功地利用了语境信息，该网络利用前一个说话者在双人对话场景中的转换来学习注意力分数，以检测一个说话者话语的情绪状态。

- 基于注意力的纠正线性单元(arellu)[14]。

- BiLSTM+ATT[6]: 一个基于帧级特征的注意力池层的 bilstm 网络。

- MDNN[17]: 由多个局部分类器和一个全局分类器组成的多路径深层神经网络。

---
title: "A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers"
description: ""
citekey: wangNovelEndtoendSpeech2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:31
lastmod: 2023-04-11 17:53:09
---

> [!info] 论文信息
>1. Title：A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers
>2. Author：Xianfeng Wang, Min Wang, Wenbo Qi, Wanqi Su, Xiangqian Wang, Huan Zhou
>3. Entry：[Zotero link](zotero://select/items/@wangNovelEndtoendSpeech2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/HW-AARC-CLUB/ICASSP_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 在现有语音情绪识别(SER)模型基础上增加 stacked transformer layers (STL)结构构建新的SER体系结构, 利用STLS模块强大的特征学习能力. 
- 使用 ==t-SNE== 技术可视化中间过渡层发现的特征质量来检查 STL 模型的有效性.

## 摘要

> [!abstract] Speech emotion recognition (SER) aims to automatically recognize emotional category for a given speech utterance. The performance of a SER system heavily relies on the effectiveness of global representation expressed at utterance level. To effectively extract such a global feature, the mainstream of recent SER architectures adopts a pipeline with two key modules, feature extraction and aggregation. Although variant module designs have brought impressive progresses, SER is still a challenging task. In contrast with those previous works, herein we propose a novel strategy for global SER feature extraction by applying an additional enhancement module on top of the current SER pipeline. To verify its effect, an end-to-end SER architecture is proposed where stacked multiple transformer layers are explored to enhance the aggregated global feature. Such an architecture is evaluated on IEMO-CAP and results strongly substantiate the effectiveness of our proposal. In terms of weighted accuracy on four emotion categories, our proposed SER system outperforms the prior arts by a large margin of relatively 20% improvement. Our codes and the pre-trained SER models are made publicly available.

> 语音情感识别(SER)的目的是自动识别给定语音话语的情感类别。SER系统的性能在很大程度上依赖于在话语层面上表达的全局表征的有效性。为了有效地提取这样的全局特征，目前主流的SER体系结构采用了一种包含两个关键模块的流水线，即特征提取和聚合。尽管各种各样的模块设计已经带来了令人印象深刻的进步，但SER仍然是一项具有挑战性的任务。与前人的工作不同，本文提出了一种新的全局SER特征提取策略，在现有SER流水线的基础上增加了一个增强模块。为了验证其效果，提出了一种端到端的SER结构，其中利用堆叠的多个转换器层来增强聚集的全局特征。这种体系结构在IEMOCAP上进行了评估，结果有力地证明了我们的建议的有效性。在四个情感类别的加权准确率方面，我们提出的SER系统比现有技术有较大幅度的提高20%。我们的代码和预先培训的SER模型是公开提供的。

## 预处理

## 概述

## 结果

EMOCAP 包含超过10K 个话语，由九个情感类别的标签标注。按照前人的研究步骤，我们使用了一个四类({高兴、愤怒、悲伤、中性})的子库，总共有5531个话语。对于每个情感类别，其关联样本分别按7/1/2的比例随机分为训练/均值/测试。

所提出的SER系统已在PyTorch中实现。利用Adam[20]优化器对分类交叉熵进行优化，同时监测验证精度，提前停止设置为8个历元。训练系统的批次为32，学习率为2×10−4，衰减率为1×10−6。

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220417162209.png)

## 精读

言语情感作为文本之外的一种 meta-information，对理解说话人的心理和反应起着重要的作用。相关研究被称为语音情感识别(SER)，其目的是自动识别给定语音表达的情感类别。由于情感通常以一种微妙和可变的方式传达，因此识别情感嵌入作为话语的表征一直是一个挑战，可以有效地对情感类别进行分类。

随着深度神经网络(DNN)的最新进展，情感嵌入已经从先前基于知识的手工制作的声学特征，例如低级描述符(LLD)，演变为基于 DNN 的深度情感特征。在最近的 SER 工作中，已经探索了各种 DNN 结构，如卷积神经网络(CNN)、长短期记忆(LSTM)、时延神经网络(TDNN)、残差网络(ResNet)、扩张残差网络(DRN)，它们本身或组合在一起[1-4]。在语义特征识别领域已经有了大量的研究成果，但由于情感与语言特征的分离，以及从长时间的话语中提取有效的情感特征，这方面的研究仍然具有挑战性。为了解决这些问题，现代 SER 系统普遍采用两个模块的流水线：1)特征提取模块，以生成情感相关的时间声学特征；以及 2)聚合模块，将这些时间特征汇集到话语级别的紧凑的全局语境表示(也称为情感嵌入)中。为了产生有效的情感嵌入，最近的 SER 工作集中在开发不同的模块架构上。例如，[2]提出了一种混合的 CNN-LSTM 体系结构，其中 CNN 从原始频谱图中提取特征序列，而 LSTM 聚合长期特征依赖关系。类似地，文献[5]提出了基于 1 D 和 2 D CNN-LSTM 的 SER 系统，并表明 2 D CNNLSTM 网络通过从谱图中捕捉局部相关性和全局上下文信息而优于 1 D 网络。后来在注意力机制的启发下，RNN 在[7]中提出了 RNN-注意，RNN 通过关注情绪显著的特征来提取时间特征和聚合长期特征依赖关系。文[8]测试了几种时间建模方法，目的是从原始波形中学习深层情感特征。在[9]中，作者引入了 CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。MHA 机制再次被[4]采用来构建 DRN-注意体系结构，其中 DRN 使网络在特征学习中保持高分辨率的时间结构。此外，文献[10]还提出了语境 LSM 注意来考虑周围话语之间的关系和依存关系。尽管在以前的工作中取得了进展，但目前的 SER 表现仍有改进的空间。例如，在基准数据集 IEMOCAP 上的 SOTA 分类准确率仅为 76%左右(在四个情感类别上)。同时，我们注意到，在其他研究领域，经常有报道称，用叠层变压器层(STL)取代重复性可以显著提高性能。例如，基于 STLS 的声学模型[11]在 Librispeech 基准上给出了最好的声学模型。在问答领域，堆积式潜在注意和多跳注意网络(MAN)[12]都表现出显著的性能改进。对于图像捕获任务，从堆叠的交叉注意网络(SCAN)[13]或堆叠的注意模块[14]获得最先进的(SOTA)结果。受其他研究领域的成功启发，在本研究中，我们有兴趣将 STL 机制应用到 SER 网络中。特别是，提出了一种通过在现有 SER 流水线上添加 STL 而构建的新的 SER 体系结构。我们建议的动机是利用 STLS 模块强大的特征学习能力来直接提高流水线输出。据作者所知，这是文献中第一次使用这种策略来研究 SER 问题。本文组织如下。在第二节中，简要介绍了香草变压器。在第三节中，我们详细地提出了 STL 增强的 SER 体系结构。第四节报道了实验。第五节总结了这项研究。


在本节中，我们将简要描述源自 Vanilla Transformer[15]的 STL 的详细信息，后者完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系。在这项研究中，我们感兴趣的是 ITS 编码器，它将输入序列转换为固定维度的全局上下文向量。具体地，编码器由六个相同的变压器层(TL)堆叠而成。每个 TL 包含具有剩余连接的两个子层：1)自关注子层和 2)位置前馈网络(FFN)子层。此外，还引入了位置编码(PE)来为模型提供显式的顺序信息。得益于自我注意机制，输出序列通过重视输入的特定部分来保留和利用输入信息。TL 的堆叠方式是，只有第一层采用输入顺序，每隔一层只采用前面的输出作为输入。这使得能够通过在潜在表示空间内的迭代关注和投影来丰富上下文向量。从数学上讲，《变形金刚》的体系结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程组(ODE)求解器。从这个角度来看，STL 的数量对应于 ODE 中的时间维度，这自然赋予 Transformer 以强大的学习能力来产生深层上下文表示。
度。



我们建议的新的 SER 框架将在这一部分介绍。图 1 概述了它的体系结构。如图所示，它依次包括 4 个模块：前端预处理模块，用于提取帧级别的 LLD；CNN-BiLSTM 模块，用于提取上下文表示；STLS，用于增强上下文表示；后端分类器，用于预测情感类别的概率。下面介绍 CNNBiLSTM 和 STLS 模块的设计细节。

CNN-BiLSTM Module

我们的 CNN-BiLSTM 模块的详细信息如图 2 所示，这类似于我们之前的研究[17]。简而言之，该模块由六对 CNN-Pooling 层和一个 BiLSTM 层构成。每个 CNN-Pooling 对包括一个卷积层(配有 batch 归一化和 ELU 函数激活)用于提取高层时间特征，以及一个池化层(具有固定丢失率)用于降低特征维.

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210042.png)

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210255.png)

特别是，模块输入是基于帧级别的 LLD 声学特征，表示为 $X=\left\{x_{1}，x_{2}，\cdots，x_{t}\right\}\in$$\mathbb{R}^{\mathrm{t}\times\mathrm{d}}$ ，其中 $\mathrm{t}$ 表示帧长度， $\mathrm{d}$ 表示 LLD 特征尺寸。对于第一个 CNN-Pooling 对，是具有 $c h_{0}$ 个 Filter 的 2 D 卷积，应用在 $X$ 上，并产生时间特征序列，表示为 $H_{0}^{\text{CNN}}=$$\left\{h_{1}^{\mathrm{CNN}}，h_{2}^{\mathrm{CNN}}，\cdots，H_{\mathrm{t}}^{\mathrm{cnn}}\right\}\in \mathbb{R}^{\mathrm{t}\times\mathrm{h}_{0}\times\mathrm{w}_{0}}$ ；然后对每个输出进行大小为 $n_{a_p}*n_{a_p}$ 的 2 D-AveragePooling 操作以降维，得到的输出 $H_{0}^{\text {pool }}=$ $\left.\left\{h_{1}^{\text {pool }}, h_{2}^{\text {pool }}, \cdots, h_{\text {ch }_{0}}^{\text {pool }}\right\} \in \mathbb{R}^{\text {ch }_{0} \times \mathrm{h}_{0} / \mathrm{n}_{\mathrm{ap}} \times \mathrm{w}_{0} / \mathrm{n}_{\mathrm{ap}}}\right)$ 被馈送到下一个卷积对。通过总共 6 个 CNN-Pooling 对，，得到最终的潜在表示 $H_{5}^{\text{pool}}\in\mathbb{R}^{\mathrm{ch}_{5}\times\mathrm{h}_{5}\times\mathrm{w}_{5}}$ 。通过进一步的 reshape 操作，将 reshape 版本 $H^{r e}\in\mathbb{R}^{\mathrm{h}_{5}\times\left(\mathrm{ch}_{5}\times\mathrm{w}_{5}\right)}$ 随后馈送到 BiLSTM 层。

BiLSTM 向前和向后从 $H^{\text{re}}$ 学习上下文依赖关系。其输出为隐藏状态序列 $H^{\text {bilstm }}=\left\{h_{1}^{\text {bilstm }}, h_{2}^{\text {bilstm }}, \cdots, h_{h_{5}}^{\text {bilstm }}\right\}$ 表示上下文特征序列，其中 $h_{i}^{\text {bilstm }}=\left[\vec{h}^{\text {bilstm }}, \overleftarrow{h}^{\text {bilstm }}\right] \in$ $\mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ ，隐藏状态大小为 $\mathrm{d}_{\mathrm{lstm}}$ 。

在经典的 SER 任务中，采用了两种聚合方法，即 reucurrence-based 方法和 attention-based,方法来产生全局情感嵌入。
$$

X^{G}=\left\{\begin{array}{cl}

h_{\mathrm{h}_{5}}^{\text {bilstm }}, & \text { recurrence - based } \\

\sum_{i=1}^{\mathrm{h}_{5} \alpha_{\mathrm{i}} \mathrm{h}_{\mathrm{i}}^{\text {bilstm }},} & \text { attention }-\text { based }
\end{array}\right.
$$

在任何一种方式中，情感嵌入 XG(或受进一步仿射变换的约束)被馈送到后端分类器以进行概率预测。


作为我们的核心贡献，提出了一种产生情感嵌入的替代方法。它应用 additional STLs 来增强上下文特征序列 $H^{\text {bilstm }}$ 。

为此，首先执行1D 卷积以将 $H^{\text{bilstm}}$ 映射到向量 $Z_{0} \in \mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ 。然后使用具有 L 层的 STLs 迭代增强
$Z_{0}$ 得到 $Z_{i}=G\left(Z_{i-1}\right)$ 。除了跳过 positional encoding 外，一系列操作都是在 $G$ 内执行的，例如多头注意、前馈和残差连接(如第 2 节所述)。最后一个 Transformer 层的输出是全局嵌入，即 $X^{G}=Z_{L-1}$ 。

最后，以端到端的方式对所提出的 SER 体系结构进行训练，目标函数如下： $$

\begin{gathered}

\hat{y}_{c}=\operatorname{softmax}\left(\mathrm{X}_{\mathrm{G}}^{\mathrm{T}} \mathrm{W}+\mathrm{b}\right), \\

L=-\log \prod_{i \in S} \sum_{c=1}^{C} \hat{y}_{i, c} \log \left(\hat{y}_{i, c}\right),

\end{gathered}

$$ ，其中 $S$ 表示用于训练的样本集， $C$ 表示情感类别的总数。

我们注意到，类似的架构被用于多通道情感识别[18]。我们的方法与以前的工作不同，因为 CMA 是对时间特征序列进行操作，这需要进一步的统计汇集；而在我们的例子中，STLS 应用于语音级表示。

3. STL 模块，用于增强上下文表示；

    STACKED Transformer:** 完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系，可以将输入序列转换为固定维度的上下文全局特征。它由六个相同的  Transformer Layers (TLs)堆叠而成，每个 Transformer Layer 包含两个附带  residual connections 的子层：self-attention 层和 position-wise feed-forward network(FFN) 子层。此外引入了 positional encoding (PE)来为模型提供显式的顺序信息。

    通过 self-attention 机制，对输入序列的特定部分加以重视, 来保留和利用输入序列信息。Transformer Layers 的堆叠，只有第一层接受输入序列，后面每隔一层只接受前一层的输出作为输入，这使得能够通过在潜在表征空间的迭代注意力机制和数据投影来丰富上下文全局特征。

    从数学上讲， Transformer 结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程(ODE)求解器[16]。从这个角度来看，STLs 的数量对应于 ODE 中的时间维度，这自然使 Transformer 具有很强的学习能力来产生**深层上下文表示**。

4. 分类器，用于预测情感类别的概率。

**[t-SNE 可视化技术](https://zhuanlan.zhihu.com/p/148170862)**




我们进行了更多的调查，以评估我们拟议的 STL 模块的有效性。首先，研究了一种不带 STLS 模块的截断 SER 系统。由此产生的 WA 值暴跌至 72.79%。这项研究的意义是双重的：
1)我们的截断系统设计得很好，性能可以与现有技术相媲美；
2)STLS 模块对系统整体性能的贡献很大(18.5%)，代价是合理增加参数数量(从 356 K 增加到 468 K)。
其次，通过使用 t-SNE 技术可视化中间层发现的特征质量来检查 STLS 的有效性[24]。
两个潜在特征分布 Z 0 和 Z 4 在图 3 中用关于类的 2 D 曲线图进行了比较和说明。同样，可以观察到，STL 之后的特征被清楚地投影到不同的集群，并且彼此很好地分开。
最后，进一步研究了 STL 的最佳层数。实验结果如图 4(A)所示，有趣的是，系统性能在开始时(&lt;5 层)迅速提高，在中间(5-10 层)几乎保持不变，然后在结束时(&gt;10 层)急剧下降。基于这样的观察，我们所提出的 STLS 模块的最佳深度被选择为 5。同时，根据我们的模型的学习曲线，在图 4(B)中画出了最佳的 5 个变压器层，其中最佳模型是在第 67个历元。

### 引文

- [2]提出了一种混合的CNN-LSTM体系结构，其中CNN从原始频谱图中提取特征序列，而LSTM聚合长期的特征依赖关系。

- MHA机制再次被[4]采用来构建DRN-注意体系结构，其中DRN使网络在特征学习中保持高分辨率的时间结构。

- 文献[5]提出了基于一维和二维CNN-LSTM的SER系统，并表明2D CNN LSTM网络通过从谱图中捕获局部相关性和全局上下文信息而优于一维 CNN LSTM网络。

- 后来在注意机制[6]的启发下，在[7]中提出了RNN-注意，RNN通过关注情绪显著的特征来提取时间特征，并聚合长期特征依赖关系。

- 文[8]测试了几种时态建模方法，目的是从原始波形中学习深层情感特征。

- 在文献[9]中，作者引入了CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。

- 文献[10]还提出了语境LSTM注意来考虑周围话语之间的关系和依存关系。

- 基于STL的声学模型[11]在Librispeech基准上给出了最好的声学模型。

- 在问题回答领域，stacked latent attention 和multihop attention networks (MAN)[12]都显示出了显著的性能改进.

- 对于图像捕获任务，从 stacked cross attention network (SCAN)[13]或stacked attention modules [14]获得最先进的结果。

- vanilla Transformer [15]

- 我们的 CNN-BiLSTM 模块的详细信息如图2所示，类似于我们之前的研究[17]。

- 类似的架构被用于多通道情感识别[18]。

- DSCNN [21]

- IAAN [22] 

- 通过使用 t-SNE 技术可视化中间层发现的特征质量来检查STL的有效性[24]。

## 摘录
---
title: "A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers"
description: ""
citekey: wangNovelEndtoendSpeech2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:31
lastmod: 2023-04-11 17:53:09
---

> [!info] 论文信息
>1. Title：A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers
>2. Author：Xianfeng Wang, Min Wang, Wenbo Qi, Wanqi Su, Xiangqian Wang, Huan Zhou
>3. Entry：[Zotero link](zotero://select/items/@wangNovelEndtoendSpeech2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/HW-AARC-CLUB/ICASSP_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 在现有语音情绪识别(SER)模型基础上增加 stacked transformer layers (STL)结构构建新的SER体系结构, 利用STLS模块强大的特征学习能力. 
- 使用 ==t-SNE== 技术可视化中间过渡层发现的特征质量来检查 STL 模型的有效性.

## 摘要

> [!abstract] Speech emotion recognition (SER) aims to automatically recognize emotional category for a given speech utterance. The performance of a SER system heavily relies on the effectiveness of global representation expressed at utterance level. To effectively extract such a global feature, the mainstream of recent SER architectures adopts a pipeline with two key modules, feature extraction and aggregation. Although variant module designs have brought impressive progresses, SER is still a challenging task. In contrast with those previous works, herein we propose a novel strategy for global SER feature extraction by applying an additional enhancement module on top of the current SER pipeline. To verify its effect, an end-to-end SER architecture is proposed where stacked multiple transformer layers are explored to enhance the aggregated global feature. Such an architecture is evaluated on IEMO-CAP and results strongly substantiate the effectiveness of our proposal. In terms of weighted accuracy on four emotion categories, our proposed SER system outperforms the prior arts by a large margin of relatively 20% improvement. Our codes and the pre-trained SER models are made publicly available.

> 语音情感识别(SER)的目的是自动识别给定语音话语的情感类别。SER系统的性能在很大程度上依赖于在话语层面上表达的全局表征的有效性。为了有效地提取这样的全局特征，目前主流的SER体系结构采用了一种包含两个关键模块的流水线，即特征提取和聚合。尽管各种各样的模块设计已经带来了令人印象深刻的进步，但SER仍然是一项具有挑战性的任务。与前人的工作不同，本文提出了一种新的全局SER特征提取策略，在现有SER流水线的基础上增加了一个增强模块。为了验证其效果，提出了一种端到端的SER结构，其中利用堆叠的多个转换器层来增强聚集的全局特征。这种体系结构在IEMOCAP上进行了评估，结果有力地证明了我们的建议的有效性。在四个情感类别的加权准确率方面，我们提出的SER系统比现有技术有较大幅度的提高20%。我们的代码和预先培训的SER模型是公开提供的。

## 预处理

## 概述

## 结果

EMOCAP 包含超过10K 个话语，由九个情感类别的标签标注。按照前人的研究步骤，我们使用了一个四类({高兴、愤怒、悲伤、中性})的子库，总共有5531个话语。对于每个情感类别，其关联样本分别按7/1/2的比例随机分为训练/均值/测试。

所提出的SER系统已在PyTorch中实现。利用Adam[20]优化器对分类交叉熵进行优化，同时监测验证精度，提前停止设置为8个历元。训练系统的批次为32，学习率为2×10−4，衰减率为1×10−6。

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220417162209.png)

## 精读

言语情感作为文本之外的一种 meta-information，对理解说话人的心理和反应起着重要的作用。相关研究被称为语音情感识别(SER)，其目的是自动识别给定语音表达的情感类别。由于情感通常以一种微妙和可变的方式传达，因此识别情感嵌入作为话语的表征一直是一个挑战，可以有效地对情感类别进行分类。

随着深度神经网络(DNN)的最新进展，情感嵌入已经从先前基于知识的手工制作的声学特征，例如低级描述符(LLD)，演变为基于 DNN 的深度情感特征。在最近的 SER 工作中，已经探索了各种 DNN 结构，如卷积神经网络(CNN)、长短期记忆(LSTM)、时延神经网络(TDNN)、残差网络(ResNet)、扩张残差网络(DRN)，它们本身或组合在一起[1-4]。在语义特征识别领域已经有了大量的研究成果，但由于情感与语言特征的分离，以及从长时间的话语中提取有效的情感特征，这方面的研究仍然具有挑战性。为了解决这些问题，现代 SER 系统普遍采用两个模块的流水线：1)特征提取模块，以生成情感相关的时间声学特征；以及 2)聚合模块，将这些时间特征汇集到话语级别的紧凑的全局语境表示(也称为情感嵌入)中。为了产生有效的情感嵌入，最近的 SER 工作集中在开发不同的模块架构上。例如，[2]提出了一种混合的 CNN-LSTM 体系结构，其中 CNN 从原始频谱图中提取特征序列，而 LSTM 聚合长期特征依赖关系。类似地，文献[5]提出了基于 1 D 和 2 D CNN-LSTM 的 SER 系统，并表明 2 D CNNLSTM 网络通过从谱图中捕捉局部相关性和全局上下文信息而优于 1 D 网络。后来在注意力机制的启发下，RNN 在[7]中提出了 RNN-注意，RNN 通过关注情绪显著的特征来提取时间特征和聚合长期特征依赖关系。文[8]测试了几种时间建模方法，目的是从原始波形中学习深层情感特征。在[9]中，作者引入了 CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。MHA 机制再次被[4]采用来构建 DRN-注意体系结构，其中 DRN 使网络在特征学习中保持高分辨率的时间结构。此外，文献[10]还提出了语境 LSM 注意来考虑周围话语之间的关系和依存关系。尽管在以前的工作中取得了进展，但目前的 SER 表现仍有改进的空间。例如，在基准数据集 IEMOCAP 上的 SOTA 分类准确率仅为 76%左右(在四个情感类别上)。同时，我们注意到，在其他研究领域，经常有报道称，用叠层变压器层(STL)取代重复性可以显著提高性能。例如，基于 STLS 的声学模型[11]在 Librispeech 基准上给出了最好的声学模型。在问答领域，堆积式潜在注意和多跳注意网络(MAN)[12]都表现出显著的性能改进。对于图像捕获任务，从堆叠的交叉注意网络(SCAN)[13]或堆叠的注意模块[14]获得最先进的(SOTA)结果。受其他研究领域的成功启发，在本研究中，我们有兴趣将 STL 机制应用到 SER 网络中。特别是，提出了一种通过在现有 SER 流水线上添加 STL 而构建的新的 SER 体系结构。我们建议的动机是利用 STLS 模块强大的特征学习能力来直接提高流水线输出。据作者所知，这是文献中第一次使用这种策略来研究 SER 问题。本文组织如下。在第二节中，简要介绍了香草变压器。在第三节中，我们详细地提出了 STL 增强的 SER 体系结构。第四节报道了实验。第五节总结了这项研究。


在本节中，我们将简要描述源自 Vanilla Transformer[15]的 STL 的详细信息，后者完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系。在这项研究中，我们感兴趣的是 ITS 编码器，它将输入序列转换为固定维度的全局上下文向量。具体地，编码器由六个相同的变压器层(TL)堆叠而成。每个 TL 包含具有剩余连接的两个子层：1)自关注子层和 2)位置前馈网络(FFN)子层。此外，还引入了位置编码(PE)来为模型提供显式的顺序信息。得益于自我注意机制，输出序列通过重视输入的特定部分来保留和利用输入信息。TL 的堆叠方式是，只有第一层采用输入顺序，每隔一层只采用前面的输出作为输入。这使得能够通过在潜在表示空间内的迭代关注和投影来丰富上下文向量。从数学上讲，《变形金刚》的体系结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程组(ODE)求解器。从这个角度来看，STL 的数量对应于 ODE 中的时间维度，这自然赋予 Transformer 以强大的学习能力来产生深层上下文表示。
度。



我们建议的新的 SER 框架将在这一部分介绍。图 1 概述了它的体系结构。如图所示，它依次包括 4 个模块：前端预处理模块，用于提取帧级别的 LLD；CNN-BiLSTM 模块，用于提取上下文表示；STLS，用于增强上下文表示；后端分类器，用于预测情感类别的概率。下面介绍 CNNBiLSTM 和 STLS 模块的设计细节。

CNN-BiLSTM Module

我们的 CNN-BiLSTM 模块的详细信息如图 2 所示，这类似于我们之前的研究[17]。简而言之，该模块由六对 CNN-Pooling 层和一个 BiLSTM 层构成。每个 CNN-Pooling 对包括一个卷积层(配有 batch 归一化和 ELU 函数激活)用于提取高层时间特征，以及一个池化层(具有固定丢失率)用于降低特征维.

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210042.png)

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210255.png)

特别是，模块输入是基于帧级别的 LLD 声学特征，表示为 $X=\left\{x_{1}，x_{2}，\cdots，x_{t}\right\}\in$$\mathbb{R}^{\mathrm{t}\times\mathrm{d}}$ ，其中 $\mathrm{t}$ 表示帧长度， $\mathrm{d}$ 表示 LLD 特征尺寸。对于第一个 CNN-Pooling 对，是具有 $c h_{0}$ 个 Filter 的 2 D 卷积，应用在 $X$ 上，并产生时间特征序列，表示为 $H_{0}^{\text{CNN}}=$$\left\{h_{1}^{\mathrm{CNN}}，h_{2}^{\mathrm{CNN}}，\cdots，H_{\mathrm{t}}^{\mathrm{cnn}}\right\}\in \mathbb{R}^{\mathrm{t}\times\mathrm{h}_{0}\times\mathrm{w}_{0}}$ ；然后对每个输出进行大小为 $n_{a_p}*n_{a_p}$ 的 2 D-AveragePooling 操作以降维，得到的输出 $H_{0}^{\text {pool }}=$ $\left.\left\{h_{1}^{\text {pool }}, h_{2}^{\text {pool }}, \cdots, h_{\text {ch }_{0}}^{\text {pool }}\right\} \in \mathbb{R}^{\text {ch }_{0} \times \mathrm{h}_{0} / \mathrm{n}_{\mathrm{ap}} \times \mathrm{w}_{0} / \mathrm{n}_{\mathrm{ap}}}\right)$ 被馈送到下一个卷积对。通过总共 6 个 CNN-Pooling 对，，得到最终的潜在表示 $H_{5}^{\text{pool}}\in\mathbb{R}^{\mathrm{ch}_{5}\times\mathrm{h}_{5}\times\mathrm{w}_{5}}$ 。通过进一步的 reshape 操作，将 reshape 版本 $H^{r e}\in\mathbb{R}^{\mathrm{h}_{5}\times\left(\mathrm{ch}_{5}\times\mathrm{w}_{5}\right)}$ 随后馈送到 BiLSTM 层。

BiLSTM 向前和向后从 $H^{\text{re}}$ 学习上下文依赖关系。其输出为隐藏状态序列 $H^{\text {bilstm }}=\left\{h_{1}^{\text {bilstm }}, h_{2}^{\text {bilstm }}, \cdots, h_{h_{5}}^{\text {bilstm }}\right\}$ 表示上下文特征序列，其中 $h_{i}^{\text {bilstm }}=\left[\vec{h}^{\text {bilstm }}, \overleftarrow{h}^{\text {bilstm }}\right] \in$ $\mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ ，隐藏状态大小为 $\mathrm{d}_{\mathrm{lstm}}$ 。

在经典的 SER 任务中，采用了两种聚合方法，即 reucurrence-based 方法和 attention-based,方法来产生全局情感嵌入。
$$

X^{G}=\left\{\begin{array}{cl}

h_{\mathrm{h}_{5}}^{\text {bilstm }}, & \text { recurrence - based } \\

\sum_{i=1}^{\mathrm{h}_{5} \alpha_{\mathrm{i}} \mathrm{h}_{\mathrm{i}}^{\text {bilstm }},} & \text { attention }-\text { based }
\end{array}\right.
$$

在任何一种方式中，情感嵌入 XG(或受进一步仿射变换的约束)被馈送到后端分类器以进行概率预测。


作为我们的核心贡献，提出了一种产生情感嵌入的替代方法。它应用 additional STLs 来增强上下文特征序列 $H^{\text {bilstm }}$ 。

为此，首先执行1D 卷积以将 $H^{\text{bilstm}}$ 映射到向量 $Z_{0} \in \mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ 。然后使用具有 L 层的 STLs 迭代增强
$Z_{0}$ 得到 $Z_{i}=G\left(Z_{i-1}\right)$ 。除了跳过 positional encoding 外，一系列操作都是在 $G$ 内执行的，例如多头注意、前馈和残差连接(如第 2 节所述)。最后一个 Transformer 层的输出是全局嵌入，即 $X^{G}=Z_{L-1}$ 。

最后，以端到端的方式对所提出的 SER 体系结构进行训练，目标函数如下： $$

\begin{gathered}

\hat{y}_{c}=\operatorname{softmax}\left(\mathrm{X}_{\mathrm{G}}^{\mathrm{T}} \mathrm{W}+\mathrm{b}\right), \\

L=-\log \prod_{i \in S} \sum_{c=1}^{C} \hat{y}_{i, c} \log \left(\hat{y}_{i, c}\right),

\end{gathered}

$$ ，其中 $S$ 表示用于训练的样本集， $C$ 表示情感类别的总数。

我们注意到，类似的架构被用于多通道情感识别[18]。我们的方法与以前的工作不同，因为 CMA 是对时间特征序列进行操作，这需要进一步的统计汇集；而在我们的例子中，STLS 应用于语音级表示。

3. STL 模块，用于增强上下文表示；

    STACKED Transformer:** 完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系，可以将输入序列转换为固定维度的上下文全局特征。它由六个相同的  Transformer Layers (TLs)堆叠而成，每个 Transformer Layer 包含两个附带  residual connections 的子层：self-attention 层和 position-wise feed-forward network(FFN) 子层。此外引入了 positional encoding (PE)来为模型提供显式的顺序信息。

    通过 self-attention 机制，对输入序列的特定部分加以重视, 来保留和利用输入序列信息。Transformer Layers 的堆叠，只有第一层接受输入序列，后面每隔一层只接受前一层的输出作为输入，这使得能够通过在潜在表征空间的迭代注意力机制和数据投影来丰富上下文全局特征。

    从数学上讲， Transformer 结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程(ODE)求解器[16]。从这个角度来看，STLs 的数量对应于 ODE 中的时间维度，这自然使 Transformer 具有很强的学习能力来产生**深层上下文表示**。

4. 分类器，用于预测情感类别的概率。

**[t-SNE 可视化技术](https://zhuanlan.zhihu.com/p/148170862)**




我们进行了更多的调查，以评估我们拟议的 STL 模块的有效性。首先，研究了一种不带 STLS 模块的截断 SER 系统。由此产生的 WA 值暴跌至 72.79%。这项研究的意义是双重的：
1)我们的截断系统设计得很好，性能可以与现有技术相媲美；
2)STLS 模块对系统整体性能的贡献很大(18.5%)，代价是合理增加参数数量(从 356 K 增加到 468 K)。
其次，通过使用 t-SNE 技术可视化中间层发现的特征质量来检查 STLS 的有效性[24]。
两个潜在特征分布 Z 0 和 Z 4 在图 3 中用关于类的 2 D 曲线图进行了比较和说明。同样，可以观察到，STL 之后的特征被清楚地投影到不同的集群，并且彼此很好地分开。
最后，进一步研究了 STL 的最佳层数。实验结果如图 4(A)所示，有趣的是，系统性能在开始时(&lt;5 层)迅速提高，在中间(5-10 层)几乎保持不变，然后在结束时(&gt;10 层)急剧下降。基于这样的观察，我们所提出的 STLS 模块的最佳深度被选择为 5。同时，根据我们的模型的学习曲线，在图 4(B)中画出了最佳的 5 个变压器层，其中最佳模型是在第 67个历元。

### 引文

- [2]提出了一种混合的CNN-LSTM体系结构，其中CNN从原始频谱图中提取特征序列，而LSTM聚合长期的特征依赖关系。

- MHA机制再次被[4]采用来构建DRN-注意体系结构，其中DRN使网络在特征学习中保持高分辨率的时间结构。

- 文献[5]提出了基于一维和二维CNN-LSTM的SER系统，并表明2D CNN LSTM网络通过从谱图中捕获局部相关性和全局上下文信息而优于一维 CNN LSTM网络。

- 后来在注意机制[6]的启发下，在[7]中提出了RNN-注意，RNN通过关注情绪显著的特征来提取时间特征，并聚合长期特征依赖关系。

- 文[8]测试了几种时态建模方法，目的是从原始波形中学习深层情感特征。

- 在文献[9]中，作者引入了CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。

- 文献[10]还提出了语境LSTM注意来考虑周围话语之间的关系和依存关系。

- 基于STL的声学模型[11]在Librispeech基准上给出了最好的声学模型。

- 在问题回答领域，stacked latent attention 和multihop attention networks (MAN)[12]都显示出了显著的性能改进.

- 对于图像捕获任务，从 stacked cross attention network (SCAN)[13]或stacked attention modules [14]获得最先进的结果。

- vanilla Transformer [15]

- 我们的 CNN-BiLSTM 模块的详细信息如图2所示，类似于我们之前的研究[17]。

- 类似的架构被用于多通道情感识别[18]。

- DSCNN [21]

- IAAN [22] 

- 通过使用 t-SNE 技术可视化中间层发现的特征质量来检查STL的有效性[24]。

---
title: "Contrastive Unsupervised Learning for Speech Emotion Recognition"
description: ""
citekey: liContrastiveUnsupervisedLearning2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:41
lastmod: 2023-04-11 11:09:01
---

> [!info] 论文信息
>1. Title：Contrastive Unsupervised Learning for Speech Emotion Recognition
>2. Author：Mao Li, Bo Yang, Joshua Levy, Andreas Stolcke, Viktor Rozgic, Spyros Matsoukas, Constantinos Papayiannis, Daniel Bone, Chao Wang
>3. Entry：[Zotero link](zotero://select/items/@liContrastiveUnsupervisedLearning2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Contrastive Unsupervised Learning for Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 为了提升情感识别效果，使用 Contrastive Predictive Coding(CPC) 无监督学习方法，预先在无标签的大型语音数据库训练一个特征提取器，最终改善了小数据量问题。
- 遇见了两个新的损失函数：infoNCE 损失函数，源自于 CPC 无监督方法；concordance correlation coefficient(CCC), 基于一致性相关系数的损失函数,测量两个随机变量的对齐度（相关程度）。

## 摘要

> [!abstract] Speech emotion recognition (SER) is a key technology to enable more natural human-machine communication. However, SER has long suffered from a lack of public large-scale labeled datasets. To circumvent this problem, we investigate how unsupervised representation learning on unlabeled datasets can benefit SER. We show that the contrastive predictive coding (CPC) method can learn salient representations from unlabeled datasets, which improves emotion recognition performance. In our experiments, this method achieved state-of-the-art concordance correlation coefficient (CCC) performance for all emotion primitives (activation, valence, and dominance) on IEMOCAP. Additionally, on the MSP-Podcast dataset, our method obtained considerable performance improvements compared to baselines.

> 语音情绪识别（SER）是实现更自然的人机通信的关键技术。但是，SER 长期以来一直缺乏公共的大规模标签数据集。为了避免这个问题，我们研究了未标记数据集上的无监督表示学习如何使 SER 受益。我们表明，对比预测编码（CPC）方法可以从未标记的数据集中学习显着表示，从而提高情绪识别性能。在我们的实验中，该方法在 IEMOCAP 上实现了所有情绪原语（激活，效价和优势）的最新一致性相关系数（CCC）性能。此外，在 MSPPodcast 数据集上，与基线相比，我们的方法获得了相当大的性能改进。

## 预处理

## 概述

## 结果

使用系数

![]({11}_Contrastive%20Unsupervised%20Learning%20for%20Speech%20Emotion%20Recognition@liContrastiveUnsupervisedLearning2021.assets/image-20220417162513.png)

IEMOCAP

![]({11}_Contrastive%20Unsupervised%20Learning%20for%20Speech%20Emotion%20Recognition@liContrastiveUnsupervisedLearning2021.assets/image-20220417162339.png)

## 精读

无监督学习方法提出背景在于，虽然深度学习(dl)方法在多个领域能实现最先进的结果，但这些方法往往是需要大量数据支撑的。

维度情绪指标通常包括激活(又名激发，非常平静或非常活跃) ，效价(积极或消极水平)和支配(非常弱或非常强)。

**基于CPC的特征提取**

Contrastive predictive coding（对比预测编码），属于对比学习范畴，通过损失函数构建和分离正反例，具体实现步骤如下。

对于在上述设置中使用的 cpc 模型，我们使用一个带有步长[5,4,4,2] ，滤波器尺寸[10,8,8,4]和128个隐藏单元的带有关联激活的4层 cnn 来编码16khz 音频波形输入。

采用一个具有256个隐含维数的单向门控递归单元(gru)网络作为自回归模型。对于 gru 的每个输出，我们预测未来的12个时间步，使用50个负样本，从同一序列中取样，在每个预测中。

我们用10s固定长度的话语训练 cpc 模型。较长的话语在10s时被切断，较短的话语通过重复来填充。

在情感识别器中，使用了一个具有512维隐藏状态的8头注意层。注意层的输出与输入维数相同。两个完全相连的层有128个隐藏单元。退出的可能性被设置为0.2的辍学层。

我们的模型在 pytorch 上实现，所有的方法都在8个 gpu 上进行，每个 gpu 的小批量大小为8个样本进行 cpc 预训练。我们使用 adam 优化器，重量衰减0.00001，学习率0.0002。我们使用50个时代的训练和保存模型，执行最好的验证集测试。

为了评估 iemocap 数据集，我们配置了5个交叉验证来评估模型。所有的实验都进行了五次，得到了平均值和标准差。

1. 在 LibriSpeech dataset 预训练 CPC模型，使用非线性编码器 $f$ 将语音序列 $x_{t}$ 映射到隐式表征 $z_{t}$。
2. 使用自回归模型 $g$ 将过去时间上连续的隐层表征转化为上下文相关的特征 $c_{t}$，上下文相关意味着包含了过去的信息，亦即 $c_{t}$ 能够通过未来的观察值 $x_{t+k}$ 预测 $z_{t+k}$。$$\hat{z}_{t+k}=h_{k}\left(c_{t}\right)=h_{k}\left(g\left(z_{\leq t}\right)\right)$$
3. 从同一序列或其他序列中随机抽取样本（即其他观察值 $x$ ），计算其隐式表征 $z_{t}$，以形成对比问题。假设每个语境的 N个样本中，含有随机抽样的 N-1 个 negative 样本和一个positive 样本。
4. 利用 infoNCE 损失函数 $\mathcal{L}$( $\tau$ 是特征聚集程度的比例因子，$k$ 是预测时间上限，$i$ 为随机抽取的 negative 样本)，学习区分 negative 样本和 positive 样本，归结为一个 N 分类问题。$\mathcal{L}=-\sum_{m=1}^{k}\left[\log \frac{\exp \left(\hat{z}_{t+m}^{\top} z_{t+m}\right) / \tau}{\exp \left(\hat{z}_{t+m}^{\top} z_{t+m}\right) / \tau+\sum_{i=1}^{N-1} \exp \left(\hat{z}_{t+m}^{\top} z_{i}\right) / \tau}\right]$

显然，在不同的音频段和时间步骤中，损失是累加的，因此在训练中，损失通常是针对批量的音频段和这些段中所有可能的时间步骤计算的，以利用基于小批量的 adam 优化器。优化结果在隐层表征和其预测对应之间产生更大的内积，比任何 negative 样本匹配的隐层表征和预测。优化目标函数的理论证明可以在[11]和[18]中找到。

基于注意力机制的情感识别

由于话语的某些部分往往在情感上比其他部分更加突出，因此我们采用一种自我注意机制来关注这些时段，以利用相关特征，进行句子级别的嵌入。具体步骤：将CPC输出的 $C^{L*D_{c}}$ 作为输入，使用多头 dot-product 注意力机制，得到多方面考虑上下文信息权重的$H_{j}^{L × D_{attn}}$ 特征, 并经过简单的串联和线性变换得到$U^{L × D_{u}} = Conat(H^{1},H^{2},\cdot\cdot\cdot,H^{n}) W_{o}^{nD_{attn}×D_{u}}$作为序列特征。

沿着时间维度计算 $U$ 的平均值和标准差，并将它们连接成序列表示，然后经过两个全连接层（Relu+dropout）以及一个隐藏层单元为最终输出数量的全连接层，得到最终输出结果。

最终使用损失函数更新参数，其中$\rho = \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}}$ 是皮尔森相关系数，用来反映两个变量X(模型预测)和Y(数据标签)的线性相关程度；$μ$ 和 $σ$是均值和标准差; 

$\mathcal{L}=1-\alpha \mathrm{CCC}_{a c t}-\beta \mathrm{CCC}_{v a l}-\gamma \mathrm{CCC}_{d o m}$

$\mathrm{CCC}(X, Y)=\rho \frac{2 \sigma_{X} \sigma_{Y}}{\sigma_{X}^{2}+\sigma_{Y}^{2}+\left(\mu_{X}-\mu_{Y}\right)^{2}}$

实验设置

1. 基线方法：使用MFCC40维特征的单独的受监督模型
2. 采用端到端的方式，从原始音频中训练 CPC 模型直接学习特征和情感识别器。加入基线方法，采用手工特征进行监控任务，以测试当特征提取部分知道下游任务时，是否有可能学到更好的特征。
3. minicpc 在相同的数据集上分两个阶段训练 cpc 模型和情感识别器。
4. 在 LibriSpeech dataset上预训练CPC，在 MSP-Podcast 和 IEMOCAP上训练含CPC的情感识别器。

### 引文

- 虽然利用非监督式学习对 ser 的研究相对较少，但之前使用自动编码器的尝试已经成功[12,13]。

- 最近，有研究表明，学习在一个时间序列中预测未来的信息是一种有用的训练前机制[14]。

- 例如，对比预测编码(cpc)[11]能够从顺序数据中提取有用的表示，并在各种任务中取得竞争性的表现，包括语音中的电话和说话人分类。

- 一般来说，有两种广泛使用的方法来表示情绪: 通过情绪类别(快乐、悲伤、愤怒等)或者通过维度情绪度量(又名情绪原语)[3,4,15]。

- 由于情绪表征是一个活跃的研究课题，我们建议感兴趣的读者参阅[15,16]。

## 摘录
---
title: "Compact Graph Architecture for Speech Emotion Recognition"
description: ""
citekey: shirianCompactGraphArchitecture 2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:50
lastmod: 2023-04-11 11:10:26
---

> [!info] 论文信息
>1. Title：Compact Graph Architecture for Speech Emotion Recognition
>2. Author：Amir Shirian, Tanaya Guha
>3. Entry：[Zotero link](zotero://select/items/@shirianCompactGraphArchitecture2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Shirian_Guha_2021_Compact Graph Architecture for Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/AmirSh 15/Compact_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 第一个将图分类方法用于 SER 的工作。
- 利用图信号处理的理论，提出了一种基于 GCN 的图分类方法，该方法可以有效地执行精确的图卷积。
- 模型可训练参数显著减少(仅为 30 K)。
- IEMOCAP 自带 spontaneity 特征- 第一个将图分类方法用于 SER 的工作。
- 利用图信号处理的理论，提出了一种基于 GCN 的图分类方法，该方法可以有效地执行精确的图卷积。
- 模型可训练参数显著减少(仅为 30 K)。
- IEMOCAP 自带 spontaneity 特征

思考：
- 图结构可以捕捉语音情感的动态特性吗？

## 摘要

> [!abstract] We propose a deep graph approach to address the task of speech emotion recognition. A compact, efficient and scalable way to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a Graph Convolution Network (GCN)-based architecture that can perform an accurate graph convolution in contrast to the approximate convolution used in standard GCNs. We evaluated the performance of our model for speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves comparable performance to the state-of-the-art with significantly fewer learnable parameters ( 30 K) indicating its applicability in resource-constrained devices. Our code is available at /github.com/AmirSh 15/Compact_SER.

> 我们提出了一种深度图方法来解决语音情感识别问题。以图表的形式表示数据是一种紧凑、高效和可伸缩的方式。遵循图信号处理的理论，我们提出将语音信号建模为循环图或折线图。这种图结构使我们能够构建一个基于图卷积网络(GCN)的体系结构，与标准 GCN 中使用的近似卷积相比，它可以执行精确的图卷积。我们在流行的 IEMOCAP 和 MSP-Improv 数据库上对我们的语音情感识别模型的性能进行了评估。我们的模型的性能优于标准 GCN 和其他相关的深图体系结构，表明了我们方法的有效性。与现有的语音情感识别方法相比，我们的模型在可学习参数(∼30 K)显著减少的情况下获得了与最先进水平相当的性能，表明其在资源受限的设备中的适用性。

## 预处理

## 概述

## 结果

MSP-IMPROV

![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220417162601.png)

## 精读

大部分 SER 方法都是遵循两种处理过程，首先从原始音频中提取一组特征，然后将特征输入到深度学习模型中，以生成离散(或连续)的情感标签[1,2,3,4]。在 SER 领域，模型训练常将 LLDs(Low-Level descriptor，手工设计的一些低水平特征)，lexical 特征[5,6]和 Log Mel spectrograms 作为输入[7]。其中 spectrogram 通常与卷积神经网络(CNNs)一起使用[7]，但 CNNs 无法捕捉语音动态变化，而这种时间动态在 SER 中具有重要意义，因为它反映了情绪的动态变化。为了捕捉情绪的动态变化，Recurrent 模型，特别是长短时记忆网络(LSTMs)[2,3,4]，在 SER 中占据了主导地位，但其常常产生具有数百万可训练参数的复杂体系结构。目前，以图的形式表示数据，是一种紧凑、高效和可扩展的方式，并且 GCNs 目前已经在部分领域中实现应用，因此本文将 SER 问题归结为一个图分类问题，提出采用深度图的方法来进行 SER。

本文的工作基于 spectral GCNS，它在图形信号处理方面有很好的应用[14]。考虑到卷积核(对角线矩阵)是可学习的，spectral GCNs 在图的拉普拉斯矩阵的谱上执行卷积运算[15]。这涉及到图的拉普拉斯矩阵的特征分解，这在计算上是昂贵的。为了减少计算量，ChebNet 用 Chebyshev 多项式逼近卷积运算(包括可学习的卷积核)[16]。最流行的 GCN 形式使用切比雪夫多项式的一阶近似，将卷积运算进一步简化为线性投影[9]。这样的 GCN 模型易于实现，并已成功地用于社交媒体网络和引文网络中的各种节点分类任务[9]。

我们将语音信号建模为一个简单的图，由于这种特殊的图结构，我们利用图信号处理[17]中的某些结果来执行精确的图形卷积(与流行的 GCN 中使用的 approximations 不同)，提出了一个轻量级的 GCN 架构，在 IEMOCAP[18]和 MSP-Improv[19]数据库上具有优异的情感识别性能。

**图结构**

首先，从每个语音样本中构造对应的图结构 G=(V,E)，其中 $V$ 是 $M$ 个节点{ $v_{i}$ }的集合， $E$ 是节点之间所有边的集合。 $G$ 的邻接矩阵表示为 $A$ ，其中元素 ( $A_{ij}$ ) 表示连接{ $v_{i}$ }和{ $v_{j}$ }的边界权值，权值为零时表示没有边连接两点，其中邻接矩阵 $A$ 的对角线元素为0。

![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220304010332.png)

1. **语音图节点的构造策略**遵循一个简单的帧到节点转换，每个节点对应于语音信号的一个 short windowed segment。即 $M$ 帧语音信号(短的、重叠的片段)构成 $G$ 中的 $M$ 个节点。

2. **构建语音信号图结构**，我们研究两个无向图结构:(1)由邻接矩阵 $A_{c}$ 定义的圆形图结构和(2)由邻接矩阵 $A_{l}$ 定义的线形图结构。![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220304010347.png)
    由上图可知，每个节点仅连接到两个相邻节点，从而可以将信号转换为折线图或循环图。并且这两种图结构的图拉普拉斯算子具有特殊的结构，可以极大地简化 spectral GCN 计算。

3. **关联节点特征**，每个节点{ $v_i$ }与节点特征向量{ $x_{i} \in R^P$ }相关联。一个节点特征向量包含了从对应的语音片段中提取的 LLDs。特一个特征矩阵 $X=[x_1，···x_M] \in R^{M×P}$ ，包含了所有节点的特征向量。

**图分类**

设计 GCN 架构，给定一组由语音转换来的图 { $G_1，…，G_N$ }和对应的真实标签 { $y_1，…y_{N}$ }，能够识别语音情感。本文架构包括两个图卷积层；一个池化层，生成图嵌入向量；一个全连接层，生成离散的情感标签。

![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220304010243.png)

1. **图卷积层**。本文模型基于 spectral GCN，在谱域中执行图卷积操作[14]： $h=x_i∗w$ ，其中 $w$ 为图卷积核(可学习)， $x_i$ 为输入节点特征，其等价于图谱域中的乘积： $\hat{\mathbf{h}}=\hat{\mathbf{x}}_{i} \odot \hat{\mathbf{w}}$ ，其中 $\hat{\mathbf{h}}$ 、 $\hat{\mathbf{x_i}}$ 和 $\hat{\mathbf{w}}$ 表示输出、节点特征和卷积滤波器经由图傅里叶变换(GFT)的谱域表示。由此扩展到输入特征矩阵，便可得到矩阵表示法： $\hat{\mathbf{H}}=\hat{\mathbf{X}} \hat{\mathbf{W}}$ 。

    为了得到 $\hat{\mathbf{X}}$ 和 $\hat{\mathbf{W}}$ ，我们通常计算归一化的图拉普拉斯矩阵 $\mathcal{L}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L D}^{-\frac{1}{2}}$ ，其中 $\mathbf{D}$ 为度矩阵(记录每个节点邻点数量的对角矩阵)， $\mathbf{L}=\mathbf{D}−\mathbf{A}$ 为原拉普拉斯矩阵， $\mathbf{A}$ 为图的邻接矩阵。

    又因 $\mathbf{L}$ 的特征分解可以写成 $\mathcal{L}=\mathbf{U} \Lambda \mathbf{U}^{T}=\sum_{i=1}^{M} \lambda_{i} \mathbf{u}_{i} \mathbf{u}_{i}{ }^{T}$ 形式，其中 $\lambda_{i}$ 是 L 的第 i 个特征值， $\mathbf{u}_{i}$ 是对应于 $\lambda_{i}$ 的特征向量， $Λ=diag(\lambda_{i})$ 为对角矩阵， $\mathbf{U}=[\mathbf{u}_1,\mathbf{u}_{2,}\cdots , \mathbf{u}_N]$ ，可得精确的图卷积运算公式： $$\begin{aligned}&\hat{\mathbf{H}}=\hat{\mathbf{X}} \hat{\mathbf{W}}= \left(\mathbf{U}^{T} \mathbf{X}\right)\left(\mathbf{U}^{T} \mathbf{W}\right) \\ &\mathbf{H}=\mathbf{U} \hat{\mathbf{H}}\end{aligned}$$
    如果进行多次卷积操作，则可扩展得到第 k 层的图卷积公式： $$\mathbf{H}^{k+1}=\mathbf{U} \left(\mathbf{U}^{T} \mathbf{H}^{k}\right)\left(\mathbf{U}^{T} \mathbf{W}^{k}\right)$$ ，其中 $\mathbf{H}^{0}=\mathbf{X}$ ，且 $\mathbf{W}$ 是可学习的。

    特殊的，若令 $A=A_c$ (循环图)，可得 $L$ 的形式为： $$\mathbf{L}=\left[\begin{array}{ccccc}2 & -1 & 0 & \cdots & -1 \\-1 & 2 & -1 & \cdots & 0 \\0 & -1 & 2 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ -1 & 0 & \cdots & -1 & 2 \end{array}\right]$$ 在此情况下，GFT 等价于离散傅里叶变换(DFT)[17]。类似地，对于 $A=A_l$ (线形图)，GFT 等价于离散余弦变换(DCT)。这使得卷积操作更加方便且计算效率高，因为我们可以避免对于任意图结构而言计算量大的特征分解。

    根据最近一篇关于 GCN[20]的工作，可知公式中的卷积核 $\left(\mathbf{U}^{T} \mathbf{W}^{k}\right)$ 能通过多层感知器(MLP)来学习得到，最后得到以下形式的卷积运算 $$\mathbf{H}^{(k+1)}=\mathbf{U}\left(\operatorname{MLP}\left(\mathbf{U}^{T} \mathbf{H}^{(k)}\right)\right)$$ 其中，只有 MLP 参数是可学习的。

2. **汇聚层**。我们的目标是对整个图结构进行分类(与一般图节点分类任务相反)。因此，我们需要一个函数来从节点的嵌入向量中获得图级表征 $h_G \in R^Q$ 。

    通过在将所有节点卷积运算得到的嵌入向量矩阵 $H_(k)$ 传递到分类层之前，在最后一层通过汇聚层池化，可以获得 $h_G \in R^Q$ 。图域中常用的池化方法有 mean, max 和 sum[9,21]三种，但 Max 和 Mean 两种方法通常不能保存有关图结构的基本信息，而 Sum 池已被证明是很好的选择[20]。因此本文中使用 Sum 方法来获得图表示（对每一维度的所有节点的嵌入特征求和）： $$\mathbf{h}_{G}=\operatorname{sumpool}\left(\mathbf{H}^{(K)}\right)=\sum_{i=1}^{M} \mathbf{h}_{i}^{(K)}$$

    池化层之后是一个全连接的层，用来生成分类标签，最后用交叉熵损失函数 loss = $=-\sum_{n} \mathbf{y}_{n} \log \tilde{\mathbf{y}}_{n}$ 训练。

**实验结果和分析**
我们使用 OpenSMILE 工具包[28]从为 InterSpeech 2009 情感挑战[27]建议的语音话语中提取一组低层表征(LLD)构建特征集，其中包括 Mel 频率倒谱系数(MFCCs)、过零率、语音概率、基频(F 0)和帧能量。对于每个样本，我们使用一个长度为 25 ms(步长为 10 ms)的滑动窗口来局部提取 LLDs。然后使用移动平均滤波器对每个特征进行平滑，并且使用平滑后的特征来计算它们各自的一阶 delta 系数。此外，我们还为 IEMOCAP 添加了 spontaneity 作为 binary feature，因为这一特征有助于 SER[29]，spontaneity 数据随数据库一起提供。综上，在 IEMOCAP 数据库生成了维度 P=35 的节点特征向量，对于 MSPIMPROV，生成了维度 P=34 的节点特征向量(无 spo
ntaneity 特征)。

### 引文

- 图卷积网络(GCNs)[9]已成功地用于解决计算机 vi 快速流和自然语言处理中的各种问题，如动作识别[10]、对象跟踪[11]和文本分类[12]。

- 在 con Fast streams 文本音频分析中，我们知道只有一个最近的工作提出了一个基于注意的图神经网络架构，用于少量音频分类[13]。

## 摘录
