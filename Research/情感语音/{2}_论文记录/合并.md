---
title: "{{title}}"
description: ""
author: ""
tags: [""]
categories: ""
keywords:  [""]
draft: true
layout: ""
date: 2023-04-11 18:13:35
lastmod: 2023-04-11 18:25:49
---


> [!info] 论文信息
>1. Title：Speech Emotion Recognition with Multi-Task Learning
>2. Author：Xingyu Cai, Jiahong Yuan, Renjie Zheng, Liang Huang, Kenneth Church
>3. Entry：[Zotero link](zotero://select/items/@caiSpeechEmotionRecognition2021a) [URL link](https://www.isca-speech.org/archive/interspeech_2021/cai21b_interspeech.html) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cai et al_2021_Speech Emotion Recognition with Multi-Task Learning.pdf>)
>4. Other：2021 - Interspeech 2021  ISCA   -   

## ⭐ 重点

- 利用预先训练好的 Wave 2 vec-2.0 进行语音特征提取，通过情感分类(Ser)和语音识别(ASR)两个任务对 SER 数据进行微调。
- 改进了训练时的 loss 值，额外添加了文本识别损失（CTC，忽略文本和语音长度差异，有效反向传播梯度）
- 语音识别(ASR)可以作为副产品获得。
- 最终预测时，将 Softmax 算子替换成 argmax 算子。- 论文源文件

## 摘要

> [!abstract] 
> Speech emotion recognition (SER) classifies speech into emotion categories such as: Happy, Angry, Sad and Neutral. Recently, deep learning has been applied to the SER task. This paper proposes a multi-task learning (MTL) framework to simultaneously perform speech-to-text recognition and emotion classification, with an end-to-end deep neural model based on wav 2 vec-2.0. Experiments on the IEMOCAP benchmark show that the proposed method achieves the state-of-the-art performance on the SER task. In addition, an ablation study establishes the effectiveness of the proposed MTL framework.

> 语音情感识别(SER)将语音分为快乐、愤怒、悲伤和中性等情感类别。最近，深度学习被应用到 SER 任务中。提出了一种同时进行语音到文本识别和情感分类的多任务学习(MTL)框架，并基于 Wav 2 vec-2.0 建立了端到端的深度神经模型。在 IEMOCAP 基准测试平台上的实验表明，该方法在 SER 任务上达到了最好的性能。此外，一项消融研究确定了拟议的 MTL 框架的有效性。

## 结果

IEMOCAP

![]({4}_Speech%20Emotion%20Recognition%20with%20Multi-Task%20Learning@caiSpeechEmotionRecognition2021a.assets/image-20220417160641.png)

## 精读

IEMOCAP

![]({4}_Speech%20Emotion%20Recognition%20with%20Multi-Task%20Learning@caiSpeechEmotionRecognition2021a.assets/image-20220304005708.png)

许多识别模型使用的频谱特征，以及韵律特征的显式表示、音质特征和基于 Teager 能量算子的特征[5]。这些方法需要很强的领域知识和对语音的深入理解。

**Multi-task Learning**

多任务学习使用共享架构模型，同时优化不同任务的多个目标，其优势是辅助信息和不同任务之间的能够交叉正则化(隐含地，任务 a 可以是任务 b 目标的调节器 )。同时，联合优化带来了挑战[18]。

**Wav 2 Vec-2.0: Pretraining with Fine-Tuning for Speech**

预训练阶段通常在无监督的情况，使用一个大数据集训练一个模型，比如 bert，让模型学习数据的有意义表示，预训练完成后，该模型就可以通过使用相对较少的带有标签的受监督的训练数据对特定的下行任务数据进行微调训练。预训练阶段所获得的信息，可以帮助训练针对下行特定任务数据的模型且不容易过拟此类特定任务的数据。

wav 2 vec-2.0 ，使用类似于 bert 所采用的无监督方法，通过对大量的音频数据进行预先训练来学习语音表达，试图恢复编码音频特征的随机掩码部分。经过训练之后，可以针对不同的下游任务上进行了微调。

**Model Architecture**

我们提出了一种端到端模型，输入原始语音波形，并输出预测的情感标签。

我们将预先训练的 Wav2vec-2.0模型表示为 $f(·)$ ，假设输入波形为 $x$ ，其长度为 $L$ (样本数量： $L$ )，从 Wav2vec-2.0中最后一个隐藏层得到的输出作为特征 $z$ ，即 $z=f(X)$ ，对应灰色块部分。后面 ASR 路径（使用由 V=32个字符组成的词汇表，其中有26个英文字母和几个标点符号），以 $z$ 为输入，通过一个完全连接层(FC 块) $g$ ，获得对字符的预测： $y=g(f(X))$ 。后面的另一个的 SER 路径，以 $z$ 为输入，通过池化层 P 和全连接层 h，获得对情感类的预测： $C=h(ΣP(z_{i}))$ 。在两条路径的末端，我们对 $y$ 和 $c$ 应用 Softmax 算子，以将它们转换为概率向量，然后通过 CTC 和 CrossEntropy 分别求取 ASR 和 SER 的损失，并最终联合为最终损失值 $\mathcal{L}=\mathcal{L}_{\mathrm{CE}}+\alpha \mathcal{L}_{\mathrm{CTC}}$ 。最终预测时，将 Softmax 算子替换成 argmax 算子。


> [!info] 论文信息
>1. Title：Speech Emotion Recognition Using Semantic Information
>2. Author：Panagiotis Tzirakis, Anh Nguyen, Stefanos Zafeiriou, Björn W. Schuller
>3. Entry：[Zotero link](zotero://select/items/@tzirakisSpeechEmotionRecognition2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Tzirakis et al_2021_Speech Emotion Recognition Using Semantic Information.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

## ⭐ 重点

- 

## 摘要

> [!abstract] Speech emotion recognition is a crucial problem manifesting in a multitude of applications such as human computer interaction and education. Although several advancements have been made in the recent years, especially with the advent of Deep Neural Networks (DNN), most of the studies in the literature fail to consider the semantic information in the speech signal. In this paper, we propose a novel framework that can capture both the semantic and the paralinguistic information in the signal. In particular, our framework is comprised of a semantic feature extractor, that captures the semantic information, and a paralinguistic feature extractor, that captures the paralinguistic information. Both semantic and paraliguistic features are then combined to a unified representation using a novel attention mechanism. The unified feature vector is passed through a LSTM to capture the temporal dynamics in the signal, before the final prediction. To validate the effectiveness of our framework, we use the popular SEWA dataset of the AVEC challenge series and compare with the three winning papers. Our model provides state-of-the-art results in the valence and liking dimensions.1

> 语音情感识别是人机交互、教育等众多应用中的一个关键问题。尽管近年来取得了一些进展，特别是随着深度神经网络(DNN)的出现，但文献中的大多数研究都没有考虑语音信号中的语义信息。在本文中，我们提出了一种新的框架，可以同时捕捉信号中的语义信息和副语言信息。特别是，我们的框架由一个捕获语义信息的语义特征抽取器和一个捕获副语言信息的副语言特征抽取器组成。然后，使用一种新的注意机制将语义特征和并列特征组合成统一的表示。在最终预测之前，统一的特征向量通过 LSTM 来捕获信号中的时间动态。为了验证我们的框架的有效性，我们使用了 AVEC 挑战系列中流行的 SEWA 数据集，并与三篇获奖论文进行了比较。我们的模型在价位和喜好维度上提供了最先进的结果。

## 结果

Speech2Vec

计算结果系数

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161416.png)

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161252.png)

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161340.png)

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220417161312.png)

## 精读

语音情感识别是人机交互和教育等众多应用中的一个关键问题。尽管近年来取得了一些进展，特别是随着深度神经网络（DNN）的出现，文献中的大多数研究都没有考虑语音信号中的语义信息。

![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220304005759.png)

我们的跨模式框架可以利用语义(高级)信息(SEC。3.1)和语音信号中的副语言(低级)动态(Sec.。3.2)。使用一种新的注意力融合策略(SEC)将低级别和高级特征集融合在一起。3.3)在将它们馈送到单层LSTM模块之前，捕获信号中的时间动态，用于最终的帧级预测。

1. 第一步捕获语音信号中的 semantic information

	训练了 Speech2Vec 和 Word2Vec 模型（可以分别使用语音数据和文本信息获得含语义信息的 Speech embedding  spaces 和 Word embedding  spaces）得到特征矩阵，然后使用[15]的方法(将得到的 embedding  spaces 通过域对抗方法[^Adversarialtraining]得到对齐后的 embedding  spaces )。即通过领域对抗性训练来学习。其中鉴别器试图最小化$L_{D}\left(\theta_{D} \mid W\right)=-\frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}\left(\text { speech }=1 \mid W s_{i}\right)-\frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}\left(\text { speech }=0 \mid t_{i}\right)$，其中$θ$是鉴别器的参数，$P_{\theta_{D}}(speech=1/0|z)$是特征矩阵$z$源自语音特征矩阵或文本特征矩阵的概率，使其能准确识别特征矩阵$z$的来源。而生成器试图最小化$L_{G}\left(W \mid \theta_{D}\right)=-\frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_{D}}\left(\text { speech }=0 \mid W s_{i}\right) -\frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_{D}}\left(\text { speech }=1 \mid t_{i}\right)$，持续优化$W$来欺骗鉴别器，使$T'=WS$和$T$尽可能相似，最终使$W$将得到的特征向量矩阵对齐。

2. 上述公式平等对待所有嵌入向量，而频率较高的词在向量空间中的嵌入质量会比频率较低的词更好。

	为此，我们使用频繁词来创建自定字典，该字典指定哪些语音嵌入向量{$S_{k}$}对应于哪些文本嵌入向量{$T_{k}$}，继续改善映射矩阵$W$。最终优化出$W^{∗}= \mathop{argmin}\limits_{W}||WS_{k}−T_{r}||_{F}$，公式解可由$S_{r} T_{r}^{T}$的奇异值分解得到的，即$S V D\left(S_{r} T_{r}^{T}\right)=$ $U \Sigma V^{T}$。**并最终得到想要的 Speech Semantic Features：$X_{s}$** 

3. 第三步为了得到语音信号中的 paralinguistic information
	使用原始波形作为输入，通过一个三层的一维卷积神经网络得到 Paralinguistic Features $X_{p}$ ![]({5}_Speech%20Emotion%20Recognition%20Using%20Semantic%20Information@tzirakisSpeechEmotionRecognition2021.assets/image-20220304012843.png)

4. 融合两个特征（简单串联或注意力机制融合）

    - 将 $X_{s}$和 $X_{p}$  简单的串联得到$X_{fusion}=[X_{s},X_{p}]$。
    - 每个特征集点乘线性映射矩阵$W_{s},W_{p}$，使其位于相同的向量空间（相同纬度），第一层使用注意力机制融合这两种特征得到$\tilde{\mathbf{x}}_{sp}=\text {Attention}\left(\tilde{\mathbf{x}}_{s}, \tilde{\mathbf{x}}_{p}\right)$，通过三个具有相同维度的全连接层(FC)，得到三种信息流$a,v,l$，刚好对应arousal, valence, liking 情感维度。然后再通过两个注意力机制一步步融合三种特征得$X_{fusion}=\text {Attention}\left(\text {Attention}\left(a, l\right), v\right)$

5. 在最终预测之前，$X_{fusion}$**通过 ==LSTM== 来捕获信号中的时间动态。**

6. 模型设置

Adam优化方法[27]、固定学习速率为10−4、小batch 大小25，序列长度为300，dropout[28]率为0.5、LSTM网络的 gradient norm clipping为5.0、原始波形长10秒、采样率22050 Hz、目标损失函数Concordance Correlation Coefficient (ρc)


> [!info] 论文信息
>1. Title：Speech Emotion Recognition Based on Listener Adaptive Models
>2. Author：Atsushi Ando, Ryo Masumura, Hiroshi Sato, Takafumi Moriya, Takanori Ashihara, Yusuke Ijima, Tomoki Toda
>3. Entry：[Zotero link](zotero://select/items/@andoSpeechEmotionRecognition2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ando et al_2021_Speech Emotion Recognition Based on Listener Adaptive Models.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   


## ⭐ 重点

- 缓解传统方法没有考虑到不同听众的个性化情感感知。
- 最终结果可以转换为传统的情绪感知结果（求平均）
- 采用的方法是自适应方法（自适应全连接（AFC），自适应LSTM（ALSTM）和自适应CNN（ACNN）
- 采用了一个更大型的、持续更新的开源库MSP-Podcast。

思考：
- 把所有听众都考虑了，每个听众都是不同的，那么这些听众是否可以进一步分类，即不同类人有不同的情感判别特性，然后根据类别设计不同模型？
- 优点：计算次数变少，把人分类后，可扩展性也更好。

## 摘要

> [!abstract] This paper presents a novel speech emotion recognition scheme that can deal with the individuality of emotion perception. Most conventional methods directly model the majority decision of multiple listener's perceived emotions. However, emotion perception varies with the listener, which means the conventional methods can mismatch the recognition results to human perception. In order to mitigate this problem, we propose a Listener Adaptive (LA) model that reflects emotion recognition criteria of each listener. One-hot listener codes with several adaptation layers are employed in the LA model. The LA model yields the posterior probabilities of the listener-specific perceived emotions. Majority-voted emotion can be also estimated by averaging, in the LA model, the posterior probabilities for all listeners. Experiments on two emotional speech datasets demonstrate that the proposed approach offers improved listener-wise perceived emotion recognition performance in natural speech.

> 提出了一种新的能够处理情感感知个性化的语音情感识别方案。大多数传统方法直接对多个听者的感知情绪的多数决策进行建模。然而，情感感知随着听者的不同而不同，这意味着传统的识别方法可能会使识别结果与人的感知不匹配。为了缓解这一问题，我们提出了一种反映每个听者情感识别标准的听者自适应(LA)模型。LA模型采用具有多个适配层的One-HotLister编码。LA模型产生特定于听者的感知情绪的后验概率。在LA模型中，多数人投票的情绪也可以通过平均所有听众的后验概率来估计。在两个情感语音数据集上的实验表明，该方法在自然语音中具有更好的听者感知情感识别性能。

## 结果

IEMOCAP&&MSP-Podcast

![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220417161651.png)

## 精读

![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220304005823.png)

![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220304005857.png)

大多数传统情感识别方法，将声音特征输入模型后，经由解码器和编码器得到后验概率，最终从中选出最大概率对应的标签，即为情感预测标签。而在此方式中所使用的训练集中的真实标签，是由一组参与情感标签标注的人，对声音序列进行标注，并取出投票数最多的情感标签作为其真实标签得到的，即 the majority decision of multiple listener’s perceived emotions 方法。然而现实情况是情绪感知会因听者而异（听众依赖问题），这意味着传统方法可能会使识别结果与人类个体的个性化感知不匹配。为了缓解传统方法没有考虑到不同听众的个性化情感感知特征，本文提出了一个 Listener Adaptive模型 (LA，听众自适应模型) ，它可以反映每个听众的情绪感知标准。

本文所提出的方法假定每个注释训练集语音的听众都是已知的，并且包括在了训练数据中，然后用一个模型来学习每个听众的情绪感知标准，以此解决听众依赖性问题。

本文中的LA模型将 One-hot listener code **v (l)** 输入到  listener embedding layer中，得到 Listener embedding 向量 **e (l)**，然后将之作为辅助信息与声学特征 **x** 共同输入到多个adaptation layers （自适应全连接层（AFC），自适应LSTM层（ALSTM）和自适应CNN层（ACNN））中。通过切换 listener code ，用单一模型估计依赖于听众的情绪感知后验概率，最终产生每个特定听众的情绪感知的后验概率。对于传统的 Majority-voted 情绪感知结果可以通过在 LA 模型中对所有听众的后验概率进行平均来估计特定标签的概率。 

==声学特征：400维对数功率谱图，其中帧长和帧移分别为40ms 和10ms。Dft长度为1600(10hz 网格分辨率) ，使用0-4千赫兹频率范围。所有的功率谱都使用训练数据集的均值和方差进行了 z 标准化$^{[7, 19]}$。==

本文使用了两个情感语音数据集: Msp-podcast和Iemocap。Msp-podcast有非常自然的语音、大量听众以及语音标注，每个语音都至少有3个听众(平均每个语音有6-7个听众)标注。这个数据集有两种情绪标注，主要情绪和次要情绪，文中只使用了主要情绪构成训练集。 其中开放数据集中的60名发言者的8215个片段用于测试，44名发言者的4418个片段用于验证，其余的25332个片段用于训练。在训练集中，标注少于100条的听众被归类为“其他听众”，因为这些听众会使得学习依赖于听者的情绪感知特征变得困难。Iemocap 是一个被广泛使用的数据集，其中包含了一些听众标注的演讲。它包含10个专业演员(5个男性和5个女性)的视听数据。本文只使用了即兴发挥的语音，并将快乐和兴奋的标签组合成 hap 类。在这个语料库中共有6个学生听众，其中每个语音会得到3个标注，因此我们统一了每个听众的多个情绪标注，规则如下: 优先选择标注中情绪标签出现次数最多的，否则选择第一个标注。对于标注少于500条的听众被归类为“其他听众”，和 msp-podcast 一样。评估表现采用一个说话人缺失交叉验证进行比较，一个说话人用于测试，另一个用于验证，其他8个说话人用于训练。

为了阐明情绪感知中听者依赖性的存在，我们首先使用Cohen’s kappa 系数来研究听者注释的相似性。通过5级匹配((4 targets + Oth)计算相似度。我们选择 msp-podcast/iemocap 中的前10名/前3名的听众来评估两个听众对的相似性，在这些听众对中，两个听众都注释了超过20个相同的话语(结果中小于20个是“-”)。结果表明，MSP-Podcast中相当数量的听众对相似度较低。听众1与听众4,9,10表现出高度相似性，但与其他听众相似性低。另一方面，iemocap 中听众对的相似度均大于0.4(中度匹配) ，而听众1和听众3的相似度相对较低。这些表明情绪感知取决于听众，至少对于 MSP-Podcast。

基线是 majority-voted 情绪识别模型和那些受过 soft-target 目标训练的模型。在依赖听者的情绪识别任务中，基线模型的输出被看作是听者的个体估计结果。由于该方法统一了 la 模型的多个输出，因此在多数情感识别中也比较了不同初始参数情感模型的集成。听众人数为每个语音的平均听众人数，即 msp 播客和 iemocap 中的平均听众人数分别为7和3。在基线模型中，

输入 batch 的大小为8（iemocap）和16（MSPPodcast），结构包含3层 cnn（16,24,32+batch normalization+ReLu激活函数+2 × 2 max-pooling 层）、1层双向 lstm（128个隐藏单元+0.2 Dropout 率）、4-head 结构注意力层（0.2 Dropout 率）和2层 fc（64个隐藏单元+0.2 Dropout 率）。最终的损失优化方法使用的是Adam（学习率为0.0001）。

==在训练步骤中，以 inverse values of the class frequencies 作为 class weights ，以 mitigate the class imbalance problem（缓解类不平衡问题）。==采用 Speed perturbation 方法进行数据增强，其因子分别设置为0.9、0.95、1.05和1.1。SpecAugment（针对ASR的数据增强方法）也应用了两个 time and frequency masking（时频掩蔽方法）。这些方法是基于LA模型的。

单独使用 afc、 alstm 和 acnn 层，并将这些适应层组合作为 LA 模型进行评估。La 模型的结构和超参数与基线相同。在 MSP-Podcast和 iemocap 中，听者嵌入向量维数分别为32和8。在 ld 模型训练中，计算每个listener的类权重，并将 listener 权重与listener注释频率的反比值相乘，得到最终的损失权重。其他的训练条件与基线相符。

评价指标为加权准确度(wa：所有语音的分类准确度)和未加权准确度(ua：个体情绪类准确度的宏观平均值)。![]({6}_Speech%20Emotion%20Recognition%20Based%20on%20Listener%20Adaptive%20Models@andoSpeechEmotionRecognition2021.assets/image-20220304012859.png)

符号(ens.)是指模型的整体结果。

在 msp-podcast 中，在听者依赖任务中，具有 afc 层的 la 模型的宏观平均值显著高于软标签模型和具有comparable UAs(p  .05)的 majority-voted 模型(配对 t 检验 p < 0.05)。在多数人投票的情感识别中也出现了类似的结果;

具有相似 UA 的基线及其集合的 WA 高出 10% 以上。从这些结果和表3可以看出，在听者的情绪感知差异很 大的情况下，例如自然语言，所提出的LA模型是有效的。

在 IEMOCAP 中，基于 AFC 分层的 LA 模型在依赖于听者和多数投票的情绪任务中分别表现出略好于基线的表现和几乎相同的表现。这些都表明，在情感感知不太依赖于听者的情况下，所提出的方法不会恶化估计性能。最后，对 LA 模型中的适配层进行了比较。AFC 适应层比 ALSTM 和 ACNN 有更好的效果。一种可能性是，个性化情绪感知的出现在对情绪线索的感知做出决定的编码器中，而不是从音频信号中提取情绪线索的解码器中。结果还表明，听众和说话人的个性应该在评估模型的不同部分进行建模；



> [!info] 论文信息
>1. Title：Hierarchical Network Based on the Fusion of Static and Dynamic Features for Speech Emotion Recognition
>2. Author：Qi Cao, Mixiao Hou, Bingzhi Chen, Zheng Zhang, Guangming Lu
>3. Entry：[Zotero link](zotero://select/items/@caoHierarchicalNetworkBased2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Cao et al_2021_Hierarchical Network Based on the Fusion of Static and Dynamic Features for.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   



## ⭐ 重点

- 通过静态 Log-Mel Filter bank 特征（Mel滤波器组）和其动态一阶导数和二阶导数特征进行互补学习，逐步了解高层次的情绪表示。 
- 利用了一种 gate-based 的多特征融合单元，用于在帧级上有效地将不同特征整合在一起。
- 使用了 Bahdanau 可微分注意力模型计算情绪
- 使用 z-score 标准化消除说话人之间的差异- 论文源文件
- 通过静态 Log-Mel Filter bank 特征（Mel滤波器组）和其动态一阶导数和二阶导数特征进行互补学习，逐步了解高层次的情绪表示。 
- 利用了一种 gate-based 的多特征融合单元，用于在帧级上有效地将不同特征整合在一起。
- 使用了 Bahdanau 可微分注意力模型计算情绪
- 使用 z-score标准化 消除说话人之间的差异

## 摘要

> [!abstract] Many studies on automatic speech emotion recognition (SER) have been devoted to extracting meaningful emotional features for generating emotion-relevant representations. However, they generally ignore the complementary learning of static and dynamic features, leading to limited performances. In this paper, we propose a novel hierarchical network called HNSD that can efficiently integrate the static and dynamic features for SER. Specifically, the proposed HNSD framework consists of three different modules. To capture the discriminative features, an effective encoding module is firstly designed to simultaneously encode both static and dynamic features. By taking the obtained features as inputs, the Gated Multi-features Unit (GMU) is conducted to explicitly determine the emotional intermediate representations for frame-level features fusion, instead of directly fusing these acoustic features. In this way, the learned static and dynamic features can jointly and comprehensively generate the unified feature representations. Benefiting from a well-designed attention mechanism, the last classification module is applied to predict the emotional states at the utterance level. Extensive experiments on the IEMOCAP benchmark dataset demonstrate the superiority of our method in comparison with state-of-the-art baselines.

> 许多自动语音情感识别(SER)的研究致力于提取有意义的情感特征以生成与情感相关的表征。然而，它们普遍忽略了静态和动态特征的互补学习，导致了性能的限制。在本文中，我们提出了一种新的层次化网络HNSD，它能有效地集成SER的静态和动态特征。具体地说，拟议的HNSD框架由三个不同的模块组成。为了获取可区分的特征，首先设计了一个有效的编码模块，同时对静态和动态特征进行编码。将获得的特征作为输入，通过门控多特征单元(GMU)明确地确定帧特征融合的情感中间表征，而不是直接融合这些声学特征。这样，学习到的静态特征和动态特征可以联合、综合地生成统一的特征表示。得益于精心设计的注意机制，最后一个分类模块用于预测话语层面的情绪状态。在IEMOCAP基准数据集上的广泛实验表明，与最先进的基线相比，我们的方法具有优越性。

## 预处理

## 概述

## 结果

IEMOCAP

![]({8}_Hierarchical%20Network%20Based%20on%20the%20Fusion%20of%20Static%20and%20Dynamic%20Features%20for%20Speech%20Emotion%20Recognition@caoHierarchicalNetworkBased2021.assets/image-20220417161851.png)

![]({8}_Hierarchical%20Network%20Based%20on%20the%20Fusion%20of%20Static%20and%20Dynamic%20Features%20for%20Speech%20Emotion%20Recognition@caoHierarchicalNetworkBased2021.assets/image-20220417161903.png)

## 精读

![]({8}_Hierarchical%20Network%20Based%20on%20the%20Fusion%20of%20Static%20and%20Dynamic%20Features%20for%20Speech%20Emotion%20Recognition@caoHierarchicalNetworkBased2021.assets/image-20220304005951.png)

静态特征中包含了足够的频域信息。 

1. 使用汉明窗将语音信号分割成25ms 窗宽、10ms 移位的短帧。
2. 利用短时距傅里叶变换信号将信号从时域转换到频域。
3. 对 mel 滤波器组（26个）的能谱进行了对数运算。将Log-Mel 特征作为静态特征，由矩阵$X^{S}∈R^{T×N}$表示，其中 $T$ 是帧数，$N$ 是 Mel滤波器的数量。 $x^{S}_{i}∈ X^{S}$是含 $N$ 个滤波器的第 $i$ 帧（i∈[1，t]）。

动态特征描述了帧间的频谱变化，并反映了情绪的变化过程，可以使用一阶差分方程获得，并由$X^{d} = [Y, Z]$表示，$x^{d}_{i}∈ X^{d}$, i 为第 i 帧（i∈[1，t]）。

1. $y_{i} =\sum_{n=1}^{M} n\left(x_{i+n}^{s}-x_{i-n}^{s}\right) / 2 \sum_{n=1}^{M} n^{2}$
2. $z_{i} =\sum_{n=1}^{M} n\left(y_{i+n}-y_{i-n}\right) / 2 \sum_{n=1}^{M} n^{2}$

第一层是一个有效编码模块。为了防止不同的特征之间彼此干扰，通过使用长短期记忆单元(LSTM，隐藏单元512)，把$X^{S}$和$X^{d}$ 进行编码，用$f^{s}_{i}$ 和 $f^{d}_{i}$ 表示。在这个模块中可以分别学习静态特征和动态特征在时间尺度上的上下文信息，并将静态和动态特征嵌入到高维特征表示中。

第二层中引入门控多特征单元（GMU），将所获得的特征$f^{s}_{i}$ 和 $f^{d}_{i}$作为输入，通过以Tanh为激活函数的神经元$（h_{d/S}=\tanh \left(W_{d/s}\left(f^{d/s}\right)^{T}\right)）$得到 $h^{S}$和$h^{d}$ 。同时定义了一个门神经元控制$f^{s}$ 和 $f^{d}$的权重，表示为$z=\sigma\left(W_{c}\left[f^{s}, f^{d}\right]^{T}\right) ∈ R^{1×T}$，最终得到协同融合特征$h=h_{s}*z+h_{d}*(1-z)∈ R^{hs×T}$

第三层采用 Bahdanau 的注意力机制模型通过前馈神经网络计算权重，每一帧的情绪分数$s_{t}=V^{T} \tanh \left(W h_{t}+b\right)$，其反映了每个帧的情感贡献。其中 $h_{t}$ 表示 $h$ 第$t$帧的融合特征， $V∈R^{d×1}$（本文中d选用的16）和$W≥R^{d×Hs}$是权重矩阵，$b∈R^{d×1}$是偏置向量。然后将情绪分数标准化得到是 t-th 帧的$\alpha_{t}=\frac{\exp \left(s_{t}\right)}{\sum_{i=1}^{T} \exp \left(s_{i}\right)}$。而话语层次的特征就可以通过对每一帧按其贡献加权得到   $u=\sum_{t=1}^{T} \alpha_{t} h_{t}$

通过全连接层（神经元个数：128）和sorftmax层得到最终的情感识别结果。训练过程中使用Adam（学习率：0.0001，weight decay ：0.001）进行反馈优化参数

通过WA（（神经元个数：128））和UA对测试结果进行评估。

### 引文

- 许多研究已经采用了基于卷积神经网络（CNN）和经常性神经网络（RNN）的模型，以产生更多辨别性声学特征，以提高SER [2,3,4,5,6]的性能。

- “例如，李等人。 [4]设计了两个不同的卷积核，用于分别从频谱图分别捕获时间和频域特性。 李等人。''

- [5]提出了一种具有多头自注意的扩展神经网络，用于从 mel frequency 倒谱系数(mfccs)研究情绪相关特征。

- 近年来，注意机制在提高注意效果方面取得了很大进展[7,8,9]。

- [7]应用RNN以检测MFCC的时间上下文信息，并引入了可靠的关注机制，专注于语音信号的情绪相关区域。

- 陈等人。 [10]和Lee等人。 [11]考虑了语音的动态特征作为网络的输入，它们将连接的静态和动态特征转移到深层神经网络中。然而，这种策略忽略了静态和动态特征将相互干扰，这导致表现不令人满意[12]。 

- [13],给出了通过门控多特征单元(gmu)寻找情绪中间表示的灵感。

- Bahdanau注意机制[14,15]致力于在最后一个模块计算情绪突出框架。

- Bi-LSTM [11]：输入为32种特征的Bi-LSTM网络（包括F0，语音概率，过零率，12维MFCC的log energy系数，以及一阶时间导数）。  

- LSTM+CTC [9]：输入为40维的log Mel-filter bank 特征的 attention-based BI-LSTM网络，与连接主义时间分类（CTC）组合。   

- CNN+ Attention[4]：基于声谱图，使用含两组 filter的CNN提取 time-specific and frequency-specific 特征，然后将其连接起来输入到后续的卷积层中，使用 Attention pooling 方法学习  the final emotional representation.。  

- Transformer[20]：基于IS09特征集（统计功能集），结合 self-attention 研究情感语音识别。 

- CNN + RNN + Attention [10]：输入为一阶和二阶动态特征的基于attention的三维CNN 模型判别信息  

- CNN-GRU-SEQCAP [21]：考虑声谱中activities的spatial relationship的基于 capsule networks (CapsNets) 的架构，the spatial relationship of activities in spectrograms  

- Transformer + auxiliary learning [22]: 输入为log Mel-Filter Bank 特征的一种基于多任务学习的多头注意力机制的深度学习网络。

---
title: "A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion Recognition"
description: ""
citekey: rajamaniNovelAttentionBasedGated 2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:22
lastmod: 2023-04-11 17:54:18
---

> [!info] 论文信息
>1. Title：A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion Recognition
>2. Author：Srividya Tirunellai Rajamani, Kumar T. Rajamani, Adria Mallol-Ragolta, Shuo Liu, Björn Schuller
>3. Entry：[Zotero link](zotero://select/items/@rajamaniNovelAttentionBasedGated2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rajamani et al_2021_A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\TBPVYNJ 2\\9414489.html>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 论文源文件
- 提出了一种使用了增强版激活函数的方法：Attention ReLU GRU，即将 attention-based Rectified Linear Unit (AReLU) 作为激活函数的 GRU 和 BiGRU。
- openSMILE 工具包[16]提取特征
- 引出了一些重要的激活函数：SELU [18], EELU [19], Mish [20], and learnable activations such as Comb [21] and PAU [22].
- 论文源文件
- 提出了一种使用了增强版激活函数的方法：Attention ReLU GRU，即将 attention-based Rectified Linear Unit (AReLU) 作为激活函数的 GRU 和 BiGRU。
- openSMILE 工具包[16]提取特征
- 引出了一些重要的激活函数：SELU [18], EELU [19], Mish [20], and learnable activations such as Comb [21] and PAU [22].

## 摘要

> [!abstract] Notwithstanding the significant advancements in the field of deep learning, the basic long short-term memory (LSTM) or Gated Recurrent Unit (GRU) units have largely remained unchanged and unexplored. There are several possibilities in advancing the state-of-art by rightly adapting and enhancing the various elements of these units. Activation functions are one such key element. In this work, we explore using diverse activation functions within GRU and bi-directional GRU (BiGRU) cells in the context of speech emotion recognition (SER). We also propose a novel Attention ReLU GRU (AR-GRU) that employs attention-based Rectified Linear Unit (AReLU) activation within GRU and BiGRU cells. We demonstrate the effectiveness of AR-GRU on one exemplary application using the recently proposed network for SER namely Interaction-Aware Attention Network (IAAN). Our proposed method utilising AR-GRU within this network yields significant performance gain and achieves an unweighted accuracy of 68.3% (2% over the baseline) and weighted accuracy of 66.9 % (2.2 % absolute over the baseline) in four class emotion recognition on the IEMOCAP database.

> 尽管深度学习领域取得了重大进展，但基本的长短期记忆(LSTM)或门控递归单位(GRU)单位在很大程度上保持不变和未被探索。通过正确地调整和加强这些单位的各种要素，有几种可能性来推进最先进的技术。激活功能就是这样的关键要素之一。在这项工作中，我们探索在语音情感识别(SER)的背景下，在 GRU 和双向 GRU(BiGRU)细胞中使用不同的激活函数。我们还提出了一种新的注意力重现 GRU(AR-GRU)，它在 GRU 和 BiGRU 细胞中使用基于注意的整流线性单元(AReLU)的激活。我们使用最近提出的 SER 网络，即交互感知注意网络(IAN)，在一个示例性应用上演示了 AR-GRU 的有效性。我们提出的方法在这个网络中使用 AR-GRU 获得了显著的性能提升，在 IEMOCAP 数据库上的四类情感识别中，未加权准确率为 68.3%(超过基线 2%)，加权准确率为 66.9%(绝对准确率为 2.2%)。

## 预处理

## 概述

## 结果

IEMOCAP

![]({9}_A%20 Novel%20 Attention-Based%20 Gated%20 Recurrent%20 Unit%20 and%20 its%20 Efficacy%20 in%20 Speech%20 Emotion%20 Recognition@rajamaniNovelAttentionBasedGated 2021.assets/image-20220417162004.png)

## 精读

蕴含在人类声音中的副语言信息揭示了说话者的情感状态。这些信息在人与人之间的互动中至关重要，因为我们人类利用这些信息来调整，例如，我们的信息内容或者我们的声调，目的是使互动更加顺畅，并且使我们对自己的语言产生共鸣。

激活函数是改善 LSTM 或 GRU 等 Unit 的一个关键因素，因此，在本文中，我们探讨了语音情感识别中 GRU 和双向 GRU Unit 内不同的激活函数，并提出了一种新的 Attention ReLU GRU(AR-GRU)方法，即使用 AReLU （Attention-based Rectified Linear Units）作为激活函数的 GRU 和 BiGRU 。

AR-GRU 有助于在富含情绪的语音间捕捉特征间的长程交互（依赖），并在解决梯度消失问题的同时提高 SER 系统的性能。

AReLU 中是通过使用 Alpha 为 0.01 和 Beta 为-4，使 AReLU 类似于 RELU 的激活函数。因为 Alpha 是控制 negative values 的比例因子，而值 0.01 与 clamp function 相结合对 negative values 影响最小，β是控制 positive values 的比例因子，将其设置为-4 会使此比例因子失效。然后不断探索取值效果，最终得到β值为 1，Alpha 为 0.01 时，结果最好。

我们使用[13]作为基线模型，它采用了一个 BiGRU 来处理说话人的当前话语，以及两个 GRU 来处理前面的话语。基于 Emobase 2010 的配置，使用 openSMILE 工具包[16]，提取声学低级描述符（LLDs），包括梅尔-频谱系数（MFCCs）、音调和它们在语料的每个短帧中的统计数据。

使用 5 折交叉验证（5-fold Cross Validation）和留一法进行验证，通过观察每 100 个训练周期在验证集上的表现来提前停止。

![]({9}_A%20 Novel%20 Attention-Based%20 Gated%20 Recurrent%20 Unit%20 and%20 its%20 Efficacy%20 in%20 Speech%20 Emotion%20 Recognition@rajamaniNovelAttentionBasedGated 2021.assets/image-20220304010017.png)

## 扩展知识

**GRU**

门控递归单元(GRU)是递归神经网络(RNN)的一种，它使用门控机制(gating mechanisms )来控制和管理神经网络中神经元之间的信息流。GRU 的结构允许自适应地从大型数据序列捕获相关性，同时确保不会丢弃来自序列较早部分的信息。这是通过选通机制实现的，该选通机制规定了在每个时间步要保留或丢弃的信息。与 LSTM 相比，GRU 能够克服消失梯度问题，并且由于需要优化的参数数量较少，因此训练速度更快。

传统 GRU 中的经典激活函数是双曲正切(TANH)。尽管使用 tanh 函数有其固有的优点，但由于密集的激活计算，它具有很高的计算复杂度，并且容易受到消失梯度问题的影响。

[**AReLU**](https://zhuanlan.zhihu.com/p/158389615)

AReLU 一种基于注意力机制的可动态学习的线性整流单元（激活函数）[14]，可以表示为 element-wise sign-based attention 机制（基于符号的注意力机制：ELSA）和标准 Rectified Linear Unit（ReLU） 的组合，其对梯度消失的抵抗力更强，只有两个额外的可学习参数，可以在较小的学习率下促进快速的网络训练。

$\mathcal{F}\left(x_{i}, \alpha, \beta\right)=\mathcal{R}\left(x_{i}\right)+\mathcal{L}\left(x_{i}, \alpha, \beta\right)\\ \begin{cases}C(\alpha) x_{i} & , x_{i}<0 \\ (1+\sigma(\beta)) x_{i} & , x_{i}=0\end{cases}$

$X = {x_{i}}$ ,是激活层的输入， ${α, β} ∈ R^{2}$ 是可学习参数，C 是剪裁操作，作用是将 alpha 的值限制在 $[0.01, 0.99]$ 之间，阻止其变为 0， $σ$ 是 sigmoid 函数。理解：对于大于零的数据梯度进行放大，小于零的则缩小。

### 引文

- 隐马尔可夫模型(hmms)或支持向量机(svms)[3,4]。

- 包括卷积神经网络[5] 

- 回归神经网络[6,7,8] 

- cnns 和 rnns [9]

- 例如长短期记忆(lstm)[10]和门控循环单元(gru)[11] ，可以捕捉序列数据的时间动态，能够捕获的时间依赖性的声学特征。

- 注意力机制可以用来帮助 rnns 关注最突出的情感信息[6,7,8]。

- 此外，上下文信息也可以用来改善服务器系统的性能，如最近的工作[12,13]所示。

- 我们使用交互感知的注意力网络（IAAN）[13]作为基线模型，Yeh et al. [13]通过交互感知注意网络(iaan)成功地利用了语境信息，该网络利用前一个说话者在双人对话场景中的转换来学习注意力分数，以检测一个说话者话语的情绪状态。

- 基于注意力的纠正线性单元(arellu)[14]。

- BiLSTM+ATT[6]: 一个基于帧级特征的注意力池层的 bilstm 网络。

- MDNN[17]: 由多个局部分类器和一个全局分类器组成的多路径深层神经网络。

---
title: "A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers"
description: ""
citekey: wangNovelEndtoendSpeech2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:31
lastmod: 2023-04-11 17:53:09
---

> [!info] 论文信息
>1. Title：A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers
>2. Author：Xianfeng Wang, Min Wang, Wenbo Qi, Wanqi Su, Xiangqian Wang, Huan Zhou
>3. Entry：[Zotero link](zotero://select/items/@wangNovelEndtoendSpeech2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/HW-AARC-CLUB/ICASSP_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 在现有语音情绪识别(SER)模型基础上增加 stacked transformer layers (STL)结构构建新的SER体系结构, 利用STLS模块强大的特征学习能力. 
- 使用 ==t-SNE== 技术可视化中间过渡层发现的特征质量来检查 STL 模型的有效性.

## 摘要

> [!abstract] Speech emotion recognition (SER) aims to automatically recognize emotional category for a given speech utterance. The performance of a SER system heavily relies on the effectiveness of global representation expressed at utterance level. To effectively extract such a global feature, the mainstream of recent SER architectures adopts a pipeline with two key modules, feature extraction and aggregation. Although variant module designs have brought impressive progresses, SER is still a challenging task. In contrast with those previous works, herein we propose a novel strategy for global SER feature extraction by applying an additional enhancement module on top of the current SER pipeline. To verify its effect, an end-to-end SER architecture is proposed where stacked multiple transformer layers are explored to enhance the aggregated global feature. Such an architecture is evaluated on IEMO-CAP and results strongly substantiate the effectiveness of our proposal. In terms of weighted accuracy on four emotion categories, our proposed SER system outperforms the prior arts by a large margin of relatively 20% improvement. Our codes and the pre-trained SER models are made publicly available.

> 语音情感识别(SER)的目的是自动识别给定语音话语的情感类别。SER系统的性能在很大程度上依赖于在话语层面上表达的全局表征的有效性。为了有效地提取这样的全局特征，目前主流的SER体系结构采用了一种包含两个关键模块的流水线，即特征提取和聚合。尽管各种各样的模块设计已经带来了令人印象深刻的进步，但SER仍然是一项具有挑战性的任务。与前人的工作不同，本文提出了一种新的全局SER特征提取策略，在现有SER流水线的基础上增加了一个增强模块。为了验证其效果，提出了一种端到端的SER结构，其中利用堆叠的多个转换器层来增强聚集的全局特征。这种体系结构在IEMOCAP上进行了评估，结果有力地证明了我们的建议的有效性。在四个情感类别的加权准确率方面，我们提出的SER系统比现有技术有较大幅度的提高20%。我们的代码和预先培训的SER模型是公开提供的。

## 预处理

## 概述

## 结果

EMOCAP 包含超过10K 个话语，由九个情感类别的标签标注。按照前人的研究步骤，我们使用了一个四类({高兴、愤怒、悲伤、中性})的子库，总共有5531个话语。对于每个情感类别，其关联样本分别按7/1/2的比例随机分为训练/均值/测试。

所提出的SER系统已在PyTorch中实现。利用Adam[20]优化器对分类交叉熵进行优化，同时监测验证精度，提前停止设置为8个历元。训练系统的批次为32，学习率为2×10−4，衰减率为1×10−6。

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220417162209.png)

## 精读

言语情感作为文本之外的一种 meta-information，对理解说话人的心理和反应起着重要的作用。相关研究被称为语音情感识别(SER)，其目的是自动识别给定语音表达的情感类别。由于情感通常以一种微妙和可变的方式传达，因此识别情感嵌入作为话语的表征一直是一个挑战，可以有效地对情感类别进行分类。

随着深度神经网络(DNN)的最新进展，情感嵌入已经从先前基于知识的手工制作的声学特征，例如低级描述符(LLD)，演变为基于 DNN 的深度情感特征。在最近的 SER 工作中，已经探索了各种 DNN 结构，如卷积神经网络(CNN)、长短期记忆(LSTM)、时延神经网络(TDNN)、残差网络(ResNet)、扩张残差网络(DRN)，它们本身或组合在一起[1-4]。在语义特征识别领域已经有了大量的研究成果，但由于情感与语言特征的分离，以及从长时间的话语中提取有效的情感特征，这方面的研究仍然具有挑战性。为了解决这些问题，现代 SER 系统普遍采用两个模块的流水线：1)特征提取模块，以生成情感相关的时间声学特征；以及 2)聚合模块，将这些时间特征汇集到话语级别的紧凑的全局语境表示(也称为情感嵌入)中。为了产生有效的情感嵌入，最近的 SER 工作集中在开发不同的模块架构上。例如，[2]提出了一种混合的 CNN-LSTM 体系结构，其中 CNN 从原始频谱图中提取特征序列，而 LSTM 聚合长期特征依赖关系。类似地，文献[5]提出了基于 1 D 和 2 D CNN-LSTM 的 SER 系统，并表明 2 D CNNLSTM 网络通过从谱图中捕捉局部相关性和全局上下文信息而优于 1 D 网络。后来在注意力机制的启发下，RNN 在[7]中提出了 RNN-注意，RNN 通过关注情绪显著的特征来提取时间特征和聚合长期特征依赖关系。文[8]测试了几种时间建模方法，目的是从原始波形中学习深层情感特征。在[9]中，作者引入了 CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。MHA 机制再次被[4]采用来构建 DRN-注意体系结构，其中 DRN 使网络在特征学习中保持高分辨率的时间结构。此外，文献[10]还提出了语境 LSM 注意来考虑周围话语之间的关系和依存关系。尽管在以前的工作中取得了进展，但目前的 SER 表现仍有改进的空间。例如，在基准数据集 IEMOCAP 上的 SOTA 分类准确率仅为 76%左右(在四个情感类别上)。同时，我们注意到，在其他研究领域，经常有报道称，用叠层变压器层(STL)取代重复性可以显著提高性能。例如，基于 STLS 的声学模型[11]在 Librispeech 基准上给出了最好的声学模型。在问答领域，堆积式潜在注意和多跳注意网络(MAN)[12]都表现出显著的性能改进。对于图像捕获任务，从堆叠的交叉注意网络(SCAN)[13]或堆叠的注意模块[14]获得最先进的(SOTA)结果。受其他研究领域的成功启发，在本研究中，我们有兴趣将 STL 机制应用到 SER 网络中。特别是，提出了一种通过在现有 SER 流水线上添加 STL 而构建的新的 SER 体系结构。我们建议的动机是利用 STLS 模块强大的特征学习能力来直接提高流水线输出。据作者所知，这是文献中第一次使用这种策略来研究 SER 问题。本文组织如下。在第二节中，简要介绍了香草变压器。在第三节中，我们详细地提出了 STL 增强的 SER 体系结构。第四节报道了实验。第五节总结了这项研究。


在本节中，我们将简要描述源自 Vanilla Transformer[15]的 STL 的详细信息，后者完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系。在这项研究中，我们感兴趣的是 ITS 编码器，它将输入序列转换为固定维度的全局上下文向量。具体地，编码器由六个相同的变压器层(TL)堆叠而成。每个 TL 包含具有剩余连接的两个子层：1)自关注子层和 2)位置前馈网络(FFN)子层。此外，还引入了位置编码(PE)来为模型提供显式的顺序信息。得益于自我注意机制，输出序列通过重视输入的特定部分来保留和利用输入信息。TL 的堆叠方式是，只有第一层采用输入顺序，每隔一层只采用前面的输出作为输入。这使得能够通过在潜在表示空间内的迭代关注和投影来丰富上下文向量。从数学上讲，《变形金刚》的体系结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程组(ODE)求解器。从这个角度来看，STL 的数量对应于 ODE 中的时间维度，这自然赋予 Transformer 以强大的学习能力来产生深层上下文表示。
度。



我们建议的新的 SER 框架将在这一部分介绍。图 1 概述了它的体系结构。如图所示，它依次包括 4 个模块：前端预处理模块，用于提取帧级别的 LLD；CNN-BiLSTM 模块，用于提取上下文表示；STLS，用于增强上下文表示；后端分类器，用于预测情感类别的概率。下面介绍 CNNBiLSTM 和 STLS 模块的设计细节。

CNN-BiLSTM Module

我们的 CNN-BiLSTM 模块的详细信息如图 2 所示，这类似于我们之前的研究[17]。简而言之，该模块由六对 CNN-Pooling 层和一个 BiLSTM 层构成。每个 CNN-Pooling 对包括一个卷积层(配有 batch 归一化和 ELU 函数激活)用于提取高层时间特征，以及一个池化层(具有固定丢失率)用于降低特征维.

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210042.png)

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210255.png)

特别是，模块输入是基于帧级别的 LLD 声学特征，表示为 $X=\left\{x_{1}，x_{2}，\cdots，x_{t}\right\}\in$$\mathbb{R}^{\mathrm{t}\times\mathrm{d}}$ ，其中 $\mathrm{t}$ 表示帧长度， $\mathrm{d}$ 表示 LLD 特征尺寸。对于第一个 CNN-Pooling 对，是具有 $c h_{0}$ 个 Filter 的 2 D 卷积，应用在 $X$ 上，并产生时间特征序列，表示为 $H_{0}^{\text{CNN}}=$$\left\{h_{1}^{\mathrm{CNN}}，h_{2}^{\mathrm{CNN}}，\cdots，H_{\mathrm{t}}^{\mathrm{cnn}}\right\}\in \mathbb{R}^{\mathrm{t}\times\mathrm{h}_{0}\times\mathrm{w}_{0}}$ ；然后对每个输出进行大小为 $n_{a_p}*n_{a_p}$ 的 2 D-AveragePooling 操作以降维，得到的输出 $H_{0}^{\text {pool }}=$ $\left.\left\{h_{1}^{\text {pool }}, h_{2}^{\text {pool }}, \cdots, h_{\text {ch }_{0}}^{\text {pool }}\right\} \in \mathbb{R}^{\text {ch }_{0} \times \mathrm{h}_{0} / \mathrm{n}_{\mathrm{ap}} \times \mathrm{w}_{0} / \mathrm{n}_{\mathrm{ap}}}\right)$ 被馈送到下一个卷积对。通过总共 6 个 CNN-Pooling 对，，得到最终的潜在表示 $H_{5}^{\text{pool}}\in\mathbb{R}^{\mathrm{ch}_{5}\times\mathrm{h}_{5}\times\mathrm{w}_{5}}$ 。通过进一步的 reshape 操作，将 reshape 版本 $H^{r e}\in\mathbb{R}^{\mathrm{h}_{5}\times\left(\mathrm{ch}_{5}\times\mathrm{w}_{5}\right)}$ 随后馈送到 BiLSTM 层。

BiLSTM 向前和向后从 $H^{\text{re}}$ 学习上下文依赖关系。其输出为隐藏状态序列 $H^{\text {bilstm }}=\left\{h_{1}^{\text {bilstm }}, h_{2}^{\text {bilstm }}, \cdots, h_{h_{5}}^{\text {bilstm }}\right\}$ 表示上下文特征序列，其中 $h_{i}^{\text {bilstm }}=\left[\vec{h}^{\text {bilstm }}, \overleftarrow{h}^{\text {bilstm }}\right] \in$ $\mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ ，隐藏状态大小为 $\mathrm{d}_{\mathrm{lstm}}$ 。

在经典的 SER 任务中，采用了两种聚合方法，即 reucurrence-based 方法和 attention-based,方法来产生全局情感嵌入。
$$

X^{G}=\left\{\begin{array}{cl}

h_{\mathrm{h}_{5}}^{\text {bilstm }}, & \text { recurrence - based } \\

\sum_{i=1}^{\mathrm{h}_{5} \alpha_{\mathrm{i}} \mathrm{h}_{\mathrm{i}}^{\text {bilstm }},} & \text { attention }-\text { based }
\end{array}\right.
$$

在任何一种方式中，情感嵌入 XG(或受进一步仿射变换的约束)被馈送到后端分类器以进行概率预测。


作为我们的核心贡献，提出了一种产生情感嵌入的替代方法。它应用 additional STLs 来增强上下文特征序列 $H^{\text {bilstm }}$ 。

为此，首先执行1D 卷积以将 $H^{\text{bilstm}}$ 映射到向量 $Z_{0} \in \mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ 。然后使用具有 L 层的 STLs 迭代增强
$Z_{0}$ 得到 $Z_{i}=G\left(Z_{i-1}\right)$ 。除了跳过 positional encoding 外，一系列操作都是在 $G$ 内执行的，例如多头注意、前馈和残差连接(如第 2 节所述)。最后一个 Transformer 层的输出是全局嵌入，即 $X^{G}=Z_{L-1}$ 。

最后，以端到端的方式对所提出的 SER 体系结构进行训练，目标函数如下： $$

\begin{gathered}

\hat{y}_{c}=\operatorname{softmax}\left(\mathrm{X}_{\mathrm{G}}^{\mathrm{T}} \mathrm{W}+\mathrm{b}\right), \\

L=-\log \prod_{i \in S} \sum_{c=1}^{C} \hat{y}_{i, c} \log \left(\hat{y}_{i, c}\right),

\end{gathered}

$$ ，其中 $S$ 表示用于训练的样本集， $C$ 表示情感类别的总数。

我们注意到，类似的架构被用于多通道情感识别[18]。我们的方法与以前的工作不同，因为 CMA 是对时间特征序列进行操作，这需要进一步的统计汇集；而在我们的例子中，STLS 应用于语音级表示。

3. STL 模块，用于增强上下文表示；

    STACKED Transformer:** 完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系，可以将输入序列转换为固定维度的上下文全局特征。它由六个相同的  Transformer Layers (TLs)堆叠而成，每个 Transformer Layer 包含两个附带  residual connections 的子层：self-attention 层和 position-wise feed-forward network(FFN) 子层。此外引入了 positional encoding (PE)来为模型提供显式的顺序信息。

    通过 self-attention 机制，对输入序列的特定部分加以重视, 来保留和利用输入序列信息。Transformer Layers 的堆叠，只有第一层接受输入序列，后面每隔一层只接受前一层的输出作为输入，这使得能够通过在潜在表征空间的迭代注意力机制和数据投影来丰富上下文全局特征。

    从数学上讲， Transformer 结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程(ODE)求解器[16]。从这个角度来看，STLs 的数量对应于 ODE 中的时间维度，这自然使 Transformer 具有很强的学习能力来产生**深层上下文表示**。

4. 分类器，用于预测情感类别的概率。

**[t-SNE 可视化技术](https://zhuanlan.zhihu.com/p/148170862)**




我们进行了更多的调查，以评估我们拟议的 STL 模块的有效性。首先，研究了一种不带 STLS 模块的截断 SER 系统。由此产生的 WA 值暴跌至 72.79%。这项研究的意义是双重的：
1)我们的截断系统设计得很好，性能可以与现有技术相媲美；
2)STLS 模块对系统整体性能的贡献很大(18.5%)，代价是合理增加参数数量(从 356 K 增加到 468 K)。
其次，通过使用 t-SNE 技术可视化中间层发现的特征质量来检查 STLS 的有效性[24]。
两个潜在特征分布 Z 0 和 Z 4 在图 3 中用关于类的 2 D 曲线图进行了比较和说明。同样，可以观察到，STL 之后的特征被清楚地投影到不同的集群，并且彼此很好地分开。
最后，进一步研究了 STL 的最佳层数。实验结果如图 4(A)所示，有趣的是，系统性能在开始时(&lt;5 层)迅速提高，在中间(5-10 层)几乎保持不变，然后在结束时(&gt;10 层)急剧下降。基于这样的观察，我们所提出的 STLS 模块的最佳深度被选择为 5。同时，根据我们的模型的学习曲线，在图 4(B)中画出了最佳的 5 个变压器层，其中最佳模型是在第 67个历元。

### 引文

- [2]提出了一种混合的CNN-LSTM体系结构，其中CNN从原始频谱图中提取特征序列，而LSTM聚合长期的特征依赖关系。

- MHA机制再次被[4]采用来构建DRN-注意体系结构，其中DRN使网络在特征学习中保持高分辨率的时间结构。

- 文献[5]提出了基于一维和二维CNN-LSTM的SER系统，并表明2D CNN LSTM网络通过从谱图中捕获局部相关性和全局上下文信息而优于一维 CNN LSTM网络。

- 后来在注意机制[6]的启发下，在[7]中提出了RNN-注意，RNN通过关注情绪显著的特征来提取时间特征，并聚合长期特征依赖关系。

- 文[8]测试了几种时态建模方法，目的是从原始波形中学习深层情感特征。

- 在文献[9]中，作者引入了CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。

- 文献[10]还提出了语境LSTM注意来考虑周围话语之间的关系和依存关系。

- 基于STL的声学模型[11]在Librispeech基准上给出了最好的声学模型。

- 在问题回答领域，stacked latent attention 和multihop attention networks (MAN)[12]都显示出了显著的性能改进.

- 对于图像捕获任务，从 stacked cross attention network (SCAN)[13]或stacked attention modules [14]获得最先进的结果。

- vanilla Transformer [15]

- 我们的 CNN-BiLSTM 模块的详细信息如图2所示，类似于我们之前的研究[17]。

- 类似的架构被用于多通道情感识别[18]。

- DSCNN [21]

- IAAN [22] 

- 通过使用 t-SNE 技术可视化中间层发现的特征质量来检查STL的有效性[24]。

## 摘录

---
title: "A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers"
description: ""
citekey: wangNovelEndtoendSpeech2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:31
lastmod: 2023-04-11 17:53:09
---

> [!info] 论文信息
>1. Title：A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers
>2. Author：Xianfeng Wang, Min Wang, Wenbo Qi, Wanqi Su, Xiangqian Wang, Huan Zhou
>3. Entry：[Zotero link](zotero://select/items/@wangNovelEndtoendSpeech2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Wang et al_2021_A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/HW-AARC-CLUB/ICASSP_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 在现有语音情绪识别(SER)模型基础上增加 stacked transformer layers (STL)结构构建新的SER体系结构, 利用STLS模块强大的特征学习能力. 
- 使用 ==t-SNE== 技术可视化中间过渡层发现的特征质量来检查 STL 模型的有效性.

## 摘要

> [!abstract] Speech emotion recognition (SER) aims to automatically recognize emotional category for a given speech utterance. The performance of a SER system heavily relies on the effectiveness of global representation expressed at utterance level. To effectively extract such a global feature, the mainstream of recent SER architectures adopts a pipeline with two key modules, feature extraction and aggregation. Although variant module designs have brought impressive progresses, SER is still a challenging task. In contrast with those previous works, herein we propose a novel strategy for global SER feature extraction by applying an additional enhancement module on top of the current SER pipeline. To verify its effect, an end-to-end SER architecture is proposed where stacked multiple transformer layers are explored to enhance the aggregated global feature. Such an architecture is evaluated on IEMO-CAP and results strongly substantiate the effectiveness of our proposal. In terms of weighted accuracy on four emotion categories, our proposed SER system outperforms the prior arts by a large margin of relatively 20% improvement. Our codes and the pre-trained SER models are made publicly available.

> 语音情感识别(SER)的目的是自动识别给定语音话语的情感类别。SER系统的性能在很大程度上依赖于在话语层面上表达的全局表征的有效性。为了有效地提取这样的全局特征，目前主流的SER体系结构采用了一种包含两个关键模块的流水线，即特征提取和聚合。尽管各种各样的模块设计已经带来了令人印象深刻的进步，但SER仍然是一项具有挑战性的任务。与前人的工作不同，本文提出了一种新的全局SER特征提取策略，在现有SER流水线的基础上增加了一个增强模块。为了验证其效果，提出了一种端到端的SER结构，其中利用堆叠的多个转换器层来增强聚集的全局特征。这种体系结构在IEMOCAP上进行了评估，结果有力地证明了我们的建议的有效性。在四个情感类别的加权准确率方面，我们提出的SER系统比现有技术有较大幅度的提高20%。我们的代码和预先培训的SER模型是公开提供的。

## 预处理

## 概述

## 结果

EMOCAP 包含超过10K 个话语，由九个情感类别的标签标注。按照前人的研究步骤，我们使用了一个四类({高兴、愤怒、悲伤、中性})的子库，总共有5531个话语。对于每个情感类别，其关联样本分别按7/1/2的比例随机分为训练/均值/测试。

所提出的SER系统已在PyTorch中实现。利用Adam[20]优化器对分类交叉熵进行优化，同时监测验证精度，提前停止设置为8个历元。训练系统的批次为32，学习率为2×10−4，衰减率为1×10−6。

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220417162209.png)

## 精读

言语情感作为文本之外的一种 meta-information，对理解说话人的心理和反应起着重要的作用。相关研究被称为语音情感识别(SER)，其目的是自动识别给定语音表达的情感类别。由于情感通常以一种微妙和可变的方式传达，因此识别情感嵌入作为话语的表征一直是一个挑战，可以有效地对情感类别进行分类。

随着深度神经网络(DNN)的最新进展，情感嵌入已经从先前基于知识的手工制作的声学特征，例如低级描述符(LLD)，演变为基于 DNN 的深度情感特征。在最近的 SER 工作中，已经探索了各种 DNN 结构，如卷积神经网络(CNN)、长短期记忆(LSTM)、时延神经网络(TDNN)、残差网络(ResNet)、扩张残差网络(DRN)，它们本身或组合在一起[1-4]。在语义特征识别领域已经有了大量的研究成果，但由于情感与语言特征的分离，以及从长时间的话语中提取有效的情感特征，这方面的研究仍然具有挑战性。为了解决这些问题，现代 SER 系统普遍采用两个模块的流水线：1)特征提取模块，以生成情感相关的时间声学特征；以及 2)聚合模块，将这些时间特征汇集到话语级别的紧凑的全局语境表示(也称为情感嵌入)中。为了产生有效的情感嵌入，最近的 SER 工作集中在开发不同的模块架构上。例如，[2]提出了一种混合的 CNN-LSTM 体系结构，其中 CNN 从原始频谱图中提取特征序列，而 LSTM 聚合长期特征依赖关系。类似地，文献[5]提出了基于 1 D 和 2 D CNN-LSTM 的 SER 系统，并表明 2 D CNNLSTM 网络通过从谱图中捕捉局部相关性和全局上下文信息而优于 1 D 网络。后来在注意力机制的启发下，RNN 在[7]中提出了 RNN-注意，RNN 通过关注情绪显著的特征来提取时间特征和聚合长期特征依赖关系。文[8]测试了几种时间建模方法，目的是从原始波形中学习深层情感特征。在[9]中，作者引入了 CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。MHA 机制再次被[4]采用来构建 DRN-注意体系结构，其中 DRN 使网络在特征学习中保持高分辨率的时间结构。此外，文献[10]还提出了语境 LSM 注意来考虑周围话语之间的关系和依存关系。尽管在以前的工作中取得了进展，但目前的 SER 表现仍有改进的空间。例如，在基准数据集 IEMOCAP 上的 SOTA 分类准确率仅为 76%左右(在四个情感类别上)。同时，我们注意到，在其他研究领域，经常有报道称，用叠层变压器层(STL)取代重复性可以显著提高性能。例如，基于 STLS 的声学模型[11]在 Librispeech 基准上给出了最好的声学模型。在问答领域，堆积式潜在注意和多跳注意网络(MAN)[12]都表现出显著的性能改进。对于图像捕获任务，从堆叠的交叉注意网络(SCAN)[13]或堆叠的注意模块[14]获得最先进的(SOTA)结果。受其他研究领域的成功启发，在本研究中，我们有兴趣将 STL 机制应用到 SER 网络中。特别是，提出了一种通过在现有 SER 流水线上添加 STL 而构建的新的 SER 体系结构。我们建议的动机是利用 STLS 模块强大的特征学习能力来直接提高流水线输出。据作者所知，这是文献中第一次使用这种策略来研究 SER 问题。本文组织如下。在第二节中，简要介绍了香草变压器。在第三节中，我们详细地提出了 STL 增强的 SER 体系结构。第四节报道了实验。第五节总结了这项研究。


在本节中，我们将简要描述源自 Vanilla Transformer[15]的 STL 的详细信息，后者完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系。在这项研究中，我们感兴趣的是 ITS 编码器，它将输入序列转换为固定维度的全局上下文向量。具体地，编码器由六个相同的变压器层(TL)堆叠而成。每个 TL 包含具有剩余连接的两个子层：1)自关注子层和 2)位置前馈网络(FFN)子层。此外，还引入了位置编码(PE)来为模型提供显式的顺序信息。得益于自我注意机制，输出序列通过重视输入的特定部分来保留和利用输入信息。TL 的堆叠方式是，只有第一层采用输入顺序，每隔一层只采用前面的输出作为输入。这使得能够通过在潜在表示空间内的迭代关注和投影来丰富上下文向量。从数学上讲，《变形金刚》的体系结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程组(ODE)求解器。从这个角度来看，STL 的数量对应于 ODE 中的时间维度，这自然赋予 Transformer 以强大的学习能力来产生深层上下文表示。
度。



我们建议的新的 SER 框架将在这一部分介绍。图 1 概述了它的体系结构。如图所示，它依次包括 4 个模块：前端预处理模块，用于提取帧级别的 LLD；CNN-BiLSTM 模块，用于提取上下文表示；STLS，用于增强上下文表示；后端分类器，用于预测情感类别的概率。下面介绍 CNNBiLSTM 和 STLS 模块的设计细节。

CNN-BiLSTM Module

我们的 CNN-BiLSTM 模块的详细信息如图 2 所示，这类似于我们之前的研究[17]。简而言之，该模块由六对 CNN-Pooling 层和一个 BiLSTM 层构成。每个 CNN-Pooling 对包括一个卷积层(配有 batch 归一化和 ELU 函数激活)用于提取高层时间特征，以及一个池化层(具有固定丢失率)用于降低特征维.

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210042.png)

![]({10}_A%20Novel%20end-to-end%20Speech%20Emotion%20Recognition%20Network%20with%20Stacked%20Transformer%20Layers@wangNovelEndtoendSpeech2021.assets/image-20220808210255.png)

特别是，模块输入是基于帧级别的 LLD 声学特征，表示为 $X=\left\{x_{1}，x_{2}，\cdots，x_{t}\right\}\in$$\mathbb{R}^{\mathrm{t}\times\mathrm{d}}$ ，其中 $\mathrm{t}$ 表示帧长度， $\mathrm{d}$ 表示 LLD 特征尺寸。对于第一个 CNN-Pooling 对，是具有 $c h_{0}$ 个 Filter 的 2 D 卷积，应用在 $X$ 上，并产生时间特征序列，表示为 $H_{0}^{\text{CNN}}=$$\left\{h_{1}^{\mathrm{CNN}}，h_{2}^{\mathrm{CNN}}，\cdots，H_{\mathrm{t}}^{\mathrm{cnn}}\right\}\in \mathbb{R}^{\mathrm{t}\times\mathrm{h}_{0}\times\mathrm{w}_{0}}$ ；然后对每个输出进行大小为 $n_{a_p}*n_{a_p}$ 的 2 D-AveragePooling 操作以降维，得到的输出 $H_{0}^{\text {pool }}=$ $\left.\left\{h_{1}^{\text {pool }}, h_{2}^{\text {pool }}, \cdots, h_{\text {ch }_{0}}^{\text {pool }}\right\} \in \mathbb{R}^{\text {ch }_{0} \times \mathrm{h}_{0} / \mathrm{n}_{\mathrm{ap}} \times \mathrm{w}_{0} / \mathrm{n}_{\mathrm{ap}}}\right)$ 被馈送到下一个卷积对。通过总共 6 个 CNN-Pooling 对，，得到最终的潜在表示 $H_{5}^{\text{pool}}\in\mathbb{R}^{\mathrm{ch}_{5}\times\mathrm{h}_{5}\times\mathrm{w}_{5}}$ 。通过进一步的 reshape 操作，将 reshape 版本 $H^{r e}\in\mathbb{R}^{\mathrm{h}_{5}\times\left(\mathrm{ch}_{5}\times\mathrm{w}_{5}\right)}$ 随后馈送到 BiLSTM 层。

BiLSTM 向前和向后从 $H^{\text{re}}$ 学习上下文依赖关系。其输出为隐藏状态序列 $H^{\text {bilstm }}=\left\{h_{1}^{\text {bilstm }}, h_{2}^{\text {bilstm }}, \cdots, h_{h_{5}}^{\text {bilstm }}\right\}$ 表示上下文特征序列，其中 $h_{i}^{\text {bilstm }}=\left[\vec{h}^{\text {bilstm }}, \overleftarrow{h}^{\text {bilstm }}\right] \in$ $\mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ ，隐藏状态大小为 $\mathrm{d}_{\mathrm{lstm}}$ 。

在经典的 SER 任务中，采用了两种聚合方法，即 reucurrence-based 方法和 attention-based,方法来产生全局情感嵌入。
$$

X^{G}=\left\{\begin{array}{cl}

h_{\mathrm{h}_{5}}^{\text {bilstm }}, & \text { recurrence - based } \\

\sum_{i=1}^{\mathrm{h}_{5} \alpha_{\mathrm{i}} \mathrm{h}_{\mathrm{i}}^{\text {bilstm }},} & \text { attention }-\text { based }
\end{array}\right.
$$

在任何一种方式中，情感嵌入 XG(或受进一步仿射变换的约束)被馈送到后端分类器以进行概率预测。


作为我们的核心贡献，提出了一种产生情感嵌入的替代方法。它应用 additional STLs 来增强上下文特征序列 $H^{\text {bilstm }}$ 。

为此，首先执行1D 卷积以将 $H^{\text{bilstm}}$ 映射到向量 $Z_{0} \in \mathbb{R}^{2 \mathrm{~d}_{\text {stm }}}$ 。然后使用具有 L 层的 STLs 迭代增强
$Z_{0}$ 得到 $Z_{i}=G\left(Z_{i-1}\right)$ 。除了跳过 positional encoding 外，一系列操作都是在 $G$ 内执行的，例如多头注意、前馈和残差连接(如第 2 节所述)。最后一个 Transformer 层的输出是全局嵌入，即 $X^{G}=Z_{L-1}$ 。

最后，以端到端的方式对所提出的 SER 体系结构进行训练，目标函数如下： $$

\begin{gathered}

\hat{y}_{c}=\operatorname{softmax}\left(\mathrm{X}_{\mathrm{G}}^{\mathrm{T}} \mathrm{W}+\mathrm{b}\right), \\

L=-\log \prod_{i \in S} \sum_{c=1}^{C} \hat{y}_{i, c} \log \left(\hat{y}_{i, c}\right),

\end{gathered}

$$ ，其中 $S$ 表示用于训练的样本集， $C$ 表示情感类别的总数。

我们注意到，类似的架构被用于多通道情感识别[18]。我们的方法与以前的工作不同，因为 CMA 是对时间特征序列进行操作，这需要进一步的统计汇集；而在我们的例子中，STLS 应用于语音级表示。

3. STL 模块，用于增强上下文表示；

    STACKED Transformer:** 完全依赖注意力机制来捕获输入和输出序列之间的长期依赖关系，可以将输入序列转换为固定维度的上下文全局特征。它由六个相同的  Transformer Layers (TLs)堆叠而成，每个 Transformer Layer 包含两个附带  residual connections 的子层：self-attention 层和 position-wise feed-forward network(FFN) 子层。此外引入了 positional encoding (PE)来为模型提供显式的顺序信息。

    通过 self-attention 机制，对输入序列的特定部分加以重视, 来保留和利用输入序列信息。Transformer Layers 的堆叠，只有第一层接受输入序列，后面每隔一层只接受前一层的输出作为输入，这使得能够通过在潜在表征空间的迭代注意力机制和数据投影来丰富上下文全局特征。

    从数学上讲， Transformer 结构被解释为多粒子动力系统中对流扩散方程的数值常微分方程(ODE)求解器[16]。从这个角度来看，STLs 的数量对应于 ODE 中的时间维度，这自然使 Transformer 具有很强的学习能力来产生**深层上下文表示**。

4. 分类器，用于预测情感类别的概率。

**[t-SNE 可视化技术](https://zhuanlan.zhihu.com/p/148170862)**




我们进行了更多的调查，以评估我们拟议的 STL 模块的有效性。首先，研究了一种不带 STLS 模块的截断 SER 系统。由此产生的 WA 值暴跌至 72.79%。这项研究的意义是双重的：
1)我们的截断系统设计得很好，性能可以与现有技术相媲美；
2)STLS 模块对系统整体性能的贡献很大(18.5%)，代价是合理增加参数数量(从 356 K 增加到 468 K)。
其次，通过使用 t-SNE 技术可视化中间层发现的特征质量来检查 STLS 的有效性[24]。
两个潜在特征分布 Z 0 和 Z 4 在图 3 中用关于类的 2 D 曲线图进行了比较和说明。同样，可以观察到，STL 之后的特征被清楚地投影到不同的集群，并且彼此很好地分开。
最后，进一步研究了 STL 的最佳层数。实验结果如图 4(A)所示，有趣的是，系统性能在开始时(&lt;5 层)迅速提高，在中间(5-10 层)几乎保持不变，然后在结束时(&gt;10 层)急剧下降。基于这样的观察，我们所提出的 STLS 模块的最佳深度被选择为 5。同时，根据我们的模型的学习曲线，在图 4(B)中画出了最佳的 5 个变压器层，其中最佳模型是在第 67个历元。

### 引文

- [2]提出了一种混合的CNN-LSTM体系结构，其中CNN从原始频谱图中提取特征序列，而LSTM聚合长期的特征依赖关系。

- MHA机制再次被[4]采用来构建DRN-注意体系结构，其中DRN使网络在特征学习中保持高分辨率的时间结构。

- 文献[5]提出了基于一维和二维CNN-LSTM的SER系统，并表明2D CNN LSTM网络通过从谱图中捕获局部相关性和全局上下文信息而优于一维 CNN LSTM网络。

- 后来在注意机制[6]的启发下，在[7]中提出了RNN-注意，RNN通过关注情绪显著的特征来提取时间特征，并聚合长期特征依赖关系。

- 文[8]测试了几种时态建模方法，目的是从原始波形中学习深层情感特征。

- 在文献[9]中，作者引入了CNN-BLSTM-注意结构，将注意力扩展到多头注意(MHA)，以探索不同位置的不同代表子空间。

- 文献[10]还提出了语境LSTM注意来考虑周围话语之间的关系和依存关系。

- 基于STL的声学模型[11]在Librispeech基准上给出了最好的声学模型。

- 在问题回答领域，stacked latent attention 和multihop attention networks (MAN)[12]都显示出了显著的性能改进.

- 对于图像捕获任务，从 stacked cross attention network (SCAN)[13]或stacked attention modules [14]获得最先进的结果。

- vanilla Transformer [15]

- 我们的 CNN-BiLSTM 模块的详细信息如图2所示，类似于我们之前的研究[17]。

- 类似的架构被用于多通道情感识别[18]。

- DSCNN [21]

- IAAN [22] 

- 通过使用 t-SNE 技术可视化中间层发现的特征质量来检查STL的有效性[24]。

---
title: "Contrastive Unsupervised Learning for Speech Emotion Recognition"
description: ""
citekey: liContrastiveUnsupervisedLearning2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:41
lastmod: 2023-04-11 11:09:01
---

> [!info] 论文信息
>1. Title：Contrastive Unsupervised Learning for Speech Emotion Recognition
>2. Author：Mao Li, Bo Yang, Joshua Levy, Andreas Stolcke, Viktor Rozgic, Spyros Matsoukas, Constantinos Papayiannis, Daniel Bone, Chao Wang
>3. Entry：[Zotero link](zotero://select/items/@liContrastiveUnsupervisedLearning2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2021_Contrastive Unsupervised Learning for Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 为了提升情感识别效果，使用 Contrastive Predictive Coding(CPC) 无监督学习方法，预先在无标签的大型语音数据库训练一个特征提取器，最终改善了小数据量问题。
- 遇见了两个新的损失函数：infoNCE 损失函数，源自于 CPC 无监督方法；concordance correlation coefficient(CCC), 基于一致性相关系数的损失函数,测量两个随机变量的对齐度（相关程度）。

## 摘要

> [!abstract] Speech emotion recognition (SER) is a key technology to enable more natural human-machine communication. However, SER has long suffered from a lack of public large-scale labeled datasets. To circumvent this problem, we investigate how unsupervised representation learning on unlabeled datasets can benefit SER. We show that the contrastive predictive coding (CPC) method can learn salient representations from unlabeled datasets, which improves emotion recognition performance. In our experiments, this method achieved state-of-the-art concordance correlation coefficient (CCC) performance for all emotion primitives (activation, valence, and dominance) on IEMOCAP. Additionally, on the MSP-Podcast dataset, our method obtained considerable performance improvements compared to baselines.

> 语音情绪识别（SER）是实现更自然的人机通信的关键技术。但是，SER 长期以来一直缺乏公共的大规模标签数据集。为了避免这个问题，我们研究了未标记数据集上的无监督表示学习如何使 SER 受益。我们表明，对比预测编码（CPC）方法可以从未标记的数据集中学习显着表示，从而提高情绪识别性能。在我们的实验中，该方法在 IEMOCAP 上实现了所有情绪原语（激活，效价和优势）的最新一致性相关系数（CCC）性能。此外，在 MSPPodcast 数据集上，与基线相比，我们的方法获得了相当大的性能改进。

## 预处理

## 概述

## 结果

使用系数

![]({11}_Contrastive%20Unsupervised%20Learning%20for%20Speech%20Emotion%20Recognition@liContrastiveUnsupervisedLearning2021.assets/image-20220417162513.png)

IEMOCAP

![]({11}_Contrastive%20Unsupervised%20Learning%20for%20Speech%20Emotion%20Recognition@liContrastiveUnsupervisedLearning2021.assets/image-20220417162339.png)

## 精读

无监督学习方法提出背景在于，虽然深度学习(dl)方法在多个领域能实现最先进的结果，但这些方法往往是需要大量数据支撑的。

维度情绪指标通常包括激活(又名激发，非常平静或非常活跃) ，效价(积极或消极水平)和支配(非常弱或非常强)。

**基于CPC的特征提取**

Contrastive predictive coding（对比预测编码），属于对比学习范畴，通过损失函数构建和分离正反例，具体实现步骤如下。

对于在上述设置中使用的 cpc 模型，我们使用一个带有步长[5,4,4,2] ，滤波器尺寸[10,8,8,4]和128个隐藏单元的带有关联激活的4层 cnn 来编码16khz 音频波形输入。

采用一个具有256个隐含维数的单向门控递归单元(gru)网络作为自回归模型。对于 gru 的每个输出，我们预测未来的12个时间步，使用50个负样本，从同一序列中取样，在每个预测中。

我们用10s固定长度的话语训练 cpc 模型。较长的话语在10s时被切断，较短的话语通过重复来填充。

在情感识别器中，使用了一个具有512维隐藏状态的8头注意层。注意层的输出与输入维数相同。两个完全相连的层有128个隐藏单元。退出的可能性被设置为0.2的辍学层。

我们的模型在 pytorch 上实现，所有的方法都在8个 gpu 上进行，每个 gpu 的小批量大小为8个样本进行 cpc 预训练。我们使用 adam 优化器，重量衰减0.00001，学习率0.0002。我们使用50个时代的训练和保存模型，执行最好的验证集测试。

为了评估 iemocap 数据集，我们配置了5个交叉验证来评估模型。所有的实验都进行了五次，得到了平均值和标准差。

1. 在 LibriSpeech dataset 预训练 CPC模型，使用非线性编码器 $f$ 将语音序列 $x_{t}$ 映射到隐式表征 $z_{t}$。
2. 使用自回归模型 $g$ 将过去时间上连续的隐层表征转化为上下文相关的特征 $c_{t}$，上下文相关意味着包含了过去的信息，亦即 $c_{t}$ 能够通过未来的观察值 $x_{t+k}$ 预测 $z_{t+k}$。$$\hat{z}_{t+k}=h_{k}\left(c_{t}\right)=h_{k}\left(g\left(z_{\leq t}\right)\right)$$
3. 从同一序列或其他序列中随机抽取样本（即其他观察值 $x$ ），计算其隐式表征 $z_{t}$，以形成对比问题。假设每个语境的 N个样本中，含有随机抽样的 N-1 个 negative 样本和一个positive 样本。
4. 利用 infoNCE 损失函数 $\mathcal{L}$( $\tau$ 是特征聚集程度的比例因子，$k$ 是预测时间上限，$i$ 为随机抽取的 negative 样本)，学习区分 negative 样本和 positive 样本，归结为一个 N 分类问题。$\mathcal{L}=-\sum_{m=1}^{k}\left[\log \frac{\exp \left(\hat{z}_{t+m}^{\top} z_{t+m}\right) / \tau}{\exp \left(\hat{z}_{t+m}^{\top} z_{t+m}\right) / \tau+\sum_{i=1}^{N-1} \exp \left(\hat{z}_{t+m}^{\top} z_{i}\right) / \tau}\right]$

显然，在不同的音频段和时间步骤中，损失是累加的，因此在训练中，损失通常是针对批量的音频段和这些段中所有可能的时间步骤计算的，以利用基于小批量的 adam 优化器。优化结果在隐层表征和其预测对应之间产生更大的内积，比任何 negative 样本匹配的隐层表征和预测。优化目标函数的理论证明可以在[11]和[18]中找到。

基于注意力机制的情感识别

由于话语的某些部分往往在情感上比其他部分更加突出，因此我们采用一种自我注意机制来关注这些时段，以利用相关特征，进行句子级别的嵌入。具体步骤：将CPC输出的 $C^{L*D_{c}}$ 作为输入，使用多头 dot-product 注意力机制，得到多方面考虑上下文信息权重的$H_{j}^{L × D_{attn}}$ 特征, 并经过简单的串联和线性变换得到$U^{L × D_{u}} = Conat(H^{1},H^{2},\cdot\cdot\cdot,H^{n}) W_{o}^{nD_{attn}×D_{u}}$作为序列特征。

沿着时间维度计算 $U$ 的平均值和标准差，并将它们连接成序列表示，然后经过两个全连接层（Relu+dropout）以及一个隐藏层单元为最终输出数量的全连接层，得到最终输出结果。

最终使用损失函数更新参数，其中$\rho = \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}}$ 是皮尔森相关系数，用来反映两个变量X(模型预测)和Y(数据标签)的线性相关程度；$μ$ 和 $σ$是均值和标准差; 

$\mathcal{L}=1-\alpha \mathrm{CCC}_{a c t}-\beta \mathrm{CCC}_{v a l}-\gamma \mathrm{CCC}_{d o m}$

$\mathrm{CCC}(X, Y)=\rho \frac{2 \sigma_{X} \sigma_{Y}}{\sigma_{X}^{2}+\sigma_{Y}^{2}+\left(\mu_{X}-\mu_{Y}\right)^{2}}$

实验设置

1. 基线方法：使用MFCC40维特征的单独的受监督模型
2. 采用端到端的方式，从原始音频中训练 CPC 模型直接学习特征和情感识别器。加入基线方法，采用手工特征进行监控任务，以测试当特征提取部分知道下游任务时，是否有可能学到更好的特征。
3. minicpc 在相同的数据集上分两个阶段训练 cpc 模型和情感识别器。
4. 在 LibriSpeech dataset上预训练CPC，在 MSP-Podcast 和 IEMOCAP上训练含CPC的情感识别器。

### 引文

- 虽然利用非监督式学习对 ser 的研究相对较少，但之前使用自动编码器的尝试已经成功[12,13]。

- 最近，有研究表明，学习在一个时间序列中预测未来的信息是一种有用的训练前机制[14]。

- 例如，对比预测编码(cpc)[11]能够从顺序数据中提取有用的表示，并在各种任务中取得竞争性的表现，包括语音中的电话和说话人分类。

- 一般来说，有两种广泛使用的方法来表示情绪: 通过情绪类别(快乐、悲伤、愤怒等)或者通过维度情绪度量(又名情绪原语)[3,4,15]。

- 由于情绪表征是一个活跃的研究课题，我们建议感兴趣的读者参阅[15,16]。

## 摘录

---
title: "Compact Graph Architecture for Speech Emotion Recognition"
description: ""
citekey: shirianCompactGraphArchitecture 2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:29:50
lastmod: 2023-04-11 11:10:26
---

> [!info] 论文信息
>1. Title：Compact Graph Architecture for Speech Emotion Recognition
>2. Author：Amir Shirian, Tanaya Guha
>3. Entry：[Zotero link](zotero://select/items/@shirianCompactGraphArchitecture2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Shirian_Guha_2021_Compact Graph Architecture for Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/AmirSh 15/Compact_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 第一个将图分类方法用于 SER 的工作。
- 利用图信号处理的理论，提出了一种基于 GCN 的图分类方法，该方法可以有效地执行精确的图卷积。
- 模型可训练参数显著减少(仅为 30 K)。
- IEMOCAP 自带 spontaneity 特征- 第一个将图分类方法用于 SER 的工作。
- 利用图信号处理的理论，提出了一种基于 GCN 的图分类方法，该方法可以有效地执行精确的图卷积。
- 模型可训练参数显著减少(仅为 30 K)。
- IEMOCAP 自带 spontaneity 特征

思考：
- 图结构可以捕捉语音情感的动态特性吗？

## 摘要

> [!abstract] We propose a deep graph approach to address the task of speech emotion recognition. A compact, efficient and scalable way to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a Graph Convolution Network (GCN)-based architecture that can perform an accurate graph convolution in contrast to the approximate convolution used in standard GCNs. We evaluated the performance of our model for speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves comparable performance to the state-of-the-art with significantly fewer learnable parameters ( 30 K) indicating its applicability in resource-constrained devices. Our code is available at /github.com/AmirSh 15/Compact_SER.

> 我们提出了一种深度图方法来解决语音情感识别问题。以图表的形式表示数据是一种紧凑、高效和可伸缩的方式。遵循图信号处理的理论，我们提出将语音信号建模为循环图或折线图。这种图结构使我们能够构建一个基于图卷积网络(GCN)的体系结构，与标准 GCN 中使用的近似卷积相比，它可以执行精确的图卷积。我们在流行的 IEMOCAP 和 MSP-Improv 数据库上对我们的语音情感识别模型的性能进行了评估。我们的模型的性能优于标准 GCN 和其他相关的深图体系结构，表明了我们方法的有效性。与现有的语音情感识别方法相比，我们的模型在可学习参数(∼30 K)显著减少的情况下获得了与最先进水平相当的性能，表明其在资源受限的设备中的适用性。

## 预处理

## 概述

## 结果

MSP-IMPROV

![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220417162601.png)

## 精读

大部分 SER 方法都是遵循两种处理过程，首先从原始音频中提取一组特征，然后将特征输入到深度学习模型中，以生成离散(或连续)的情感标签[1,2,3,4]。在 SER 领域，模型训练常将 LLDs(Low-Level descriptor，手工设计的一些低水平特征)，lexical 特征[5,6]和 Log Mel spectrograms 作为输入[7]。其中 spectrogram 通常与卷积神经网络(CNNs)一起使用[7]，但 CNNs 无法捕捉语音动态变化，而这种时间动态在 SER 中具有重要意义，因为它反映了情绪的动态变化。为了捕捉情绪的动态变化，Recurrent 模型，特别是长短时记忆网络(LSTMs)[2,3,4]，在 SER 中占据了主导地位，但其常常产生具有数百万可训练参数的复杂体系结构。目前，以图的形式表示数据，是一种紧凑、高效和可扩展的方式，并且 GCNs 目前已经在部分领域中实现应用，因此本文将 SER 问题归结为一个图分类问题，提出采用深度图的方法来进行 SER。

本文的工作基于 spectral GCNS，它在图形信号处理方面有很好的应用[14]。考虑到卷积核(对角线矩阵)是可学习的，spectral GCNs 在图的拉普拉斯矩阵的谱上执行卷积运算[15]。这涉及到图的拉普拉斯矩阵的特征分解，这在计算上是昂贵的。为了减少计算量，ChebNet 用 Chebyshev 多项式逼近卷积运算(包括可学习的卷积核)[16]。最流行的 GCN 形式使用切比雪夫多项式的一阶近似，将卷积运算进一步简化为线性投影[9]。这样的 GCN 模型易于实现，并已成功地用于社交媒体网络和引文网络中的各种节点分类任务[9]。

我们将语音信号建模为一个简单的图，由于这种特殊的图结构，我们利用图信号处理[17]中的某些结果来执行精确的图形卷积(与流行的 GCN 中使用的 approximations 不同)，提出了一个轻量级的 GCN 架构，在 IEMOCAP[18]和 MSP-Improv[19]数据库上具有优异的情感识别性能。

**图结构**

首先，从每个语音样本中构造对应的图结构 G=(V,E)，其中 $V$ 是 $M$ 个节点{ $v_{i}$ }的集合， $E$ 是节点之间所有边的集合。 $G$ 的邻接矩阵表示为 $A$ ，其中元素 ( $A_{ij}$ ) 表示连接{ $v_{i}$ }和{ $v_{j}$ }的边界权值，权值为零时表示没有边连接两点，其中邻接矩阵 $A$ 的对角线元素为0。

![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220304010332.png)

1. **语音图节点的构造策略**遵循一个简单的帧到节点转换，每个节点对应于语音信号的一个 short windowed segment。即 $M$ 帧语音信号(短的、重叠的片段)构成 $G$ 中的 $M$ 个节点。

2. **构建语音信号图结构**，我们研究两个无向图结构:(1)由邻接矩阵 $A_{c}$ 定义的圆形图结构和(2)由邻接矩阵 $A_{l}$ 定义的线形图结构。![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220304010347.png)
    由上图可知，每个节点仅连接到两个相邻节点，从而可以将信号转换为折线图或循环图。并且这两种图结构的图拉普拉斯算子具有特殊的结构，可以极大地简化 spectral GCN 计算。

3. **关联节点特征**，每个节点{ $v_i$ }与节点特征向量{ $x_{i} \in R^P$ }相关联。一个节点特征向量包含了从对应的语音片段中提取的 LLDs。特一个特征矩阵 $X=[x_1，···x_M] \in R^{M×P}$ ，包含了所有节点的特征向量。

**图分类**

设计 GCN 架构，给定一组由语音转换来的图 { $G_1，…，G_N$ }和对应的真实标签 { $y_1，…y_{N}$ }，能够识别语音情感。本文架构包括两个图卷积层；一个池化层，生成图嵌入向量；一个全连接层，生成离散的情感标签。

![]({12}_Compact%20 Graph%20 Architecture%20 for%20 Speech%20 Emotion%20 Recognition@shirianCompactGraphArchitecture 2021.assets/image-20220304010243.png)

1. **图卷积层**。本文模型基于 spectral GCN，在谱域中执行图卷积操作[14]： $h=x_i∗w$ ，其中 $w$ 为图卷积核(可学习)， $x_i$ 为输入节点特征，其等价于图谱域中的乘积： $\hat{\mathbf{h}}=\hat{\mathbf{x}}_{i} \odot \hat{\mathbf{w}}$ ，其中 $\hat{\mathbf{h}}$ 、 $\hat{\mathbf{x_i}}$ 和 $\hat{\mathbf{w}}$ 表示输出、节点特征和卷积滤波器经由图傅里叶变换(GFT)的谱域表示。由此扩展到输入特征矩阵，便可得到矩阵表示法： $\hat{\mathbf{H}}=\hat{\mathbf{X}} \hat{\mathbf{W}}$ 。

    为了得到 $\hat{\mathbf{X}}$ 和 $\hat{\mathbf{W}}$ ，我们通常计算归一化的图拉普拉斯矩阵 $\mathcal{L}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L D}^{-\frac{1}{2}}$ ，其中 $\mathbf{D}$ 为度矩阵(记录每个节点邻点数量的对角矩阵)， $\mathbf{L}=\mathbf{D}−\mathbf{A}$ 为原拉普拉斯矩阵， $\mathbf{A}$ 为图的邻接矩阵。

    又因 $\mathbf{L}$ 的特征分解可以写成 $\mathcal{L}=\mathbf{U} \Lambda \mathbf{U}^{T}=\sum_{i=1}^{M} \lambda_{i} \mathbf{u}_{i} \mathbf{u}_{i}{ }^{T}$ 形式，其中 $\lambda_{i}$ 是 L 的第 i 个特征值， $\mathbf{u}_{i}$ 是对应于 $\lambda_{i}$ 的特征向量， $Λ=diag(\lambda_{i})$ 为对角矩阵， $\mathbf{U}=[\mathbf{u}_1,\mathbf{u}_{2,}\cdots , \mathbf{u}_N]$ ，可得精确的图卷积运算公式： $$\begin{aligned}&\hat{\mathbf{H}}=\hat{\mathbf{X}} \hat{\mathbf{W}}= \left(\mathbf{U}^{T} \mathbf{X}\right)\left(\mathbf{U}^{T} \mathbf{W}\right) \\ &\mathbf{H}=\mathbf{U} \hat{\mathbf{H}}\end{aligned}$$
    如果进行多次卷积操作，则可扩展得到第 k 层的图卷积公式： $$\mathbf{H}^{k+1}=\mathbf{U} \left(\mathbf{U}^{T} \mathbf{H}^{k}\right)\left(\mathbf{U}^{T} \mathbf{W}^{k}\right)$$ ，其中 $\mathbf{H}^{0}=\mathbf{X}$ ，且 $\mathbf{W}$ 是可学习的。

    特殊的，若令 $A=A_c$ (循环图)，可得 $L$ 的形式为： $$\mathbf{L}=\left[\begin{array}{ccccc}2 & -1 & 0 & \cdots & -1 \\-1 & 2 & -1 & \cdots & 0 \\0 & -1 & 2 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ -1 & 0 & \cdots & -1 & 2 \end{array}\right]$$ 在此情况下，GFT 等价于离散傅里叶变换(DFT)[17]。类似地，对于 $A=A_l$ (线形图)，GFT 等价于离散余弦变换(DCT)。这使得卷积操作更加方便且计算效率高，因为我们可以避免对于任意图结构而言计算量大的特征分解。

    根据最近一篇关于 GCN[20]的工作，可知公式中的卷积核 $\left(\mathbf{U}^{T} \mathbf{W}^{k}\right)$ 能通过多层感知器(MLP)来学习得到，最后得到以下形式的卷积运算 $$\mathbf{H}^{(k+1)}=\mathbf{U}\left(\operatorname{MLP}\left(\mathbf{U}^{T} \mathbf{H}^{(k)}\right)\right)$$ 其中，只有 MLP 参数是可学习的。

2. **汇聚层**。我们的目标是对整个图结构进行分类(与一般图节点分类任务相反)。因此，我们需要一个函数来从节点的嵌入向量中获得图级表征 $h_G \in R^Q$ 。

    通过在将所有节点卷积运算得到的嵌入向量矩阵 $H_(k)$ 传递到分类层之前，在最后一层通过汇聚层池化，可以获得 $h_G \in R^Q$ 。图域中常用的池化方法有 mean, max 和 sum[9,21]三种，但 Max 和 Mean 两种方法通常不能保存有关图结构的基本信息，而 Sum 池已被证明是很好的选择[20]。因此本文中使用 Sum 方法来获得图表示（对每一维度的所有节点的嵌入特征求和）： $$\mathbf{h}_{G}=\operatorname{sumpool}\left(\mathbf{H}^{(K)}\right)=\sum_{i=1}^{M} \mathbf{h}_{i}^{(K)}$$

    池化层之后是一个全连接的层，用来生成分类标签，最后用交叉熵损失函数 loss = $=-\sum_{n} \mathbf{y}_{n} \log \tilde{\mathbf{y}}_{n}$ 训练。

**实验结果和分析**
我们使用 OpenSMILE 工具包[28]从为 InterSpeech 2009 情感挑战[27]建议的语音话语中提取一组低层表征(LLD)构建特征集，其中包括 Mel 频率倒谱系数(MFCCs)、过零率、语音概率、基频(F 0)和帧能量。对于每个样本，我们使用一个长度为 25 ms(步长为 10 ms)的滑动窗口来局部提取 LLDs。然后使用移动平均滤波器对每个特征进行平滑，并且使用平滑后的特征来计算它们各自的一阶 delta 系数。此外，我们还为 IEMOCAP 添加了 spontaneity 作为 binary feature，因为这一特征有助于 SER[29]，spontaneity 数据随数据库一起提供。综上，在 IEMOCAP 数据库生成了维度 P=35 的节点特征向量，对于 MSPIMPROV，生成了维度 P=34 的节点特征向量(无 spo
ntaneity 特征)。

### 引文

- 图卷积网络(GCNs)[9]已成功地用于解决计算机 vi 快速流和自然语言处理中的各种问题，如动作识别[10]、对象跟踪[11]和文本分类[12]。

- 在 con Fast streams 文本音频分析中，我们知道只有一个最近的工作提出了一个基于注意的图神经网络架构，用于少量音频分类[13]。
---
title: "Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation"
description: ""
citekey: xuSpeechEmotionRecognition2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:30:19
lastmod: 2023-04-11 11:16:13
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation
>2. Author：Mingke Xu, Fan Zhang, Xiaodong Cui, Wei Zhang
>3. Entry：[Zotero link](zotero://select/items/@xuSpeechEmotionRecognition2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xu et al_2021_Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/lessonxmk/Optimized_attention_for_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 第一次引入多尺度区域注意力机制，从不同尺度处理需要 attention 数据
- 引入nlpaug library中的Vocal Tract Length Perturbation (VTLP)，利用VTLP，在语音特征层面上施加一个随机的扭曲因子，对原始数据进行处理，生成新的数据。
- 使用长方形卷积核，从时间和频域维度提取特征

思考：
- 不同注意力机制总结
	- 通道注意力
	- 空间注意力
	- 残差注意力
	- 混合注意力
	- 双重注意力
	- 自注意力
	- 类别注意力
	- 时间注意力
	- 频率注意力
	- 全局注意力
	- 高阶注意力

> [注意力机制研究现状综述（Attention mechanism） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/361893386)
>[一文看尽深度学习中的各种注意力机制 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/379501097)
>[14 - 第五节 Transformer （2022：各式各样的自注意力机制变型） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/488632024)

## 摘要

> [!abstract] In Speech Emotion Recognition (SER), emotional characteristics often appear in diverse forms of energy patterns in spectrograms. Typical attention neural network classifiers of SER are usually optimized on a fixed attention granularity. In this paper, we apply multiscale area attention in a deep convolutional neural network to attend emotional characteristics with varied granularities and therefore the classifier can benefit from an ensemble of attentions with different scales. To deal with data sparsity, we conduct data augmentation with vocal tract length perturbation (VTLP) to improve the generalization capability of the classifier. Experiments are carried out on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved 79.34% weighted accuracy (WA) and 77.54% unweighted accuracy (UA), which, to the best of our knowledge, is the state of the art on this dataset.

> 在语音情感识别(SER)中，情感特征往往以不同形式的能量模式出现在谱图中。SER中经典的基于注意力的神经网络分类器通常是在固定的注意力粒度（attention granularity）上进行优化的。本文将多尺度区域注意力机制（multiscale area attention）应用于深度卷积神经网络中，以适应不同粒度的情感特征，从而使分类器能够从不同尺度的注意集合中获益。针对数据稀疏性问题，采用声道长度扰动(VTLP)进行数据增强，提高了分类器的泛化能力。在(IEMOCAP)数据集上进行实验获得了79.34%的加权准确率(WA)和77.54%的未加权准确率(UA)。

## 预处理

## 概述

## 结果

IEMOCAP

![]({15}_Speech%20Emotion%20Recognition%20with%20Multiscale%20Area%20Attention%20and%20Data%20Augmentation@xuSpeechEmotionRecognition2021.assets/image-20220417181906.png)

原始数据和基于VTLP的扩展增强数据上最大区域大小的选择。
> 当在原始数据集上训练时，最大区域大小为4x4的模型获得了最高的ACC，其次是3x3。在扩充后的数据集上训练时，最大面积为3x3的模型获得了最高的ACC。在大多数情况下，增强数据的使用带来了超过0.5%的绝对精度提升。因此，我们建议使用3x3的最大区域大小，并使用VTLP进行数据增强。

VTLP下扩充数据量对SER性能的影响。

> 随着训练中加入更多扩充数据的副本，准确率提高

使用不同 area features特征的。
> Max、Mean和Sample三种选择, Max达到最大值,Mean和Sample三种选择最小值

## 精读

![]({15}_Speech%20Emotion%20Recognition%20with%20Multiscale%20Area%20Attention%20and%20Data%20Augmentation@xuSpeechEmotionRecognition2021.assets/image-20220417182945.png)

如上图所示，本文网络结构由5个卷积层、一个attention层和一个全连接层构成。本文使用Librosa[19]提取logMel谱图作为特征，并将其送入两个平行的卷积层中，分别从时间轴和频率轴提取纹理。其中每个卷积层之后都应用了 Batch normalization。

在这一部分中，我们扩展了李彦宏等人对这一领域的关注。[13]至SER。注意机制可以看作是一种软寻址操作，它使用key-value对来表示存储在存储器中的内容，元素由地址(key)和值(value)组成。query可以匹配到根据query和key之间的相关程度从存储器中检索到的对应value的key。Query、Key、Value通常先乘以一个参数矩阵W，得到Q、K、V。公式1表示注意力分数的计算，其中DK是K[20]的维度，以防止结果太大。

![]({15}_Speech%20Emotion%20Recognition%20with%20Multiscale%20Area%20Attention%20and%20Data%20Augmentation@xuSpeechEmotionRecognition2021.assets/image-20220607171540.png)

在self-attention中，query、key和value来自同一个输入X。通过计算自我注意，模型可以关注输入不同部分之间的联系。在SER中，情感特征的分布往往跨越更大的尺度，在语音情感识别中使用自我注意提高了准确率。

然而，在常规 attention 机制下，该模型只使用预设的粒度作为计算的基本单位，例如，单词用于词级翻译模型， 网格单元用于基于图像的模型等。然而，很难知道哪种粒度最适合复杂的任务。区域注意允许模型以多个尺度和粒度进行注意，并学习最合适的粒度。如图2所示，对于连续的存储块，可以创建多个区域以适应不同的粒度，例如1x2、2x1、2x2等。为了以区域为单位计算attention，我们需要定义区域的key和value。例如，我们可以将一个area的平均值定义为key，将一个area的sum定义为value，这样就可以用一种类似于普通注意力的方式来评估注意力。(公式1)。

对大的内存块的注意力进行详尽的评估在计算上可能是令人望而却步的。对调查区域设置最大长度和最大宽度。

使用nlpaug库中的声道长度扰动(VTLP)[14]作为数据增强的手段，生成了原始数据的另外7个副本[21]，增加 IEMOCAP 的数据量。然后数据集随机分为训练集(80%的数据)和测试集(20%的数据)进行5次交叉验证。每个话语被分成2秒的片段，片段之间有1秒(训练中)或1.6秒(测试中)的重叠。以同一话语所有片段的平均预测结果作为最终结果。

### 引文

- 2014年，由han等人提出了第一个基于深度学习的SER模型。[4]。
- 最近，为了同样的目的，M.Chen等人提出了自己的观点。[5]组合卷积神经网络(CNN)和长短期记忆(LSTM)；
- X.Wu等人。[6]用胶囊网络(CapsNet)取代了CNN；
- Y.Xu等人。[7]使用GRU(Gate Recurn Unit)从帧和语音级计算特征
- S.Parthasarathy[11]使用梯形网络将无监督辅助任务和预测情绪属性的主要任务结合起来。
- 最近，人们对基于注意力的SER模型感兴趣，以获得更高的精度[8，9，12]。
- Y.Li等人。[13]建议的区域注意力，允许模型同时计算多个粒度的注意力，这一想法在SER中尚未探索。
- 数据不足阻碍了SER的进展。数据扩充已成为自动语音识别(ASR)相关领域中增加训练数据的一种流行方法[14-17]。
- 然而，它并没有受到SER的广泛关注。它用于平衡情绪类别[18]，而不是增加训练数据的总量。

## 摘录

---
title: "Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation"
description: ""
citekey: xuSpeechEmotionRecognition2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:30:19
lastmod: 2023-04-11 11:16:13
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation
>2. Author：Mingke Xu, Fan Zhang, Xiaodong Cui, Wei Zhang
>3. Entry：[Zotero link](zotero://select/items/@xuSpeechEmotionRecognition2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xu et al_2021_Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/lessonxmk/Optimized_attention_for_SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 第一次引入多尺度区域注意力机制，从不同尺度处理需要 attention 数据
- 引入nlpaug library中的Vocal Tract Length Perturbation (VTLP)，利用VTLP，在语音特征层面上施加一个随机的扭曲因子，对原始数据进行处理，生成新的数据。
- 使用长方形卷积核，从时间和频域维度提取特征

思考：
- 不同注意力机制总结
	- 通道注意力
	- 空间注意力
	- 残差注意力
	- 混合注意力
	- 双重注意力
	- 自注意力
	- 类别注意力
	- 时间注意力
	- 频率注意力
	- 全局注意力
	- 高阶注意力

> [注意力机制研究现状综述（Attention mechanism） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/361893386)
>[一文看尽深度学习中的各种注意力机制 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/379501097)
>[14 - 第五节 Transformer （2022：各式各样的自注意力机制变型） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/488632024)

## 摘要

> [!abstract] In Speech Emotion Recognition (SER), emotional characteristics often appear in diverse forms of energy patterns in spectrograms. Typical attention neural network classifiers of SER are usually optimized on a fixed attention granularity. In this paper, we apply multiscale area attention in a deep convolutional neural network to attend emotional characteristics with varied granularities and therefore the classifier can benefit from an ensemble of attentions with different scales. To deal with data sparsity, we conduct data augmentation with vocal tract length perturbation (VTLP) to improve the generalization capability of the classifier. Experiments are carried out on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved 79.34% weighted accuracy (WA) and 77.54% unweighted accuracy (UA), which, to the best of our knowledge, is the state of the art on this dataset.

> 在语音情感识别(SER)中，情感特征往往以不同形式的能量模式出现在谱图中。SER中经典的基于注意力的神经网络分类器通常是在固定的注意力粒度（attention granularity）上进行优化的。本文将多尺度区域注意力机制（multiscale area attention）应用于深度卷积神经网络中，以适应不同粒度的情感特征，从而使分类器能够从不同尺度的注意集合中获益。针对数据稀疏性问题，采用声道长度扰动(VTLP)进行数据增强，提高了分类器的泛化能力。在(IEMOCAP)数据集上进行实验获得了79.34%的加权准确率(WA)和77.54%的未加权准确率(UA)。

## 预处理

## 概述

## 结果

IEMOCAP

![]({15}_Speech%20Emotion%20Recognition%20with%20Multiscale%20Area%20Attention%20and%20Data%20Augmentation@xuSpeechEmotionRecognition2021.assets/image-20220417181906.png)

原始数据和基于VTLP的扩展增强数据上最大区域大小的选择。
> 当在原始数据集上训练时，最大区域大小为4x4的模型获得了最高的ACC，其次是3x3。在扩充后的数据集上训练时，最大面积为3x3的模型获得了最高的ACC。在大多数情况下，增强数据的使用带来了超过0.5%的绝对精度提升。因此，我们建议使用3x3的最大区域大小，并使用VTLP进行数据增强。

VTLP下扩充数据量对SER性能的影响。

> 随着训练中加入更多扩充数据的副本，准确率提高

使用不同 area features特征的。
> Max、Mean和Sample三种选择, Max达到最大值,Mean和Sample三种选择最小值

## 精读

![]({15}_Speech%20Emotion%20Recognition%20with%20Multiscale%20Area%20Attention%20and%20Data%20Augmentation@xuSpeechEmotionRecognition2021.assets/image-20220417182945.png)

如上图所示，本文网络结构由5个卷积层、一个attention层和一个全连接层构成。本文使用Librosa[19]提取logMel谱图作为特征，并将其送入两个平行的卷积层中，分别从时间轴和频率轴提取纹理。其中每个卷积层之后都应用了 Batch normalization。

在这一部分中，我们扩展了李彦宏等人对这一领域的关注。[13]至SER。注意机制可以看作是一种软寻址操作，它使用key-value对来表示存储在存储器中的内容，元素由地址(key)和值(value)组成。query可以匹配到根据query和key之间的相关程度从存储器中检索到的对应value的key。Query、Key、Value通常先乘以一个参数矩阵W，得到Q、K、V。公式1表示注意力分数的计算，其中DK是K[20]的维度，以防止结果太大。

![]({15}_Speech%20Emotion%20Recognition%20with%20Multiscale%20Area%20Attention%20and%20Data%20Augmentation@xuSpeechEmotionRecognition2021.assets/image-20220607171540.png)

在self-attention中，query、key和value来自同一个输入X。通过计算自我注意，模型可以关注输入不同部分之间的联系。在SER中，情感特征的分布往往跨越更大的尺度，在语音情感识别中使用自我注意提高了准确率。

然而，在常规 attention 机制下，该模型只使用预设的粒度作为计算的基本单位，例如，单词用于词级翻译模型， 网格单元用于基于图像的模型等。然而，很难知道哪种粒度最适合复杂的任务。区域注意允许模型以多个尺度和粒度进行注意，并学习最合适的粒度。如图2所示，对于连续的存储块，可以创建多个区域以适应不同的粒度，例如1x2、2x1、2x2等。为了以区域为单位计算attention，我们需要定义区域的key和value。例如，我们可以将一个area的平均值定义为key，将一个area的sum定义为value，这样就可以用一种类似于普通注意力的方式来评估注意力。(公式1)。

对大的内存块的注意力进行详尽的评估在计算上可能是令人望而却步的。对调查区域设置最大长度和最大宽度。

使用nlpaug库中的声道长度扰动(VTLP)[14]作为数据增强的手段，生成了原始数据的另外7个副本[21]，增加 IEMOCAP 的数据量。然后数据集随机分为训练集(80%的数据)和测试集(20%的数据)进行5次交叉验证。每个话语被分成2秒的片段，片段之间有1秒(训练中)或1.6秒(测试中)的重叠。以同一话语所有片段的平均预测结果作为最终结果。

### 引文

- 2014年，由han等人提出了第一个基于深度学习的SER模型。[4]。
- 最近，为了同样的目的，M.Chen等人提出了自己的观点。[5]组合卷积神经网络(CNN)和长短期记忆(LSTM)；
- X.Wu等人。[6]用胶囊网络(CapsNet)取代了CNN；
- Y.Xu等人。[7]使用GRU(Gate Recurn Unit)从帧和语音级计算特征
- S.Parthasarathy[11]使用梯形网络将无监督辅助任务和预测情绪属性的主要任务结合起来。
- 最近，人们对基于注意力的SER模型感兴趣，以获得更高的精度[8，9，12]。
- Y.Li等人。[13]建议的区域注意力，允许模型同时计算多个粒度的注意力，这一想法在SER中尚未探索。
- 数据不足阻碍了SER的进展。数据扩充已成为自动语音识别(ASR)相关领域中增加训练数据的一种流行方法[14-17]。
- 然而，它并没有受到SER的广泛关注。它用于平衡情绪类别[18]，而不是增加训练数据的总量。
---
title: "Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention"
description: ""
citekey: pengEfficientSpeechEmotion2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:30:28
lastmod: 2023-04-11 11:18:25
---

> [!info] 论文信息
>1. Title：Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention
>2. Author：Zixuan Peng, Yu Lu, Shengfeng Pan, Yunfeng Liu
>3. Entry：[Zotero link](zotero://select/items/@pengEfficientSpeechEmotion2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Peng et al_2021_Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/julianyulu/icassp2021-mscnn-spu
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 引入多尺度卷积(MSCNN)，用不同大小的卷积核对某特征图进行卷积操作，得到新的大小不同的特征图，之后经过处理得到下一层的输入特征图。
- 提出了一个统计合并单元(SPU)，它由沿序列方向的三个平行的一维pooling组成：a)全局最大合并；b)全局平均合并；c)全局标准差合并。
- 使用Google的Speech-to-Text API获取文本识别结果输入进模型中，与输入语音实际文本的结果做对比。


思考：
- 不同卷积方式
	- 1×1 卷积（Bottleneck 结构）
	- 3 D 卷积
	- 
	- 扩张卷积
	- 转置卷积（反卷积）
	- 分组卷积（混洗分组卷积，逐点分组卷积）
	- 可分离卷积（空间可分卷积，深度可分卷积）
	- 平展卷积
	- 微步卷积
	- 空洞卷积（膨胀卷积）
	- 可变形卷积
	- 图卷积

> [A Comprehensive Introduction to Different Types of Convolutions in Deep Learning | by Kunlun Bai | Towards Data Science](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)
> [CNN中千奇百怪的卷积方式大汇总 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/29367273)
> [变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作。 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/28749411)
> [一文看尽深度学习中的20种卷积（附源码整理和论文解读） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/381839221)

## 摘要

> [!abstract] Emotion recognition from speech is a challenging task. Recent advances in deep learning have led bi-directional recurrent neural network (Bi-RNN) and attention mechanism as a standard method for speech emotion recognition, extracting and attending multi-modal features - audio and text, and then fusing them for downstream emotion classification tasks. In this paper, we propose a simple yet efficient neural network architecture to exploit both acoustic and lexical information from speech. The proposed framework using multi-scale convolutional layers (MSCNN) to obtain both audio and text hidden representations. Then, a statistical pooling unit (SPU) is used to further extract the features in each modality. Besides, an attention module can be built on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the performance. Extensive experiments show that the proposed model outperforms previous state-of-the-art methods on IEMOCAP dataset with four emotion categories (i.e., angry, happy, sad and neutral) in both weighted accuracy (WA) and unweighted accuracy (UA), with an improvement of 5.0% and 5.2% respectively under the ASR setting.

> 语音情感识别是一项具有挑战性的任务。深度学习的最新进展使双向递归神经网络(bi-RNN)和注意机制成为语音情感识别的标准方法，提取并处理音频和文本的多模式特征，然后将它们融合用于下游情感分类任务。在本文中，我们提出了一种简单而高效的神经网络结构，以利用语音中的声学信息和词汇信息。该框架使用多尺度卷积层(MSCNN)来同时获得音频和文本的隐藏表示。然后，使用统计汇集单元(SPU)进一步提取每个通道的特征。此外，还可以在MSCNNSPU(音频)和MSCNN(文本)的基础上构建注意力模块，以进一步提高性能。大量实验表明，该模型在IEMOCAP数据集上的加权准确率(WA)和未加权准确率(UA)上均优于已有的方法，在ASR环境下分别提高了5.0%和5.2%。

## 预处理

## 概述

## 结果

IEMOCAP

![]({16}_Efficient%20Speech%20Emotion%20Recognition%20Using%20Multi-Scale%20CNN%20and%20Attention@pengEfficientSpeechEmotion2021.assets/image-20220418090457.png)

## 精读

本文首先提出了一种简单的卷积神经网络(CNN)和 pooling-based 模型，称为带有 statistical pooling units 的 multi scale CNN(MSCNN-SPU)，该模型能够有效地同时学习语音和文本模式用于情感识别。此外，通过在 MSCNN-SPU 之上构建注意力模块，从而产生 MSCNN-SPU-ATT，可以进一步提高整体性能。

![]({16}_Efficient%20Speech%20Emotion%20Recognition%20Using%20Multi-Scale%20CNN%20and%20Attention@pengEfficientSpeechEmotion2021.assets/image-20220418092230.png)

使用一组具有不同kernel大小的filters来构建multiple CNN层，分别用于文本和音频这两个独立的路径。

具有ReLU激活的各种单层二维卷积[13]与文本和音频的输入特征并行应用。

设$\Omega^{A, T}=\left\{\left(s, d^{A, T}\right), s \in S^{A, T}\right\}$是音频通道(上标‘A’)和文本通道(上标‘T’)的kernel的集合，其中s是沿输入序列维度的kernel大小，dA和dT分别是mfcc和单词嵌入向量的维度。

$$

\begin{aligned}

&\mathrm{G}_{\alpha}^{\mathrm{MSCNN}}(\boldsymbol{x}, \theta)=\left\{y_{\alpha}^{A, T} \mid \alpha \in \Omega\right\}\\

&y_{\alpha=(s, d) \in \Omega}^{A, T}[i, j]=\sum_{m=\left\lfloor-\frac{s}{2}\right\rfloor}^{\left\lceil\frac{s}{2}\right\rceil} \sum_{n=\left\lfloor-\frac{d}{2}\right\rfloor}^{\left\lceil\frac{d}{2}\right\rceil} \boldsymbol{k}[m, n] \cdot \boldsymbol{x}[i-m, j-n]

\end{aligned}

$$

因此，我们提出了一个统计合并单元(SPU)，它由沿序列建模方向的三个平行的一维合并组成：a)全局最大合并；b)全局平均合并；c)全局标准差合并。

$$

\mathrm{E}=\left\{\mathrm{G}_{\gamma}^{\mathrm{SPU}}\left(\mathrm{G}_{\alpha}^{\mathrm{MSCNN}}(\boldsymbol{x})\right) \mid \alpha \in \Omega, \gamma \in\{\max , \text { avg, std }\}\right\}

$$

受[1，3]中注意机制概念的启发，我们提出了一种建立在Audio MSCNN-SPU和 Text-MSCNN 之上的双通道attention层。与以前的工作不同，我们将前者的输出视为上下文向量$\boldsymbol{e}_{\gamma}$(即来自音频分支的最大池、平均池、标准池特征向量)。根据来自max-pooling、avgpooling和std-pooling的输出，分别将加权系数$s_{k}^{\gamma}$计算自上下文向量$\boldsymbol{e}_{\gamma}$和来自文本MSCNN $\boldsymbol{h}_{k}$的第k个输出特征映射之间的乘积。由此得到的注意力向量S是通过用加权系数$s_{k}^{\gamma}$对$\boldsymbol{h}_{k}$加权得到的。

$$
\begin{gathered}

s_{k}^{\gamma}=\frac{\exp \left(\boldsymbol{e}_{\gamma}^{\mathrm{T}} \boldsymbol{h}_{k}\right)}{\sum_{k} \exp \left(\boldsymbol{e}_{\gamma}^{\mathrm{T}} \boldsymbol{h}_{k}\right)}, \text{ where } \gamma \in\{\text{max}, \text{avg}, \text{std} \}\\

\boldsymbol{S}^{\gamma}=\sum_{k} s_{k}^{\gamma} \boldsymbol{h}_{k} \\

\boldsymbol{S}=\operatorname{concat}\left(\boldsymbol{S}^{\mathrm{max}}, \boldsymbol{S}^{\mathrm{avg}}, \boldsymbol{S}^{\mathrm{std}}\right)

\end{gathered}

$$

此外，在说话人识别任务中，使用Kaldi语音识别工具包[18]，X向量嵌入[16]被用作从VoxCeleb数据集[17]上的预先训练的TDNN模型中提取的互补音频特征。

模型设置：
每个CNN层的filter数量被设置为128；在文本encoder中，SWEM-max和SWEMavg特征[14]是从单词嵌入中获得的，然后被附加到文本SPU的输出中；另一方面，将X-vector嵌入附加到audioSPU的输出中；Adam优化，学习率为0.0005；采用范数为1的梯度剪裁；dropout率为0.3；batch size为64。

其次，我们将我们提出的方法与其他多模式方法进行比较。一种直接的方法是为每种模态训练一个 LSTM 网络，然后从每个模式连接最后一个隐藏状态，如 MDRE[1]所示。学习对齐[2]采用 LSTM 网络对音频和文本的序列进行建模。然后，使用模型中的注意力执行音频和文本之间的软对齐。在 MHA[3]中，提出了一种所谓的多跳注意，使用一种模态的隐藏表示作为上下文向量并将注意方法应用于另一种模态，然后重复这种方案数次。

### 引文

- 近年来，基于深度学习的情感识别方法在情感识别中表现出了很好的性能[1，2，3]。
- [1]通过在文本和音频之间引入注意机制来融合学习特征。
- 在[2]中，注意力网络被用来学习语音和文本之间的对齐，而BiLSTM网络被用来对情感识别中的序列进行建模。
- 此外，文献[3]还提出了一种多跳关注法，选择文本数据的相关部分，然后关注音频特征，用于以后的分类目的。使用Audio-BRE(LSTM)。
- 研究人员已经证明了CNN在具有音频特征[7,8]和文本信息[9]的情绪分类中的有效性。
- 受[9]中使用的文本-CNN架构的激励
- 在[11]中，一种使用WordNet和词性标注的混合方法与标准音频特征相结合，然后使用支持向量机进行分类。
- 使用DNN，[12]从多分辨率CNN中提取文本特征，从BiLSTM中提取音频信息，并使用分类损失和验证损失的加权和来优化任务。
- 使用CNN进行情感识别任务的工作经常使用单层全局最大池或全局平均池，并在[7，8]中被证明是有效的。
- Swem向量，它是直接在学习的单词嵌入上连接各种池的结果[14]
- 使用300维GloVe[19]嵌入作为标记化记录的预训练单词嵌入。
- CNN+LSTM in [20]
- [21]中的TDNN+LSTM

## 摘录

---
title: "Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention"
description: ""
citekey: pengEfficientSpeechEmotion2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:30:28
lastmod: 2023-04-11 11:18:25
---

> [!info] 论文信息
>1. Title：Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention
>2. Author：Zixuan Peng, Yu Lu, Shengfeng Pan, Yunfeng Liu
>3. Entry：[Zotero link](zotero://select/items/@pengEfficientSpeechEmotion2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Peng et al_2021_Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/julianyulu/icassp2021-mscnn-spu
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 引入多尺度卷积(MSCNN)，用不同大小的卷积核对某特征图进行卷积操作，得到新的大小不同的特征图，之后经过处理得到下一层的输入特征图。
- 提出了一个统计合并单元(SPU)，它由沿序列方向的三个平行的一维pooling组成：a)全局最大合并；b)全局平均合并；c)全局标准差合并。
- 使用Google的Speech-to-Text API获取文本识别结果输入进模型中，与输入语音实际文本的结果做对比。


思考：
- 不同卷积方式
	- 1×1 卷积（Bottleneck 结构）
	- 3 D 卷积
	- 
	- 扩张卷积
	- 转置卷积（反卷积）
	- 分组卷积（混洗分组卷积，逐点分组卷积）
	- 可分离卷积（空间可分卷积，深度可分卷积）
	- 平展卷积
	- 微步卷积
	- 空洞卷积（膨胀卷积）
	- 可变形卷积
	- 图卷积

> [A Comprehensive Introduction to Different Types of Convolutions in Deep Learning | by Kunlun Bai | Towards Data Science](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)
> [CNN中千奇百怪的卷积方式大汇总 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/29367273)
> [变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作。 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/28749411)
> [一文看尽深度学习中的20种卷积（附源码整理和论文解读） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/381839221)

## 摘要

> [!abstract] Emotion recognition from speech is a challenging task. Recent advances in deep learning have led bi-directional recurrent neural network (Bi-RNN) and attention mechanism as a standard method for speech emotion recognition, extracting and attending multi-modal features - audio and text, and then fusing them for downstream emotion classification tasks. In this paper, we propose a simple yet efficient neural network architecture to exploit both acoustic and lexical information from speech. The proposed framework using multi-scale convolutional layers (MSCNN) to obtain both audio and text hidden representations. Then, a statistical pooling unit (SPU) is used to further extract the features in each modality. Besides, an attention module can be built on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the performance. Extensive experiments show that the proposed model outperforms previous state-of-the-art methods on IEMOCAP dataset with four emotion categories (i.e., angry, happy, sad and neutral) in both weighted accuracy (WA) and unweighted accuracy (UA), with an improvement of 5.0% and 5.2% respectively under the ASR setting.

> 语音情感识别是一项具有挑战性的任务。深度学习的最新进展使双向递归神经网络(bi-RNN)和注意机制成为语音情感识别的标准方法，提取并处理音频和文本的多模式特征，然后将它们融合用于下游情感分类任务。在本文中，我们提出了一种简单而高效的神经网络结构，以利用语音中的声学信息和词汇信息。该框架使用多尺度卷积层(MSCNN)来同时获得音频和文本的隐藏表示。然后，使用统计汇集单元(SPU)进一步提取每个通道的特征。此外，还可以在MSCNNSPU(音频)和MSCNN(文本)的基础上构建注意力模块，以进一步提高性能。大量实验表明，该模型在IEMOCAP数据集上的加权准确率(WA)和未加权准确率(UA)上均优于已有的方法，在ASR环境下分别提高了5.0%和5.2%。

## 预处理

## 概述

## 结果

IEMOCAP

![]({16}_Efficient%20Speech%20Emotion%20Recognition%20Using%20Multi-Scale%20CNN%20and%20Attention@pengEfficientSpeechEmotion2021.assets/image-20220418090457.png)

## 精读

本文首先提出了一种简单的卷积神经网络(CNN)和 pooling-based 模型，称为带有 statistical pooling units 的 multi scale CNN(MSCNN-SPU)，该模型能够有效地同时学习语音和文本模式用于情感识别。此外，通过在 MSCNN-SPU 之上构建注意力模块，从而产生 MSCNN-SPU-ATT，可以进一步提高整体性能。

![]({16}_Efficient%20Speech%20Emotion%20Recognition%20Using%20Multi-Scale%20CNN%20and%20Attention@pengEfficientSpeechEmotion2021.assets/image-20220418092230.png)

使用一组具有不同kernel大小的filters来构建multiple CNN层，分别用于文本和音频这两个独立的路径。

具有ReLU激活的各种单层二维卷积[13]与文本和音频的输入特征并行应用。

设$\Omega^{A, T}=\left\{\left(s, d^{A, T}\right), s \in S^{A, T}\right\}$是音频通道(上标‘A’)和文本通道(上标‘T’)的kernel的集合，其中s是沿输入序列维度的kernel大小，dA和dT分别是mfcc和单词嵌入向量的维度。

$$

\begin{aligned}

&\mathrm{G}_{\alpha}^{\mathrm{MSCNN}}(\boldsymbol{x}, \theta)=\left\{y_{\alpha}^{A, T} \mid \alpha \in \Omega\right\}\\

&y_{\alpha=(s, d) \in \Omega}^{A, T}[i, j]=\sum_{m=\left\lfloor-\frac{s}{2}\right\rfloor}^{\left\lceil\frac{s}{2}\right\rceil} \sum_{n=\left\lfloor-\frac{d}{2}\right\rfloor}^{\left\lceil\frac{d}{2}\right\rceil} \boldsymbol{k}[m, n] \cdot \boldsymbol{x}[i-m, j-n]

\end{aligned}

$$

因此，我们提出了一个统计合并单元(SPU)，它由沿序列建模方向的三个平行的一维合并组成：a)全局最大合并；b)全局平均合并；c)全局标准差合并。

$$

\mathrm{E}=\left\{\mathrm{G}_{\gamma}^{\mathrm{SPU}}\left(\mathrm{G}_{\alpha}^{\mathrm{MSCNN}}(\boldsymbol{x})\right) \mid \alpha \in \Omega, \gamma \in\{\max , \text { avg, std }\}\right\}

$$

受[1，3]中注意机制概念的启发，我们提出了一种建立在Audio MSCNN-SPU和 Text-MSCNN 之上的双通道attention层。与以前的工作不同，我们将前者的输出视为上下文向量$\boldsymbol{e}_{\gamma}$(即来自音频分支的最大池、平均池、标准池特征向量)。根据来自max-pooling、avgpooling和std-pooling的输出，分别将加权系数$s_{k}^{\gamma}$计算自上下文向量$\boldsymbol{e}_{\gamma}$和来自文本MSCNN $\boldsymbol{h}_{k}$的第k个输出特征映射之间的乘积。由此得到的注意力向量S是通过用加权系数$s_{k}^{\gamma}$对$\boldsymbol{h}_{k}$加权得到的。

$$
\begin{gathered}

s_{k}^{\gamma}=\frac{\exp \left(\boldsymbol{e}_{\gamma}^{\mathrm{T}} \boldsymbol{h}_{k}\right)}{\sum_{k} \exp \left(\boldsymbol{e}_{\gamma}^{\mathrm{T}} \boldsymbol{h}_{k}\right)}, \text{ where } \gamma \in\{\text{max}, \text{avg}, \text{std} \}\\

\boldsymbol{S}^{\gamma}=\sum_{k} s_{k}^{\gamma} \boldsymbol{h}_{k} \\

\boldsymbol{S}=\operatorname{concat}\left(\boldsymbol{S}^{\mathrm{max}}, \boldsymbol{S}^{\mathrm{avg}}, \boldsymbol{S}^{\mathrm{std}}\right)

\end{gathered}

$$

此外，在说话人识别任务中，使用Kaldi语音识别工具包[18]，X向量嵌入[16]被用作从VoxCeleb数据集[17]上的预先训练的TDNN模型中提取的互补音频特征。

模型设置：
每个CNN层的filter数量被设置为128；在文本encoder中，SWEM-max和SWEMavg特征[14]是从单词嵌入中获得的，然后被附加到文本SPU的输出中；另一方面，将X-vector嵌入附加到audioSPU的输出中；Adam优化，学习率为0.0005；采用范数为1的梯度剪裁；dropout率为0.3；batch size为64。

其次，我们将我们提出的方法与其他多模式方法进行比较。一种直接的方法是为每种模态训练一个 LSTM 网络，然后从每个模式连接最后一个隐藏状态，如 MDRE[1]所示。学习对齐[2]采用 LSTM 网络对音频和文本的序列进行建模。然后，使用模型中的注意力执行音频和文本之间的软对齐。在 MHA[3]中，提出了一种所谓的多跳注意，使用一种模态的隐藏表示作为上下文向量并将注意方法应用于另一种模态，然后重复这种方案数次。

### 引文

- 近年来，基于深度学习的情感识别方法在情感识别中表现出了很好的性能[1，2，3]。
- [1]通过在文本和音频之间引入注意机制来融合学习特征。
- 在[2]中，注意力网络被用来学习语音和文本之间的对齐，而BiLSTM网络被用来对情感识别中的序列进行建模。
- 此外，文献[3]还提出了一种多跳关注法，选择文本数据的相关部分，然后关注音频特征，用于以后的分类目的。使用Audio-BRE(LSTM)。
- 研究人员已经证明了CNN在具有音频特征[7,8]和文本信息[9]的情绪分类中的有效性。
- 受[9]中使用的文本-CNN架构的激励
- 在[11]中，一种使用WordNet和词性标注的混合方法与标准音频特征相结合，然后使用支持向量机进行分类。
- 使用DNN，[12]从多分辨率CNN中提取文本特征，从BiLSTM中提取音频信息，并使用分类损失和验证损失的加权和来优化任务。
- 使用CNN进行情感识别任务的工作经常使用单层全局最大池或全局平均池，并在[7，8]中被证明是有效的。
- Swem向量，它是直接在学习的单词嵌入上连接各种池的结果[14]
- 使用300维GloVe[19]嵌入作为标记化记录的预训练单词嵌入。
- CNN+LSTM in [20]
- [21]中的TDNN+LSTM

---
title: "CopyPaste: An Augmentation Method for Speech Emotion Recognition"
description: ""
citekey: pappagariCopyPasteAugmentationMethod2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:30:37
lastmod: 2023-04-11 11:20:12
---

> [!info] 论文信息
>1. Title：CopyPaste: An Augmentation Method for Speech Emotion Recognition
>2. Author：Raghavendra Pappagari, Jesús Villalba, Piotr Żelasko, Laureano Moro-Velazquez, Najim Dehak
>3. Entry：[Zotero link](zotero://select/items/@pappagariCopyPasteAugmentationMethod2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Pappagari et al_2021_CopyPaste.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 与只用干净的数据训练的模型相比，用噪声增强的数据训练的模型表现得更好，但在噪声环境下噪声增强的性能更好
- 预训练可以显著提高所有数据集上的模型性能
- 引入声纹识别领域的 x-vector 模型进行迁移学习。- 与只用干净的数据训练的模型相比，用噪声增强的数据训练的模型表现得更好，但在噪声环境下噪声增强的性能更好
- 预训练可以显著提高所有数据集上的模型性能
- 引入声纹识别领域的x-vector模型进行迁移学习。

## 摘要

> [!abstract] Data augmentation is a widely used strategy for training robust machine learning models. It partially alleviates the problem of limited data for tasks like speech emotion recognition (SER), where collecting data is expensive and challenging. This study proposes CopyPaste, a perceptually motivated novel augmentation procedure for SER. Assuming that the presence of emotions other than neutral dictates a speaker's overall perceived emotion in a recording, concatenation of an emotional (emotion E) and a neutral utterance can still be labeled with emotion E. We hypothesize that SER performance can be improved using these concatenated utterances in model training. To verify this, three CopyPaste schemes are tested on two deep learning models: one trained independently and another using transfer learning from an x-vector model, a speaker recognition model. We observed that all three CopyPaste schemes improve SER performance on all the three datasets considered: MSP-Podcast, Crema-D, and IEMOCAP. Additionally, CopyPaste performs better than noise augmentation and, using them together improves the SER performance further. Our experiments on noisy test sets suggested that CopyPaste is effective even in noisy test conditions.

> Data augmentation 是训练稳健的机器学习模型的一种广泛使用的策略。它部分缓解了语音情感识别(SER)等任务数据有限的问题，在这些任务中，收集数据既昂贵又具有挑战性。本文提出的 CopyPaste，假设非中性情绪的存在决定了说话人在录音中的总体感知情绪，因此含某情绪的语音和其他中性话语的串联，仍是此情绪。通过在两个深度学习模型上测试了三种 CopyPaste 方案：一个是独立训练的，另一个是从 x 向量模型(说话人识别模型)进行迁移学习的。在 MSP-Podcast、CREMA-D 和 IEMOCAP 这三个数据集上，所有三种 CopyPaste 方案都提高了 SER 性能。此外，CopyPaste 的性能比噪声增强更好，而且即使在噪声测试条件下，CopyPaste 也是有效的。

## 预处理

## 概述

## 结果

CopyPaste 方案对干净数据和噪声增强数据都是有效的。但与预先训练的 ResNet 模型相比，CopyPaste 方案在随机初始化的 ResNet 模型上获得的改进相对更高。

Clean+Noise：SER模型训练针对干净和噪声增强数据进行训练

Clean：SER模型训练仅针对CLEAN数据进行训练

括号内数据：显示与不使用CopyPaste(无CP)训练的模型相比的提高值。

No CP：表示没有CopyPaste训练的模型

## 不同数据库中不同方案得到的情感分类平均结果

表2：使用随机初始化ResNet模型得到的结果(加权F1-分数)。

![]({17}_CopyPaste_%20An%20Augmentation%20Method%20for%20Speech%20Emotion%20Recognition@pappagariCopyPasteAugmentationMethod2021.assets/image-20220419153233.png)

表3：使用针对说话人分类预训练的ResNet模型得到的结果(加权F1分数)。

![]({17}_CopyPaste_%20An%20Augmentation%20Method%20for%20Speech%20Emotion%20Recognition@pappagariCopyPasteAugmentationMethod2021.assets/image-20220419153243.png)

## 精读

在本文中假设，除了中性情绪外，其他情绪的存在决定了录音中说话人的整体感知情绪。换句话说，如果说话者在较长的发声中表达了一种非中性的情绪，即使是很短的持续时间，那么该说话者就被认为是在表达这种情绪。因此，在SER模型训练中，可以将新的串联话语（有情绪语句与中性语串联）与原始话语一起使用将提高SER性能。本文使用两个模型来评估上述假设：一个是独立训练的模型，另一个是从说话人识别模型中进行迁移学习的模型。我们遵循[14]中提出的框架来构建模型。

CopyPaste数据增强方案：

- Neutral CopyPaste (N-CP)：将有情感语音(比方说情感E)和中性语音连接起来，以产生另一种带有情感E的话语。
- Same Emotion CopyPaste (SE-CP)：将具有相同情感的两个E情感语音连接在一起以产生另一个具有情感E的语音。
- N+SE-CP：在模型训练中同时使用N-CP和SE-CP。

![]({17}_CopyPaste_%20An%20Augmentation%20Method%20for%20Speech%20Emotion%20Recognition@pappagariCopyPasteAugmentationMethod2021.assets/image-20220418213738.png)

本文 ResNet 模型由三个模块组成：帧级表征学习网络、pooling网络和语音级分类器。帧级表征学习网络对输入的帧级特征进行操作，例如Mel频率倒谱系数(MFCC)和滤波器组系数。

使用的ResNet-34[22]结构，由一系列具有残差链接的2D卷积层组成。pooling网络包括多头注意力机制，对ResNet的输出进行操作。每个头部$h$的归一化注意力权重$w_{h,t}=\frac{\exp \left(-s_{h}\left\|\mathbf{x}_{t}-\boldsymbol{\mu}_{h}\right\|\right)}{\sum_{t=1}^{T} \exp \left(-s_{h}\left\|\mathbf{x}_{t}-\boldsymbol{\mu}_{h}\right\|\right)}$，头$h$的输出嵌入向量是其输入沿时间轴的加权平均值

不同的头被设计来捕捉输入信号的不同语音方面。我们将注意力头部的输出连接起来，并将其通过一个完全连通的层，以获得总结输入的单个向量嵌入。然后，将完全连接层的输出通过语音级分类器来获得模型判决。

为了使用ResNet结构实现SER，我们训练了两个模型：一个是用随机初始化训练的模型，另一个是用预先训练好的说话人分类模型初始化的模型。我们使用23维MFCC表示作为ResNet模型的输入，并且在通过ResNet之前，应用了基于能量的语音活动检测（energy-based voice activity detection）和平均归一化（mean normalization）。在训练过程中，我们使用带有默认参数的ADAM优化器来最小化交叉熵损失函数。选择在development集上具有最佳加权F1分数的纪元在测试集上进行评估。记录测试集上3次运行的加权F1分数的平均值。

在预训练中使用了几个带有说话人标签的数据集：VoxCeleb1、VoxCeleb2、NISTSRE4-10和Switchboard数据集，总共包含大约12000名说话人。然后使用噪声和音乐增强来提高说话人的分类性能。在说话人识别领域，这种模型通常被称为x向量模型。更多详情，请参考[21]。

为了将在该模型中学习到的知识转移到SER中，我们遵循了[14]中描述的微调过程，即用一个情感鉴别层替换最终的说话人鉴别层，并进行微调以最大限度地减少情感损失。换言之，我们将预训练中学习到的权重用于除最后一层之外的所有层，然后对所有权重进行优化以进行情感分类。

在训练过程中，随机抽取128个语料，并根据情感类别标签进行了CopyPaste，并且在每个时期只对80%的batch执行CopyPaste增强。在相同的前提下，在N+SE-CP方案中，对每个时期中40%的批次遵循N-CP和SE-CP方案，相当于80%的批次具有CopyPaste增强。为了避免过度匹配，我们从每个记录中随机选取4个用于串联，而不是整个记录。我们注意到，在我们的数据集中，训练记录的平均长度不到6s。因此，我们的假设只受到可以忽略不计的可能性的影响，因为只挑选每个记录中的4s进行连接。

并且在本文中，后面通过添加来自MUSAN语料库的噪声和音乐来扩展和增加训练数据[23]，即由训练集分别添加10、5和0分贝的噪声和音乐后得到6个副本。其中由干净和扩充数据组成的数据集训练的模型称为Clean+Noise。由于研究人员表明，与干净的测试数据相比，在有噪声的测试数据上向训练数据添加噪声的有效性更明显[24]，因此本文在有噪声的测试条件下比较了噪声增强和CopyPaste。但由于情感数据集通常是干净的，并且具有较高的信噪比，因此考虑在测试数据中添加噪声。，共创建了两组测试数据，一组信噪比为10分贝，另一组为0分贝，用于与CopyPaste进行比较。

### 引文

- 一些例子，如[5，6，7，8]，探索了标准特征表示，如MFCC和OpenSMILE[9]特征与深度学习模型，如CNN和LSTM的使用。
- 很少有研究使用原始波形或语谱图作为输入，探索联合学习语音表征的情感识别模型[10，11，12]。
- 很少有其他研究小组探索利用具有与情绪无关的注释的大型数据集，如音素和说话人身份标签[13，14]。在这方面，作者在[13]中表明，在训练用于预测音素的ASR模型中学习的知识可以转移到SER。类似地，从诸如X向量模型的说话人识别模型中提取的特征被示出为包含情感信息[14]。
- ResNet34体系结构[14, 21, 22]
- 在[14，16]中表明，在干净的录音中添加噪音有助于模型更好地识别情绪。改变说话速度[16]和声道长度扰动[17]也有助于SER。
- 预训练显著提高了所有数据集上的模型性能，就像在[14]中的情况一样。
- 与只用干净的数据训练的模型相比，用噪声增强的数据训练的模型表现得更好，这与之前的研究证实了[14，16]。
- 很少有研究[18，19]冒险使用先进的技术，如CycleGans和StarGans来产生情感语音特征。
- 在这方面，一些作者观察到，当情绪中立的语音片段和情绪E的情绪片段按顺序播放时，人类听众通常将整个序列归类为情绪E[20]。
- x向量模型[21]。
- 由于研究人员表明，与干净的测试数据相比，在有噪声的测试数据上向训练数据添加噪声的有效性更明显[24]

## 摘录

---
title: "Speech Emotion Recognition Considering Nonverbal Vocalization in Affective Conversations"
description: ""
citekey: hsuSpeechEmotionRecognition2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:30:56
lastmod: 2023-04-11 11:22:24
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition Considering Nonverbal Vocalization in Affective Conversations
>2. Author：Jia-Hao Hsu, Ming-Hsiang Su, Chung-Hsien Wu, Yi-Hsuan Chen
>3. Entry：[Zotero link](zotero://select/items/@hsuSpeechEmotionRecognition2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hsu et al_2021_Speech Emotion Recognition Considering Nonverbal Vocalization in Affective.pdf>)
>4. Other：2021 - IEEE/ACM Transactions on Audio, Speech, and Language Processing     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] In real-life communication, nonverbal vocalization such as laughter, cries or other emotion interjections, within an utterance play an important role for emotion expression. In previous studies, only few emotion recognition systems consider nonverbal vocalization, which naturally exists in our daily conversation. In this work, both verbal and nonverbal sounds within an utterance are considered for emotion recognition of real-life affective conversations. Firstly, a support vector machine (SVM)-based verbal and nonverbal sound detector is developed. A prosodic phrase auto-tagger is further employed to extract the verbal/nonverbal sound segments. For each segment, the emotion and sound feature embeddings are respectively extracted using the deep residual networks (ResNets). Finally, a sequence of the extracted feature embeddings for the entire dialog turn are fed to an attentive long short-term memory (LSTM)-based sequence-to-sequence model to output an emotional sequence as recognition result. The NNIME corpus (The NTHU-NTUA Chinese interactive multimodal emotion corpus), which consists of verbal and nonverbal sounds, was adopted for system training and testing. 4766 single speaker dialogue turns in the audio data of the NNIME corpus were selected for evaluation. The experimental results showed that nonverbal vocalization was helpful for speech emotion recognition. For comparison, the proposed method based on decision-level fusion achieved an accuracy of 61.92% for speech emotion recognition outperforming the traditional methods as well as the feature-level and model-level fusion approaches.

> 在实际沟通过程中，语音交谈中的非语言发声，如笑声、哭声或其他情感感叹词，在情感表达中起着重要的作用。在以往的研究中，很少有情感识别系统考虑自然存在于我们日常对话中的非语言发声，因此在本文工作中，语音中的语言和非语言声音部分都被用作对现实生活中情感对话的情感识别。本文首先设计了一种基于支持向量机的**语言语音和非语言语音检测器**，然后进一步使用韵律短语自动标记器（prosodic phrase autotagger）来提取语言语音和非语言语音片段。对于每个片段，分别使用深度残差网络(ResNets)提取情感和语音嵌入特征。最后，将从整个对话提取的嵌入特征序列馈送到基于注意力的长短期记忆(LSTM)的序列到序列模型，以输出情感序列作为识别结果。采用 NNIME 语料库(NTHU-NTUA 汉语交互式多通道情感语料库)进行系统训练和测试。在 NNIME 语料库的音频数据中选取4766个单人对话进行评估。实验结果表明，非语言声音有助于语音情感识别。相比之下，基于决策层融合的语音情感识别方法的准确率达到61.92%，优于传统方法以及特征层和模型层的融合方法。

## 预处理

## 概述

## 结果

## 精读

近年来，随着科学技术的快速进步，使智能设备在我们的日常生活中变得更加流行。聊天机器人、心理诊断助手、智能医疗、销售广告、智能娱乐等智能服务，考虑的不仅仅是服务的完成性，更重要的是人机交互的人性化。如何实现智能化的人机交互平台成为一个重要的研究问题。对于语音交互系统的应用，一些先进的企业会使用聊天机器人来改善他们的客户服务，并为公司增加业务成绩[1]，[2]并且与直接和客户沟通不同的是，与情感相关的共情系统一般会被纳入到语音交互系统的设计中，以改善用户在人机交互中的体验。更重要的是，要让语音交互系统被用户视为一种社会角色，共情是必不可少的一步。基于以上动机，本研究旨在提高情感识别过程的准确率。

随着使用移动设备越来越普及，语音已经成为人与人、人与机器沟通的最常见的方式之一。语音情感识别是实现计算机情感智能的重要技术之一[4]-[9]，因为人们可以通过语音中包含的信息来理解他人的情感[3]。目前，语音情感识别的研究主要集中在情感特征提取和识别建模两个方面。此外，情感语音数据库的选择对于开发稳健的语音情感识别架构也很重要[10]，[11]。目前的情感识别系统受到语音数据资源缺乏的限制，很少关注日常自发交流中的非语言发声。NNIME 和 BAUM-1 语料库中数据的声音类型和情感类别之间的关系如图1所示，例如，大笑很容易被认为是幸福的，而大喊大叫则与惊喜高度相关。现有的关于非语言情绪识别的文献描述了某些非语言声音(如笑声和喊叫)有利于积极情绪(如快乐和惊讶)的识别[12]-[17]。然而，关于其他非语言声音类型以及隐晦或负面情绪(如悲伤和沮丧)的影响的相关实验和讨论并不多。在现实生活中，对话中的非语言声音在人们识别他人情绪方面发挥着重要作用[18]。当人类大脑分析情绪语音时，非语言声音可以有效地帮助大脑获得情绪表达的差异[19]。因此，本研究旨在将非语言声音的特征应用于语音情感识别。

本研究提出了以下有待解决的问题。

- 第一个问题是语音情感识别基本单元的选择。当我们在对话中识别别人的情绪时，我们不需要听别人的整个反应来识别情绪。从文献中的报告来看，句子级别的特征比粗略对应于用于情感识别的几个单词的持续时间的语音片段更差[26]。先前的研究表明，情绪的识别至少需要一秒钟以上的语音信号[33]。文献[28]中的研究表明，离散的韵律动作单位可以代表言语情感。由于停顿在汉语语音中起着韵律边界标记的作用[41]，我们选择韵律短语作为情感识别的基本单位。

- 对于第二个问题，自然语言中有许多非语言的声音，如笑声和哭声，这有助于区分他人的情绪。在我们只听到鼻息声的情况下，我们无法确定他人的情绪。如果我们听到低能量和多次停顿的鼻息声参与对话，我们可以识别它是对话过程中悲伤情绪中的抽泣声。文献[19]中的研究表明，当人们识别他人的情绪时，我们的大脑独立地处理语音表征和情绪，然后通过声音效应更清楚地获得情绪的差异。由于语音发声和非语音发声在语音情感表达上是相辅相成的，本研究采用两种不同的深度残差网络(ResNets)来提取语音发声的情感特征和非语言声音的情感特征，用于语音情感识别。

- 对于最后一个问题，我们认为在一次对话中可能存在多种情感表达。人们在不同的对话中表达不同程度或不同类型的情绪。人们在感兴趣的内容上往往会表现出高度的 arousal 情绪，在描述日常生活事件时会考虑中性的情绪。针对这一问题，我们将音频信号分为语言段和非语言段进行特征提取。然后使用长短期记忆(LSTM)模型来刻画语音信号的连续情感表达。也就是说，构建LSTM模型，从一系列分段的语音信号中获得说话人在对话中的情感变化。

图2显示了建议的系统框架。在训练阶段，主要分为语音/非语音分割、特征嵌入提取和情感模型构建三个阶段。首先，语音信号经过静默检测、语言和非语言的语音片段检测和韵律短语分割过程，得到语音/语音片段。其次，使用语言和非语言的语音/语音片段来训练相应的ResNet模型，以提取情感和声音的嵌入特征。通过去除输出层，将经过情感/语音类型分类训练的ResNet模型用作特征提取器。最后用 ResNet 提取的语音和情感的特征作为每个片段的代表嵌入特征，该特征嵌入序列将用于情感识别，以获得考虑时间上下文中情感变化的每个片段的情感识别结果。

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426103153.png)


本研究选取NTHU-NTUA中文互动多通道情绪语料库(NNIME)作为系统评测的资料库。在NNIME中，有愤怒、悲伤、快乐、挫折、中立和惊讶6种情感情绪。NNIME的设计有三个主要要素：(1)第一个要素是采用二元交互作用进行情感行为的自然诱导，(2)第二个要素是同时收集二元组的外部行为和内部生理信息，(3)第三个要素是从不同的角度诠释交互作用的两个组的丰富情绪属性[25]。NNIME语料库包括44名来自台湾艺术大学戏剧系的受试者的录音，其中22名女性，20名男性。NNIME语料库由49名受试者标注，其中包括学生和教授。该数据库包含音频、视频和心电记录。在这项研究中，我们只使用了包含102个二元交互会话的音频数据，大约11个小时(每个会话的持续时间μ=195.35秒，σ=73.26秒)。

由于本研究只使用了NNIME数据库中的音频文件，即整部剧的音频数据，按照指定的分割标准

1. 注释单个发言者的通话开始/结束时间。
2. 注释多个发言者的开始/结束通话时间。
3. 如果被其他发言者打断，请在话轮的上一次停顿时停下来。
4. 纯粹的非语言声音可以看作是一种转折。

将音频文件手工分割为7672个单人对话回合，每个会话的音频文件逐个标注说话人语音信号的开头和结尾，称为会话中的单人对话回合。下图显示了注释文件的一个例子。

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426110552.png)

由于NNIME语料库每个会话有一个/两个记录 channel，我们只选择了其中较清晰的一个频道。最后，从7672个单人话轮中选出4766个单人话轮(每个话轮的时长和标准差μ=3.25秒，σ=5.42秒)，并对其进行情感类型和声音类型的标注。

在这项研究中，我们使用了与NNIME中定义的相同的六种情绪，包括愤怒、沮丧、悲伤、惊讶、中立和快乐。由于很难区分说话人的 “silence“ 和 “background noise” 片段，我们增加了一个“background” 标签来代表 silence/background 片段。这项研究总共使用了6个情绪标签来表达情绪，以及一个“background”标签来表征不太可能被贴上特定情绪标签的静音/背景噪音片段。由于演讲者在NNIME的对话回合中不仅表达了一种情绪，我们重新注释了每个片段的情绪和“background”标签。

除了情感类型，本研究还定义了四种声音类型。每个语言片段被分配了一个情感类别，每个非语言片段被分配到四种声音类型中的一种，如表II所示。前三种非语言类声音类型，包括笑声、呼吸和喊叫，第四类是背景音，包括沉默、噪音、观众声音等。由于NNIME的录音环境有来自观众和演员自发行为的噪音，非语言的 “background” 标签不仅包括对话中的沉默，还包括来自录音环境的噪音和观众声音。

音频分割是我们提出的方法中的一个重要步骤，因此我们需要一个边界验证集来测试语音和非语音区间的自动边界检测结果。我们从4766个单人对话语料中随机抽取 300 个对话进行边界标注和情感标注。在选择的300个对话的边界已经被标注之后，使用边界验证集来训练用于语言/非语言语音分割的支持向量机模型。重新标注NNIME语料库有四个步骤。

- 首先，使用Praat软件对选定的300个对话的文本网格文件进行静音检测。
- 其次，使用基于韵律短语现象(phenomena of prosodic phrase，PPH)和如下标准的三个标注器，对音频片段进行标注。
	1. 语句暂停超过 $0.3$ 秒。
	2. Intonation raising(在 raising pitch 轮廓的最高点标注)。
	3. 延长结尾处(Prolonged ending，在延长的起始处标注)。
	4. 下降强度(在下降强度的最低点标注)

	1. 消除Praat对静默间隔的错误检测。
	2. 不要修剪少于$0.1$秒的间隔。
	3. 调整Praat检测到的静默区间的错误边界。

- 第三，标注器对每个片段的情感类型和声音类型进行标注，并将音频文件分类为语言片段和非语言片段。最后，将这些经过标注的音频作为边界验证集。

表四显示了边界验证集合的统计数据

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426114306.png)

表五显示了边界验证集合中声音类型的分布。利用该数据集验证了本文提出的自动分割方法的性能。

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426114314.png)

在边界标注中，确保每个标签至少由两个标注器标注。重新标注的情感标签和声音类型标签的kappa分数[42]分别为0.63和0.69，均大于0.6，这意味着基本一致。重新标注的数据的一致性足够高，足以证明其可靠性。
近年来，随着科学技术的快速进步，使智能设备在我们的日常生活中变得更加流行。聊天机器人、心理诊断助手、智能医疗、销售广告、智能娱乐等智能服务，考虑的不仅仅是服务的完成性，更重要的是人机交互的人性化。如何实现智能化的人机交互平台成为一个重要的研究问题。对于语音交互系统的应用，一些先进的企业会使用聊天机器人来改善他们的客户服务，并为公司增加业务成绩[1]，[2]并且与直接和客户沟通不同的是，与情感相关的共情系统一般会被纳入到语音交互系统的设计中，以改善用户在人机交互中的体验。更重要的是，要让语音交互系统被用户视为一种社会角色，共情是必不可少的一步。基于以上动机，本研究旨在提高情感识别过程的准确率。

随着使用移动设备越来越普及，语音已经成为人与人、人与机器沟通的最常见的方式之一。语音情感识别是实现计算机情感智能的重要技术之一[4]-[9]，因为人们可以通过语音中包含的信息来理解他人的情感[3]。目前，语音情感识别的研究主要集中在情感特征提取和识别建模两个方面。此外，情感语音数据库的选择对于开发稳健的语音情感识别架构也很重要[10]，[11]。目前的情感识别系统受到语音数据资源缺乏的限制，很少关注日常自发交流中的非语言发声。NNIME 和 BAUM-1 语料库中数据的声音类型和情感类别之间的关系如图1所示，例如，大笑很容易被认为是幸福的，而大喊大叫则与惊喜高度相关。现有的关于非语言情绪识别的文献描述了某些非语言声音(如笑声和喊叫)有利于积极情绪(如快乐和惊讶)的识别[12]-[17]。然而，关于其他非语言声音类型以及隐晦或负面情绪(如悲伤和沮丧)的影响的相关实验和讨论并不多。在现实生活中，对话中的非语言声音在人们识别他人情绪方面发挥着重要作用[18]。当人类大脑分析情绪语音时，非语言声音可以有效地帮助大脑获得情绪表达的差异[19]。因此，本研究旨在将非语言声音的特征应用于语音情感识别。

本研究提出了以下有待解决的问题。

- 第一个问题是语音情感识别基本单元的选择。当我们在对话中识别别人的情绪时，我们不需要听别人的整个反应来识别情绪。从文献中的报告来看，句子级别的特征比粗略对应于用于情感识别的几个单词的持续时间的语音片段更差[26]。先前的研究表明，情绪的识别至少需要一秒钟以上的语音信号[33]。文献[28]中的研究表明，离散的韵律动作单位可以代表言语情感。由于停顿在汉语语音中起着韵律边界标记的作用[41]，我们选择韵律短语作为情感识别的基本单位。

- 对于第二个问题，自然语言中有许多非语言的声音，如笑声和哭声，这有助于区分他人的情绪。在我们只听到鼻息声的情况下，我们无法确定他人的情绪。如果我们听到低能量和多次停顿的鼻息声参与对话，我们可以识别它是对话过程中悲伤情绪中的抽泣声。文献[19]中的研究表明，当人们识别他人的情绪时，我们的大脑独立地处理语音表征和情绪，然后通过声音效应更清楚地获得情绪的差异。由于语音发声和非语音发声在语音情感表达上是相辅相成的，本研究采用两种不同的深度残差网络(ResNets)来提取语音发声的情感特征和非语言声音的情感特征，用于语音情感识别。

- 对于最后一个问题，我们认为在一次对话中可能存在多种情感表达。人们在不同的对话中表达不同程度或不同类型的情绪。人们在感兴趣的内容上往往会表现出高度的 arousal 情绪，在描述日常生活事件时会考虑中性的情绪。针对这一问题，我们将音频信号分为语言段和非语言段进行特征提取。然后使用长短期记忆(LSTM)模型来刻画语音信号的连续情感表达。也就是说，构建LSTM模型，从一系列分段的语音信号中获得说话人在对话中的情感变化。

图2显示了建议的系统框架。在训练阶段，主要分为语音/非语音分割、特征嵌入提取和情感模型构建三个阶段。首先，语音信号经过静默检测、语言和非语言的语音片段检测和韵律短语分割过程，得到语音/语音片段。其次，使用语言和非语言的语音/语音片段来训练相应的ResNet模型，以提取情感和声音的嵌入特征。通过去除输出层，将经过情感/语音类型分类训练的ResNet模型用作特征提取器。最后用 ResNet 提取的语音和情感的特征作为每个片段的代表嵌入特征，该特征嵌入序列将用于情感识别，以获得考虑时间上下文中情感变化的每个片段的情感识别结果。

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426103153.png)


本研究选取NTHU-NTUA中文互动多通道情绪语料库(NNIME)作为系统评测的资料库。在NNIME中，有愤怒、悲伤、快乐、挫折、中立和惊讶6种情感情绪。NNIME的设计有三个主要要素：(1)第一个要素是采用二元交互作用进行情感行为的自然诱导，(2)第二个要素是同时收集二元组的外部行为和内部生理信息，(3)第三个要素是从不同的角度诠释交互作用的两个组的丰富情绪属性[25]。NNIME语料库包括44名来自台湾艺术大学戏剧系的受试者的录音，其中22名女性，20名男性。NNIME语料库由49名受试者标注，其中包括学生和教授。该数据库包含音频、视频和心电记录。在这项研究中，我们只使用了包含102个二元交互会话的音频数据，大约11个小时(每个会话的持续时间μ=195.35秒，σ=73.26秒)。

由于本研究只使用了NNIME数据库中的音频文件，即整部剧的音频数据，按照指定的分割标准

1. 注释单个发言者的通话开始/结束时间。
2. 注释多个发言者的开始/结束通话时间。
3. 如果被其他发言者打断，请在话轮的上一次停顿时停下来。
4. 纯粹的非语言声音可以看作是一种转折。

将音频文件手工分割为7672个单人对话回合，每个会话的音频文件逐个标注说话人语音信号的开头和结尾，称为会话中的单人对话回合。下图显示了注释文件的一个例子。

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426110552.png)

由于NNIME语料库每个会话有一个/两个记录 channel，我们只选择了其中较清晰的一个频道。最后，从7672个单人话轮中选出4766个单人话轮(每个话轮的时长和标准差μ=3.25秒，σ=5.42秒)，并对其进行情感类型和声音类型的标注。

在这项研究中，我们使用了与NNIME中定义的相同的六种情绪，包括愤怒、沮丧、悲伤、惊讶、中立和快乐。由于很难区分说话人的 “silence“ 和 “background noise” 片段，我们增加了一个“background” 标签来代表 silence/background 片段。这项研究总共使用了6个情绪标签来表达情绪，以及一个“background”标签来表征不太可能被贴上特定情绪标签的静音/背景噪音片段。由于演讲者在NNIME的对话回合中不仅表达了一种情绪，我们重新注释了每个片段的情绪和“background”标签。

除了情感类型，本研究还定义了四种声音类型。每个语言片段被分配了一个情感类别，每个非语言片段被分配到四种声音类型中的一种，如表II所示。前三种非语言类声音类型，包括笑声、呼吸和喊叫，第四类是背景音，包括沉默、噪音、观众声音等。由于NNIME的录音环境有来自观众和演员自发行为的噪音，非语言的 “background” 标签不仅包括对话中的沉默，还包括来自录音环境的噪音和观众声音。

音频分割是我们提出的方法中的一个重要步骤，因此我们需要一个边界验证集来测试语音和非语音区间的自动边界检测结果。我们从4766个单人对话语料中随机抽取 300 个对话进行边界标注和情感标注。在选择的300个对话的边界已经被标注之后，使用边界验证集来训练用于语言/非语言语音分割的支持向量机模型。重新标注NNIME语料库有四个步骤。

- 首先，使用Praat软件对选定的300个对话的文本网格文件进行静音检测。
- 其次，使用基于韵律短语现象(phenomena of prosodic phrase，PPH)和如下标准的三个标注器，对音频片段进行标注。
	1. 语句暂停超过 $0.3$ 秒。
	2. Intonation raising(在 raising pitch 轮廓的最高点标注)。
	3. 延长结尾处(Prolonged ending，在延长的起始处标注)。
	4. 下降强度(在下降强度的最低点标注)

	1. 消除Praat对静默间隔的错误检测。
	2. 不要修剪少于$0.1$秒的间隔。
	3. 调整Praat检测到的静默区间的错误边界。

- 第三，标注器对每个片段的情感类型和声音类型进行标注，并将音频文件分类为语言片段和非语言片段。最后，将这些经过标注的音频作为边界验证集。

表四显示了边界验证集合的统计数据

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426114306.png)

表五显示了边界验证集合中声音类型的分布。利用该数据集验证了本文提出的自动分割方法的性能。

![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426114314.png)

在边界标注中，确保每个标签至少由两个标注器标注。重新标注的情感标签和声音类型标签的kappa分数[42]分别为0.63和0.69，均大于0.6，这意味着基本一致。重新标注的数据的一致性足够高，足以证明其可靠性。

### 引文

随着对情感计算的研究越来越多，学者们已经建立了几个**情感数据库**[20]-[24]。这些数据库包含不同格式(如音频、视频、文本和运动)和不同标注(如语言、情感标签和脚本)的数据。为了分析情感语音中的非语言声音，一个合适的情感语音数据库是值得研究的。

与其他剧本类情感语料库或电视剧中的分段音频不同，NTHU-NTUA 汉语交互式多通道情感语料库(**NNIME**)是一个自发的汉语语音情感数据库，包含了各种情感的非语言声音，如笑声、抽泣和叹息[25]等，符合本研究的要求，因此被采用。

在语音情感识别机制中，如何从语音信号中提取情感特征是有效情感识别的关键问题。许多研究都在寻找合适的音频特征或合适的**音频特征集**[26]、[27]来表征情绪特征。其他研究试图用不同的特征单位来表示音频[28]。近年来，随着神经网络(NN)的发展，许多研究人员试图从原始的音频波形或频谱中提取特征用于情感识别[29]-[32]。一些研究认为，现有的声学特征集可能缺乏主观情感特征，因为特征提取算法的参数大多是人工调整的[31]。他们期望基于神经网络的端到端情感特征提取方法能够从语音中获取信息。

基于不同类型的情感特征，语音情感识别模型的选择应考虑所提取特征的属性。长短时记忆(LSTM)是一种能够对连续时间属性[34]、[35]特征的情绪类型进行分类的记忆。**谢等人** 提出了一种基于注意的长短期记忆(LSTM)递归神经网络，用于基于帧水平语音特征的语音情感识别[34]。他们在CASIA、eNTERFACE和GEMEP情感语料库上的实验表明，所提出的方法的性能优于迄今报道的最先进的算法。

卷积神经网络(CNN)可以通过卷积步骤组合局部特征，从而获得更大范围的组合信息的分类结果[33]。**赵等人**。将一维和二维CNN与LSTM网络相结合用于语音情感识别。建立一维CNN是为了从语音中学习与局部和全球情感相关的特征，而二维CNN是用来学习音谱图的[36]。CNN和LSTM的结合被称为**CLDNN**，预计将从两种类型的模型中受益，并通过涉及注意力来获得更好的表现[37]。

随着深层网络结构的逐渐成熟，使用更深层的基于CNN的模型也具有良好的图像识别性能。利用**ResNet**提取语音的幅值谱特征，在语音情感识别中也取得了许多优秀的结果[38][40]。

---
title: "Speech Emotion Classification Using Attention-Based LSTM"
description: ""
citekey: xieSpeechEmotionClassification2019
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：Speech Emotion Classification Using Attention-Based LSTM
>2. Author：Yue Xie, Ruiyu Liang, Zhenlin Liang, Chengwei Huang, Cairong Zou, Björn Schuller
>3. Entry：[Zotero link](zotero://select/items/@xieSpeechEmotionClassification2019) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xie et al_2019_Speech Emotion Classification Using Attention-Based LSTM.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\GYS5QCAH\\Xie 等。 - 2019 - Speech Emotion Classification Using Attention-Base.pdf>)
>4. Other：2019 - IEEE/ACM Transactions on Audio, Speech, and Language Processing     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 提出了一种基于注意力的长短期记忆(LSTM)递归神经网络的帧级语音特征的语音识别方法
	- 修改传统 LSTM 的遗忘门来降低计算复杂度
	- 使用注意力机制, 分别提取时间和特征方面的关联特征信息，在特征层面上，区分时间维度中的情感差异以及特征维度中的情感表征能力

## 摘要

> [!abstract] Automatic speech emotion recognition has been a research hotspot in the field of human-computer interaction over the past decade. However, due to the lack of research on the inherent temporal relationship of the speech waveform, the current recognition accuracy needs improvement. To make full use of the difference of emotional saturation between time frames, a novel method is proposed for speech recognition using frame-level speech features combined with attention-based long short-term memory (LSTM) recurrent neural networks. Frame-level speech features were extracted from waveform to replace traditional statistical features, which could preserve the timing relations in the original speech through the sequence of frames. To distinguish emotional saturation in different frames, two improvement strategies are proposed for LSTM based on the attention mechanism: first, the algorithm reduces the computational complexity by modifying the forgetting gate of traditional LSTM without sacrificing performance and second, in the final output of the LSTM, an attention mechanism is applied to both the time and the feature dimension to obtain the information related to the task, rather than using the output from the last iteration of the traditional algorithm. Extensive experiments on the CASIA, eNTERFACE, and GEMEP emotion corpora demonstrate that the performance of the proposed approach is able to outperform the state-of-the-art algorithms reported to date.

> 语音情感自动识别是近十年来人机交互领域的研究热点。然而，由于缺乏对语音波形内在时间关系(the inherent temporal relationship)的研究，目前的识别准确率还有待提高。为了充分利用时间帧之间情感饱和度的差异，提出了一种结合基于注意力的长短期记忆(LSTM)递归神经网络的帧级语音特征的语音识别方法。从波形中提取帧级语音特征来代替传统的统计特征，通过帧序列来保留原始语音中的时间关系。为了区分不同帧中的情感饱和，提出了两种基于注意力机制的LSTM改进策略：第一，在不牺牲性能的情况下，通过修改传统LSTM的遗忘门来降低计算复杂度；第二，在LSTM的最终输出中，将注意力机制应用到时间和特征维度上，以获取与任务相关的信息，而不是使用传统算法最后一次迭代的输出。在CASIA、eNTERFACE和 GEMEP 情感语料库上的大量实验表明，所提出的方法的性能能够超过迄今报道的最先进的算法。

## 预处理

## 概述

## 结果

## 精读

情感识别在人机交互中具有重要的实用价值[1]-[4]，并具有广泛的应用前景。为了实现基于语音的情感分类，人们在机器学习算法上投入了大量的研究工作，如支持向量机[5]-[7]、贝叶斯分类器[8]、[9]和 K 最近邻分类法[10]、[11]。近年来，深度学习也被广泛应用于自动语音情感识别。但大多数传统的机器学习算法和深度学习网络(如自动编码器和卷积神经网络)都只能接受固定维度的数据作为输入，这对于具有可变语音长度的发音级情感识别来说似乎是矛盾的。为了解决这个问题，首先，最流行的方法[15]-[18]从短时语音帧中提取与情感相关的特征(本文称为帧级特征)，然后将静态统计函数(例如，均值、方差、最大值或线性回归系数)应用于帧级特征，并将结果拼接成固定维度的向量来表示完整的语音波形。虽然这些固定维度的特征满足了模型输入的要求，但经过统计分析处理的语音特征丢失了原始语音中的时间信息。解决这一矛盾的另一种方法是设计一种可以接受可变长度特征的模型。

近年来，为了增强LSTM在特定任务中处理数据的能力，人们对LSTM的内部结构提出了许多改进建议。Schmidhuber[19]提出了一种通过使用历史细胞状态作为输入信息的窥视孔连接，以增强学习历史信息的能力。
姚[20]通过引入深度门来连接各层之间的存储单元，从而控制存储单元之间的数据流。

此外，在许多LSTM应用[14]、[21]-[23]中，LSTM最后一次的输出通常被选为下一个模型的输入(因为其他模型只能接受固定尺寸的输入)。然而，在语音情感识别任务中，语音在最后大多是无声的，几乎没有任何情感信息。因此，情绪信息会在最后一刻被削弱。如何在任何时刻(而不是在最后一刻)有效地使用LSTM输出是提高语音情感识别性能的关键。针对上述问题，提出了一种改进的LSTM模型用于语音情感识别任务。首先，该模型使用帧级别的语音特征作为输入。特征的维度随着实际语音长度的变化而变化，而原始语音中的时间信息被帧之间的序列保留。因此，它更适合于具有处理可变长度序列的能力的LSTM的输入。其次，为了使LSTM的存储单元能够有效地利用历史状态中的关键信息，提出了一种替代传统LSTM中遗忘门的关注门。这一改进不仅降低了LSTM的计算复杂度，而且优化了情感识别性能。此外，语音的不同时间段的情感饱和度不同(沉默的片段包含的情感信息较少)，各种语音特征区分情感的能力也不同[24]。因此，利用权重系数区分差异是可行的，可以充分利用情感信息，提高情感识别性能。因此，针对语音情感识别的特殊性，本文提出了一种基于注意力机制的LSTM输出加权方法，并成功应用于图像处理领域[25]-[27]。该加权操作不仅作用于时间维度，还作用于特征维度。最后，在CASIA、eNTERFACE和GEMEP语料库上验证了该模型的性能。

从理论上讲，LSTM网络的最后时刻应该获得较大的权重。因此，本研究以上一次的输出为参照，通过使用注意机制来确保其能够获得较大的权重。此外，考虑到语音特征之间区分能力的差异，还将注意力机制应用于LSTM输出的特征维度。本文研究侧重于细胞内部的计算，并使用自我注意算法对LSTM的遗忘门进行了改进[43]。因此，遗忘门的计算不同于以前的LSTM。由于自我注意算法仅与历史单元状态本身相关，而与当前输入和历史隐藏无关。

基于OpenSMILE ComParE的帧级别的语音特征(即没有统计函数的特征)。直接用于情感分类，如下表所示。基本原因是：(1)统计函数的定长特征计算丢失了原始语音中的大量信息，如时间信息。(2)Hinton[45]认为深度学习具有自动学习特征变化的能力，可以从潜在的语音特征中学习与任务相关的深层特征。因此，帧级别特征看起来更适合作为在此建议的深度学习网络的输入。

| Features | Description |
| :--- | :--- |
| voiceProb | Voicing probability（发音概率） |
| HNR | Log harmonics-to-noise ratio（对数谐波均方根能量(Harmonic ERMS)与声门噪声均方根能量(NoiseERMS)之比） |
| F0 | Pitch frequency（基音频率） |
| F0raw | Raw F0 candidate without threshold in unvoiced segments（在无声段中无阈值的原生F0）|
| F0env | F0 envelope（F0 包络） |
| jitterLocal | The Average Absolute Difference between consecutive periods(连续周期之间的平均绝对差值) |
| jitterDDP | The Average Absolute Difference between consecutive differences between consecutive periods(连续周期间连续差的平均绝对差值) |
| shimmerLocal | The Average Absolute Difference between the interpolated peak amplitudes of consecutive periods(连续周期内插峰值幅度的平均绝对差) |
| harmonicERMS | Harmonic component RMS[Root Mean Square] energy（谐波分量均方根能量） |
| noiseERMS | Noise component RMS[Root Mean Square] energy（噪声分量均方根能量） |
| pcm_loudness_sma | Loudness（响度） |
| pcm_loudness_sma_de | Delta regression of loudness（响度的前向差分回归） |
| mfcc_sma[0]-[14] | Mel-Frequency Cepstral Coefficients（梅尔倒谱系数） |
| mfcc_sma_de[0]-[14] | Delta regression of mfcc（梅尔倒谱系数的前向差分回归） |
| pcm_Mag[0]-[25] | Mel Spectral（MEL 谱） |
| logMelFreqBand[0]-[7] | log Mel frequency bands（对数MEL频带） |
| lpcCoeff[0]-[7] | Linear predictive coding coefficients（线性预测编码系数） |
| lspFreq[0]-[7] | Line spectral pair frequency（线谱对频率） |
| pcm_zcr | Zero-crossing rate（过零率） |

谐波与噪声比(HNR)：：

$$
\mathrm{HNR}=10 \log _{10}\left\{\sum_{n=1}^{N} g^{2}(n) / \sum_{n=1}^{N} n^{2}(n)\right\}
$$

然而，HNR模糊了不同情绪类别之间的差异，因为在比例上存在分歧。相反，谐音ERMS和噪声ERMS特征可以保留情感类别之间的差异。同时，分别提取声门谐波能量（glottal harmonic energy）和声门噪声能量（glottal noise energy）作为反映声门关闭状态的情感特征。

为了可视化这三种特征对分类的影响，从CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]中获取了一些样本(X轴)，然后计算这些样本的三个特征在一段时间内的平均值。如下图所示，在eNTERFACE语料库上，情绪之间的区别在谐波ERMS和噪声ERMS等值线上明显，而在HNR(谐波ERMS和噪声ERMS的比率)等值线中由于除法运算而严重降低。同样，在CASIA语料库上，HNR维度上的情绪差异小于和谐ERMS和NoiseERMS维度上的差异。此外，愤怒情绪在这些语料库上处于较高水平(这也是上述语料库的共性)，因此相对更容易与其他情绪区分开来。在CASIA语料库上，中性情绪在三个特征轮廓中处于最低水平，因此也相对容易区分。在eNTERFACE语料库上，处于最低水平的悲伤情绪在理论上具有较强的区分性，而厌恶、恐惧和惊讶情绪相互重叠，可能很难区分。在GEMEP语料库上，所有情感的轮廓相互重叠，其中一个可能的原因是一些情感描述带有非语义的短文本AAA。因此，GEMEP的情感区分度低于其他两个语料库，这表明在GEMEP上的平均识别率最低。总而言之，不同的特征在不同的数据库中具有不同的区分情绪的能力。

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429171451.png)

注意机制首次应用于图像处理领域[25]-[27]，取得了很好的效果。其核心思想是，人脑对整个画面的关注是不平衡的，并存在一定的权重区分。受这一现象的启发，本文将自我注意机制引入到LSTM的遗忘门计算中，在保证模型性能的前提下减少了模型运算量。同时，情感识别中使用的帧级语音特征不仅包括时间信息，还包括特征级信息。这些不同的特征可能对最终分类性能具有不同程度的影响。为此，还将特征级别信息乘以关注度加权系数，以提高模型的最终性能。

LSTM信元的遗忘门用于确定在前一时刻的信元状态中应该丢弃哪些信息，并直接参与更新信元状态。

整体网络架构如下图所示

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429175757.png)

为了展示所建议方法的性能，我们选择了三个不同的流行数据库来避免基于单一语料库评估的观察。实验使用了CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]。

CASIA是中科院自动化所引进的情感语料库，包含6类情感(即愤怒、恐惧、高兴、中性、悲伤和惊讶)。语料库包含来自4名说话人(2名男性和2名女性)的7200个语音样本，其中随机选择1000个样本作为测试集。

eNTERFACE是一个英语音频和视频情感语料库，记录了来自14个国家的43名说话者，并根据以下6种情绪对样本进行分类：愤怒、厌恶、恐惧、高兴、悲伤和惊讶。

在这项工作中只使用了来自该语料库的音频数据。获得了1260个有效语音样本用于情感识别研究，其中260个样本作为测试集。GEMEP是一个法语内容语料库，拥有18个语音情感类别和1260个话语样本。在我们的实验中，我们选择了12种情绪(娱乐(Amu)、焦虑(Inq)、绝望(Des)、愤怒(Coll)、喜悦(Joi)、恐慌恐惧(Peu)、兴趣(Int)、激怒(Irr)、快乐(Pla)、骄傲(Fie)、解脱(Sou)、悲伤(Tri)；注：缩写来自法语)，如[8]、[52]。这是属于所选类别的10个说话人的1080个样本，其中随机选择200个样本作为测试集。根据作者目前的知识，基于语音的识别准确率在CASIA[53]、[54]上低于90%，在eNTERFACE[8]、[44]上低于80%，在GEMEP[8]、[55]上低于50%。提出的模型包括基于时间维度注意力加权的LSTM模型(LSTM-T)、基于特征维度注意力加权的LSTM(LSTMF)、基于改进注意力机制遗忘门的LSTM-AT(LSTM-AT)、基于时间和特征维度注意力加权的LSTM(LSTM-TF)及其改进的遗忘门LSTM-AT(LSTM-TF-AT)，相关参数的设置如表II所示。输入的维度为[128，TimeStep，93]，其中128是批次大小，TimeStep是帧的数量，93是从语音中提取的特征的数量。为了比较时间复杂度，这些参数在所有没有筛选的语料库上都是相同的。只有学习率根据训练集上收敛的稳定性进行调整。CASIA的初始学习率为0.0001，eNTERFACE和GEMEP的初始学习率均为0.001。输出的维度由语料库中情绪的数量决定(CASIA和ENTERFACE有6个类别，而GEMEP有12个类别)。由于对LSTM的输出矩阵执行两次注意力加权运算，并且结果以[outputT，outputF]的形式连接作为后续全连接层的输入，因此全连接层的信元数加倍。表II中的全连接层参数[256,128]对应于基于传统LSTM的网络，而[512,128]是基于时间和特征维度的注意力机制的LSTM网络的参数设置。其他参数保持不变，以保证实验结果的有效性。

为了验证基于注意力机制的改进型遗忘门在保证系统性能的前提下能够有效地减少训练时间，本文对两组实验进行了对比实验。一个实验是在LSTM-AT和传统的LSTM模型之间，另一个实验是在LSTM-TF模型和LSTM-TF-AT之间。如图5所示，它描述了在相应语料库上相同训练步骤下的训练时间。这四个模型分别在CASIA、ENTERFACE和GEMEP上训练，CASIA为1200个历元，ENTERFACE为1000个历元，GEMEP为1500个历元。换句话说，这些模型在相同的数据库上执行相同的迭代。从图中可以看出，相同训练步数的每个模型的训练时间是不同的。与未修改的LSTM模型相比，基于关注门的LSTM模型的时间开销更小。比较这些语料库的训练时间，CASIA需要更多的时间，LSTM-AT与LSTM(3.5h)、LSTM-Tf-At与LSTM-Tf(1h)的训练时间差异大于eNTERFACE(0.8h和0.9h)和GEMEP(0.7h)。这表明训练时间越长，基于注意门的LSTM的优势越显著。在计算复杂性方面，GRU的训练时间少于提出的基于注意力的遗忘门(Lstmat)，如图5所示。然而，LSTM-AT在拥有比eNTERFACE和GEMEP更多样本的CASIA语料库上取得了比GRU更好的性能。虽然GRU具有较低的计算复杂度，但在大数据集上表现不佳。布里茨[56]推翻了类似的结论。在他的工作中，LSTM细胞的表现一直好于GRU细胞。因此，LSTM-AT在不牺牲性能的情况下降低了计算复杂度。在更新细胞状态方面，陶[40]将注意力机制应用于LSTM的细胞状态更新，关注细胞之间的信息。但本研究关注的是细胞内部，并对LSTM的遗忘门进行了改造。因此，遗忘门的计算不同于以前的LSTM和[40]中的计算。在所有提到的语料库上，LSTM-AT的性能与TAO相似。然而，后者需要更多的时间来训练，因为需要计算更多的先前的小区状态，如图5所示。在本研究中，LSTM-AT侧重于小区状态的内部计算，并综合考虑了计算复杂度和性能。为了定量分析基于关注门的LSTM模型的识别性能，分析了每个模型的最佳识别性能，如表三、表四和表五所示。基于关注门的LSTM减少了模型内部的矩阵运算，并且不会对所有数据库的UAR产生负面影响，事实上，有时甚至可以观察到性能的提高。与传统LSTM相比，LSTM-AT在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.6%、5.7%和2.5%，在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.8%、2.7%和3%。综上所述，LSTM-TF-at通过在时间和特征维度中引入注意机制，增强了与情绪相关的信息，显著提高了UAR，如表VI所示，该表描述了LSTM和改进后的P值之间的左尾T检验。然而，在CASIA语料库上，由于基线较高，0.052的P值对LSTM-AT的改善并不显著。此外，设计了基于注意力机制的遗忘门，在保证性能的前提下，降低了模型的计算复杂度，加快了模型的收敛速度，缩短了训练时间。将LSTM-Tf和LSTM-At结合起来，由于LSTM-Tf-At得到的P值较小，其意义更加明显。因此，该模型在情感分类中具有明显的优势。

由于本文使用的特征集是在OpenSMILE比较特征集[16]的基础上进行修改的，所以还对原始特征集和修改后的两个特征集进行了比较。然而，由于函数应用于LLD后的最终OpenSMILE比较特征集是一维特征向量，内部数据不具有时序关系，不适合作为提出的用于分类情感识别的LSTM模型的输入。因此，将OpenSMILE比较函数特征集与传统的机器学习算法支持向量机相结合，作为比较基线。结果见表七、表八和表九，分别对应于CASIA、eNTERFACE和GEMEP数据库。如表所示，该方法在CASIA、eNTERFACE和GEMEP上的UAR分别提高了5.4%、33.8%和17.0%。尤其是在CASIA和eNTERFACE上，每类情绪的回忆次数都有所增加，这表明了LSTM-TF-AT具有框架级特征的优势。

本文提出了一种改进的基于注意力的LSTM情感分类方法。在遗忘门和LSTM的输出中都引入了注意机制。改进后的门只与历史单元状态有关，与当前输入无关，降低了计算复杂度。实验表明，新的关注门也能提高识别率。此外，由于LSTM-TFat模型考虑了不同时间段的情感饱和度，并将注意力机制应用到LSTM输出的时间和特征维中，使得LSTM-TFat模型具有更好的性能，特别是在大数据集上。在小数据集上，LSTM-TF-AT和Mirsamadi的模型具有相似的UAR，但后者在算法复杂度上具有优势。此外，与经典的支持向量机分类器相比，在CASIA和eNTERFACE语料库上，LSTM-TF-AT对每类情感的召回率都有所提高。下一步的工作包括以下几个方面。首先，虽然该方法在分类任务上是有效的，但它对连续情感的分类有很大的意义。因此，有必要研究改进的LSTM用于连续情感识别。其次，该算法考虑了不同特征对情感的区分能力。因此，在我们未来的研究中，它将被用于特征过滤。此外，随着我们的研究，这种基于注意力的LSTM有望在更多的应用中进行。

情感识别在人机交互中具有重要的实用价值[1]-[4]，并具有广泛的应用前景。为了实现基于语音的情感分类，人们在机器学习算法上投入了大量的研究工作，如支持向量机[5]-[7]、贝叶斯分类器[8]、[9]和 K 最近邻分类法[10]、[11]。近年来，深度学习也被广泛应用于自动语音情感识别。但大多数传统的机器学习算法和深度学习网络(如自动编码器和卷积神经网络)都只能接受固定维度的数据作为输入，这对于具有可变语音长度的发音级情感识别来说似乎是矛盾的。为了解决这个问题，首先，最流行的方法[15]-[18]从短时语音帧中提取与情感相关的特征(本文称为帧级特征)，然后将静态统计函数(例如，均值、方差、最大值或线性回归系数)应用于帧级特征，并将结果拼接成固定维度的向量来表示完整的语音波形。虽然这些固定维度的特征满足了模型输入的要求，但经过统计分析处理的语音特征丢失了原始语音中的时间信息。解决这一矛盾的另一种方法是设计一种可以接受可变长度特征的模型。

近年来，为了增强LSTM在特定任务中处理数据的能力，人们对LSTM的内部结构提出了许多改进建议。Schmidhuber[19]提出了一种通过使用历史细胞状态作为输入信息的窥视孔连接，以增强学习历史信息的能力。
姚[20]通过引入深度门来连接各层之间的存储单元，从而控制存储单元之间的数据流。

此外，在许多LSTM应用[14]、[21]-[23]中，LSTM最后一次的输出通常被选为下一个模型的输入(因为其他模型只能接受固定尺寸的输入)。然而，在语音情感识别任务中，语音在最后大多是无声的，几乎没有任何情感信息。因此，情绪信息会在最后一刻被削弱。如何在任何时刻(而不是在最后一刻)有效地使用LSTM输出是提高语音情感识别性能的关键。针对上述问题，提出了一种改进的LSTM模型用于语音情感识别任务。首先，该模型使用帧级别的语音特征作为输入。特征的维度随着实际语音长度的变化而变化，而原始语音中的时间信息被帧之间的序列保留。因此，它更适合于具有处理可变长度序列的能力的LSTM的输入。其次，为了使LSTM的存储单元能够有效地利用历史状态中的关键信息，提出了一种替代传统LSTM中遗忘门的关注门。这一改进不仅降低了LSTM的计算复杂度，而且优化了情感识别性能。此外，语音的不同时间段的情感饱和度不同(沉默的片段包含的情感信息较少)，各种语音特征区分情感的能力也不同[24]。因此，利用权重系数区分差异是可行的，可以充分利用情感信息，提高情感识别性能。因此，针对语音情感识别的特殊性，本文提出了一种基于注意力机制的LSTM输出加权方法，并成功应用于图像处理领域[25]-[27]。该加权操作不仅作用于时间维度，还作用于特征维度。最后，在CASIA、eNTERFACE和GEMEP语料库上验证了该模型的性能。

从理论上讲，LSTM网络的最后时刻应该获得较大的权重。因此，本研究以上一次的输出为参照，通过使用注意机制来确保其能够获得较大的权重。此外，考虑到语音特征之间区分能力的差异，还将注意力机制应用于LSTM输出的特征维度。本文研究侧重于细胞内部的计算，并使用自我注意算法对LSTM的遗忘门进行了改进[43]。因此，遗忘门的计算不同于以前的LSTM。由于自我注意算法仅与历史单元状态本身相关，而与当前输入和历史隐藏无关。

基于OpenSMILE ComParE的帧级别的语音特征(即没有统计函数的特征)。直接用于情感分类，如下表所示。基本原因是：(1)统计函数的定长特征计算丢失了原始语音中的大量信息，如时间信息。(2)Hinton[45]认为深度学习具有自动学习特征变化的能力，可以从潜在的语音特征中学习与任务相关的深层特征。因此，帧级别特征看起来更适合作为在此建议的深度学习网络的输入。

| Features | Description |
| :--- | :--- |
| voiceProb | Voicing probability（发音概率） |
| HNR | Log harmonics-to-noise ratio（对数谐波均方根能量(Harmonic ERMS)与声门噪声均方根能量(NoiseERMS)之比） |
| F0 | Pitch frequency（基音频率） |
| F0raw | Raw F0 candidate without threshold in unvoiced segments（在无声段中无阈值的原生F0）|
| F0env | F0 envelope（F0 包络） |
| jitterLocal | The Average Absolute Difference between consecutive periods(连续周期之间的平均绝对差值) |
| jitterDDP | The Average Absolute Difference between consecutive differences between consecutive periods(连续周期间连续差的平均绝对差值) |
| shimmerLocal | The Average Absolute Difference between the interpolated peak amplitudes of consecutive periods(连续周期内插峰值幅度的平均绝对差) |
| harmonicERMS | Harmonic component RMS[Root Mean Square] energy（谐波分量均方根能量） |
| noiseERMS | Noise component RMS[Root Mean Square] energy（噪声分量均方根能量） |
| pcm_loudness_sma | Loudness（响度） |
| pcm_loudness_sma_de | Delta regression of loudness（响度的前向差分回归） |
| mfcc_sma[0]-[14] | Mel-Frequency Cepstral Coefficients（梅尔倒谱系数） |
| mfcc_sma_de[0]-[14] | Delta regression of mfcc（梅尔倒谱系数的前向差分回归） |
| pcm_Mag[0]-[25] | Mel Spectral（MEL 谱） |
| logMelFreqBand[0]-[7] | log Mel frequency bands（对数MEL频带） |
| lpcCoeff[0]-[7] | Linear predictive coding coefficients（线性预测编码系数） |
| lspFreq[0]-[7] | Line spectral pair frequency（线谱对频率） |
| pcm_zcr | Zero-crossing rate（过零率） |

谐波与噪声比(HNR)：：

$$
\mathrm{HNR}=10 \log _{10}\left\{\sum_{n=1}^{N} g^{2}(n) / \sum_{n=1}^{N} n^{2}(n)\right\}
$$

然而，HNR模糊了不同情绪类别之间的差异，因为在比例上存在分歧。相反，谐音ERMS和噪声ERMS特征可以保留情感类别之间的差异。同时，分别提取声门谐波能量（glottal harmonic energy）和声门噪声能量（glottal noise energy）作为反映声门关闭状态的情感特征。

为了可视化这三种特征对分类的影响，从CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]中获取了一些样本(X轴)，然后计算这些样本的三个特征在一段时间内的平均值。如下图所示，在eNTERFACE语料库上，情绪之间的区别在谐波ERMS和噪声ERMS等值线上明显，而在HNR(谐波ERMS和噪声ERMS的比率)等值线中由于除法运算而严重降低。同样，在CASIA语料库上，HNR维度上的情绪差异小于和谐ERMS和NoiseERMS维度上的差异。此外，愤怒情绪在这些语料库上处于较高水平(这也是上述语料库的共性)，因此相对更容易与其他情绪区分开来。在CASIA语料库上，中性情绪在三个特征轮廓中处于最低水平，因此也相对容易区分。在eNTERFACE语料库上，处于最低水平的悲伤情绪在理论上具有较强的区分性，而厌恶、恐惧和惊讶情绪相互重叠，可能很难区分。在GEMEP语料库上，所有情感的轮廓相互重叠，其中一个可能的原因是一些情感描述带有非语义的短文本AAA。因此，GEMEP的情感区分度低于其他两个语料库，这表明在GEMEP上的平均识别率最低。总而言之，不同的特征在不同的数据库中具有不同的区分情绪的能力。

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429171451.png)

注意机制首次应用于图像处理领域[25]-[27]，取得了很好的效果。其核心思想是，人脑对整个画面的关注是不平衡的，并存在一定的权重区分。受这一现象的启发，本文将自我注意机制引入到LSTM的遗忘门计算中，在保证模型性能的前提下减少了模型运算量。同时，情感识别中使用的帧级语音特征不仅包括时间信息，还包括特征级信息。这些不同的特征可能对最终分类性能具有不同程度的影响。为此，还将特征级别信息乘以关注度加权系数，以提高模型的最终性能。

LSTM信元的遗忘门用于确定在前一时刻的信元状态中应该丢弃哪些信息，并直接参与更新信元状态。

整体网络架构如下图所示

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429175757.png)

为了展示所建议方法的性能，我们选择了三个不同的流行数据库来避免基于单一语料库评估的观察。实验使用了CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]。

CASIA是中科院自动化所引进的情感语料库，包含6类情感(即愤怒、恐惧、高兴、中性、悲伤和惊讶)。语料库包含来自4名说话人(2名男性和2名女性)的7200个语音样本，其中随机选择1000个样本作为测试集。

eNTERFACE是一个英语音频和视频情感语料库，记录了来自14个国家的43名说话者，并根据以下6种情绪对样本进行分类：愤怒、厌恶、恐惧、高兴、悲伤和惊讶。

在这项工作中只使用了来自该语料库的音频数据。获得了1260个有效语音样本用于情感识别研究，其中260个样本作为测试集。GEMEP是一个法语内容语料库，拥有18个语音情感类别和1260个话语样本。在我们的实验中，我们选择了12种情绪(娱乐(Amu)、焦虑(Inq)、绝望(Des)、愤怒(Coll)、喜悦(Joi)、恐慌恐惧(Peu)、兴趣(Int)、激怒(Irr)、快乐(Pla)、骄傲(Fie)、解脱(Sou)、悲伤(Tri)；注：缩写来自法语)，如[8]、[52]。这是属于所选类别的10个说话人的1080个样本，其中随机选择200个样本作为测试集。根据作者目前的知识，基于语音的识别准确率在CASIA[53]、[54]上低于90%，在eNTERFACE[8]、[44]上低于80%，在GEMEP[8]、[55]上低于50%。提出的模型包括基于时间维度注意力加权的LSTM模型(LSTM-T)、基于特征维度注意力加权的LSTM(LSTMF)、基于改进注意力机制遗忘门的LSTM-AT(LSTM-AT)、基于时间和特征维度注意力加权的LSTM(LSTM-TF)及其改进的遗忘门LSTM-AT(LSTM-TF-AT)，相关参数的设置如表II所示。输入的维度为[128，TimeStep，93]，其中128是批次大小，TimeStep是帧的数量，93是从语音中提取的特征的数量。为了比较时间复杂度，这些参数在所有没有筛选的语料库上都是相同的。只有学习率根据训练集上收敛的稳定性进行调整。CASIA的初始学习率为0.0001，eNTERFACE和GEMEP的初始学习率均为0.001。输出的维度由语料库中情绪的数量决定(CASIA和ENTERFACE有6个类别，而GEMEP有12个类别)。由于对LSTM的输出矩阵执行两次注意力加权运算，并且结果以[outputT，outputF]的形式连接作为后续全连接层的输入，因此全连接层的信元数加倍。表II中的全连接层参数[256,128]对应于基于传统LSTM的网络，而[512,128]是基于时间和特征维度的注意力机制的LSTM网络的参数设置。其他参数保持不变，以保证实验结果的有效性。

为了验证基于注意力机制的改进型遗忘门在保证系统性能的前提下能够有效地减少训练时间，本文对两组实验进行了对比实验。一个实验是在LSTM-AT和传统的LSTM模型之间，另一个实验是在LSTM-TF模型和LSTM-TF-AT之间。如图5所示，它描述了在相应语料库上相同训练步骤下的训练时间。这四个模型分别在CASIA、ENTERFACE和GEMEP上训练，CASIA为1200个历元，ENTERFACE为1000个历元，GEMEP为1500个历元。换句话说，这些模型在相同的数据库上执行相同的迭代。从图中可以看出，相同训练步数的每个模型的训练时间是不同的。与未修改的LSTM模型相比，基于关注门的LSTM模型的时间开销更小。比较这些语料库的训练时间，CASIA需要更多的时间，LSTM-AT与LSTM(3.5h)、LSTM-Tf-At与LSTM-Tf(1h)的训练时间差异大于eNTERFACE(0.8h和0.9h)和GEMEP(0.7h)。这表明训练时间越长，基于注意门的LSTM的优势越显著。在计算复杂性方面，GRU的训练时间少于提出的基于注意力的遗忘门(Lstmat)，如图5所示。然而，LSTM-AT在拥有比eNTERFACE和GEMEP更多样本的CASIA语料库上取得了比GRU更好的性能。虽然GRU具有较低的计算复杂度，但在大数据集上表现不佳。布里茨[56]推翻了类似的结论。在他的工作中，LSTM细胞的表现一直好于GRU细胞。因此，LSTM-AT在不牺牲性能的情况下降低了计算复杂度。在更新细胞状态方面，陶[40]将注意力机制应用于LSTM的细胞状态更新，关注细胞之间的信息。但本研究关注的是细胞内部，并对LSTM的遗忘门进行了改造。因此，遗忘门的计算不同于以前的LSTM和[40]中的计算。在所有提到的语料库上，LSTM-AT的性能与TAO相似。然而，后者需要更多的时间来训练，因为需要计算更多的先前的小区状态，如图5所示。在本研究中，LSTM-AT侧重于小区状态的内部计算，并综合考虑了计算复杂度和性能。为了定量分析基于关注门的LSTM模型的识别性能，分析了每个模型的最佳识别性能，如表三、表四和表五所示。基于关注门的LSTM减少了模型内部的矩阵运算，并且不会对所有数据库的UAR产生负面影响，事实上，有时甚至可以观察到性能的提高。与传统LSTM相比，LSTM-AT在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.6%、5.7%和2.5%，在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.8%、2.7%和3%。综上所述，LSTM-TF-at通过在时间和特征维度中引入注意机制，增强了与情绪相关的信息，显著提高了UAR，如表VI所示，该表描述了LSTM和改进后的P值之间的左尾T检验。然而，在CASIA语料库上，由于基线较高，0.052的P值对LSTM-AT的改善并不显著。此外，设计了基于注意力机制的遗忘门，在保证性能的前提下，降低了模型的计算复杂度，加快了模型的收敛速度，缩短了训练时间。将LSTM-Tf和LSTM-At结合起来，由于LSTM-Tf-At得到的P值较小，其意义更加明显。因此，该模型在情感分类中具有明显的优势。

由于本文使用的特征集是在OpenSMILE比较特征集[16]的基础上进行修改的，所以还对原始特征集和修改后的两个特征集进行了比较。然而，由于函数应用于LLD后的最终OpenSMILE比较特征集是一维特征向量，内部数据不具有时序关系，不适合作为提出的用于分类情感识别的LSTM模型的输入。因此，将OpenSMILE比较函数特征集与传统的机器学习算法支持向量机相结合，作为比较基线。结果见表七、表八和表九，分别对应于CASIA、eNTERFACE和GEMEP数据库。如表所示，该方法在CASIA、eNTERFACE和GEMEP上的UAR分别提高了5.4%、33.8%和17.0%。尤其是在CASIA和eNTERFACE上，每类情绪的回忆次数都有所增加，这表明了LSTM-TF-AT具有框架级特征的优势。

本文提出了一种改进的基于注意力的LSTM情感分类方法。在遗忘门和LSTM的输出中都引入了注意机制。改进后的门只与历史单元状态有关，与当前输入无关，降低了计算复杂度。实验表明，新的关注门也能提高识别率。此外，由于LSTM-TFat模型考虑了不同时间段的情感饱和度，并将注意力机制应用到LSTM输出的时间和特征维中，使得LSTM-TFat模型具有更好的性能，特别是在大数据集上。在小数据集上，LSTM-TF-AT和Mirsamadi的模型具有相似的UAR，但后者在算法复杂度上具有优势。此外，与经典的支持向量机分类器相比，在CASIA和eNTERFACE语料库上，LSTM-TF-AT对每类情感的召回率都有所提高。下一步的工作包括以下几个方面。首先，虽然该方法在分类任务上是有效的，但它对连续情感的分类有很大的意义。因此，有必要研究改进的LSTM用于连续情感识别。其次，该算法考虑了不同特征对情感的区分能力。因此，在我们未来的研究中，它将被用于特征过滤。此外，随着我们的研究，这种基于注意力的LSTM有望在更多的应用中进行。

### 引文

- 邓[12]使用了带有自动编码器的半监督学习和少量的情感标签数据来进行SER。
- Neumann[13]和Wöllmer[14]分别将卷积神经网络和LSTM应用于SER。Wöllmer[14]首次将LSTM应用于连续情感识别，为每句话提取了4843个特征作为LSTM的输入。在他的进一步工作[15]中，静态特征被用作双向LSTM(BLSTM)的输入，以预测口语的情感表达。
- Schmidhuber[19]针对递归神经网络(RNN)提出了长短期记忆(LSTM)结构。该方法为处理诸如语音的可变长度的时间序列提供了可行性。
- 在特征方面，由于全局统计忽略了语音的时间结构，语音中的时间信息没有得到充分利用[31]。为了增强特征，文[32]将时间窗口逐帧引入递归层，并进行了情感分类实验，取得了较好的效果。
- 在早期的工作中，SER[31]、[33]-[36]直接使用了帧级特征，通过帧之间的序列来保存时间信息。众所周知，LSTM擅长处理序列数据，因此帧级别的特征比统计特征更适合于它的输入。
- 除了LSTM的输入外，还需要改进常规LSTM的输出。在LSTM[21][23]的大多数应用中，选择LSTM中最后时刻的输出作为下一个模型的输入(因为其他模型只接受具有固定维度的输入，而LSTM的输出的维度与其实数维度不一致的输入相同)。这可能会导致在其他历史时刻对LSTM输出信息的不完全使用；具体地说，由于长期依赖的时间跨度不是无限的[37]，[38]，在最后时刻LSTM的累积信息是有损失的。
- 在情感分类任务中，Keren[32]在LSTM的输出中引入了卷积神经网络的汇集操作。
- Mirsamadi[39]提出了一种用于计算具有注意力参数向量的帧的权重的注意力机制。由于LSTM的存储容量，在上一次的输出中积累的信息最丰富。因此，最后一次的输出通常被认为是LSTM的最终输出(在本研究和[39]中，这种方法都可以识别情绪)。
- 注意机制不仅可以用来优化LSTM的输出，而且还可以用来更新存储单元。一些研究已经调查了如何更新存储单元。陶[40]将注意力机制应用于LSTM的细胞状态更新，该机制关注细胞之间的信息，并更多地考虑先前的细胞状态。
- 在计算方面，Bradbury[41]提出了用于神经序列建模的准递归神经网络，它允许输出依赖于序列中元素的整体顺序，并且在训练和测试时间比传统的LSTM具有更快的速度。
- Cho[42]提出了门控递归单元(GRU)，它将输入门和遗忘门组合成一个更新门，并将单元状态和隐层状态混合在一起，从而简化了LSTM的计算。
- Greff[22]引入了一种耦合的LSTM，它只使用一个门来控制历史小区状态和候选小区状态对当前小区状态的影响，从而简化了候选小区状态权重的计算。
- Schuller等人提出的ComParE openSMILE特征。在语音情感识别中应用最为广泛[12]，[44]，其中一个[17]基于低级描述符(LLD，例如零交叉率、均方根帧能量、基音频率和Mel频率倒谱系数1-12)的提取，添加它们的增量，并应用统计函数，具有6373个特征的维度。
- 如[7]，证实了语音的harmonic information信息可以用于区分CASIA和EMODB数据库中的情感类别。
- 研究[46]还表明，声门波（glottis waves）包含某些情绪信息。
- 在Hochreiter[19]提出的原始LSTM算法中，单元状态的更新算法与前一时刻的隐含层输出和当前时刻的输入有关。此外，他们增加了窥视孔连接，并将前一时刻的细胞状态作为参数来更新当前状态。

## 摘录

---
title: "Speech Emotion Classification Using Attention-Based LSTM"
description: ""
citekey: xieSpeechEmotionClassification2019
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：Speech Emotion Classification Using Attention-Based LSTM
>2. Author：Yue Xie, Ruiyu Liang, Zhenlin Liang, Chengwei Huang, Cairong Zou, Björn Schuller
>3. Entry：[Zotero link](zotero://select/items/@xieSpeechEmotionClassification2019) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Xie et al_2019_Speech Emotion Classification Using Attention-Based LSTM.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\GYS5QCAH\\Xie 等。 - 2019 - Speech Emotion Classification Using Attention-Base.pdf>)
>4. Other：2019 - IEEE/ACM Transactions on Audio, Speech, and Language Processing     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 提出了一种基于注意力的长短期记忆(LSTM)递归神经网络的帧级语音特征的语音识别方法
	- 修改传统 LSTM 的遗忘门来降低计算复杂度
	- 使用注意力机制, 分别提取时间和特征方面的关联特征信息，在特征层面上，区分时间维度中的情感差异以及特征维度中的情感表征能力

## 摘要

> [!abstract] Automatic speech emotion recognition has been a research hotspot in the field of human-computer interaction over the past decade. However, due to the lack of research on the inherent temporal relationship of the speech waveform, the current recognition accuracy needs improvement. To make full use of the difference of emotional saturation between time frames, a novel method is proposed for speech recognition using frame-level speech features combined with attention-based long short-term memory (LSTM) recurrent neural networks. Frame-level speech features were extracted from waveform to replace traditional statistical features, which could preserve the timing relations in the original speech through the sequence of frames. To distinguish emotional saturation in different frames, two improvement strategies are proposed for LSTM based on the attention mechanism: first, the algorithm reduces the computational complexity by modifying the forgetting gate of traditional LSTM without sacrificing performance and second, in the final output of the LSTM, an attention mechanism is applied to both the time and the feature dimension to obtain the information related to the task, rather than using the output from the last iteration of the traditional algorithm. Extensive experiments on the CASIA, eNTERFACE, and GEMEP emotion corpora demonstrate that the performance of the proposed approach is able to outperform the state-of-the-art algorithms reported to date.

> 语音情感自动识别是近十年来人机交互领域的研究热点。然而，由于缺乏对语音波形内在时间关系(the inherent temporal relationship)的研究，目前的识别准确率还有待提高。为了充分利用时间帧之间情感饱和度的差异，提出了一种结合基于注意力的长短期记忆(LSTM)递归神经网络的帧级语音特征的语音识别方法。从波形中提取帧级语音特征来代替传统的统计特征，通过帧序列来保留原始语音中的时间关系。为了区分不同帧中的情感饱和，提出了两种基于注意力机制的LSTM改进策略：第一，在不牺牲性能的情况下，通过修改传统LSTM的遗忘门来降低计算复杂度；第二，在LSTM的最终输出中，将注意力机制应用到时间和特征维度上，以获取与任务相关的信息，而不是使用传统算法最后一次迭代的输出。在CASIA、eNTERFACE和 GEMEP 情感语料库上的大量实验表明，所提出的方法的性能能够超过迄今报道的最先进的算法。

## 预处理

## 概述

## 结果

## 精读

情感识别在人机交互中具有重要的实用价值[1]-[4]，并具有广泛的应用前景。为了实现基于语音的情感分类，人们在机器学习算法上投入了大量的研究工作，如支持向量机[5]-[7]、贝叶斯分类器[8]、[9]和 K 最近邻分类法[10]、[11]。近年来，深度学习也被广泛应用于自动语音情感识别。但大多数传统的机器学习算法和深度学习网络(如自动编码器和卷积神经网络)都只能接受固定维度的数据作为输入，这对于具有可变语音长度的发音级情感识别来说似乎是矛盾的。为了解决这个问题，首先，最流行的方法[15]-[18]从短时语音帧中提取与情感相关的特征(本文称为帧级特征)，然后将静态统计函数(例如，均值、方差、最大值或线性回归系数)应用于帧级特征，并将结果拼接成固定维度的向量来表示完整的语音波形。虽然这些固定维度的特征满足了模型输入的要求，但经过统计分析处理的语音特征丢失了原始语音中的时间信息。解决这一矛盾的另一种方法是设计一种可以接受可变长度特征的模型。

近年来，为了增强LSTM在特定任务中处理数据的能力，人们对LSTM的内部结构提出了许多改进建议。Schmidhuber[19]提出了一种通过使用历史细胞状态作为输入信息的窥视孔连接，以增强学习历史信息的能力。
姚[20]通过引入深度门来连接各层之间的存储单元，从而控制存储单元之间的数据流。

此外，在许多LSTM应用[14]、[21]-[23]中，LSTM最后一次的输出通常被选为下一个模型的输入(因为其他模型只能接受固定尺寸的输入)。然而，在语音情感识别任务中，语音在最后大多是无声的，几乎没有任何情感信息。因此，情绪信息会在最后一刻被削弱。如何在任何时刻(而不是在最后一刻)有效地使用LSTM输出是提高语音情感识别性能的关键。针对上述问题，提出了一种改进的LSTM模型用于语音情感识别任务。首先，该模型使用帧级别的语音特征作为输入。特征的维度随着实际语音长度的变化而变化，而原始语音中的时间信息被帧之间的序列保留。因此，它更适合于具有处理可变长度序列的能力的LSTM的输入。其次，为了使LSTM的存储单元能够有效地利用历史状态中的关键信息，提出了一种替代传统LSTM中遗忘门的关注门。这一改进不仅降低了LSTM的计算复杂度，而且优化了情感识别性能。此外，语音的不同时间段的情感饱和度不同(沉默的片段包含的情感信息较少)，各种语音特征区分情感的能力也不同[24]。因此，利用权重系数区分差异是可行的，可以充分利用情感信息，提高情感识别性能。因此，针对语音情感识别的特殊性，本文提出了一种基于注意力机制的LSTM输出加权方法，并成功应用于图像处理领域[25]-[27]。该加权操作不仅作用于时间维度，还作用于特征维度。最后，在CASIA、eNTERFACE和GEMEP语料库上验证了该模型的性能。

从理论上讲，LSTM网络的最后时刻应该获得较大的权重。因此，本研究以上一次的输出为参照，通过使用注意机制来确保其能够获得较大的权重。此外，考虑到语音特征之间区分能力的差异，还将注意力机制应用于LSTM输出的特征维度。本文研究侧重于细胞内部的计算，并使用自我注意算法对LSTM的遗忘门进行了改进[43]。因此，遗忘门的计算不同于以前的LSTM。由于自我注意算法仅与历史单元状态本身相关，而与当前输入和历史隐藏无关。

基于OpenSMILE ComParE的帧级别的语音特征(即没有统计函数的特征)。直接用于情感分类，如下表所示。基本原因是：(1)统计函数的定长特征计算丢失了原始语音中的大量信息，如时间信息。(2)Hinton[45]认为深度学习具有自动学习特征变化的能力，可以从潜在的语音特征中学习与任务相关的深层特征。因此，帧级别特征看起来更适合作为在此建议的深度学习网络的输入。

| Features | Description |
| :--- | :--- |
| voiceProb | Voicing probability（发音概率） |
| HNR | Log harmonics-to-noise ratio（对数谐波均方根能量(Harmonic ERMS)与声门噪声均方根能量(NoiseERMS)之比） |
| F0 | Pitch frequency（基音频率） |
| F0raw | Raw F0 candidate without threshold in unvoiced segments（在无声段中无阈值的原生F0）|
| F0env | F0 envelope（F0 包络） |
| jitterLocal | The Average Absolute Difference between consecutive periods(连续周期之间的平均绝对差值) |
| jitterDDP | The Average Absolute Difference between consecutive differences between consecutive periods(连续周期间连续差的平均绝对差值) |
| shimmerLocal | The Average Absolute Difference between the interpolated peak amplitudes of consecutive periods(连续周期内插峰值幅度的平均绝对差) |
| harmonicERMS | Harmonic component RMS[Root Mean Square] energy（谐波分量均方根能量） |
| noiseERMS | Noise component RMS[Root Mean Square] energy（噪声分量均方根能量） |
| pcm_loudness_sma | Loudness（响度） |
| pcm_loudness_sma_de | Delta regression of loudness（响度的前向差分回归） |
| mfcc_sma[0]-[14] | Mel-Frequency Cepstral Coefficients（梅尔倒谱系数） |
| mfcc_sma_de[0]-[14] | Delta regression of mfcc（梅尔倒谱系数的前向差分回归） |
| pcm_Mag[0]-[25] | Mel Spectral（MEL 谱） |
| logMelFreqBand[0]-[7] | log Mel frequency bands（对数MEL频带） |
| lpcCoeff[0]-[7] | Linear predictive coding coefficients（线性预测编码系数） |
| lspFreq[0]-[7] | Line spectral pair frequency（线谱对频率） |
| pcm_zcr | Zero-crossing rate（过零率） |

谐波与噪声比(HNR)：：

$$
\mathrm{HNR}=10 \log _{10}\left\{\sum_{n=1}^{N} g^{2}(n) / \sum_{n=1}^{N} n^{2}(n)\right\}
$$

然而，HNR模糊了不同情绪类别之间的差异，因为在比例上存在分歧。相反，谐音ERMS和噪声ERMS特征可以保留情感类别之间的差异。同时，分别提取声门谐波能量（glottal harmonic energy）和声门噪声能量（glottal noise energy）作为反映声门关闭状态的情感特征。

为了可视化这三种特征对分类的影响，从CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]中获取了一些样本(X轴)，然后计算这些样本的三个特征在一段时间内的平均值。如下图所示，在eNTERFACE语料库上，情绪之间的区别在谐波ERMS和噪声ERMS等值线上明显，而在HNR(谐波ERMS和噪声ERMS的比率)等值线中由于除法运算而严重降低。同样，在CASIA语料库上，HNR维度上的情绪差异小于和谐ERMS和NoiseERMS维度上的差异。此外，愤怒情绪在这些语料库上处于较高水平(这也是上述语料库的共性)，因此相对更容易与其他情绪区分开来。在CASIA语料库上，中性情绪在三个特征轮廓中处于最低水平，因此也相对容易区分。在eNTERFACE语料库上，处于最低水平的悲伤情绪在理论上具有较强的区分性，而厌恶、恐惧和惊讶情绪相互重叠，可能很难区分。在GEMEP语料库上，所有情感的轮廓相互重叠，其中一个可能的原因是一些情感描述带有非语义的短文本AAA。因此，GEMEP的情感区分度低于其他两个语料库，这表明在GEMEP上的平均识别率最低。总而言之，不同的特征在不同的数据库中具有不同的区分情绪的能力。

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429171451.png)

注意机制首次应用于图像处理领域[25]-[27]，取得了很好的效果。其核心思想是，人脑对整个画面的关注是不平衡的，并存在一定的权重区分。受这一现象的启发，本文将自我注意机制引入到LSTM的遗忘门计算中，在保证模型性能的前提下减少了模型运算量。同时，情感识别中使用的帧级语音特征不仅包括时间信息，还包括特征级信息。这些不同的特征可能对最终分类性能具有不同程度的影响。为此，还将特征级别信息乘以关注度加权系数，以提高模型的最终性能。

LSTM信元的遗忘门用于确定在前一时刻的信元状态中应该丢弃哪些信息，并直接参与更新信元状态。

整体网络架构如下图所示

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429175757.png)

为了展示所建议方法的性能，我们选择了三个不同的流行数据库来避免基于单一语料库评估的观察。实验使用了CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]。

CASIA是中科院自动化所引进的情感语料库，包含6类情感(即愤怒、恐惧、高兴、中性、悲伤和惊讶)。语料库包含来自4名说话人(2名男性和2名女性)的7200个语音样本，其中随机选择1000个样本作为测试集。

eNTERFACE是一个英语音频和视频情感语料库，记录了来自14个国家的43名说话者，并根据以下6种情绪对样本进行分类：愤怒、厌恶、恐惧、高兴、悲伤和惊讶。

在这项工作中只使用了来自该语料库的音频数据。获得了1260个有效语音样本用于情感识别研究，其中260个样本作为测试集。GEMEP是一个法语内容语料库，拥有18个语音情感类别和1260个话语样本。在我们的实验中，我们选择了12种情绪(娱乐(Amu)、焦虑(Inq)、绝望(Des)、愤怒(Coll)、喜悦(Joi)、恐慌恐惧(Peu)、兴趣(Int)、激怒(Irr)、快乐(Pla)、骄傲(Fie)、解脱(Sou)、悲伤(Tri)；注：缩写来自法语)，如[8]、[52]。这是属于所选类别的10个说话人的1080个样本，其中随机选择200个样本作为测试集。根据作者目前的知识，基于语音的识别准确率在CASIA[53]、[54]上低于90%，在eNTERFACE[8]、[44]上低于80%，在GEMEP[8]、[55]上低于50%。提出的模型包括基于时间维度注意力加权的LSTM模型(LSTM-T)、基于特征维度注意力加权的LSTM(LSTMF)、基于改进注意力机制遗忘门的LSTM-AT(LSTM-AT)、基于时间和特征维度注意力加权的LSTM(LSTM-TF)及其改进的遗忘门LSTM-AT(LSTM-TF-AT)，相关参数的设置如表II所示。输入的维度为[128，TimeStep，93]，其中128是批次大小，TimeStep是帧的数量，93是从语音中提取的特征的数量。为了比较时间复杂度，这些参数在所有没有筛选的语料库上都是相同的。只有学习率根据训练集上收敛的稳定性进行调整。CASIA的初始学习率为0.0001，eNTERFACE和GEMEP的初始学习率均为0.001。输出的维度由语料库中情绪的数量决定(CASIA和ENTERFACE有6个类别，而GEMEP有12个类别)。由于对LSTM的输出矩阵执行两次注意力加权运算，并且结果以[outputT，outputF]的形式连接作为后续全连接层的输入，因此全连接层的信元数加倍。表II中的全连接层参数[256,128]对应于基于传统LSTM的网络，而[512,128]是基于时间和特征维度的注意力机制的LSTM网络的参数设置。其他参数保持不变，以保证实验结果的有效性。

为了验证基于注意力机制的改进型遗忘门在保证系统性能的前提下能够有效地减少训练时间，本文对两组实验进行了对比实验。一个实验是在LSTM-AT和传统的LSTM模型之间，另一个实验是在LSTM-TF模型和LSTM-TF-AT之间。如图5所示，它描述了在相应语料库上相同训练步骤下的训练时间。这四个模型分别在CASIA、ENTERFACE和GEMEP上训练，CASIA为1200个历元，ENTERFACE为1000个历元，GEMEP为1500个历元。换句话说，这些模型在相同的数据库上执行相同的迭代。从图中可以看出，相同训练步数的每个模型的训练时间是不同的。与未修改的LSTM模型相比，基于关注门的LSTM模型的时间开销更小。比较这些语料库的训练时间，CASIA需要更多的时间，LSTM-AT与LSTM(3.5h)、LSTM-Tf-At与LSTM-Tf(1h)的训练时间差异大于eNTERFACE(0.8h和0.9h)和GEMEP(0.7h)。这表明训练时间越长，基于注意门的LSTM的优势越显著。在计算复杂性方面，GRU的训练时间少于提出的基于注意力的遗忘门(Lstmat)，如图5所示。然而，LSTM-AT在拥有比eNTERFACE和GEMEP更多样本的CASIA语料库上取得了比GRU更好的性能。虽然GRU具有较低的计算复杂度，但在大数据集上表现不佳。布里茨[56]推翻了类似的结论。在他的工作中，LSTM细胞的表现一直好于GRU细胞。因此，LSTM-AT在不牺牲性能的情况下降低了计算复杂度。在更新细胞状态方面，陶[40]将注意力机制应用于LSTM的细胞状态更新，关注细胞之间的信息。但本研究关注的是细胞内部，并对LSTM的遗忘门进行了改造。因此，遗忘门的计算不同于以前的LSTM和[40]中的计算。在所有提到的语料库上，LSTM-AT的性能与TAO相似。然而，后者需要更多的时间来训练，因为需要计算更多的先前的小区状态，如图5所示。在本研究中，LSTM-AT侧重于小区状态的内部计算，并综合考虑了计算复杂度和性能。为了定量分析基于关注门的LSTM模型的识别性能，分析了每个模型的最佳识别性能，如表三、表四和表五所示。基于关注门的LSTM减少了模型内部的矩阵运算，并且不会对所有数据库的UAR产生负面影响，事实上，有时甚至可以观察到性能的提高。与传统LSTM相比，LSTM-AT在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.6%、5.7%和2.5%，在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.8%、2.7%和3%。综上所述，LSTM-TF-at通过在时间和特征维度中引入注意机制，增强了与情绪相关的信息，显著提高了UAR，如表VI所示，该表描述了LSTM和改进后的P值之间的左尾T检验。然而，在CASIA语料库上，由于基线较高，0.052的P值对LSTM-AT的改善并不显著。此外，设计了基于注意力机制的遗忘门，在保证性能的前提下，降低了模型的计算复杂度，加快了模型的收敛速度，缩短了训练时间。将LSTM-Tf和LSTM-At结合起来，由于LSTM-Tf-At得到的P值较小，其意义更加明显。因此，该模型在情感分类中具有明显的优势。

由于本文使用的特征集是在OpenSMILE比较特征集[16]的基础上进行修改的，所以还对原始特征集和修改后的两个特征集进行了比较。然而，由于函数应用于LLD后的最终OpenSMILE比较特征集是一维特征向量，内部数据不具有时序关系，不适合作为提出的用于分类情感识别的LSTM模型的输入。因此，将OpenSMILE比较函数特征集与传统的机器学习算法支持向量机相结合，作为比较基线。结果见表七、表八和表九，分别对应于CASIA、eNTERFACE和GEMEP数据库。如表所示，该方法在CASIA、eNTERFACE和GEMEP上的UAR分别提高了5.4%、33.8%和17.0%。尤其是在CASIA和eNTERFACE上，每类情绪的回忆次数都有所增加，这表明了LSTM-TF-AT具有框架级特征的优势。

本文提出了一种改进的基于注意力的LSTM情感分类方法。在遗忘门和LSTM的输出中都引入了注意机制。改进后的门只与历史单元状态有关，与当前输入无关，降低了计算复杂度。实验表明，新的关注门也能提高识别率。此外，由于LSTM-TFat模型考虑了不同时间段的情感饱和度，并将注意力机制应用到LSTM输出的时间和特征维中，使得LSTM-TFat模型具有更好的性能，特别是在大数据集上。在小数据集上，LSTM-TF-AT和Mirsamadi的模型具有相似的UAR，但后者在算法复杂度上具有优势。此外，与经典的支持向量机分类器相比，在CASIA和eNTERFACE语料库上，LSTM-TF-AT对每类情感的召回率都有所提高。下一步的工作包括以下几个方面。首先，虽然该方法在分类任务上是有效的，但它对连续情感的分类有很大的意义。因此，有必要研究改进的LSTM用于连续情感识别。其次，该算法考虑了不同特征对情感的区分能力。因此，在我们未来的研究中，它将被用于特征过滤。此外，随着我们的研究，这种基于注意力的LSTM有望在更多的应用中进行。

情感识别在人机交互中具有重要的实用价值[1]-[4]，并具有广泛的应用前景。为了实现基于语音的情感分类，人们在机器学习算法上投入了大量的研究工作，如支持向量机[5]-[7]、贝叶斯分类器[8]、[9]和 K 最近邻分类法[10]、[11]。近年来，深度学习也被广泛应用于自动语音情感识别。但大多数传统的机器学习算法和深度学习网络(如自动编码器和卷积神经网络)都只能接受固定维度的数据作为输入，这对于具有可变语音长度的发音级情感识别来说似乎是矛盾的。为了解决这个问题，首先，最流行的方法[15]-[18]从短时语音帧中提取与情感相关的特征(本文称为帧级特征)，然后将静态统计函数(例如，均值、方差、最大值或线性回归系数)应用于帧级特征，并将结果拼接成固定维度的向量来表示完整的语音波形。虽然这些固定维度的特征满足了模型输入的要求，但经过统计分析处理的语音特征丢失了原始语音中的时间信息。解决这一矛盾的另一种方法是设计一种可以接受可变长度特征的模型。

近年来，为了增强LSTM在特定任务中处理数据的能力，人们对LSTM的内部结构提出了许多改进建议。Schmidhuber[19]提出了一种通过使用历史细胞状态作为输入信息的窥视孔连接，以增强学习历史信息的能力。
姚[20]通过引入深度门来连接各层之间的存储单元，从而控制存储单元之间的数据流。

此外，在许多LSTM应用[14]、[21]-[23]中，LSTM最后一次的输出通常被选为下一个模型的输入(因为其他模型只能接受固定尺寸的输入)。然而，在语音情感识别任务中，语音在最后大多是无声的，几乎没有任何情感信息。因此，情绪信息会在最后一刻被削弱。如何在任何时刻(而不是在最后一刻)有效地使用LSTM输出是提高语音情感识别性能的关键。针对上述问题，提出了一种改进的LSTM模型用于语音情感识别任务。首先，该模型使用帧级别的语音特征作为输入。特征的维度随着实际语音长度的变化而变化，而原始语音中的时间信息被帧之间的序列保留。因此，它更适合于具有处理可变长度序列的能力的LSTM的输入。其次，为了使LSTM的存储单元能够有效地利用历史状态中的关键信息，提出了一种替代传统LSTM中遗忘门的关注门。这一改进不仅降低了LSTM的计算复杂度，而且优化了情感识别性能。此外，语音的不同时间段的情感饱和度不同(沉默的片段包含的情感信息较少)，各种语音特征区分情感的能力也不同[24]。因此，利用权重系数区分差异是可行的，可以充分利用情感信息，提高情感识别性能。因此，针对语音情感识别的特殊性，本文提出了一种基于注意力机制的LSTM输出加权方法，并成功应用于图像处理领域[25]-[27]。该加权操作不仅作用于时间维度，还作用于特征维度。最后，在CASIA、eNTERFACE和GEMEP语料库上验证了该模型的性能。

从理论上讲，LSTM网络的最后时刻应该获得较大的权重。因此，本研究以上一次的输出为参照，通过使用注意机制来确保其能够获得较大的权重。此外，考虑到语音特征之间区分能力的差异，还将注意力机制应用于LSTM输出的特征维度。本文研究侧重于细胞内部的计算，并使用自我注意算法对LSTM的遗忘门进行了改进[43]。因此，遗忘门的计算不同于以前的LSTM。由于自我注意算法仅与历史单元状态本身相关，而与当前输入和历史隐藏无关。

基于OpenSMILE ComParE的帧级别的语音特征(即没有统计函数的特征)。直接用于情感分类，如下表所示。基本原因是：(1)统计函数的定长特征计算丢失了原始语音中的大量信息，如时间信息。(2)Hinton[45]认为深度学习具有自动学习特征变化的能力，可以从潜在的语音特征中学习与任务相关的深层特征。因此，帧级别特征看起来更适合作为在此建议的深度学习网络的输入。

| Features | Description |
| :--- | :--- |
| voiceProb | Voicing probability（发音概率） |
| HNR | Log harmonics-to-noise ratio（对数谐波均方根能量(Harmonic ERMS)与声门噪声均方根能量(NoiseERMS)之比） |
| F0 | Pitch frequency（基音频率） |
| F0raw | Raw F0 candidate without threshold in unvoiced segments（在无声段中无阈值的原生F0）|
| F0env | F0 envelope（F0 包络） |
| jitterLocal | The Average Absolute Difference between consecutive periods(连续周期之间的平均绝对差值) |
| jitterDDP | The Average Absolute Difference between consecutive differences between consecutive periods(连续周期间连续差的平均绝对差值) |
| shimmerLocal | The Average Absolute Difference between the interpolated peak amplitudes of consecutive periods(连续周期内插峰值幅度的平均绝对差) |
| harmonicERMS | Harmonic component RMS[Root Mean Square] energy（谐波分量均方根能量） |
| noiseERMS | Noise component RMS[Root Mean Square] energy（噪声分量均方根能量） |
| pcm_loudness_sma | Loudness（响度） |
| pcm_loudness_sma_de | Delta regression of loudness（响度的前向差分回归） |
| mfcc_sma[0]-[14] | Mel-Frequency Cepstral Coefficients（梅尔倒谱系数） |
| mfcc_sma_de[0]-[14] | Delta regression of mfcc（梅尔倒谱系数的前向差分回归） |
| pcm_Mag[0]-[25] | Mel Spectral（MEL 谱） |
| logMelFreqBand[0]-[7] | log Mel frequency bands（对数MEL频带） |
| lpcCoeff[0]-[7] | Linear predictive coding coefficients（线性预测编码系数） |
| lspFreq[0]-[7] | Line spectral pair frequency（线谱对频率） |
| pcm_zcr | Zero-crossing rate（过零率） |

谐波与噪声比(HNR)：：

$$
\mathrm{HNR}=10 \log _{10}\left\{\sum_{n=1}^{N} g^{2}(n) / \sum_{n=1}^{N} n^{2}(n)\right\}
$$

然而，HNR模糊了不同情绪类别之间的差异，因为在比例上存在分歧。相反，谐音ERMS和噪声ERMS特征可以保留情感类别之间的差异。同时，分别提取声门谐波能量（glottal harmonic energy）和声门噪声能量（glottal noise energy）作为反映声门关闭状态的情感特征。

为了可视化这三种特征对分类的影响，从CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]中获取了一些样本(X轴)，然后计算这些样本的三个特征在一段时间内的平均值。如下图所示，在eNTERFACE语料库上，情绪之间的区别在谐波ERMS和噪声ERMS等值线上明显，而在HNR(谐波ERMS和噪声ERMS的比率)等值线中由于除法运算而严重降低。同样，在CASIA语料库上，HNR维度上的情绪差异小于和谐ERMS和NoiseERMS维度上的差异。此外，愤怒情绪在这些语料库上处于较高水平(这也是上述语料库的共性)，因此相对更容易与其他情绪区分开来。在CASIA语料库上，中性情绪在三个特征轮廓中处于最低水平，因此也相对容易区分。在eNTERFACE语料库上，处于最低水平的悲伤情绪在理论上具有较强的区分性，而厌恶、恐惧和惊讶情绪相互重叠，可能很难区分。在GEMEP语料库上，所有情感的轮廓相互重叠，其中一个可能的原因是一些情感描述带有非语义的短文本AAA。因此，GEMEP的情感区分度低于其他两个语料库，这表明在GEMEP上的平均识别率最低。总而言之，不同的特征在不同的数据库中具有不同的区分情绪的能力。

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429171451.png)

注意机制首次应用于图像处理领域[25]-[27]，取得了很好的效果。其核心思想是，人脑对整个画面的关注是不平衡的，并存在一定的权重区分。受这一现象的启发，本文将自我注意机制引入到LSTM的遗忘门计算中，在保证模型性能的前提下减少了模型运算量。同时，情感识别中使用的帧级语音特征不仅包括时间信息，还包括特征级信息。这些不同的特征可能对最终分类性能具有不同程度的影响。为此，还将特征级别信息乘以关注度加权系数，以提高模型的最终性能。

LSTM信元的遗忘门用于确定在前一时刻的信元状态中应该丢弃哪些信息，并直接参与更新信元状态。

整体网络架构如下图所示

![]({20}_Speech%20Emotion%20Classification%20Using%20Attention-Based%20LSTM@xieSpeechEmotionClassification2019.assets/image-20220429175757.png)

为了展示所建议方法的性能，我们选择了三个不同的流行数据库来避免基于单一语料库评估的观察。实验使用了CASIA[47]、eNTERFACE[48]和GEMEP语料库[49]。

CASIA是中科院自动化所引进的情感语料库，包含6类情感(即愤怒、恐惧、高兴、中性、悲伤和惊讶)。语料库包含来自4名说话人(2名男性和2名女性)的7200个语音样本，其中随机选择1000个样本作为测试集。

eNTERFACE是一个英语音频和视频情感语料库，记录了来自14个国家的43名说话者，并根据以下6种情绪对样本进行分类：愤怒、厌恶、恐惧、高兴、悲伤和惊讶。

在这项工作中只使用了来自该语料库的音频数据。获得了1260个有效语音样本用于情感识别研究，其中260个样本作为测试集。GEMEP是一个法语内容语料库，拥有18个语音情感类别和1260个话语样本。在我们的实验中，我们选择了12种情绪(娱乐(Amu)、焦虑(Inq)、绝望(Des)、愤怒(Coll)、喜悦(Joi)、恐慌恐惧(Peu)、兴趣(Int)、激怒(Irr)、快乐(Pla)、骄傲(Fie)、解脱(Sou)、悲伤(Tri)；注：缩写来自法语)，如[8]、[52]。这是属于所选类别的10个说话人的1080个样本，其中随机选择200个样本作为测试集。根据作者目前的知识，基于语音的识别准确率在CASIA[53]、[54]上低于90%，在eNTERFACE[8]、[44]上低于80%，在GEMEP[8]、[55]上低于50%。提出的模型包括基于时间维度注意力加权的LSTM模型(LSTM-T)、基于特征维度注意力加权的LSTM(LSTMF)、基于改进注意力机制遗忘门的LSTM-AT(LSTM-AT)、基于时间和特征维度注意力加权的LSTM(LSTM-TF)及其改进的遗忘门LSTM-AT(LSTM-TF-AT)，相关参数的设置如表II所示。输入的维度为[128，TimeStep，93]，其中128是批次大小，TimeStep是帧的数量，93是从语音中提取的特征的数量。为了比较时间复杂度，这些参数在所有没有筛选的语料库上都是相同的。只有学习率根据训练集上收敛的稳定性进行调整。CASIA的初始学习率为0.0001，eNTERFACE和GEMEP的初始学习率均为0.001。输出的维度由语料库中情绪的数量决定(CASIA和ENTERFACE有6个类别，而GEMEP有12个类别)。由于对LSTM的输出矩阵执行两次注意力加权运算，并且结果以[outputT，outputF]的形式连接作为后续全连接层的输入，因此全连接层的信元数加倍。表II中的全连接层参数[256,128]对应于基于传统LSTM的网络，而[512,128]是基于时间和特征维度的注意力机制的LSTM网络的参数设置。其他参数保持不变，以保证实验结果的有效性。

为了验证基于注意力机制的改进型遗忘门在保证系统性能的前提下能够有效地减少训练时间，本文对两组实验进行了对比实验。一个实验是在LSTM-AT和传统的LSTM模型之间，另一个实验是在LSTM-TF模型和LSTM-TF-AT之间。如图5所示，它描述了在相应语料库上相同训练步骤下的训练时间。这四个模型分别在CASIA、ENTERFACE和GEMEP上训练，CASIA为1200个历元，ENTERFACE为1000个历元，GEMEP为1500个历元。换句话说，这些模型在相同的数据库上执行相同的迭代。从图中可以看出，相同训练步数的每个模型的训练时间是不同的。与未修改的LSTM模型相比，基于关注门的LSTM模型的时间开销更小。比较这些语料库的训练时间，CASIA需要更多的时间，LSTM-AT与LSTM(3.5h)、LSTM-Tf-At与LSTM-Tf(1h)的训练时间差异大于eNTERFACE(0.8h和0.9h)和GEMEP(0.7h)。这表明训练时间越长，基于注意门的LSTM的优势越显著。在计算复杂性方面，GRU的训练时间少于提出的基于注意力的遗忘门(Lstmat)，如图5所示。然而，LSTM-AT在拥有比eNTERFACE和GEMEP更多样本的CASIA语料库上取得了比GRU更好的性能。虽然GRU具有较低的计算复杂度，但在大数据集上表现不佳。布里茨[56]推翻了类似的结论。在他的工作中，LSTM细胞的表现一直好于GRU细胞。因此，LSTM-AT在不牺牲性能的情况下降低了计算复杂度。在更新细胞状态方面，陶[40]将注意力机制应用于LSTM的细胞状态更新，关注细胞之间的信息。但本研究关注的是细胞内部，并对LSTM的遗忘门进行了改造。因此，遗忘门的计算不同于以前的LSTM和[40]中的计算。在所有提到的语料库上，LSTM-AT的性能与TAO相似。然而，后者需要更多的时间来训练，因为需要计算更多的先前的小区状态，如图5所示。在本研究中，LSTM-AT侧重于小区状态的内部计算，并综合考虑了计算复杂度和性能。为了定量分析基于关注门的LSTM模型的识别性能，分析了每个模型的最佳识别性能，如表三、表四和表五所示。基于关注门的LSTM减少了模型内部的矩阵运算，并且不会对所有数据库的UAR产生负面影响，事实上，有时甚至可以观察到性能的提高。与传统LSTM相比，LSTM-AT在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.6%、5.7%和2.5%，在CASIA、eNTERFACE和GEMEP语料库上的UAR分别提高了约0.8%、2.7%和3%。综上所述，LSTM-TF-at通过在时间和特征维度中引入注意机制，增强了与情绪相关的信息，显著提高了UAR，如表VI所示，该表描述了LSTM和改进后的P值之间的左尾T检验。然而，在CASIA语料库上，由于基线较高，0.052的P值对LSTM-AT的改善并不显著。此外，设计了基于注意力机制的遗忘门，在保证性能的前提下，降低了模型的计算复杂度，加快了模型的收敛速度，缩短了训练时间。将LSTM-Tf和LSTM-At结合起来，由于LSTM-Tf-At得到的P值较小，其意义更加明显。因此，该模型在情感分类中具有明显的优势。

由于本文使用的特征集是在OpenSMILE比较特征集[16]的基础上进行修改的，所以还对原始特征集和修改后的两个特征集进行了比较。然而，由于函数应用于LLD后的最终OpenSMILE比较特征集是一维特征向量，内部数据不具有时序关系，不适合作为提出的用于分类情感识别的LSTM模型的输入。因此，将OpenSMILE比较函数特征集与传统的机器学习算法支持向量机相结合，作为比较基线。结果见表七、表八和表九，分别对应于CASIA、eNTERFACE和GEMEP数据库。如表所示，该方法在CASIA、eNTERFACE和GEMEP上的UAR分别提高了5.4%、33.8%和17.0%。尤其是在CASIA和eNTERFACE上，每类情绪的回忆次数都有所增加，这表明了LSTM-TF-AT具有框架级特征的优势。

本文提出了一种改进的基于注意力的LSTM情感分类方法。在遗忘门和LSTM的输出中都引入了注意机制。改进后的门只与历史单元状态有关，与当前输入无关，降低了计算复杂度。实验表明，新的关注门也能提高识别率。此外，由于LSTM-TFat模型考虑了不同时间段的情感饱和度，并将注意力机制应用到LSTM输出的时间和特征维中，使得LSTM-TFat模型具有更好的性能，特别是在大数据集上。在小数据集上，LSTM-TF-AT和Mirsamadi的模型具有相似的UAR，但后者在算法复杂度上具有优势。此外，与经典的支持向量机分类器相比，在CASIA和eNTERFACE语料库上，LSTM-TF-AT对每类情感的召回率都有所提高。下一步的工作包括以下几个方面。首先，虽然该方法在分类任务上是有效的，但它对连续情感的分类有很大的意义。因此，有必要研究改进的LSTM用于连续情感识别。其次，该算法考虑了不同特征对情感的区分能力。因此，在我们未来的研究中，它将被用于特征过滤。此外，随着我们的研究，这种基于注意力的LSTM有望在更多的应用中进行。

### 引文

- 邓[12]使用了带有自动编码器的半监督学习和少量的情感标签数据来进行SER。
- Neumann[13]和Wöllmer[14]分别将卷积神经网络和LSTM应用于SER。Wöllmer[14]首次将LSTM应用于连续情感识别，为每句话提取了4843个特征作为LSTM的输入。在他的进一步工作[15]中，静态特征被用作双向LSTM(BLSTM)的输入，以预测口语的情感表达。
- Schmidhuber[19]针对递归神经网络(RNN)提出了长短期记忆(LSTM)结构。该方法为处理诸如语音的可变长度的时间序列提供了可行性。
- 在特征方面，由于全局统计忽略了语音的时间结构，语音中的时间信息没有得到充分利用[31]。为了增强特征，文[32]将时间窗口逐帧引入递归层，并进行了情感分类实验，取得了较好的效果。
- 在早期的工作中，SER[31]、[33]-[36]直接使用了帧级特征，通过帧之间的序列来保存时间信息。众所周知，LSTM擅长处理序列数据，因此帧级别的特征比统计特征更适合于它的输入。
- 除了LSTM的输入外，还需要改进常规LSTM的输出。在LSTM[21][23]的大多数应用中，选择LSTM中最后时刻的输出作为下一个模型的输入(因为其他模型只接受具有固定维度的输入，而LSTM的输出的维度与其实数维度不一致的输入相同)。这可能会导致在其他历史时刻对LSTM输出信息的不完全使用；具体地说，由于长期依赖的时间跨度不是无限的[37]，[38]，在最后时刻LSTM的累积信息是有损失的。
- 在情感分类任务中，Keren[32]在LSTM的输出中引入了卷积神经网络的汇集操作。
- Mirsamadi[39]提出了一种用于计算具有注意力参数向量的帧的权重的注意力机制。由于LSTM的存储容量，在上一次的输出中积累的信息最丰富。因此，最后一次的输出通常被认为是LSTM的最终输出(在本研究和[39]中，这种方法都可以识别情绪)。
- 注意机制不仅可以用来优化LSTM的输出，而且还可以用来更新存储单元。一些研究已经调查了如何更新存储单元。陶[40]将注意力机制应用于LSTM的细胞状态更新，该机制关注细胞之间的信息，并更多地考虑先前的细胞状态。
- 在计算方面，Bradbury[41]提出了用于神经序列建模的准递归神经网络，它允许输出依赖于序列中元素的整体顺序，并且在训练和测试时间比传统的LSTM具有更快的速度。
- Cho[42]提出了门控递归单元(GRU)，它将输入门和遗忘门组合成一个更新门，并将单元状态和隐层状态混合在一起，从而简化了LSTM的计算。
- Greff[22]引入了一种耦合的LSTM，它只使用一个门来控制历史小区状态和候选小区状态对当前小区状态的影响，从而简化了候选小区状态权重的计算。
- Schuller等人提出的ComParE openSMILE特征。在语音情感识别中应用最为广泛[12]，[44]，其中一个[17]基于低级描述符(LLD，例如零交叉率、均方根帧能量、基音频率和Mel频率倒谱系数1-12)的提取，添加它们的增量，并应用统计函数，具有6373个特征的维度。
- 如[7]，证实了语音的harmonic information信息可以用于区分CASIA和EMODB数据库中的情感类别。
- 研究[46]还表明，声门波（glottis waves）包含某些情绪信息。
- 在Hochreiter[19]提出的原始LSTM算法中，单元状态的更新算法与前一时刻的隐含层输出和当前时刻的输入有关。此外，他们增加了窥视孔连接，并将前一时刻的细胞状态作为参数来更新当前状态。

---
title: "An Attention Pooling based Representation Learning Method for Speech Emotion Recognition"
description: ""
citekey: liAttentionPoolingBased 2018
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:31:17
lastmod: 2023-04-11 11:24:45
---

> [!info] 论文信息
>1. Title：An Attention Pooling based Representation Learning Method for Speech Emotion Recognition
>2. Author：Pengcheng Li, Yan Song, Ian Vince McLoughlin, Wu Guo, Li-Rong Dai
>3. Entry：[Zotero link](zotero://select/items/@liAttentionPoolingBased2018) [URL link](http://dx.doi.org/10.21437/Interspeech.2018-1242) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2018_An Attention Pooling based Representation Learning Method for Speech Emotion.pdf>)
>4. Other：2018 -   International Speech Communication Association Hyderabad, India  -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 分析了时域和频域不同卷积核对应的不同感受域对语音情感识别的影响
- 新思想：将语音分成重叠片段增加数据量
- 新方法：使用 u 率压扩减少特征最大值与最小值之间的差距
- 引用了新的 Pooling 池化方法及其矩阵分解解释、注意力机制解释- 分析了时域和频域不同卷积核对应的不同感受域对语音情感识别的影响
- 新思想：将语音分成重叠片段增加数据量
- 新方法：使用 u 率压扩减少特征最大值与最小值之间的差距
- 引用了新的 Pooling 池化方法及其矩阵分解解释、注意力机制解释

## 摘要

> [!abstract] This paper proposes an attention pooling based representation learning method for speech emotion recognition (SER). The emotional representation is learned in an end-to-end fashion by applying a deep convolutional neural network (CNN) directly to spectrograms extracted from speech utterances. Motivated by the success of GoogleNet, two groups of filters with different shapes are designed to capture both temporal and frequency domain context information from the input spectrogram. The learned features are concatenated and fed into the subsequent convolutional layers. To learn the final emotional representation, a novel attention pooling method is further proposed. Compared with the existing pooling methods, such as max-pooling and average-pooling, the proposed attention pooling can effectively incorporate class-agnostic bottom-up, and class-specific top-down, attention maps. We conduct extensive evaluations on benchmark IEMOCAP data to assess the effectiveness of the proposed representation. Results demonstrate a recognition performance of 71.8% weighted accuracy (WA) and 68% unweighted accuracy (UA) over four emotions, which outperforms the state-of-the-art method by about 3% absolute for WA and 4% for UA.

> 针对语音情感识别，提出了一种基于注意力池化的表征学习方法。首先通过将深度卷积神经网络(CNN)直接应用于语音，从中提取特征谱图，以端到端的方式学习情感表征。并且受 GoogLeNet 的启发，设计了两组不同形状的滤波器，从输入的语谱图中同时捕获时间域和频域的上下文信息，并将最后学习到的特征通过级联，馈送到随后的卷积层中。为了学习最终的情感表征，本文进一步提出了一种新的注意力池池化方法。与现有的最大池化和平均池化方法相比，所提出的注意力池化方法能够有效地融合 class-agnostic bottom-up,class-specific top-down, 和 attention maps 方法。我们对基准 IEMOCAP 数据进行了广泛的评估，以评估模型提取的特征的有效性。实验结果表明，该方法对四种情感的识别率分别为 71.8%和 68%，比现有方法提高了约 3%和 4%。

> [!tip]
> Top-down 方法确定特征对对象的贡献度。
> Bottom-up 方法是将对象每个区域的一些重要特征提取出来，构成特征向量。
> class-agnostic 不可知类方法
> class-specifi 可知类方法

## 预处理

## 概述

## 结果

## 精读

简单的平均池化或最大池化可能不足以为需要分析更高阶统计量的复杂情感表达得出有效的表示。最近的一些工作显示了引入注意机制的表征学习的好处[12，13，10]。然而，它们通常以自下而上（bottomup）的方式从特征获得显著区域。为了解决这些问题，我们提出了一种基于注意力池的 SER 表征学习方法，如下图所示。CNN 被直接应用于语音语谱图，其中设计了两组不同形状的卷积滤波器来分别捕获时间域和频域的上下文信息。在 GoogLeNet[17]的启发下，学习的特征被进一步串联并馈送到后面的卷积层中。并且为了有效地进行表征学习，本文提出了一种新的注意力池方法。第一个注意图是以自下而上的方式得出的，而第二个注意图与情绪类型直接相关。

**数据处理**

数据扩充： 

CNN 的输入是从语音中提取的语谱图。给定一个语音，将其分割成 2 s 的片段进行训练，其中训练集的分割间隔 1 s，测试集的分割间隔 0.4s。

数据归类：

每个片段被分配到原语音对应的标签，最后会通过平均每个片段的分数来获得语音最终预测。

**频谱处理**

对每个 2 s 的语音片段，使用 40 ms 的汉明窗，以 10 ms 的帧移得到一系列的语音帧。然后对每个语音帧进行 DFT 变换（根据论文[11]结论，可知较高频率分辨率可以增强识别性能，因此选择参数 NFFT=1600），得到每帧的频谱。将所有帧的频谱进行合并得到对应语音片段的频谱图（帧数 x(NFFT/2+1)）。选择 0 到 4 kHz 的频率范围(去除了频率较高的一部分)，得到 199 x 400 的谱图，然后再经转置和 padding（补零）可以得到最终的 400×200的频谱矩阵。

频谱处理：

先将频谱矩阵中所有数据归一化到[-1, 1]，然后对其做一个μ为 255 的 u 率压扩。u 率压扩通过增大数值较小的元素，人为的改变数据的分布，减少矩阵中最大值与最小值之间的差距，在信号处理领域认为可以改善信噪比率而不需要增添更多的数据。

$$

F(x)=\operatorname{sgn}(x) \frac{\ln (1+\mu|x|)}{\ln (1+\mu)},-1 \leq x \leq 1

$$
![]({21}_An%20Attention%20Pooling%20based%20Representation%20Learning%20Method%20for%20Speech%20Emotion%20Recognition@liAttentionPoolingBased2018.assets/image-20220501184932.png)

**论文模型**

论文的模型如下图，输入声谱图 $(features × frames × 1）$ ，先通过有着不同的卷积核的两个 CNN，分别提取时域特征和频域特征，concat 后喂给后面的 CNN，在最后一层使用 attention pooling 的技术。

标准的 CNN 结构可以通过多个卷积层对输入的声谱图进行处理，其中第 k 个卷积层计算输出特征图 $C_{o u t_{k}}$

$$

C_{o u t_{k}}=b_{i}+\sum_{i=1}^{C_{i n}} \omega\left(C_{o u t_{k}}, i\right) * i n p u t\left(C_{i n_{i}}\right)

$$
其中 $b$ 是偏置， $ω$ 是卷积权重矩阵， $∗$ 是卷积运算。CNN 每个卷积层的 Filter，在各个通道对输入的特征图进行卷积，最终对每个通道求和得到输出特征图。

![]({21}_An%20Attention%20Pooling%20based%20Representation%20Learning%20Method%20for%20Speech%20Emotion%20Recognition@liAttentionPoolingBased2018.assets/image-20220501123211.png)

对于第一块的两个卷积层，可以通过改变卷积核的接受域，从原始语谱图中分别提取时频信息特征，并且分析感受野对系统识别性能的影响。如图所示，Conv 1 a 中的 Filter，频率轴上的卷积域设置为最小值 2，然后时间轴上的卷积域可以自定义设置。Conv 1 b 中的 Filter，时间轴上的卷积域设置为最小值 2，然后频率轴上的卷积域可以自定义设置。上述过程的目的是找出在限制时间/频率跨度的条件下，
在[17]中 INSTIMATION 模块的启发下，将得到的两种特征通过级联，作为后续的 4 个标准卷积层（3×3）的输入，然后最终通过 2×2 的最大 Poolling 提取时域和频域的高维特征信息。

通常，CNN 使用几个全连接(FC)层来计算目标标签上的概率分布。然而，如果直接将卷积后的特征处理后送入 FC 层可能会导致过度参数化，这会使得训练变得困难，特别是对于小规模数据集。因此使用一个能够对特征图进行下采样，并保持高维特征表征能力的池化函数非常重要，本文中引入的是 [全局Attentional 池化](obsidian://open?vault=%E7%AC%94%E8%AE%B0&file=emotion%2F3.%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95%2F22.Attentional%20Pooling%20for%20Action%20Recognition) [19]。在实现上，我们在 Conv 5 之后使用 1×1 卷积层来生成一个自上而下的注意图(大小为 H×W×4，对应于 4 个情感标签)，并使用另一个 1×1 卷积层来生成自下而上的注意图(大小为 H×W×1)。然后对自下而上的注意图应用 Softmax 操作。最后，将两种类型的注意图按元素相乘并进行空间平均，得到所有 4 种情绪类别的预测分数。

采用前人工作[10，11，23，24]的方法，采用留一策略（leave-oneout）进行 10 倍交叉验证。在每个训练过程中，使用 9 个说话人作为训练数据，剩下的一个用于测试数据。对于 CNN 的培训，我们使用了 PyTorch[25]深度学习框架。优化方法是标准随机梯度下降法(SGD)，小批量为 64。我们使用的 Nesterov momentum 为 0.9，权重衰减率为 0.0001。CNN 经过 50 多个 epoch 的训练。初始学习率为 0.05，在第 21、31 和 41 个 epoch 减少 10 倍。我们在每一卷积层之后采用 batch normalization[26]层，所用的激活函数是校正线性单元(REU)。优化目标函数采用交叉熵(CE)准则。

**filter 大小对系统的影响**

更改频率维度
![]({21}_An%20Attention%20Pooling%20based%20Representation%20Learning%20Method%20for%20Speech%20Emotion%20Recognition@liAttentionPoolingBased2018.assets/image-20220503115559.png)

随着高度(频率)的增加，Wa 和中性类的精度也增加。然而，这一趋势在大约 10(100 赫兹)的高度趋于平缓，超过这一高度后，感受场频率范围的进一步增加不会带来显着改善。

**更改时间维度**

![]({21}_An%20Attention%20Pooling%20based%20Representation%20Learning%20Method%20for%20Speech%20Emotion%20Recognition@liAttentionPoolingBased2018.assets/image-20220503115616.png)

当增加宽度(时间)时，Wa 在 8(80 ms)左右的宽度处先增加到一个峰值，然后逐渐减小。有趣的是，当接受域的时间跨度变大时，愤怒类的准确率迅速下降，这表明愤怒情绪是通过短时间表征来表达的。

在这些实验中，Happy 类的准确率最低，与 filter 形状无关。这可能表明，happy 需要从更高级别的表征中学习。

根据上文所述，本文 filter 的大小选用了 10×2 和 2×8。

### 引文

- 斯兰尼等人。应用具有 Mel 频率倒谱系数的高斯混合模型(GMM)进行 SER[1]。
- 在文献[2，3]中，提取韵律特征来训练支持向量机分类器。
- 在[6，7]中，采用了多阶段的方法，训练 DNN 和 CNN 网络进行前端特征提取，然后使用后端情感识别器，如支持向量机和极限学习机(ELM)。
- Trigorgis et.。Al[9]将原始音频送入 CNN 进行前端特征提取，然后使用长短期记忆(LSTM)层进行情绪表征学习。模型参数可与反向传播算法联合优化。
- 在[8，10]中，随着时间的推移应用最大合并运算以从显著区域获得话语表示。
- 诺伊曼·艾尔。Al[12]在最大池化操作后进一步引入了注意机制。
- 而 Mirsamadi et.。Al[13]将加权池化应用于 RNN 输出。
- 在[14，15]中，人们提出了 2 D 时频 LSTM 和 Grid-LSTM 来建模大规模自动语音识别(ASR)中随时间和频率的变化。然而，复杂的模型体系结构容易在诸如 IEMOCAP[16]这样的小规模数据集上过度拟合。
---
title: "Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information"
description: ""
citekey: zouSpeechEmotionRecognition2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:15
lastmod: 2023-04-11 11:31:26
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information
>2. Author：Heqing Zou, Yuke Si, Chen Chen, Deepu Rajan, Eng Siong Chng
>3. Entry：[Zotero link](zotero://select/items/@zouSpeechEmotionRecognition2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zou et al_2022_Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/vincent-zhq/ca-mser
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 利用注意力机制, 将多级特征融合

## 摘要

> [!abstract] Speech Emotion Recognition (SER) aims to help the machine to understand human's subjective emotion from only audio in-formation. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the pro-posed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub.

> 语音情感识别的目的是帮助机器仅从音频信息中理解人的主观情感。然而，提取和利用全面的深度音频信息仍然是一项具有挑战性的任务。在本文中，我们提出了一种端到端的语音情感识别系统，该系统使用了多级声学信息，并设计了一个新的共同注意模块。首先分别用CNN、BiLSTM和Wave2ve2提取多层声学信息，包括MFCC、频谱图和嵌入的高层声学信息。然后将提取的特征作为多通道输入，并利用所提出的共同注意机制进行融合。在IEMOCAP数据集上进行了实验，与两种不同的说话人无关交叉验证策略相比，我们的模型获得了与之相当的性能。

## 预处理

## 概述

## 结果

## 精读

情绪的自动识别有几个应用，如人机交互[1]和监视[2]。一些研究人员建议将声音信息和文本信息结合起来，并学习高级上下文信息来帮助做出最终的情绪预测[3]。然而，相应的转录并不总是可用于大多数情感识别应用。此外，当前的自动语音识别(ASR)系统生成的文本还会引入单词识别错误，干扰情感识别任务。与附加文本和视觉信号的多模式情感识别相比，仅从音频信号进行情感感知要容易得多，因为单音频数据更容易获得。

在本文中，我们介绍了三种不同的用于多级声学信息的编码器：用于频谱图的CNN，用于MFCC的BiLSTM和用于原始音频信号的基于变压器的声学提取网络Wav2ve2[11]。在所设计的协同注意模块中，我们利用从MFCC提取的有效信息和频谱特征对每一帧进行加权后进行优化，得到最终的W2E嵌入。我们将提取的三个特征连接在一起，并用最终融合的信息做出最终的情感预测。在广泛使用的IEMOCAP数据集上，该模型采用了去掉一位说话人和去掉一次会话的交叉验证策略，优于现有的竞争模型。

![]({26}_Speech%20Emotion%20Recognition%20with%20Co-Attention%20Based%20Multi-Level%20Acoustic%20Information@zouSpeechEmotionRecognition2022.assets/image-20220602154158.png)

我们将来自同一音频段的MFCC、语谱图和Wav2ve2分别表示为$x_{m}\in\mathbf{R}^{T_{m}\times D_{m}}$，$x_{s}\in\mathbf{R}^{T_{s}\times D_{s}}$和$x_{w}\in\mathbf{R}^{T_{w}\times 1}$。将提取的MFCC特征$x_{m}^{\prime}$和谱图特征$x_{s}^{\prime}$连接并与线性层变换，得到wav2vec输出$x_{w}^{\prime}$的不同帧的权重。与这些生成的权重相乘后，我们从原始的WAV2vec输出中获得最终的W2E向量。将最终得到的W2E$x_{w}^{\prime\prime}$与之前的MFCC特征$x_{m}^{\prime}$和语谱图特征$x_{s}^{\prime}$连接起来，用于最终的情绪识别任务。由MFCC和频谱图特征生成的Wav2vec帧的权重和最终特征组合分别表示为$x_{\text{COATT}}^{\prime}$和$x^{\prime}$。数据的目标用$y$表示，最终预测用$\hat{y}$表示。

这里我们将多层声学信息定义为基于人类知识的低层MFCC、基于深度学习的高层谱图和W2E的组合，从而涵盖了语音信号在频域和时间域的特征。MFCC序列由双向LSTM处理，该双向LSTM具有0.5的差错和平坦化。平坦化的矢量被输入到具有RELU的线性层，作为具有0.1的丢失值的激活函数，以获得

$$

x_{m}^{\prime}=f_{m}\left(B i \operatorname{TSSM}\left(x_{m}\right)\right), \text{ 

 where  } x_{m}^{\prime} \in \mathbf{R}^{D_{m}^{\prime}}
$$
首先用预先训练的Alexnet重塑谱图图像。对AlexNet提取的特征进行与MFCC特征类似的操作以获得

$$

x_{s}^{\prime}=f_{s}\left(A l e x N e t\left(x_{s}\right)\right)\text{ 

 where  } x_{s}^{\prime} \in \mathbf{R}^{D_{s}^{\prime}}
$$

原始音频片段被直接发送到相应的Wave2ve2处理器和Wav2Vec2模型，以获得原始Wav2Vec2输出

$$

x_{w}^{\prime}=W a v 2 \operatorname{Vec} 2\left(x_{w}\right), \text{  where  } x_{w}^{\prime} \in \mathbf{R}^{T_{w}^{\prime} \times D_{w}^{\prime}}

$$

考虑到三种声学信息源在最终情感预测中的作用相似，我们利用它们之间的相关性来指导特征自适应。通常，波2ve2输出的最后一帧或平均值被用来表示波2ve2特征。显然，我们在序列维度中丢失了一些有效信息。在这里，我们引入了一种共同注意模块，将W2E的不同帧组合在一起，并利用MFCC特征和谱图特征生成帧权重。

先，我们从MFCC特征$x_{m}^{\prime}$和谱图特征$x_{s}^{\prime}$创建一维矩阵，其变换层由

$$x_{a t}^{\prime}=f_{a t}\left(x_{m}^{\prime}\oplus x_{s}^{\prime}\right)$$

给出, 其中 $x_{att}^{\prime} \in \mathbf{R}^{1\times T_{w}^{\prime}}$

将WAV2ve2输出与先前生成的权重相乘，得到最终加权的WAV2ve2特征，如$$x_{w}^{\prime\prime}=\left(x_{a t t}^{\prime}\cdot x_{w}^{\prime}\right)^{T}$$其中$x_{w}^{\prime\prime}\in\mathbf{R}^{D_{w}^{\prime}}$。

将最终的MFCC、语谱图特征和加权的W2E连接起来，并将语音情感预测写为$$\hat{y}=f\left(x_{m}^{\prime}\oplus x_{s}^{\prime}\oplus x_{w}^{\prime\prime}\right)$$

### 引文

SER问题特征方法: 

- 绝大多数的涉及提取关键的音频特征，如Mel频率倒谱系数(MFCC)、恒定Q变换(CQT)或构造相应的频谱图来将问题作为图像分类问题来处理[4]。MFCC和语谱图都反映了语音信号在频域中的更多信息。MFCC可以看作是一种基于人类知识的低层特征。谱图可以通过深度神经网络进行进一步处理，获得高层信息。
- 将语音情感识别(SER)问题转化为多个声学信息的多层次融合问题是一种潜在的利用完整音频信息的有效方法。


针对不同的声学信号，设计具有不同体系结构细节的各种编码器，如用于频谱图的CNN和用于MFCC的CNN/LSTM。

在[3]中，利用一系列具有不同核大小的CNN来挖掘声学信息。

一些方法提出引入网络的组合来提取声学信息，例如[5]结合LSTM和门控多特征单元(GMU)来提取静态和动态语音信号。

在[6]中，Gao等人提出了一种领域对抗性自动编码器，利用预先训练的谱图信息来提取区分表示。从不同来源提取特征需要对应的来源特定的神经网络。

已经提出了不同类型的注意机制来处理提取的特征，如常用的自我注意[5，7]和跨通道注意[8]。

对于输入组合比较复杂的模型，引入了新的注意机制。[9]融合两个通道，然后使用所提出的注意通道-跳跃机制将结果与另一个通道合并。

在[10]中，设计了一种基于注意力的分层时间卷积网络来融合谱图图像的通道间和通道内特征。

## 摘录

---
title: "Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information"
description: ""
citekey: zouSpeechEmotionRecognition2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:15
lastmod: 2023-04-11 11:31:26
---

> [!info] 论文信息
>1. Title：Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information
>2. Author：Heqing Zou, Yuke Si, Chen Chen, Deepu Rajan, Eng Siong Chng
>3. Entry：[Zotero link](zotero://select/items/@zouSpeechEmotionRecognition2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Zou et al_2022_Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/vincent-zhq/ca-mser
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 利用注意力机制, 将多级特征融合

## 摘要

> [!abstract] Speech Emotion Recognition (SER) aims to help the machine to understand human's subjective emotion from only audio in-formation. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the pro-posed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub.

> 语音情感识别的目的是帮助机器仅从音频信息中理解人的主观情感。然而，提取和利用全面的深度音频信息仍然是一项具有挑战性的任务。在本文中，我们提出了一种端到端的语音情感识别系统，该系统使用了多级声学信息，并设计了一个新的共同注意模块。首先分别用CNN、BiLSTM和Wave2ve2提取多层声学信息，包括MFCC、频谱图和嵌入的高层声学信息。然后将提取的特征作为多通道输入，并利用所提出的共同注意机制进行融合。在IEMOCAP数据集上进行了实验，与两种不同的说话人无关交叉验证策略相比，我们的模型获得了与之相当的性能。

## 预处理

## 概述

## 结果

## 精读

情绪的自动识别有几个应用，如人机交互[1]和监视[2]。一些研究人员建议将声音信息和文本信息结合起来，并学习高级上下文信息来帮助做出最终的情绪预测[3]。然而，相应的转录并不总是可用于大多数情感识别应用。此外，当前的自动语音识别(ASR)系统生成的文本还会引入单词识别错误，干扰情感识别任务。与附加文本和视觉信号的多模式情感识别相比，仅从音频信号进行情感感知要容易得多，因为单音频数据更容易获得。

在本文中，我们介绍了三种不同的用于多级声学信息的编码器：用于频谱图的CNN，用于MFCC的BiLSTM和用于原始音频信号的基于变压器的声学提取网络Wav2ve2[11]。在所设计的协同注意模块中，我们利用从MFCC提取的有效信息和频谱特征对每一帧进行加权后进行优化，得到最终的W2E嵌入。我们将提取的三个特征连接在一起，并用最终融合的信息做出最终的情感预测。在广泛使用的IEMOCAP数据集上，该模型采用了去掉一位说话人和去掉一次会话的交叉验证策略，优于现有的竞争模型。

![]({26}_Speech%20Emotion%20Recognition%20with%20Co-Attention%20Based%20Multi-Level%20Acoustic%20Information@zouSpeechEmotionRecognition2022.assets/image-20220602154158.png)

我们将来自同一音频段的MFCC、语谱图和Wav2ve2分别表示为$x_{m}\in\mathbf{R}^{T_{m}\times D_{m}}$，$x_{s}\in\mathbf{R}^{T_{s}\times D_{s}}$和$x_{w}\in\mathbf{R}^{T_{w}\times 1}$。将提取的MFCC特征$x_{m}^{\prime}$和谱图特征$x_{s}^{\prime}$连接并与线性层变换，得到wav2vec输出$x_{w}^{\prime}$的不同帧的权重。与这些生成的权重相乘后，我们从原始的WAV2vec输出中获得最终的W2E向量。将最终得到的W2E$x_{w}^{\prime\prime}$与之前的MFCC特征$x_{m}^{\prime}$和语谱图特征$x_{s}^{\prime}$连接起来，用于最终的情绪识别任务。由MFCC和频谱图特征生成的Wav2vec帧的权重和最终特征组合分别表示为$x_{\text{COATT}}^{\prime}$和$x^{\prime}$。数据的目标用$y$表示，最终预测用$\hat{y}$表示。

这里我们将多层声学信息定义为基于人类知识的低层MFCC、基于深度学习的高层谱图和W2E的组合，从而涵盖了语音信号在频域和时间域的特征。MFCC序列由双向LSTM处理，该双向LSTM具有0.5的差错和平坦化。平坦化的矢量被输入到具有RELU的线性层，作为具有0.1的丢失值的激活函数，以获得

$$

x_{m}^{\prime}=f_{m}\left(B i \operatorname{TSSM}\left(x_{m}\right)\right), \text{ 

 where  } x_{m}^{\prime} \in \mathbf{R}^{D_{m}^{\prime}}
$$
首先用预先训练的Alexnet重塑谱图图像。对AlexNet提取的特征进行与MFCC特征类似的操作以获得

$$

x_{s}^{\prime}=f_{s}\left(A l e x N e t\left(x_{s}\right)\right)\text{ 

 where  } x_{s}^{\prime} \in \mathbf{R}^{D_{s}^{\prime}}
$$

原始音频片段被直接发送到相应的Wave2ve2处理器和Wav2Vec2模型，以获得原始Wav2Vec2输出

$$

x_{w}^{\prime}=W a v 2 \operatorname{Vec} 2\left(x_{w}\right), \text{  where  } x_{w}^{\prime} \in \mathbf{R}^{T_{w}^{\prime} \times D_{w}^{\prime}}

$$

考虑到三种声学信息源在最终情感预测中的作用相似，我们利用它们之间的相关性来指导特征自适应。通常，波2ve2输出的最后一帧或平均值被用来表示波2ve2特征。显然，我们在序列维度中丢失了一些有效信息。在这里，我们引入了一种共同注意模块，将W2E的不同帧组合在一起，并利用MFCC特征和谱图特征生成帧权重。

先，我们从MFCC特征$x_{m}^{\prime}$和谱图特征$x_{s}^{\prime}$创建一维矩阵，其变换层由

$$x_{a t}^{\prime}=f_{a t}\left(x_{m}^{\prime}\oplus x_{s}^{\prime}\right)$$

给出, 其中 $x_{att}^{\prime} \in \mathbf{R}^{1\times T_{w}^{\prime}}$

将WAV2ve2输出与先前生成的权重相乘，得到最终加权的WAV2ve2特征，如$$x_{w}^{\prime\prime}=\left(x_{a t t}^{\prime}\cdot x_{w}^{\prime}\right)^{T}$$其中$x_{w}^{\prime\prime}\in\mathbf{R}^{D_{w}^{\prime}}$。

将最终的MFCC、语谱图特征和加权的W2E连接起来，并将语音情感预测写为$$\hat{y}=f\left(x_{m}^{\prime}\oplus x_{s}^{\prime}\oplus x_{w}^{\prime\prime}\right)$$

### 引文

SER问题特征方法: 

- 绝大多数的涉及提取关键的音频特征，如Mel频率倒谱系数(MFCC)、恒定Q变换(CQT)或构造相应的频谱图来将问题作为图像分类问题来处理[4]。MFCC和语谱图都反映了语音信号在频域中的更多信息。MFCC可以看作是一种基于人类知识的低层特征。谱图可以通过深度神经网络进行进一步处理，获得高层信息。
- 将语音情感识别(SER)问题转化为多个声学信息的多层次融合问题是一种潜在的利用完整音频信息的有效方法。


针对不同的声学信号，设计具有不同体系结构细节的各种编码器，如用于频谱图的CNN和用于MFCC的CNN/LSTM。

在[3]中，利用一系列具有不同核大小的CNN来挖掘声学信息。

一些方法提出引入网络的组合来提取声学信息，例如[5]结合LSTM和门控多特征单元(GMU)来提取静态和动态语音信号。

在[6]中，Gao等人提出了一种领域对抗性自动编码器，利用预先训练的谱图信息来提取区分表示。从不同来源提取特征需要对应的来源特定的神经网络。

已经提出了不同类型的注意机制来处理提取的特征，如常用的自我注意[5，7]和跨通道注意[8]。

对于输入组合比较复杂的模型，引入了新的注意机制。[9]融合两个通道，然后使用所提出的注意通道-跳跃机制将结果与另一个通道合并。

在[10]中，设计了一种基于注意力的分层时间卷积网络来融合谱图图像的通道间和通道内特征。

---
title: "Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?"
description: ""
citekey: rajanCrossAttentionPreferableSelfAttention2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:24
lastmod: 2023-04-11 11:52:32
---

> [!info] 论文信息
>1. Title：Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?
>2. Author：Vandana Rajan, Alessio Brutti, Andrea Cavallaro
>3. Entry：[Zotero link](zotero://select/items/@rajanCrossAttentionPreferableSelfAttention2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rajan et al_2022_Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/smartcameras/SelfCrossAttn
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 讨论了注意力机制对不同模态间特征融合的影响
- 通过将两个不同模态的输入分别分配给注意力机制的Q和KV，实现多模态运算之间的数据交互。
- 结论：多模态特征融合过程中，使用自注意力机制和使用跨模态交互注意力机制都有用，但是效果类似。

## 摘要

> [!abstract] Humans express their emotions via facial expressions, voice intonation and word choices. To infer the nature of the underlying emotion, recognition models may use a single modality, such as vision, audio, and text, or a combination of modalities. Generally, models that fuse complementary information from multiple modalities outperform their uni-modal counterparts. However, a successful model that fuses modalities requires components that can effectively aggregate task-relevant information from each modality. As cross-modal attention is seen as an effective mechanism for multi-modal fusion, in this paper we quantify the gain that such a mechanism brings compared to the corresponding self-attention mechanism. To this end, we implement and compare a cross-attention and a self-attention model. In addition to attention, each model uses convolutional layers for local feature extraction and recurrent layers for global sequential modelling. We compare the models using different modality combinations for a 7-class emotion classification task using the IEMOCAP dataset. Experimental results indicate that albeit both models improve upon the state-of-the-art in terms of weighted and unweighted accuracy for tri- and bi-modal configurations, their performance is generally statistically comparable. The code to replicate the experiments is available at https://github.com/smartcameras/SelfCrossAttn

> 人类通过面部表情、语调和用词来表达自己的情感。为了推断潜在情绪的性质，识别模型可以使用单个通道，例如视觉、音频和文本，或通道的组合。一般来说，融合来自多个通道的互补信息的模型表现优于单通道对应的模型。然而，融合通道的成功模型需要能够有效地聚合来自每个通道的与任务相关的信息的组件。由于跨通道注意被认为是一种有效的多通道融合机制，本文量化了这种机制与相应的自我注意机制相比所带来的收益。为此，我们实施并比较了交叉注意模型和自我注意模型。除了注意力外，每个模型都使用卷积层来进行局部特征提取，并使用递归层来进行全局顺序建模。我们使用IEMOCAP数据集对7类情绪分类任务中使用不同通道组合的模型进行了比较。实验结果表明，尽管这两个模型在加权和未加权精度方面都优于最先进的三模和双模配置，但它们的性能通常在统计上是可比较的。

## 预处理

## 概述

## 结果

在本节中，我们将讨论使用交叉自我注意模型进行7类双模态和三模态情绪识别的数据集和结果。

我们还讨论了与两种模型的最新方法和具有附加模型配置的实验的比较。

数据集设置

交互式情绪二元运动捕捉（IEMOCAP）[18]数据集，有些人通过合并不同的类（快乐和兴奋，愤怒和沮丧）将其用于4级分类[13]，而其他人[3,7,19,16]则进行7级分类。我们跟随后者，使用与[3,7,19]相同的数据集分区和功能。最终的数据集总共包含7487个话语（1103个愤怒，1041个兴奋，595个快乐，1084个悲伤，1849个沮丧，107个惊喜和1708个中立）。类规模小于100个话语的（恐惧，厌恶，其他）被消除[3]。

训练设置

5折交叉验证来评估模型性能。每折中的数据分为训练，开发和测试集（8:0.5:1.5）。每折叠训练和评估模型10次（具有10个不同的随机种子），并且根据加权准确度（WA）和未加权准确度（UWA）度量来评估性能。

对于音频模态，提取40维 MFCC特征（帧大小为25ms，帧移速率为10ms，窗类型为汉明窗）及其其一阶和二阶导数连接以获得120维的最终声学特征维度。通过减去均值，缩放到单位方差，来标准化音频特征。

对于视觉模态，裁剪后的说话人面部图像被馈送到ResNet101[20]中，以3Hz的帧移获得2048维的特征。

对于文本模态，话语中的每个单词都由300维的GloVe[10]嵌入表征。

请注意，模态以不同的速率采样，音频，视觉和文本模态的最大序列长度分别设置为1000,32和128。这些模型是使用PyTorch实现的[21]。



三模态模型的双模态和单模态版本是通过删除与未使用的模态/模态相对应的组件来创建的。


我们使用Adam[22]Optimizer，学习率为0.001。当验证损失连续10个时期停止减少时，学习率降低0.1倍。批量大小为32，所有模型都使用分类交叉熵损失进行训练。音频和视觉编码器每个都包含一个1D卷积层。内核大小和步长都设置为1。音频卷积层的输入和输出通道数分别为1000和500，而视觉则分别为32和25。所有3种模式的bi-GRU层数为1。每个bi-GRU层中隐藏的神经元数量为60个。所有MHA模块中的注意头数量为6，并且应用0.1的辍学率以减少过度拟合。第一和第二完全连接的输出层中的神经元数量分别为60（与bi-GRU神经元的数量相同）和7（输出类的数量）。

当UWA在连续10个时期的验证集中没有改善时停止训练，并且使用具有最佳验证UWA的模型进行测试。

所有参数都是根据验证集的性能选择的。表1显示了自我和交叉注意模型在7类单模态，双模态和三模态情绪识别任务中的表现。我们报告了每个模型在50次运行中获得的平均值和标准偏差（5折×10次重复）。我们还应用了具有零假设的two-tailed t-test，即自我和交叉注意模型的准确度值具有相同的平均（预期）值。单模态性能的比较表明，文本优于视觉和音频模式。这个结果与以前的工作一致[13,19]。由于使用跨模态模型无法进行单模态性能评估，因此我们使用单模态版本的自我关注模型报告结果。在双模态模型中，视觉和文本模态的组合为两种模型提供了最佳性能。这些结果与以前的工作一致[7,19]。总体而言，这两种模型都为双模态和三模态情况提供了可比的性能。仅对于T+A（文本和音频）和T+V+A（文本，视频和音频）的WA，自我注意显着优于交叉注意（P值<.05）。

基线模型比较

将三模态模型与AMH[3]进行比较，AMH[3]是当前最先进的模型，它使用单模态GRU层和迭代注意机制的组合1。请注意，就WA和UWA而言，自我关注模型分别超过AMH的表现4.0和2.5个百分点（pp）。交叉注意模型的类似数字是3.1 pp和1.9 pp。

我们还与MDRE[19]进行了比较，MDRE[19]使用递归层对单模态信号进行建模，然后使用完全连接的层进行聚合和分类。与MDRE相比，自我和交叉注意模型以及AMH的更好表现可归因于注意机制的有效性。

对于双模态模型，我们与称为MHA[7]和MDRE的AMH双模态版本进行比较。同样，这两种型号在所有3种双模态情况下都优于MHA和MDRE。请注意，我们通过消除三模态模型而不是针对单个双模态情况进行微调来获得双模态结果。此外，AMH，MHA和MDRE除了MFCC特征外还使用韵律特征进行音频处理，而我们仅使用MFCC特征。通过[16]（0.560 WA和0.612 UWA）获得文本+音频案例的最新结果，该结果显着高于双模态T+A（文本和音频）结果。我们假设有两个原因：（1）与[16]不同，双模态模型没有针对双模态情况进行微调；（2） [16]使用包含其他参数的变压器编码器，这些参数可能有助于学习更复杂的模态间关系，而我们仅使用多头注意机制。尽管如此，这两种模型都改善了AMH的最新三模式结果。图3显示了自我和交叉注意模型的混淆矩阵。对于这两种模式，我们可以观察到，愤怒和沮丧的类经常彼此混淆，类快乐与兴奋混淆（这两个类本质上是相似的）。两种模型在训练中的表现都很差，这可以归因于它在数据集中具有最小的样本量。这些观察结果与以前的文献一致[3]。除了两种描述的模型配置外，我们还尝试了三模态模型的不同变化。我们从两个模型中删除了统计池层以评估其重要性。来自所有时间平均模块（参见图1和2）的输出被连接并传递给分类器模块。这些模型在表2中显示为Cross-noSP和Self-noSP。我们可以做两个观察。首先，即使在消除统计池之后，自我注意模型也优于交叉注意模型（WA的P值<.05）。其次，如果没有统计池层，两个模型的性能都会下降。我们还评估了通过合并自我和交叉关注模型（cross+self）创建的组合模型的性能。将来自两个模型的统计池输出连接并馈送到公共分类器模块。我们可以看到，表现类似于自我关注模型。这可能表明，与自我关注模型相比，交叉关注模型不会提供任何额外的相关信息。

被交叉注意机制在多通道融合中的流行所吸引，我们使用IEMOCAP数据集对基于自我注意和交叉注意的模型进行了三通道和双通道七类分类的比较。结果表明，两种模型的结果没有显著差异。因此，在我们使用的数据集和体系结构的背景下，我们得出结论，在多通道情绪识别中，交叉注意并不优于自我注意。此外，自我注意模型和交叉注意模型都提高了识别任务的最新水平。未来的工作包括研究交叉注意和自我注意模型对其他多通道任务和通道的有效性。致谢。我们感谢[3，7，19]的作者提供了经过处理和分区的IEMOCAP数据集。我们承认使用了ESPRC资助的Tier 2设施Jade。

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602164542.png)

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602164602.png)

## 精读

情绪识别(Emotion recognition, ER)模型使用一种或多种模式，如音频(语言和副语言)、图像(面部表情和肢体姿势)和文本(语言)来推断潜在的情感类别[1]。多模式模型被设计为有效地融合来自不同模式的相关信息，并且通常优于单模式模型[2，3]。

ER模型可以使用原始信号(语音或面部图像)[4、5、6]或手工制作的特征[3、7]作为输入。

- 常用的语音特征是低级描述符，例如共振峰、基音、对数能量、过零率和梅尔倒谱系数(MFCC)[3，7]。
- 面部表情可以由基于始终存在于面部的实体(如眼睛、嘴巴和眉毛)的固定特征和/或基于临时实体(如皱纹和凸起)的临时特征来表示[8]。
- 可以使用单词嵌入算法(如word2vec[9]或Glove[10])将标记化的单词映射为语言特征。


基于深度神经网络(DNN)的ER模型可以包含卷积层，以从输入层和递归层提取局部任务相关分量，从而促进全局序贯建模[4，5]。DNN体系结构中集成的注意机制[11]鼓励ER模型关注与任务相关的时刻[3，6]。注意机制的一般目的是为序列中的不同时间步长提供不同水平的权重。注意机制有两种，即自我(或通道内)注意和跨(或通道间)注意。自我注意机制通过关联相同序列的不同位置来计算单模态序列的表示[6，12]。跨通道注意机制使用一种通道来估计每个位置在另一通道中的相关性[13]。例如，可以使用2个递归层之间的自我注意机制来强调输入语音信号中的任务相关时间步骤[6]，而迭代多跳交叉注意机制可以从利用门控递归单元(GRU)层获得的多模式特征中选择和聚集信息[3，7]。包含多头注意(MHA)模块的Transformers[14]在建模单模和多模情感数据[15，13，16，17]中也变得流行起来。跨模式转换器使用交叉注意来计算使用不同源模式的目标模式表征中的每个时间步长的相关性[13，17]。交叉和自我注意变压器的串联[13，17]或并行[16]组合旨在捕获用于多模式融合的跨模式和模式内关系。


考虑到对基于自我注意和交叉注意的变压器编码者结合模型的兴趣[13，16，17]，我们进行了第一个研究，比较了两种类型的注意机制(不包括其他变压器组件)。为了了解两种类型的注意机制之间的差异，我们广泛地比较了一个只基于交叉注意的模型和一个只基于自我注意的双模式和三模式组合的模型。我们在IEMOCAP[18]的7类情绪分类数据集上对这两种模型进行了比较，结果表明，交叉注意模型并不优于自我注意模型。然而，这两个模型都在加权和未加权准确度方面改善了三通道和双通道情绪识别任务的最新结果。

自我注意和交叉注意模型首先使用特定于模态的编码器处理单个模态的数据。然后，经过编码后的特征分别被馈送到多头自注意力模块或多头交叉注意力模块(Multi-Head Attention, MHA)[14]中。在每个注意力模块的输出端生成话语片段的全局表征作为时间平均值。然后，将得到的特征连接起来，并使用统计汇聚层获得它们的平均值和标准差。然后，将串联的平均和标准偏差向量馈送到全连接层。最终的情感类预测通过Softmax运算获得。具体说明如下：

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602181045.png)

1. 设$X_{a} \in \mathbb{R}^{t_{a}}\times d_{a}$是对应于一个音频片段的音频特征，其中$t_{a}$是序列长度，$d_{a}$是特征维度。音频编码器由一个$1 \mathbb{D}$卷积层和一个双向GRU组成。

- 卷积层通过查找与任务相关的特征来优化输入要素序列，其操作如下：$$X_{a}^{\prime}\left(t^{\prime}\right)=b\left(t^{\prime}\right)+\sum_{k=0}^{t_{a}-1}\left(W\left(t^{\prime}，K\right)*X_{a}(K)\right)$$其中$X_{a}^{\prime}\in\mathbb{R}^{t_{a}^{\prime}\times d_{a}^{\prime}}$是长度为$t_{a}^{\prime}$的输出，维度为$d_{a}^{\prime}$的输出，$t^{\prime}\in\left[0, t_{a}^{\prime}-1\right]$，\*是卷积运算符，$W$是权重，$b$是与层关联的偏差。因此，卷积层修改序列长度以及特征尺寸。

- 双向GRU层模拟特征随时间变化的上下文相互依赖关系。对于序列中的每个元素，Bi-GRU层计算以下函数：
$$ \left\{\begin{array}{l} r_{t}=\sigma\left(W_{i r} X_{a}^{\prime}(t)+b_{i r}+W_{h r} h_{t-1}+b_{h r}\right) \\ z_{t}=\sigma\left(W_{i z} X_{a}^{\prime}(t)+b_{i z}+W_{h z} h_{t-1}+b_{h z}\right) \\ n_{t}=\phi_{h}\left(W_{i n} X_{a}^{\prime}(t)+b_{i n}+r_{t} \odot\left(W_{h n} h_{t-1}+b_{h n}\right)\right) \\ h_{t}=\left(1-z_{t}\right) \odot n_{t}+z_{t} \odot\left(h_{t-1}\right) \end{array}\right.
$$
其中$h_{t}$和$h_{t-1}$是时间$t$和$t-1$的隐藏状态向量，$X_{a}^{\prime}(T)$是时间$t$的输入。$R_{t}、z_{t}$和$n_{t}$是重置门、更新门和新门，$W$和$b$是相应的权重和偏差，$\sigma$和$\Phi_{h}$是Sigmoid和双曲正切函数，$\odot$是Hadamard乘积。在bi-GRU的输出端，每个时间步长的前向和后向隐藏状态被串联，并且改进的音频特征可以表示为$e_{a}\in\mathbb{R}^{t_{a}^{\prime}\times d^{\prime\prime}}$，其中$d^{\prime\prime}$是GRU中隐藏神经元数目的两倍。

2. 与音频类似，视觉编码器由一个$1\mathbb{D}$卷积层和一个双GRU层组成。如果$X_{v}\in\mathbb{R}^{t_{v}\times d_{v}}$表示与话语对应的视觉特征，则在视觉编码器的输出处，这些特征被细化为$e_{v}\in\mathbb{R}^{t_{v}^{\prime}\times d^{\prime\prime}}$。

3. 对于文本通道，编码器仅由一个bi-GRU层组成。文本编码器的输入和输出可以分别用$X_{l}\in\mathbb{R}^{t_{l}\times d_{l}}$和$e_{l}\in\mathbb{R}^{t_{l}\times d^{\prime\prime}}$表示。

- 我们使用多头注意力模块[14]进行自我和交叉注意力建模。

多头注意力模块由多个注意操作组成，以捕获对序列的更丰富的解释。每个多头注意力模块需要3个输入，即查询$(Q)$、Кey$(K)$和值$(V)$，每个输入首先使用线性映射层, 投影到$H$个不同的子空间，即令多头注意力模块的Head数目为$H$。每个子空间的投影特征$h \in\{0, \ldots, H-1\}$ ，可计算为

$$
\begin{aligned}

Q_{h} &=W_{h}^{Q} e_{m} \\

K_{h} &=W_{h}^{K} e_{m} \\

V_{h} &=W_{h}^{V} e_{m}

\end{aligned}
$$

其中，$m \in\{a, v, l\}$表示哪种模态。在这些子空间中，通过scaled dot-product attention方法对映射特征进行运算。对子空间$h$，运算过程如下所示

$$
\operatorname{Att}_{h}\left(Q_{h}, K_{h}, V_{h}\right)=\operatorname{Softmax}\left(\frac{Q_{h} K_{h}^{T}}{\sqrt{d_{k}}}\right) V_{h}
$$


其中$A t t_{h}(\cdot)$和$d_{k}$分别指子空间$h$中的中的注意力运算和特征维度。多头注意模块的所有$H$个输出，被串联并通过线性映射层以获得多头注意模块的最终输出。

在交叉注意模型中，源模态使用源自身的输入数据通过线性映射，得到$K$和$V$向量矩阵，然后使用其他模态的数据，再得到$Q$向量矩阵(见图1)。这种方法背后的直觉是通过使源模态适应目标模态来实现跨模态交互[13]。

将音频作为目标通道，而视觉作为源通道举例：

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602200508.png)

将精炼的音频特征($e_{a}\in\mathbb{R}^{t_{a}^{\prime}\times d^{\prime\prime}}$)通过线性映射转换为$Q$，然后再将精炼的视频特征($e_{v}\in\mathbb{R}^{t_{v}^{\prime}\times d^{\prime\prime}}$)通过线性映射转换为$K$和$V$。跨模态的多头注意力模块会将视频映射到音频模态，并输出与音频相适应的视觉特征$e_{a v}^{w}\in\mathbb{R}^{t_{a}^{\prime}\times d^{\prime\prime}}$。注意，交叉注意加权输出的序列长度与目标通道音频相同。

对于3个模态，总共有6个源-目标模态的组合，因此我们使用了6个MHA模块。

- 而在自我注意模型的情况下，同一模态的输入序列才会被用作计算$Q、K$和$V$(见图2)，这有助于捕获每个模态中的模态内交互特征。

统计池化方法statistical pooling，在交叉注意模型中，是跨6个模态组合序列的时间平均的级联计算的，而对于自我注意模型，是跨所有3个模态的自注意序列的时间平均的级联计算的。
这两种模型的分类器都是：
$$
\hat{y}=\operatorname{Softmax}\left(f_{\theta_{2}}\left(f_{\theta_{1}}([\mu \| \sigma])\right)\right)
$$
其中，$\mu$和$\sigma$是统计汇聚层输出的均值和标准差，$||$表示串联操作，$f_{\theta_{1}}$和$f_{\theta_{2}}$分别表示参数为$\theta_{1}$和$\theta_{2}$的两个全连接层，$\hat{y}$表示情绪预测的onehot向量。

### 引文

## 摘录

---
title: "Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?"
description: ""
citekey: rajanCrossAttentionPreferableSelfAttention2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:24
lastmod: 2023-04-11 11:52:32
---

> [!info] 论文信息
>1. Title：Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?
>2. Author：Vandana Rajan, Alessio Brutti, Andrea Cavallaro
>3. Entry：[Zotero link](zotero://select/items/@rajanCrossAttentionPreferableSelfAttention2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Rajan et al_2022_Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/smartcameras/SelfCrossAttn
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 讨论了注意力机制对不同模态间特征融合的影响
- 通过将两个不同模态的输入分别分配给注意力机制的Q和KV，实现多模态运算之间的数据交互。
- 结论：多模态特征融合过程中，使用自注意力机制和使用跨模态交互注意力机制都有用，但是效果类似。

## 摘要

> [!abstract] Humans express their emotions via facial expressions, voice intonation and word choices. To infer the nature of the underlying emotion, recognition models may use a single modality, such as vision, audio, and text, or a combination of modalities. Generally, models that fuse complementary information from multiple modalities outperform their uni-modal counterparts. However, a successful model that fuses modalities requires components that can effectively aggregate task-relevant information from each modality. As cross-modal attention is seen as an effective mechanism for multi-modal fusion, in this paper we quantify the gain that such a mechanism brings compared to the corresponding self-attention mechanism. To this end, we implement and compare a cross-attention and a self-attention model. In addition to attention, each model uses convolutional layers for local feature extraction and recurrent layers for global sequential modelling. We compare the models using different modality combinations for a 7-class emotion classification task using the IEMOCAP dataset. Experimental results indicate that albeit both models improve upon the state-of-the-art in terms of weighted and unweighted accuracy for tri- and bi-modal configurations, their performance is generally statistically comparable. The code to replicate the experiments is available at https://github.com/smartcameras/SelfCrossAttn

> 人类通过面部表情、语调和用词来表达自己的情感。为了推断潜在情绪的性质，识别模型可以使用单个通道，例如视觉、音频和文本，或通道的组合。一般来说，融合来自多个通道的互补信息的模型表现优于单通道对应的模型。然而，融合通道的成功模型需要能够有效地聚合来自每个通道的与任务相关的信息的组件。由于跨通道注意被认为是一种有效的多通道融合机制，本文量化了这种机制与相应的自我注意机制相比所带来的收益。为此，我们实施并比较了交叉注意模型和自我注意模型。除了注意力外，每个模型都使用卷积层来进行局部特征提取，并使用递归层来进行全局顺序建模。我们使用IEMOCAP数据集对7类情绪分类任务中使用不同通道组合的模型进行了比较。实验结果表明，尽管这两个模型在加权和未加权精度方面都优于最先进的三模和双模配置，但它们的性能通常在统计上是可比较的。

## 预处理

## 概述

## 结果

在本节中，我们将讨论使用交叉自我注意模型进行7类双模态和三模态情绪识别的数据集和结果。

我们还讨论了与两种模型的最新方法和具有附加模型配置的实验的比较。

数据集设置

交互式情绪二元运动捕捉（IEMOCAP）[18]数据集，有些人通过合并不同的类（快乐和兴奋，愤怒和沮丧）将其用于4级分类[13]，而其他人[3,7,19,16]则进行7级分类。我们跟随后者，使用与[3,7,19]相同的数据集分区和功能。最终的数据集总共包含7487个话语（1103个愤怒，1041个兴奋，595个快乐，1084个悲伤，1849个沮丧，107个惊喜和1708个中立）。类规模小于100个话语的（恐惧，厌恶，其他）被消除[3]。

训练设置

5折交叉验证来评估模型性能。每折中的数据分为训练，开发和测试集（8:0.5:1.5）。每折叠训练和评估模型10次（具有10个不同的随机种子），并且根据加权准确度（WA）和未加权准确度（UWA）度量来评估性能。

对于音频模态，提取40维 MFCC特征（帧大小为25ms，帧移速率为10ms，窗类型为汉明窗）及其其一阶和二阶导数连接以获得120维的最终声学特征维度。通过减去均值，缩放到单位方差，来标准化音频特征。

对于视觉模态，裁剪后的说话人面部图像被馈送到ResNet101[20]中，以3Hz的帧移获得2048维的特征。

对于文本模态，话语中的每个单词都由300维的GloVe[10]嵌入表征。

请注意，模态以不同的速率采样，音频，视觉和文本模态的最大序列长度分别设置为1000,32和128。这些模型是使用PyTorch实现的[21]。



三模态模型的双模态和单模态版本是通过删除与未使用的模态/模态相对应的组件来创建的。


我们使用Adam[22]Optimizer，学习率为0.001。当验证损失连续10个时期停止减少时，学习率降低0.1倍。批量大小为32，所有模型都使用分类交叉熵损失进行训练。音频和视觉编码器每个都包含一个1D卷积层。内核大小和步长都设置为1。音频卷积层的输入和输出通道数分别为1000和500，而视觉则分别为32和25。所有3种模式的bi-GRU层数为1。每个bi-GRU层中隐藏的神经元数量为60个。所有MHA模块中的注意头数量为6，并且应用0.1的辍学率以减少过度拟合。第一和第二完全连接的输出层中的神经元数量分别为60（与bi-GRU神经元的数量相同）和7（输出类的数量）。

当UWA在连续10个时期的验证集中没有改善时停止训练，并且使用具有最佳验证UWA的模型进行测试。

所有参数都是根据验证集的性能选择的。表1显示了自我和交叉注意模型在7类单模态，双模态和三模态情绪识别任务中的表现。我们报告了每个模型在50次运行中获得的平均值和标准偏差（5折×10次重复）。我们还应用了具有零假设的two-tailed t-test，即自我和交叉注意模型的准确度值具有相同的平均（预期）值。单模态性能的比较表明，文本优于视觉和音频模式。这个结果与以前的工作一致[13,19]。由于使用跨模态模型无法进行单模态性能评估，因此我们使用单模态版本的自我关注模型报告结果。在双模态模型中，视觉和文本模态的组合为两种模型提供了最佳性能。这些结果与以前的工作一致[7,19]。总体而言，这两种模型都为双模态和三模态情况提供了可比的性能。仅对于T+A（文本和音频）和T+V+A（文本，视频和音频）的WA，自我注意显着优于交叉注意（P值<.05）。

基线模型比较

将三模态模型与AMH[3]进行比较，AMH[3]是当前最先进的模型，它使用单模态GRU层和迭代注意机制的组合1。请注意，就WA和UWA而言，自我关注模型分别超过AMH的表现4.0和2.5个百分点（pp）。交叉注意模型的类似数字是3.1 pp和1.9 pp。

我们还与MDRE[19]进行了比较，MDRE[19]使用递归层对单模态信号进行建模，然后使用完全连接的层进行聚合和分类。与MDRE相比，自我和交叉注意模型以及AMH的更好表现可归因于注意机制的有效性。

对于双模态模型，我们与称为MHA[7]和MDRE的AMH双模态版本进行比较。同样，这两种型号在所有3种双模态情况下都优于MHA和MDRE。请注意，我们通过消除三模态模型而不是针对单个双模态情况进行微调来获得双模态结果。此外，AMH，MHA和MDRE除了MFCC特征外还使用韵律特征进行音频处理，而我们仅使用MFCC特征。通过[16]（0.560 WA和0.612 UWA）获得文本+音频案例的最新结果，该结果显着高于双模态T+A（文本和音频）结果。我们假设有两个原因：（1）与[16]不同，双模态模型没有针对双模态情况进行微调；（2） [16]使用包含其他参数的变压器编码器，这些参数可能有助于学习更复杂的模态间关系，而我们仅使用多头注意机制。尽管如此，这两种模型都改善了AMH的最新三模式结果。图3显示了自我和交叉注意模型的混淆矩阵。对于这两种模式，我们可以观察到，愤怒和沮丧的类经常彼此混淆，类快乐与兴奋混淆（这两个类本质上是相似的）。两种模型在训练中的表现都很差，这可以归因于它在数据集中具有最小的样本量。这些观察结果与以前的文献一致[3]。除了两种描述的模型配置外，我们还尝试了三模态模型的不同变化。我们从两个模型中删除了统计池层以评估其重要性。来自所有时间平均模块（参见图1和2）的输出被连接并传递给分类器模块。这些模型在表2中显示为Cross-noSP和Self-noSP。我们可以做两个观察。首先，即使在消除统计池之后，自我注意模型也优于交叉注意模型（WA的P值<.05）。其次，如果没有统计池层，两个模型的性能都会下降。我们还评估了通过合并自我和交叉关注模型（cross+self）创建的组合模型的性能。将来自两个模型的统计池输出连接并馈送到公共分类器模块。我们可以看到，表现类似于自我关注模型。这可能表明，与自我关注模型相比，交叉关注模型不会提供任何额外的相关信息。

被交叉注意机制在多通道融合中的流行所吸引，我们使用IEMOCAP数据集对基于自我注意和交叉注意的模型进行了三通道和双通道七类分类的比较。结果表明，两种模型的结果没有显著差异。因此，在我们使用的数据集和体系结构的背景下，我们得出结论，在多通道情绪识别中，交叉注意并不优于自我注意。此外，自我注意模型和交叉注意模型都提高了识别任务的最新水平。未来的工作包括研究交叉注意和自我注意模型对其他多通道任务和通道的有效性。致谢。我们感谢[3，7，19]的作者提供了经过处理和分区的IEMOCAP数据集。我们承认使用了ESPRC资助的Tier 2设施Jade。

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602164542.png)

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602164602.png)

## 精读

情绪识别(Emotion recognition, ER)模型使用一种或多种模式，如音频(语言和副语言)、图像(面部表情和肢体姿势)和文本(语言)来推断潜在的情感类别[1]。多模式模型被设计为有效地融合来自不同模式的相关信息，并且通常优于单模式模型[2，3]。

ER模型可以使用原始信号(语音或面部图像)[4、5、6]或手工制作的特征[3、7]作为输入。

- 常用的语音特征是低级描述符，例如共振峰、基音、对数能量、过零率和梅尔倒谱系数(MFCC)[3，7]。
- 面部表情可以由基于始终存在于面部的实体(如眼睛、嘴巴和眉毛)的固定特征和/或基于临时实体(如皱纹和凸起)的临时特征来表示[8]。
- 可以使用单词嵌入算法(如word2vec[9]或Glove[10])将标记化的单词映射为语言特征。


基于深度神经网络(DNN)的ER模型可以包含卷积层，以从输入层和递归层提取局部任务相关分量，从而促进全局序贯建模[4，5]。DNN体系结构中集成的注意机制[11]鼓励ER模型关注与任务相关的时刻[3，6]。注意机制的一般目的是为序列中的不同时间步长提供不同水平的权重。注意机制有两种，即自我(或通道内)注意和跨(或通道间)注意。自我注意机制通过关联相同序列的不同位置来计算单模态序列的表示[6，12]。跨通道注意机制使用一种通道来估计每个位置在另一通道中的相关性[13]。例如，可以使用2个递归层之间的自我注意机制来强调输入语音信号中的任务相关时间步骤[6]，而迭代多跳交叉注意机制可以从利用门控递归单元(GRU)层获得的多模式特征中选择和聚集信息[3，7]。包含多头注意(MHA)模块的Transformers[14]在建模单模和多模情感数据[15，13，16，17]中也变得流行起来。跨模式转换器使用交叉注意来计算使用不同源模式的目标模式表征中的每个时间步长的相关性[13，17]。交叉和自我注意变压器的串联[13，17]或并行[16]组合旨在捕获用于多模式融合的跨模式和模式内关系。


考虑到对基于自我注意和交叉注意的变压器编码者结合模型的兴趣[13，16，17]，我们进行了第一个研究，比较了两种类型的注意机制(不包括其他变压器组件)。为了了解两种类型的注意机制之间的差异，我们广泛地比较了一个只基于交叉注意的模型和一个只基于自我注意的双模式和三模式组合的模型。我们在IEMOCAP[18]的7类情绪分类数据集上对这两种模型进行了比较，结果表明，交叉注意模型并不优于自我注意模型。然而，这两个模型都在加权和未加权准确度方面改善了三通道和双通道情绪识别任务的最新结果。

自我注意和交叉注意模型首先使用特定于模态的编码器处理单个模态的数据。然后，经过编码后的特征分别被馈送到多头自注意力模块或多头交叉注意力模块(Multi-Head Attention, MHA)[14]中。在每个注意力模块的输出端生成话语片段的全局表征作为时间平均值。然后，将得到的特征连接起来，并使用统计汇聚层获得它们的平均值和标准差。然后，将串联的平均和标准偏差向量馈送到全连接层。最终的情感类预测通过Softmax运算获得。具体说明如下：

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602181045.png)

1. 设$X_{a} \in \mathbb{R}^{t_{a}}\times d_{a}$是对应于一个音频片段的音频特征，其中$t_{a}$是序列长度，$d_{a}$是特征维度。音频编码器由一个$1 \mathbb{D}$卷积层和一个双向GRU组成。

- 卷积层通过查找与任务相关的特征来优化输入要素序列，其操作如下：$$X_{a}^{\prime}\left(t^{\prime}\right)=b\left(t^{\prime}\right)+\sum_{k=0}^{t_{a}-1}\left(W\left(t^{\prime}，K\right)*X_{a}(K)\right)$$其中$X_{a}^{\prime}\in\mathbb{R}^{t_{a}^{\prime}\times d_{a}^{\prime}}$是长度为$t_{a}^{\prime}$的输出，维度为$d_{a}^{\prime}$的输出，$t^{\prime}\in\left[0, t_{a}^{\prime}-1\right]$，\*是卷积运算符，$W$是权重，$b$是与层关联的偏差。因此，卷积层修改序列长度以及特征尺寸。

- 双向GRU层模拟特征随时间变化的上下文相互依赖关系。对于序列中的每个元素，Bi-GRU层计算以下函数：
$$ \left\{\begin{array}{l} r_{t}=\sigma\left(W_{i r} X_{a}^{\prime}(t)+b_{i r}+W_{h r} h_{t-1}+b_{h r}\right) \\ z_{t}=\sigma\left(W_{i z} X_{a}^{\prime}(t)+b_{i z}+W_{h z} h_{t-1}+b_{h z}\right) \\ n_{t}=\phi_{h}\left(W_{i n} X_{a}^{\prime}(t)+b_{i n}+r_{t} \odot\left(W_{h n} h_{t-1}+b_{h n}\right)\right) \\ h_{t}=\left(1-z_{t}\right) \odot n_{t}+z_{t} \odot\left(h_{t-1}\right) \end{array}\right.
$$
其中$h_{t}$和$h_{t-1}$是时间$t$和$t-1$的隐藏状态向量，$X_{a}^{\prime}(T)$是时间$t$的输入。$R_{t}、z_{t}$和$n_{t}$是重置门、更新门和新门，$W$和$b$是相应的权重和偏差，$\sigma$和$\Phi_{h}$是Sigmoid和双曲正切函数，$\odot$是Hadamard乘积。在bi-GRU的输出端，每个时间步长的前向和后向隐藏状态被串联，并且改进的音频特征可以表示为$e_{a}\in\mathbb{R}^{t_{a}^{\prime}\times d^{\prime\prime}}$，其中$d^{\prime\prime}$是GRU中隐藏神经元数目的两倍。

2. 与音频类似，视觉编码器由一个$1\mathbb{D}$卷积层和一个双GRU层组成。如果$X_{v}\in\mathbb{R}^{t_{v}\times d_{v}}$表示与话语对应的视觉特征，则在视觉编码器的输出处，这些特征被细化为$e_{v}\in\mathbb{R}^{t_{v}^{\prime}\times d^{\prime\prime}}$。

3. 对于文本通道，编码器仅由一个bi-GRU层组成。文本编码器的输入和输出可以分别用$X_{l}\in\mathbb{R}^{t_{l}\times d_{l}}$和$e_{l}\in\mathbb{R}^{t_{l}\times d^{\prime\prime}}$表示。

- 我们使用多头注意力模块[14]进行自我和交叉注意力建模。

多头注意力模块由多个注意操作组成，以捕获对序列的更丰富的解释。每个多头注意力模块需要3个输入，即查询$(Q)$、Кey$(K)$和值$(V)$，每个输入首先使用线性映射层, 投影到$H$个不同的子空间，即令多头注意力模块的Head数目为$H$。每个子空间的投影特征$h \in\{0, \ldots, H-1\}$ ，可计算为

$$
\begin{aligned}

Q_{h} &=W_{h}^{Q} e_{m} \\

K_{h} &=W_{h}^{K} e_{m} \\

V_{h} &=W_{h}^{V} e_{m}

\end{aligned}
$$

其中，$m \in\{a, v, l\}$表示哪种模态。在这些子空间中，通过scaled dot-product attention方法对映射特征进行运算。对子空间$h$，运算过程如下所示

$$
\operatorname{Att}_{h}\left(Q_{h}, K_{h}, V_{h}\right)=\operatorname{Softmax}\left(\frac{Q_{h} K_{h}^{T}}{\sqrt{d_{k}}}\right) V_{h}
$$


其中$A t t_{h}(\cdot)$和$d_{k}$分别指子空间$h$中的中的注意力运算和特征维度。多头注意模块的所有$H$个输出，被串联并通过线性映射层以获得多头注意模块的最终输出。

在交叉注意模型中，源模态使用源自身的输入数据通过线性映射，得到$K$和$V$向量矩阵，然后使用其他模态的数据，再得到$Q$向量矩阵(见图1)。这种方法背后的直觉是通过使源模态适应目标模态来实现跨模态交互[13]。

将音频作为目标通道，而视觉作为源通道举例：

![]({27}_Is%20Cross-Attention%20Preferable%20to%20Self-Attention%20for%20Multi-Modal%20Emotion%20Recognition_@rajanCrossAttentionPreferableSelfAttention2022.assets/image-20220602200508.png)

将精炼的音频特征($e_{a}\in\mathbb{R}^{t_{a}^{\prime}\times d^{\prime\prime}}$)通过线性映射转换为$Q$，然后再将精炼的视频特征($e_{v}\in\mathbb{R}^{t_{v}^{\prime}\times d^{\prime\prime}}$)通过线性映射转换为$K$和$V$。跨模态的多头注意力模块会将视频映射到音频模态，并输出与音频相适应的视觉特征$e_{a v}^{w}\in\mathbb{R}^{t_{a}^{\prime}\times d^{\prime\prime}}$。注意，交叉注意加权输出的序列长度与目标通道音频相同。

对于3个模态，总共有6个源-目标模态的组合，因此我们使用了6个MHA模块。

- 而在自我注意模型的情况下，同一模态的输入序列才会被用作计算$Q、K$和$V$(见图2)，这有助于捕获每个模态中的模态内交互特征。

统计池化方法statistical pooling，在交叉注意模型中，是跨6个模态组合序列的时间平均的级联计算的，而对于自我注意模型，是跨所有3个模态的自注意序列的时间平均的级联计算的。
这两种模型的分类器都是：
$$
\hat{y}=\operatorname{Softmax}\left(f_{\theta_{2}}\left(f_{\theta_{1}}([\mu \| \sigma])\right)\right)
$$
其中，$\mu$和$\sigma$是统计汇聚层输出的均值和标准差，$||$表示串联操作，$f_{\theta_{1}}$和$f_{\theta_{2}}$分别表示参数为$\theta_{1}$和$\theta_{2}$的两个全连接层，$\hat{y}$表示情绪预测的onehot向量。

### 引文

---
title: "LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition"
description: ""
citekey: aftabLIGHTSERNETLightweightFully2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:36
lastmod: 2023-04-11 11:53:50
---

> [!info] 论文信息
>1. Title：LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition
>2. Author：Arya Aftab, Alireza Morsali, Shahrokh Ghaemmaghami, Benoit Champagne
>3. Entry：[Zotero link](zotero://select/items/@aftabLIGHTSERNETLightweightFully2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Aftab et al_2022_LIGHT-SERNET.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/AryaAftab/LIGHT-SERNET
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 对多维数据，分路径使用特定于某维度的接受野进行卷积计算。可以减少卷积参数量， 例如分别提取时间 维度的依赖关系，频域维度的依赖关系。
- F-LOSS 有时候会比 CE-Loss 有更高的精度，- 对多维数据，分路径使用特定于某维度的接受野进行卷积计算。可以减少卷积参数量，例如分别提取时间维度的依赖关系，频域维度的依赖关系。
- F-LOSS有时候会比CE-Loss有更高的精度，

## 摘要

> [!abstract] Detecting emotions directly from a speech signal plays an important role in effective human-computer interactions. Existing speech emotion recognition models require massive computational and storage resources, making them hard to implement concurrently with other machine-interactive tasks in embedded systems. In this paper, we propose an efficient and lightweight fully convolutional neural network for speech emotion recognition in systems with limited hard-ware resources. In the proposed FCNN model, various feature maps are extracted via three parallel paths with different filter sizes. This helps deep convolution blocks to extract high-level features, while ensuring sufficient separability. The extracted features are used to classify the emotion of the input speech segment. While our model has a smaller size than that of the state-of-the-art models, it achieves a higher performance on the IEMOCAP and EMO-DB datasets. The source code is available https://github.com/AryaAftab/LIGHT-SERNET

> 直接从语音信号中检测情感在有效的人机交互中起着重要的作用。现有的语音情感识别模型需要大量的计算和存储资源，使得它们很难与嵌入式系统中的其他机器交互任务并行执行。本文针对硬件资源有限的语音情感识别问题，提出了一种高效、轻量级的全卷积神经网络。在所提出的FCNN模型中，通过三条具有不同滤波器大小的并行路径来提取各种特征映射。这有助于深度卷积块提取高级特征，同时确保足够的可分性。提取的特征被用于对输入语音片段的情感进行分类。虽然我们的模型比最先进的模型具有更小的尺寸，但它在IEMOCAP和EMO-DB数据集上实现了更高的性能。

## 预处理

## 概述

## 结果

实验设置

在对−1和1之间的音频信号进行归一化之后，计算信号的MFCC。为此，我们使用汉明窗口将音频信号分割成具有16ms重叠的64ms帧大小，这些帧可以被认为是准平稳段。在对每一帧应用1024点快速傅立叶变换(FFT)之后，信号在40赫兹到7600赫兹的范围内接受梅尔尺度滤波器组分析。然后使用反离散余弦变换计算每一帧的MFCC，其中选择前40个系数来训练模型。

训练设置

基于10次交叉验证，每折经历300个历元的训练，批次大小为32。使用初始学习速率为10−4的ADAM优化器。从时段50及以上开始的学习速率每20个时段以e−0.15的速率下降。

由于缺乏足够的数据来训练模型，可能会遇到过拟合的问题，所以我们引入了正则化来解决这个问题。我们在每个卷积层之后使用批归一化，在Softmax层之前以0.3%的速率丢弃，并且对于LFLB以10−6的速率进行加权衰减(L2正则化)。

模型权值在训练时具有32位浮点精度。在对模型进行训练后，我们将训练后的模型权值的精度改为16位浮点数，将模型的规模缩小了一半。所有报告的结果都是针对具有此精度的权重。

损失函数的影响：

我们选择两个损失函数来训练所提出的模型：焦点损失（F损失）和交叉熵损失（CE损失）。提出了F损失来解决类别不平衡和具有挑战性的样本[20]。在实验中，使用γ=2的F-损耗。表1显示了EMO-DB和IEMOCAP数据集上两个损失函数的结果。与表1中的UAR相比，显示FLoss在IEMOCAP（即兴+脚本）上实现了比CE损失更高的准确度，而对于IEMOCAP（即兴）和EMO-DB数据集，CE损失表现更好。这些结果表明，模型的UAR可以提高性能，在某些情况下，CE损失简单（表1）。

并行路径的影响：在这里，我们评估并行路径对CE损失的IEMOCAP和EMO-DB数据集的影响。与单独使用每条路径相比，同时使用这三条路径分别使IEMOCAP（脚本+即兴）数据集上的WA，UAR和F1分别增加了1.38%，0.91%和1.06%。在EMO-DB数据集上，这一改进分别为1.86%，1.35%和1.57%。为了公平比较，在同时使用三条路径和单独使用每条路径时都采用了相同数量的滤波器。

输入长度的影响：由于IEMOCAP数据集话语的长度可变（即在0.58到34.13秒的范围内），我们评估了所提出的输入长度为3秒和7秒的模型。输入长度越长的主要问题是计算成本和峰值内存使用量（PMU）。输入长度为3秒和7秒的计算成本分别为322和7.6亿浮点操作（mfLOP），输入长度为3秒和7秒的PMU分别为1610和3797千字节。还发现使用7秒输入长度而不是3秒输入长度将IEMOCAP（即兴）上的评估指标增加超过2.13%，IEMOCAP（脚本+即兴）上的评估指标增加超过3.69%（表1）。

## 精读

直接从语音信号检测情绪在有效的人机交互中起着重要作用[1]。自动情绪识别可以用于各种智能设备，特别是智能对话系统和语音助理，如Apple Siri，Amazon Alexa和Google Assistant。最近，从他们的言语中识别说话者的情绪状态受到了相当大的关注[2-8]。现有的语音情绪识别（SER）方法基准主要由特征提取器和分类器组成，以获得情绪状态[2]。最近，基于深度学习（DL）的技术彻底改变了语音处理领域，并且在许多情况下优于经典方法[2,9]。基于DL的方法成功的主要原因之一是深度神经网络（DNN）通过学习过程从数据中提取复杂特征的能力[10]。特别是，与传统方法相比，卷积神经网络（CNN）在SER方面取得了显着进步[11-13]。CNN对于忽视输入信号传递的可能与目标任务无关的信息特别强大[14]。当输入是复杂的非结构化信号（例如图像或语音信号）时，此特征特别有用。


在本文中，我们提出了一种新的SER模型，它可以从Mel频率cepstral系数（MFCC）中学习频谱时间信息，该模型仅利用CNN。首先，开发了分层DL模型来自动化和替换手部工程特征的过程。事实上，我们利用三个并行CNN块从MFCC能量图中提取具有不同属性的特征。然后将提取的特征连接并馈送到深度CNN以捕获最终用softmax层分类的高级表示。所提出的模型非常轻，这使其适用于在线SER应用程序以及在资源有限的小型嵌入式系统和物联网设备上实施。与基准方法相比，CNN的使用不仅降低了模型的复杂性，而且提供了更好的泛化。我们在IEMOCAP和EMO-DB数据集上评估所提出的SER模型的实验证实，我们的模型需要相当少的参数，同时实现与现有技术模型相同或更好的性能。

在正文第一部分，将三个并行的CNN应用于MFCC，以提取时间和频谱特征。这种结构可以在其特征提取器1中实现光谱信息和时间信息之间的平衡。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143834.png)

在文献[15]中，分类精度与接收野大小之间存在直接关系，即接收野越大，分类精度越高。因此，我们使用以下技术来增加卷积网络的接受场：
1. 增加层数(更深的网络)，
2. 使用子采样块，例如合并或更高的步长，
3. 使用膨胀卷积，以及
4. 执行深度卷积。越深的网络有更高的感受野，因为每增加一层，感受野就增加核大小[16]。但是，增加层数会增加模型参数的数量，从而导致模型过度拟合。

对于多维信号，可以分别考虑每个维度来计算接受场[15]。因此，我们使用大小为9×1、1×11和3×3的核分别提取频谱、时间和频谱-时间依赖关系，如图2所示。与只有一条接收野大小相同的路径相比，使用这种技术的优点是将模型的这一部分的参数数量和计算成本减少了9×11(9×1+1×11+3×3)。最后，将提取的每条路径的特征拼接并馈送到主体II中。图1中的第二个框说明了主体部分I的结构。

Body Part II由几个LFLB组成，它们具有不同的配置，应用于Body Part I中串联的低级特征，以捕获高级特征。LFLB是受赵等人的工作启发的连续层的集合。[17]。该算法由卷积层、批归一化层(BN)、指数线性单元(ELU)和最大合并层组成。在我们的工作中，ELU层和最大池层分别被一个校正的线性单元(REU)和平均池所取代。最后一个LFLB使用全局平均池(GAP)，而不是平均池，使得我们的模型能够在不改变体系结构的情况下对不同长度的数据集进行训练。主体部分II的规格如图1所示。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143850.png)

## 引文

Yenigalla等[6]通过使用具有大卷积滤波器和音素嵌入的几个并行路径来提高识别率。

Chen等[5]使用Mel谱图，delta和delta-delta作为输入，并提出了一种基于三维注意力的卷积递归神经网络，以保持有效的情绪信息并减少无关情绪因素的影响。

Li等[3]提出了扩张残差网络和多头自我注意的组合，以减轻渐进分辨率降低中语音时间结构的损失，同时忽略超分割特征序列中元素之间的相对依赖性。

为了减少模型大小和计算成本，Zhong等[8]将神经网络的权重从原始全精度值量化为二进制值，然后可以更容易地存储和处理。

Zhong等[4]将注意力机制和焦点损失相结合，将训练过程集中在学习硬样本和减重易样本上，以解决具有挑战性的样本问题。

直接从语音信号检测情绪在有效的人机交互中起着重要作用[1]。自动情绪识别可以用于各种智能设备，特别是智能对话系统和语音助理，如Apple Siri，Amazon Alexa和Google Assistant。最近，从他们的言语中识别说话者的情绪状态受到了相当大的关注[2-8]。现有的语音情绪识别（SER）方法基准主要由特征提取器和分类器组成，以获得情绪状态[2]。最近，基于深度学习（DL）的技术彻底改变了语音处理领域，并且在许多情况下优于经典方法[2,9]。基于DL的方法成功的主要原因之一是深度神经网络（DNN）通过学习过程从数据中提取复杂特征的能力[10]。特别是，与传统方法相比，卷积神经网络（CNN）在SER方面取得了显着进步[11-13]。CNN对于忽视输入信号传递的可能与目标任务无关的信息特别强大[14]。当输入是复杂的非结构化信号（例如图像或语音信号）时，此特征特别有用。


在本文中，我们提出了一种新的SER模型，它可以从Mel频率cepstral系数（MFCC）中学习频谱时间信息，该模型仅利用CNN。首先，开发了分层DL模型来自动化和替换手部工程特征的过程。事实上，我们利用三个并行CNN块从MFCC能量图中提取具有不同属性的特征。然后将提取的特征连接并馈送到深度CNN以捕获最终用softmax层分类的高级表示。所提出的模型非常轻，这使其适用于在线SER应用程序以及在资源有限的小型嵌入式系统和物联网设备上实施。与基准方法相比，CNN的使用不仅降低了模型的复杂性，而且提供了更好的泛化。我们在IEMOCAP和EMO-DB数据集上评估所提出的SER模型的实验证实，我们的模型需要相当少的参数，同时实现与现有技术模型相同或更好的性能。

在正文第一部分，将三个并行的CNN应用于MFCC，以提取时间和频谱特征。这种结构可以在其特征提取器1中实现光谱信息和时间信息之间的平衡。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143834.png)

在文献[15]中，分类精度与接收野大小之间存在直接关系，即接收野越大，分类精度越高。因此，我们使用以下技术来增加卷积网络的接受场：
1. 增加层数(更深的网络)，
2. 使用子采样块，例如合并或更高的步长，
3. 使用膨胀卷积，以及
4. 执行深度卷积。越深的网络有更高的感受野，因为每增加一层，感受野就增加核大小[16]。但是，增加层数会增加模型参数的数量，从而导致模型过度拟合。

对于多维信号，可以分别考虑每个维度来计算接受场[15]。因此，我们使用大小为9×1、1×11和3×3的核分别提取频谱、时间和频谱-时间依赖关系，如图2所示。与只有一条接收野大小相同的路径相比，使用这种技术的优点是将模型的这一部分的参数数量和计算成本减少了9×11(9×1+1×11+3×3)。最后，将提取的每条路径的特征拼接并馈送到主体II中。图1中的第二个框说明了主体部分I的结构。

Body Part II由几个LFLB组成，它们具有不同的配置，应用于Body Part I中串联的低级特征，以捕获高级特征。LFLB是受赵等人的工作启发的连续层的集合。[17]。该算法由卷积层、批归一化层(BN)、指数线性单元(ELU)和最大合并层组成。在我们的工作中，ELU层和最大池层分别被一个校正的线性单元(REU)和平均池所取代。最后一个LFLB使用全局平均池(GAP)，而不是平均池，使得我们的模型能够在不改变体系结构的情况下对不同长度的数据集进行训练。主体部分II的规格如图1所示。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143850.png)

### 引文

Yenigalla等[6]通过使用具有大卷积滤波器和音素嵌入的几个并行路径来提高识别率。

Chen等[5]使用Mel谱图，delta和delta-delta作为输入，并提出了一种基于三维注意力的卷积递归神经网络，以保持有效的情绪信息并减少无关情绪因素的影响。

Li等[3]提出了扩张残差网络和多头自我注意的组合，以减轻渐进分辨率降低中语音时间结构的损失，同时忽略超分割特征序列中元素之间的相对依赖性。

为了减少模型大小和计算成本，Zhong等[8]将神经网络的权重从原始全精度值量化为二进制值，然后可以更容易地存储和处理。

Zhong等[4]将注意力机制和焦点损失相结合，将训练过程集中在学习硬样本和减重易样本上，以解决具有挑战性的样本问题。

## 摘录

---
title: "LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition"
description: ""
citekey: aftabLIGHTSERNETLightweightFully2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:36
lastmod: 2023-04-11 11:53:50
---

> [!info] 论文信息
>1. Title：LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition
>2. Author：Arya Aftab, Alireza Morsali, Shahrokh Ghaemmaghami, Benoit Champagne
>3. Entry：[Zotero link](zotero://select/items/@aftabLIGHTSERNETLightweightFully2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Aftab et al_2022_LIGHT-SERNET.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/AryaAftab/LIGHT-SERNET
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 对多维数据，分路径使用特定于某维度的接受野进行卷积计算。可以减少卷积参数量， 例如分别提取时间 维度的依赖关系，频域维度的依赖关系。
- F-LOSS 有时候会比 CE-Loss 有更高的精度，- 对多维数据，分路径使用特定于某维度的接受野进行卷积计算。可以减少卷积参数量，例如分别提取时间维度的依赖关系，频域维度的依赖关系。
- F-LOSS有时候会比CE-Loss有更高的精度，

## 摘要

> [!abstract] Detecting emotions directly from a speech signal plays an important role in effective human-computer interactions. Existing speech emotion recognition models require massive computational and storage resources, making them hard to implement concurrently with other machine-interactive tasks in embedded systems. In this paper, we propose an efficient and lightweight fully convolutional neural network for speech emotion recognition in systems with limited hard-ware resources. In the proposed FCNN model, various feature maps are extracted via three parallel paths with different filter sizes. This helps deep convolution blocks to extract high-level features, while ensuring sufficient separability. The extracted features are used to classify the emotion of the input speech segment. While our model has a smaller size than that of the state-of-the-art models, it achieves a higher performance on the IEMOCAP and EMO-DB datasets. The source code is available https://github.com/AryaAftab/LIGHT-SERNET

> 直接从语音信号中检测情感在有效的人机交互中起着重要的作用。现有的语音情感识别模型需要大量的计算和存储资源，使得它们很难与嵌入式系统中的其他机器交互任务并行执行。本文针对硬件资源有限的语音情感识别问题，提出了一种高效、轻量级的全卷积神经网络。在所提出的FCNN模型中，通过三条具有不同滤波器大小的并行路径来提取各种特征映射。这有助于深度卷积块提取高级特征，同时确保足够的可分性。提取的特征被用于对输入语音片段的情感进行分类。虽然我们的模型比最先进的模型具有更小的尺寸，但它在IEMOCAP和EMO-DB数据集上实现了更高的性能。

## 预处理

## 概述

## 结果

实验设置

在对−1和1之间的音频信号进行归一化之后，计算信号的MFCC。为此，我们使用汉明窗口将音频信号分割成具有16ms重叠的64ms帧大小，这些帧可以被认为是准平稳段。在对每一帧应用1024点快速傅立叶变换(FFT)之后，信号在40赫兹到7600赫兹的范围内接受梅尔尺度滤波器组分析。然后使用反离散余弦变换计算每一帧的MFCC，其中选择前40个系数来训练模型。

训练设置

基于10次交叉验证，每折经历300个历元的训练，批次大小为32。使用初始学习速率为10−4的ADAM优化器。从时段50及以上开始的学习速率每20个时段以e−0.15的速率下降。

由于缺乏足够的数据来训练模型，可能会遇到过拟合的问题，所以我们引入了正则化来解决这个问题。我们在每个卷积层之后使用批归一化，在Softmax层之前以0.3%的速率丢弃，并且对于LFLB以10−6的速率进行加权衰减(L2正则化)。

模型权值在训练时具有32位浮点精度。在对模型进行训练后，我们将训练后的模型权值的精度改为16位浮点数，将模型的规模缩小了一半。所有报告的结果都是针对具有此精度的权重。

损失函数的影响：

我们选择两个损失函数来训练所提出的模型：焦点损失（F损失）和交叉熵损失（CE损失）。提出了F损失来解决类别不平衡和具有挑战性的样本[20]。在实验中，使用γ=2的F-损耗。表1显示了EMO-DB和IEMOCAP数据集上两个损失函数的结果。与表1中的UAR相比，显示FLoss在IEMOCAP（即兴+脚本）上实现了比CE损失更高的准确度，而对于IEMOCAP（即兴）和EMO-DB数据集，CE损失表现更好。这些结果表明，模型的UAR可以提高性能，在某些情况下，CE损失简单（表1）。

并行路径的影响：在这里，我们评估并行路径对CE损失的IEMOCAP和EMO-DB数据集的影响。与单独使用每条路径相比，同时使用这三条路径分别使IEMOCAP（脚本+即兴）数据集上的WA，UAR和F1分别增加了1.38%，0.91%和1.06%。在EMO-DB数据集上，这一改进分别为1.86%，1.35%和1.57%。为了公平比较，在同时使用三条路径和单独使用每条路径时都采用了相同数量的滤波器。

输入长度的影响：由于IEMOCAP数据集话语的长度可变（即在0.58到34.13秒的范围内），我们评估了所提出的输入长度为3秒和7秒的模型。输入长度越长的主要问题是计算成本和峰值内存使用量（PMU）。输入长度为3秒和7秒的计算成本分别为322和7.6亿浮点操作（mfLOP），输入长度为3秒和7秒的PMU分别为1610和3797千字节。还发现使用7秒输入长度而不是3秒输入长度将IEMOCAP（即兴）上的评估指标增加超过2.13%，IEMOCAP（脚本+即兴）上的评估指标增加超过3.69%（表1）。

## 精读

直接从语音信号检测情绪在有效的人机交互中起着重要作用[1]。自动情绪识别可以用于各种智能设备，特别是智能对话系统和语音助理，如Apple Siri，Amazon Alexa和Google Assistant。最近，从他们的言语中识别说话者的情绪状态受到了相当大的关注[2-8]。现有的语音情绪识别（SER）方法基准主要由特征提取器和分类器组成，以获得情绪状态[2]。最近，基于深度学习（DL）的技术彻底改变了语音处理领域，并且在许多情况下优于经典方法[2,9]。基于DL的方法成功的主要原因之一是深度神经网络（DNN）通过学习过程从数据中提取复杂特征的能力[10]。特别是，与传统方法相比，卷积神经网络（CNN）在SER方面取得了显着进步[11-13]。CNN对于忽视输入信号传递的可能与目标任务无关的信息特别强大[14]。当输入是复杂的非结构化信号（例如图像或语音信号）时，此特征特别有用。


在本文中，我们提出了一种新的SER模型，它可以从Mel频率cepstral系数（MFCC）中学习频谱时间信息，该模型仅利用CNN。首先，开发了分层DL模型来自动化和替换手部工程特征的过程。事实上，我们利用三个并行CNN块从MFCC能量图中提取具有不同属性的特征。然后将提取的特征连接并馈送到深度CNN以捕获最终用softmax层分类的高级表示。所提出的模型非常轻，这使其适用于在线SER应用程序以及在资源有限的小型嵌入式系统和物联网设备上实施。与基准方法相比，CNN的使用不仅降低了模型的复杂性，而且提供了更好的泛化。我们在IEMOCAP和EMO-DB数据集上评估所提出的SER模型的实验证实，我们的模型需要相当少的参数，同时实现与现有技术模型相同或更好的性能。

在正文第一部分，将三个并行的CNN应用于MFCC，以提取时间和频谱特征。这种结构可以在其特征提取器1中实现光谱信息和时间信息之间的平衡。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143834.png)

在文献[15]中，分类精度与接收野大小之间存在直接关系，即接收野越大，分类精度越高。因此，我们使用以下技术来增加卷积网络的接受场：
1. 增加层数(更深的网络)，
2. 使用子采样块，例如合并或更高的步长，
3. 使用膨胀卷积，以及
4. 执行深度卷积。越深的网络有更高的感受野，因为每增加一层，感受野就增加核大小[16]。但是，增加层数会增加模型参数的数量，从而导致模型过度拟合。

对于多维信号，可以分别考虑每个维度来计算接受场[15]。因此，我们使用大小为9×1、1×11和3×3的核分别提取频谱、时间和频谱-时间依赖关系，如图2所示。与只有一条接收野大小相同的路径相比，使用这种技术的优点是将模型的这一部分的参数数量和计算成本减少了9×11(9×1+1×11+3×3)。最后，将提取的每条路径的特征拼接并馈送到主体II中。图1中的第二个框说明了主体部分I的结构。

Body Part II由几个LFLB组成，它们具有不同的配置，应用于Body Part I中串联的低级特征，以捕获高级特征。LFLB是受赵等人的工作启发的连续层的集合。[17]。该算法由卷积层、批归一化层(BN)、指数线性单元(ELU)和最大合并层组成。在我们的工作中，ELU层和最大池层分别被一个校正的线性单元(REU)和平均池所取代。最后一个LFLB使用全局平均池(GAP)，而不是平均池，使得我们的模型能够在不改变体系结构的情况下对不同长度的数据集进行训练。主体部分II的规格如图1所示。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143850.png)

## 引文

Yenigalla等[6]通过使用具有大卷积滤波器和音素嵌入的几个并行路径来提高识别率。

Chen等[5]使用Mel谱图，delta和delta-delta作为输入，并提出了一种基于三维注意力的卷积递归神经网络，以保持有效的情绪信息并减少无关情绪因素的影响。

Li等[3]提出了扩张残差网络和多头自我注意的组合，以减轻渐进分辨率降低中语音时间结构的损失，同时忽略超分割特征序列中元素之间的相对依赖性。

为了减少模型大小和计算成本，Zhong等[8]将神经网络的权重从原始全精度值量化为二进制值，然后可以更容易地存储和处理。

Zhong等[4]将注意力机制和焦点损失相结合，将训练过程集中在学习硬样本和减重易样本上，以解决具有挑战性的样本问题。

直接从语音信号检测情绪在有效的人机交互中起着重要作用[1]。自动情绪识别可以用于各种智能设备，特别是智能对话系统和语音助理，如Apple Siri，Amazon Alexa和Google Assistant。最近，从他们的言语中识别说话者的情绪状态受到了相当大的关注[2-8]。现有的语音情绪识别（SER）方法基准主要由特征提取器和分类器组成，以获得情绪状态[2]。最近，基于深度学习（DL）的技术彻底改变了语音处理领域，并且在许多情况下优于经典方法[2,9]。基于DL的方法成功的主要原因之一是深度神经网络（DNN）通过学习过程从数据中提取复杂特征的能力[10]。特别是，与传统方法相比，卷积神经网络（CNN）在SER方面取得了显着进步[11-13]。CNN对于忽视输入信号传递的可能与目标任务无关的信息特别强大[14]。当输入是复杂的非结构化信号（例如图像或语音信号）时，此特征特别有用。


在本文中，我们提出了一种新的SER模型，它可以从Mel频率cepstral系数（MFCC）中学习频谱时间信息，该模型仅利用CNN。首先，开发了分层DL模型来自动化和替换手部工程特征的过程。事实上，我们利用三个并行CNN块从MFCC能量图中提取具有不同属性的特征。然后将提取的特征连接并馈送到深度CNN以捕获最终用softmax层分类的高级表示。所提出的模型非常轻，这使其适用于在线SER应用程序以及在资源有限的小型嵌入式系统和物联网设备上实施。与基准方法相比，CNN的使用不仅降低了模型的复杂性，而且提供了更好的泛化。我们在IEMOCAP和EMO-DB数据集上评估所提出的SER模型的实验证实，我们的模型需要相当少的参数，同时实现与现有技术模型相同或更好的性能。

在正文第一部分，将三个并行的CNN应用于MFCC，以提取时间和频谱特征。这种结构可以在其特征提取器1中实现光谱信息和时间信息之间的平衡。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143834.png)

在文献[15]中，分类精度与接收野大小之间存在直接关系，即接收野越大，分类精度越高。因此，我们使用以下技术来增加卷积网络的接受场：
1. 增加层数(更深的网络)，
2. 使用子采样块，例如合并或更高的步长，
3. 使用膨胀卷积，以及
4. 执行深度卷积。越深的网络有更高的感受野，因为每增加一层，感受野就增加核大小[16]。但是，增加层数会增加模型参数的数量，从而导致模型过度拟合。

对于多维信号，可以分别考虑每个维度来计算接受场[15]。因此，我们使用大小为9×1、1×11和3×3的核分别提取频谱、时间和频谱-时间依赖关系，如图2所示。与只有一条接收野大小相同的路径相比，使用这种技术的优点是将模型的这一部分的参数数量和计算成本减少了9×11(9×1+1×11+3×3)。最后，将提取的每条路径的特征拼接并馈送到主体II中。图1中的第二个框说明了主体部分I的结构。

Body Part II由几个LFLB组成，它们具有不同的配置，应用于Body Part I中串联的低级特征，以捕获高级特征。LFLB是受赵等人的工作启发的连续层的集合。[17]。该算法由卷积层、批归一化层(BN)、指数线性单元(ELU)和最大合并层组成。在我们的工作中，ELU层和最大池层分别被一个校正的线性单元(REU)和平均池所取代。最后一个LFLB使用全局平均池(GAP)，而不是平均池，使得我们的模型能够在不改变体系结构的情况下对不同长度的数据集进行训练。主体部分II的规格如图1所示。

![]({28}_LIGHT-SERNET_%20A%20Lightweight%20Fully%20Convolutional%20Neural%20Network%20for%20Speech%20Emotion%20Recognition@aftabLIGHTSERNETLightweightFully2022.assets/image-20220603143850.png)

### 引文

Yenigalla等[6]通过使用具有大卷积滤波器和音素嵌入的几个并行路径来提高识别率。

Chen等[5]使用Mel谱图，delta和delta-delta作为输入，并提出了一种基于三维注意力的卷积递归神经网络，以保持有效的情绪信息并减少无关情绪因素的影响。

Li等[3]提出了扩张残差网络和多头自我注意的组合，以减轻渐进分辨率降低中语音时间结构的损失，同时忽略超分割特征序列中元素之间的相对依赖性。

为了减少模型大小和计算成本，Zhong等[8]将神经网络的权重从原始全精度值量化为二进制值，然后可以更容易地存储和处理。

Zhong等[4]将注意力机制和焦点损失相结合，将训练过程集中在学习硬样本和减重易样本上，以解决具有挑战性的样本问题。
---
title: "The Role of Task and Acoustic Similarity in Audio Transfer Learning: Insights from the Speech Emotion Recognition Case"
description: ""
citekey: triantafyllopoulosRoleTaskAcoustic2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:56
lastmod: 2023-04-11 11:55:52
---

> [!info] 论文信息
>1. Title：The Role of Task and Acoustic Similarity in Audio Transfer Learning: Insights from the Speech Emotion Recognition Case
>2. Author：Andreas Triantafyllopoulos, Björn W. Schuller
>3. Entry：[Zotero link](zotero://select/items/@triantafyllopoulosRoleTaskAcoustic2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Triantafyllopoulos_Schuller_2021_The Role of Task and Acoustic Similarity in Audio Transfer Learning.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 使用迁移学习时，越相关的任务，迁移效果会更好
- 最终微调过程，靠近输入的层比仅微调输出分类层有更多的适应
- 为了处理目标标签的不平衡问题，我们使用了一种平衡的非负似然损失(NLLoss)
- arousal、valence、dominance 三个情感维度并不是完全无关的，可以通过一定方式找出其相关性。- 使用迁移学习时，越相关的任务，迁移效果会更好
- 最终微调过程，靠近输入的层比仅微调输出分类层有更多的适应
- 为了处理目标标签的不平衡问题，我们使用了一种平衡的非负似然损失(NLLoss)
- arousal、valence、dominance三个情感维度并不是完全无关的，可以通过一定方式找出其相关性。

## 摘要

> [!abstract] With the rise of deep learning, deep knowledge transfer has emerged as one of the most effective techniques for getting state-of-the-art performance using deep neural networks. A lot of recent research has focused on understanding the mechanisms of transfer learning in the image and language domains. We perform a similar investigation for the case of speech emotion recognition (SER), and conclude that transfer learning for SER is influenced both by the choice of pre-training task and by the differences in acoustic conditions between the upstream and downstream data sets, with the former having a bigger impact. The effect of each factor is isolated by first transferring knowledge between different tasks on the same data, and then from the original data to corrupted versions of it but for the same task. We also demonstrate that layers closer to the input see more adaptation than ones closer to the output in both cases, a finding which explains why previous works often found it necessary to fine-tune all layers during transfer learning.

> 随着深度学习的兴起，深度知识迁移已经成为利用深度神经网络获得最先进性能的最有效技术之一。最近的许多研究都集中在理解意象和语言领域的迁移学习机制。我们对语音情感识别(SER)进行了类似的研究，得出结论：语音情感识别的迁移学习既受预训练任务选择的影响，也受上下游数据集声学条件差异的影响，其中前者影响更大。通过首先在相同数据的不同任务之间传输知识，然后将原始数据传输到其损坏的版本，但对于相同的任务，每个因素的影响是隔离的。我们还证明，在这两种情况下，靠近输入的层比靠近输出的层有更多的适应，这一发现解释了为什么以前的研究经常发现在迁移学习过程中有必要微调所有层。

## 预处理

## 概述

## 结果

作为迁移学习的下游数据集，我们使用了最近推出的SER数据集MSPPodcast(v1.7)[29]。它被分成了不同于说话人的分区：
·一个由38179个片段组成的训练集
·一个由7538个片段组成的开发集，从44个说话者(22名男性-22名女性)中收集
·12 902个片段组成的测试集，由60名说话者(30名男性-30名女性)组成，MSP-Podcast已经为arousal、valence、dominance以及8个情绪类别的情感维度加上一个额外的其他类别进行了注释。

在目前的工作中，我们将重点放在情感方面。这些已经在发声层面上以7-point 李克特量表（Likert scale）进行了注解，个别注释者的分数被平均以获得一致投票。与文献[30，31]中的其他方法类似，我们将连续值归入3分标度。我们使用以下映射：

低：[1-3]·中：(3-5)·高：(5-7)


这会导致了严重的不平衡分布，9%的数据在低范围内，67%在中范围内，24%在高范围内。

这些维度是能够用来定义影响，并且经过充分验证的结构[32]，其已经被证明通过不同的声学提示（acoustic cues）来表现[33]。虽然它们量化了情感表达的不同方面，但它们并不是彼此完全无关的。他们的相似性可以通过三个情绪维度之间的皮尔逊相关性Pearson correlation来衡量。在MSP-Podcast训练集上，arousal与valence的相关性为0.2412，与dominance的相关性为0.7953。这表明了这三个方案之间的任务相似程度，这是我们在迁移学习实验中利用的一个事实。

我们还利用三个额外的数据集进行预训练，我们将把它们称为上游数据集：AudioSet[27]，原始作者表明它有助于SER，VoxCeleb1[34]，我们预训练说话人识别，因为它被证明转换成SER[26]，以及IEMOCAP[35]，它被注释为相同的情感维度。由于篇幅所限，我们参考原始出版物了解每个数据集的详细说明。

我们进行了三组实验：
·在不同的数据集上进行预训练；在MSPPodcast上针对arousal任务进行微调
·在MSP-Podcast上针对不同的任务进行训练；在arousal上进行微调
·在干净的arousal数据上进行训练；在带宽受限的arousal数据上进行微调。

## 精读

深度学习（DL）方法在后几年获得了显着的突出地位。对其有效性最常被接受的解释之一是深度神经网络（DNN）体系结构可以学习在不同任务之间进行迁移的通用表征[1,2]。最近随着自我监督学习的出现，这引出了大量关于利用过去信息提高性能和增加新任务和/或数据集收敛性的文献研究[3,4]。然而，这种技术并不总是能产生更好的性能，会导致“负迁移”的良好记录效应[5,6]。这就提出了迁移学习何时成功的问题，特别是它在多大程度上受到预训练、下游任务、输入特征、体系结构类型以及不同数据集之间并置的影响。

为此，Neyshabur等[7]研究了视觉领域的迁移学习，并假设下游任务受益于预训练，因为模型正在学习可迁移的高级特征，并且因为他们学习低级统计。这些观察结果可归因于在视觉领域广泛研究的组合性概念[8]。许多先前的工作都集中在学习音频域中的一般表征，无论是使用监督[9,10,11]，无监督[12,13,14]还是自我监督方法[15,16,17,18]]。随着数据稀缺的众多应用程序的兴起，例如在医疗领域[19]，该主题变得越来越相关。在这项工作中，我们将语音情绪识别（SER）作为下游应用，这是一个在社区中受到相当关注但大数据尚未广泛应用的领域[20]。虽然DNN已经超越了传统方法[21]，但并非所有任务和数据集都如此[22]。这导致社区采用迁移学习方法，从基于特征的[23]开始，最近转向DL方法[24,25,26]。因此，了解迁移学习的工作原理可能会设计更强大的算法，释放DL对SER和其他低资源音频任务的全部潜力。我们的主要贡献在于将预训练任务的影响与各个数据集之间的声学不匹配的影响区分开来；我们期望在成功迁移中发挥重要作用的两个因素。为此，我们首先利用三种不同的数据集和任务进行预训练，以说明这两种因素在实际应用中的相对重要性。然后，我们利用这样一个事实，即我们的SER数据集已被注释为多个不同的情绪方案，它们之间具有不同程度的相似性。这使我们能够通过训练一个方案并将知识迁移到同一数据集上的另一个方案来隔离任务相似性的影响。最后，我们通过将知识从相同数据的干净版本迁移到损坏版本来隔离声学相似性的影响。我们的实验表明，虽然这两个因素都很重要，但主要是缺乏任务相似性导致负迁移，而即使是极端的声发散也可以克服。此外，我们做出了令人惊讶的观察，即更接近输入的层更容易适应，这一发现可以解释为什么之前作品中的作者发现有必要对所有层进行微调而不是最后一层[10,17]。这是一个重要发现，因为微调更多层会增加优化的开销，因为需要调整更多参数。


我们的实验是使用Kong等人最近引入的Cnn14架构进行的[10]。它已经过培训，可以在AudioSet上进行音频标记[27]。作者开放了他们的代码和训练有素的权重1。我们使用16 kHz变体，因为我们使用的数据集也来自16 kHz。作为特征，我们使用了使用64个Mel bins，32 ms的窗口大小和10 ms的帧移，计算的log-Mel频谱图.C nn14遵循VGG架构设计[28]。在最后一个卷积层之后，使用最大和平均池将特征汇集到特征维度上，随后馈入两个线性层。在每第二个卷积层之后应用概率为0.2的辍学。

![]({30}_The%20Role%20of%20Task%20and%20Acoustic%20Similarity%20in%20Audio%20Transfer%20Learning_%20Insights%20from%20the%20Speech%20Emotion%20Recognition%20Case@triantafyllopoulosRoleTaskAcoustic2021.assets/image-20220603181014.png)

除非另有说明，所有的实验都是使用标准的随机梯度下降优化器进行的，恒定学习率为0.001，Nesterov momentum设置为0.9[36]，批次大小为8。这些网络总共训练了60个epoch。我们只显示在验证集上产生最佳性能的epoch的结果。

为了处理目标标签的不平衡问题，我们使用了一种平衡的非负似然损失(NLLoss)，该损失是通过将每一项与训练集中对应类别的频率的倒数相乘而得到的。我们首先通过从随机初始化训练模型来训练标准基线。经过58个时代的训练，我们的UAR达到了68.76%。我们将这种模型称为Cnn14-Baseline。


对于所有迁移学习实验，我们尝试两个变体：

·微调所有层：通过这个实验，我们有兴趣了解网络如何适应新数据。
·仅微调线性层：通过此实验，我们有兴趣了解网络如何能够利用在预培训中学到的特征。

为了衡量模型对新任务的适应情况，一个好的指标是训练前和训练后各层权重之间的距离，如Neyshabur等人所示。[7]。为此，我们使用余弦距离：

$$

d(x, y)=1-\frac{x \cdot y}{\|x\|\|y\|}

$$

我们的第一个实验是使用在第三节提到的上游数据集上预先训练的模型进行的。

- 对于AudioSet我们使用孔等人发布的权重。[10]并将该预训练模型称为Cnn14 AudioSet。
- 对于VoxCeleb1我们使用标准NLLoss对网络进行100个epoch的训练，并选择在验证集上提供最佳准确性的检查点(在epoch92上为64%)，我们将该模型称为Cnn14 VoxCeleb。
- 最后，对于IEMOCAP我们训练网络60个epoch，并选择在验证集上给出最佳UAR的检查点(在epoch33上为64%)。我们使用与张等人相同的装箱和训练/开发/测试拆分。[31][中英文摘要]。我们将这种模型称为Cnn14 IEMOCAP。


为了理清任务的影响和上下游任务之间的声学差异，我们利用了MSP Podcast被注释为三种不同情绪属性的事实，如第三节所述。因此，我们预训练了一个关于Valence的模型和另一个关于dominance能力的模型，我们分别称之为Cnn14-Valence和Cnn14-Dominance，并为arousal对它们进行了微调。在原始任务的预训练过程中，Valence模型在第52时段达到峰值，UAR为63.56%，dominance模型在第33时段达到54.87%。最后，我们有兴趣系统地研究从影响任务相似性中分离出来的声学相似性的影响。我们通过将数据通过窄带通滤波器来模拟不同程度的声学相似性  去除大部分频谱，而通过将知识迁移到arousal分类任务和从arousal分类任务迁移来保持任务相似性不变。我们使用中心频率为500 Hz的四阶Butterworth滤波器，并测试出以下频带宽度：[20,40,60,80,100,200,300,400]。

关于迁移学习的有效性，结果好坏参半：尽管在使用原始数据时，我们无法超过基线表现，但表1和图2a都表明，预培训加速了收敛。此外，微调所有层总是更有利于仅微调线性层，这一发现与以前的工作一致[10，17]。这也与图3a中LayerWise余弦距离显示的趋势一致。较早的层在初始化方面比后一层有更多的适应，除了CNN14-VoxCeleb之外的所有模型的进一步训练加剧了这一趋势。我们的主要关注点是区分任务和声学相似性的影响。使用表1中所示的不同上游数据集进行的初步实验表明，这两种影响都在发挥作用。对于arousal分类，Cnn14-AudioSet的表现不如Cnn14-IEMOCAP和Cnn14-VoxCeleb，这表明任务相似性比声学相似性更重要；虽然AudioSet比其他两个数据集更大，呈现更多的声学多样性，但音频标记任务也与arousal分类有本质的不同。在MSP-Podcast上对不同标注方案到觉醒的知识迁移实验进一步说明了任务相似性的重要性。与dominance能力相比，Valence训练与arousal的相关性要小得多，这也会导致更糟糕的表现，并需要更长的时间才能收敛。Cnn14-Valence和Cnn14-IEMOCAP之间的直接比较也突显了任务相似性与声学相似性的相对重要性。后者是针对总体较少的数据进行预训练的，这些数据来自记录在非常狭窄的一组条件中的不同数据集，但针对相同的任务。然而，它的表现比前者要好得多，前者是在相同的数据上进行预训练的，但任务不同。最后，我们关于将知识迁移到相同数据的损坏版本但用于相同任务(arousal)的实验表明，成功的知识迁移是可能的，虽然取决于声学失配的程度。图2B中的UAR结果表明，从预先训练的网络开始会导致更好的性能、更快的收敛和更好的初始化，即使被破坏的声音信号与原始的声音信号根本不同。正如预期的那样，更宽的带宽会带来整体更好的性能，无论是在进行预培训的情况下，还是在没有预培训的情况下。图3b所示的权重空间距离表明，微调主要影响较早的层，这表明这些层还负责适应上行和下行数据集之间的声学条件变化。

我们已经通过实验证明，SER 的迁移学习取决于声学和任务相似性，后者是决定因素。结果表明，错误选择任务可能不利于迁移学习成绩。另外，我们已经确定，靠近输入的层比接近输出的层更适应；这一事实解释了为什么许多以前的作品在微调所有层而不是简单的最后层时表现出更好的性能。这些发现应该指导 SER 的体系结构设计和预培训策略。如图所示，不同的情感维度可能导致根本不同的表征，这一事实使得寻求通用表征非常具有挑战性。为了获得语音片段的整体情感表征，我们需要能够概括几个不同任务的表征，这些表征应该反映在训练前的团中。

### 引文

## 摘录

---
title: "The Role of Task and Acoustic Similarity in Audio Transfer Learning: Insights from the Speech Emotion Recognition Case"
description: ""
citekey: triantafyllopoulosRoleTaskAcoustic2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:32:56
lastmod: 2023-04-11 11:55:52
---

> [!info] 论文信息
>1. Title：The Role of Task and Acoustic Similarity in Audio Transfer Learning: Insights from the Speech Emotion Recognition Case
>2. Author：Andreas Triantafyllopoulos, Björn W. Schuller
>3. Entry：[Zotero link](zotero://select/items/@triantafyllopoulosRoleTaskAcoustic2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Triantafyllopoulos_Schuller_2021_The Role of Task and Acoustic Similarity in Audio Transfer Learning.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 使用迁移学习时，越相关的任务，迁移效果会更好
- 最终微调过程，靠近输入的层比仅微调输出分类层有更多的适应
- 为了处理目标标签的不平衡问题，我们使用了一种平衡的非负似然损失(NLLoss)
- arousal、valence、dominance 三个情感维度并不是完全无关的，可以通过一定方式找出其相关性。- 使用迁移学习时，越相关的任务，迁移效果会更好
- 最终微调过程，靠近输入的层比仅微调输出分类层有更多的适应
- 为了处理目标标签的不平衡问题，我们使用了一种平衡的非负似然损失(NLLoss)
- arousal、valence、dominance三个情感维度并不是完全无关的，可以通过一定方式找出其相关性。

## 摘要

> [!abstract] With the rise of deep learning, deep knowledge transfer has emerged as one of the most effective techniques for getting state-of-the-art performance using deep neural networks. A lot of recent research has focused on understanding the mechanisms of transfer learning in the image and language domains. We perform a similar investigation for the case of speech emotion recognition (SER), and conclude that transfer learning for SER is influenced both by the choice of pre-training task and by the differences in acoustic conditions between the upstream and downstream data sets, with the former having a bigger impact. The effect of each factor is isolated by first transferring knowledge between different tasks on the same data, and then from the original data to corrupted versions of it but for the same task. We also demonstrate that layers closer to the input see more adaptation than ones closer to the output in both cases, a finding which explains why previous works often found it necessary to fine-tune all layers during transfer learning.

> 随着深度学习的兴起，深度知识迁移已经成为利用深度神经网络获得最先进性能的最有效技术之一。最近的许多研究都集中在理解意象和语言领域的迁移学习机制。我们对语音情感识别(SER)进行了类似的研究，得出结论：语音情感识别的迁移学习既受预训练任务选择的影响，也受上下游数据集声学条件差异的影响，其中前者影响更大。通过首先在相同数据的不同任务之间传输知识，然后将原始数据传输到其损坏的版本，但对于相同的任务，每个因素的影响是隔离的。我们还证明，在这两种情况下，靠近输入的层比靠近输出的层有更多的适应，这一发现解释了为什么以前的研究经常发现在迁移学习过程中有必要微调所有层。

## 预处理

## 概述

## 结果

作为迁移学习的下游数据集，我们使用了最近推出的SER数据集MSPPodcast(v1.7)[29]。它被分成了不同于说话人的分区：
·一个由38179个片段组成的训练集
·一个由7538个片段组成的开发集，从44个说话者(22名男性-22名女性)中收集
·12 902个片段组成的测试集，由60名说话者(30名男性-30名女性)组成，MSP-Podcast已经为arousal、valence、dominance以及8个情绪类别的情感维度加上一个额外的其他类别进行了注释。

在目前的工作中，我们将重点放在情感方面。这些已经在发声层面上以7-point 李克特量表（Likert scale）进行了注解，个别注释者的分数被平均以获得一致投票。与文献[30，31]中的其他方法类似，我们将连续值归入3分标度。我们使用以下映射：

低：[1-3]·中：(3-5)·高：(5-7)


这会导致了严重的不平衡分布，9%的数据在低范围内，67%在中范围内，24%在高范围内。

这些维度是能够用来定义影响，并且经过充分验证的结构[32]，其已经被证明通过不同的声学提示（acoustic cues）来表现[33]。虽然它们量化了情感表达的不同方面，但它们并不是彼此完全无关的。他们的相似性可以通过三个情绪维度之间的皮尔逊相关性Pearson correlation来衡量。在MSP-Podcast训练集上，arousal与valence的相关性为0.2412，与dominance的相关性为0.7953。这表明了这三个方案之间的任务相似程度，这是我们在迁移学习实验中利用的一个事实。

我们还利用三个额外的数据集进行预训练，我们将把它们称为上游数据集：AudioSet[27]，原始作者表明它有助于SER，VoxCeleb1[34]，我们预训练说话人识别，因为它被证明转换成SER[26]，以及IEMOCAP[35]，它被注释为相同的情感维度。由于篇幅所限，我们参考原始出版物了解每个数据集的详细说明。

我们进行了三组实验：
·在不同的数据集上进行预训练；在MSPPodcast上针对arousal任务进行微调
·在MSP-Podcast上针对不同的任务进行训练；在arousal上进行微调
·在干净的arousal数据上进行训练；在带宽受限的arousal数据上进行微调。

## 精读

深度学习（DL）方法在后几年获得了显着的突出地位。对其有效性最常被接受的解释之一是深度神经网络（DNN）体系结构可以学习在不同任务之间进行迁移的通用表征[1,2]。最近随着自我监督学习的出现，这引出了大量关于利用过去信息提高性能和增加新任务和/或数据集收敛性的文献研究[3,4]。然而，这种技术并不总是能产生更好的性能，会导致“负迁移”的良好记录效应[5,6]。这就提出了迁移学习何时成功的问题，特别是它在多大程度上受到预训练、下游任务、输入特征、体系结构类型以及不同数据集之间并置的影响。

为此，Neyshabur等[7]研究了视觉领域的迁移学习，并假设下游任务受益于预训练，因为模型正在学习可迁移的高级特征，并且因为他们学习低级统计。这些观察结果可归因于在视觉领域广泛研究的组合性概念[8]。许多先前的工作都集中在学习音频域中的一般表征，无论是使用监督[9,10,11]，无监督[12,13,14]还是自我监督方法[15,16,17,18]]。随着数据稀缺的众多应用程序的兴起，例如在医疗领域[19]，该主题变得越来越相关。在这项工作中，我们将语音情绪识别（SER）作为下游应用，这是一个在社区中受到相当关注但大数据尚未广泛应用的领域[20]。虽然DNN已经超越了传统方法[21]，但并非所有任务和数据集都如此[22]。这导致社区采用迁移学习方法，从基于特征的[23]开始，最近转向DL方法[24,25,26]。因此，了解迁移学习的工作原理可能会设计更强大的算法，释放DL对SER和其他低资源音频任务的全部潜力。我们的主要贡献在于将预训练任务的影响与各个数据集之间的声学不匹配的影响区分开来；我们期望在成功迁移中发挥重要作用的两个因素。为此，我们首先利用三种不同的数据集和任务进行预训练，以说明这两种因素在实际应用中的相对重要性。然后，我们利用这样一个事实，即我们的SER数据集已被注释为多个不同的情绪方案，它们之间具有不同程度的相似性。这使我们能够通过训练一个方案并将知识迁移到同一数据集上的另一个方案来隔离任务相似性的影响。最后，我们通过将知识从相同数据的干净版本迁移到损坏版本来隔离声学相似性的影响。我们的实验表明，虽然这两个因素都很重要，但主要是缺乏任务相似性导致负迁移，而即使是极端的声发散也可以克服。此外，我们做出了令人惊讶的观察，即更接近输入的层更容易适应，这一发现可以解释为什么之前作品中的作者发现有必要对所有层进行微调而不是最后一层[10,17]。这是一个重要发现，因为微调更多层会增加优化的开销，因为需要调整更多参数。


我们的实验是使用Kong等人最近引入的Cnn14架构进行的[10]。它已经过培训，可以在AudioSet上进行音频标记[27]。作者开放了他们的代码和训练有素的权重1。我们使用16 kHz变体，因为我们使用的数据集也来自16 kHz。作为特征，我们使用了使用64个Mel bins，32 ms的窗口大小和10 ms的帧移，计算的log-Mel频谱图.C nn14遵循VGG架构设计[28]。在最后一个卷积层之后，使用最大和平均池将特征汇集到特征维度上，随后馈入两个线性层。在每第二个卷积层之后应用概率为0.2的辍学。

![]({30}_The%20Role%20of%20Task%20and%20Acoustic%20Similarity%20in%20Audio%20Transfer%20Learning_%20Insights%20from%20the%20Speech%20Emotion%20Recognition%20Case@triantafyllopoulosRoleTaskAcoustic2021.assets/image-20220603181014.png)

除非另有说明，所有的实验都是使用标准的随机梯度下降优化器进行的，恒定学习率为0.001，Nesterov momentum设置为0.9[36]，批次大小为8。这些网络总共训练了60个epoch。我们只显示在验证集上产生最佳性能的epoch的结果。

为了处理目标标签的不平衡问题，我们使用了一种平衡的非负似然损失(NLLoss)，该损失是通过将每一项与训练集中对应类别的频率的倒数相乘而得到的。我们首先通过从随机初始化训练模型来训练标准基线。经过58个时代的训练，我们的UAR达到了68.76%。我们将这种模型称为Cnn14-Baseline。


对于所有迁移学习实验，我们尝试两个变体：

·微调所有层：通过这个实验，我们有兴趣了解网络如何适应新数据。
·仅微调线性层：通过此实验，我们有兴趣了解网络如何能够利用在预培训中学到的特征。

为了衡量模型对新任务的适应情况，一个好的指标是训练前和训练后各层权重之间的距离，如Neyshabur等人所示。[7]。为此，我们使用余弦距离：

$$

d(x, y)=1-\frac{x \cdot y}{\|x\|\|y\|}

$$

我们的第一个实验是使用在第三节提到的上游数据集上预先训练的模型进行的。

- 对于AudioSet我们使用孔等人发布的权重。[10]并将该预训练模型称为Cnn14 AudioSet。
- 对于VoxCeleb1我们使用标准NLLoss对网络进行100个epoch的训练，并选择在验证集上提供最佳准确性的检查点(在epoch92上为64%)，我们将该模型称为Cnn14 VoxCeleb。
- 最后，对于IEMOCAP我们训练网络60个epoch，并选择在验证集上给出最佳UAR的检查点(在epoch33上为64%)。我们使用与张等人相同的装箱和训练/开发/测试拆分。[31][中英文摘要]。我们将这种模型称为Cnn14 IEMOCAP。


为了理清任务的影响和上下游任务之间的声学差异，我们利用了MSP Podcast被注释为三种不同情绪属性的事实，如第三节所述。因此，我们预训练了一个关于Valence的模型和另一个关于dominance能力的模型，我们分别称之为Cnn14-Valence和Cnn14-Dominance，并为arousal对它们进行了微调。在原始任务的预训练过程中，Valence模型在第52时段达到峰值，UAR为63.56%，dominance模型在第33时段达到54.87%。最后，我们有兴趣系统地研究从影响任务相似性中分离出来的声学相似性的影响。我们通过将数据通过窄带通滤波器来模拟不同程度的声学相似性  去除大部分频谱，而通过将知识迁移到arousal分类任务和从arousal分类任务迁移来保持任务相似性不变。我们使用中心频率为500 Hz的四阶Butterworth滤波器，并测试出以下频带宽度：[20,40,60,80,100,200,300,400]。

关于迁移学习的有效性，结果好坏参半：尽管在使用原始数据时，我们无法超过基线表现，但表1和图2a都表明，预培训加速了收敛。此外，微调所有层总是更有利于仅微调线性层，这一发现与以前的工作一致[10，17]。这也与图3a中LayerWise余弦距离显示的趋势一致。较早的层在初始化方面比后一层有更多的适应，除了CNN14-VoxCeleb之外的所有模型的进一步训练加剧了这一趋势。我们的主要关注点是区分任务和声学相似性的影响。使用表1中所示的不同上游数据集进行的初步实验表明，这两种影响都在发挥作用。对于arousal分类，Cnn14-AudioSet的表现不如Cnn14-IEMOCAP和Cnn14-VoxCeleb，这表明任务相似性比声学相似性更重要；虽然AudioSet比其他两个数据集更大，呈现更多的声学多样性，但音频标记任务也与arousal分类有本质的不同。在MSP-Podcast上对不同标注方案到觉醒的知识迁移实验进一步说明了任务相似性的重要性。与dominance能力相比，Valence训练与arousal的相关性要小得多，这也会导致更糟糕的表现，并需要更长的时间才能收敛。Cnn14-Valence和Cnn14-IEMOCAP之间的直接比较也突显了任务相似性与声学相似性的相对重要性。后者是针对总体较少的数据进行预训练的，这些数据来自记录在非常狭窄的一组条件中的不同数据集，但针对相同的任务。然而，它的表现比前者要好得多，前者是在相同的数据上进行预训练的，但任务不同。最后，我们关于将知识迁移到相同数据的损坏版本但用于相同任务(arousal)的实验表明，成功的知识迁移是可能的，虽然取决于声学失配的程度。图2B中的UAR结果表明，从预先训练的网络开始会导致更好的性能、更快的收敛和更好的初始化，即使被破坏的声音信号与原始的声音信号根本不同。正如预期的那样，更宽的带宽会带来整体更好的性能，无论是在进行预培训的情况下，还是在没有预培训的情况下。图3b所示的权重空间距离表明，微调主要影响较早的层，这表明这些层还负责适应上行和下行数据集之间的声学条件变化。

我们已经通过实验证明，SER 的迁移学习取决于声学和任务相似性，后者是决定因素。结果表明，错误选择任务可能不利于迁移学习成绩。另外，我们已经确定，靠近输入的层比接近输出的层更适应；这一事实解释了为什么许多以前的作品在微调所有层而不是简单的最后层时表现出更好的性能。这些发现应该指导 SER 的体系结构设计和预培训策略。如图所示，不同的情感维度可能导致根本不同的表征，这一事实使得寻求通用表征非常具有挑战性。为了获得语音片段的整体情感表征，我们需要能够概括几个不同任务的表征，这些表征应该反映在训练前的团中。

### 引文

---
title: "Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition"
description: ""
citekey: sunMultimodalCrossSelfAttention2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:33:05
lastmod: 2023-04-11 11:57:43
---

> [!info] 论文信息
>1. Title：Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition
>2. Author：Licai Sun, Bin Liu, Jianhua Tao, Zheng Lian
>3. Entry：[Zotero link](zotero://select/items/@sunMultimodalCrossSelfAttention2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sun et al_2021_Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现： https://github.com/david-yoon/attentive-modality-hopping-for-SER, https://github.com/SysCV/pcan
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 语音文本双模态任务引入跨模态交互注意力机制, 并于自注意力机制相结合, 在模态间传播信息。

## 摘要

> [!abstract] Speech Emotion Recognition (SER) requires a thorough understanding of both the linguistic content of an utterance (i.e., textual information) and how the speaker utters it (i.e., acoustic information). The one vital challenge in SER is how to effectively fuse these two kinds of information. In this paper, we propose a novel Multimodal Cross- and Self-Attention Network (MCSAN) to tackle this problem. The core of MCSAN is to employ the parallel cross- and self-attention modules to explicitly model both inter- and intra-modal interactions of audio and text. Specifically, the cross-attention module utilizes the cross-attention mechanism to guide one modality to attend to the other modality and update the features accordingly. Similarly, the self-attention module employs the self-attention mechanism to propagate information within each modality. We evaluate MCSAN on two benchmark datasets, IEMOCAP and MELD. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on both datasets.

> 语音情感识别(SER)需要对话语的语言内容(即文本信息)和说话人如何说出(即声学信息)进行深入的理解。SER面临的一个重要挑战是如何有效地融合这两种信息。在本文中，我们提出了一种新的多模式交叉和自我注意网络(MCSAN)来解决这一问题。MCSAN的核心是使用并行的交叉和自我注意模块来显式地建模音频和文本的模式间和模式内的交互。具体地说，交叉注意模块利用交叉注意机制来引导一个模态关注另一个模态并相应地更新特征。同样，自我注意模块使用自我注意机制在每个模态内传播信息。我们在IEMOCAP和MELD两个基准数据集上对MCSAN进行了评估。实验结果表明，我们提出的模型在两个数据集上都达到了最好的性能。

## 预处理

## 概述

## 结果

IEMOCAP 按照[20,14]进行10倍交叉验证，其中8:1:1分别用于训练，验证和测试。采用加权准确度（WA，即总体准确度）和未加权准确度（UA，即所有情绪类别的平均准确度）作为评估指标。MELD[21]整个数据集分为三部分：训练（9989），验证（1109）和测试（2610）。在[21,22]之后，我们报告了该数据集上的加权平均 F1分数。

我们从语音信号中提取40维Mel频率Cepstral系数（MFCC）。窗口大小和帧移大小分别设置为25毫秒和10毫秒。MFCC特征序列的最大长度设置为1000。我们在将它们输入音频编码器之前执行z-normalization。

对于文本特征，我们首先将word-tokenizer应用于数据集提供的transcripts。然后使用预先训练的GloVe模型将话语中的每个单词嵌入到300维矢量中[23]。我们在PyTorch框架内实现了我们的模型。模型中隐藏的神经元数量是128个。交叉和自我注意模块中的堆叠层数为1。头数设置为8。音频编码器中卷积层和最大池层的内核大小为3。为了训练模型，我们使用Adam优化器[24]，IEMOCAP的学习率为0.001，MELD的学习率为0.0005。批量大小为256。我们在IEMOCAP上训练最多30个epoch，在MELD上训练20个epoch。

## 精读

情绪在人类交流中起着重要作用。言语情绪识别（SER）旨在赋予机器感知情绪的能力。SER在人机交互领域具有广泛的应用[1]。以无处不在的语音助理（如亚马逊的Alexa和Apple的Siri）为例。他们有必要推断用户的情绪并做出适当的回应以增强用户体验。最近关于SER的研究仅关注声学信息。已经开发了各种深度学习模型来从手工声学特征或原始语音信号中提取与情绪相关的信息。如卷积神经网络（CNN）[2,3]，递归神经网络（RNN）[4,5]，自我注意机制[6]及其组合[7,8,9]。嵌入语音中的文本信息较少被利用。这些信息对于SER也是至关重要的，因为在某些情况下，话语的情绪可以由语言语义决定。例如，“这真的是一个不幸的日子？”表示说话者情绪悲伤。然而，融合声学和文本信息并不是微不足道的。在融合多模态信息时，通常需要考虑两种交互，即模态内交互和模态间交互[10]。模态内交互是指单一模态内的细粒度特征交互。例如声学特征中的帧-帧关系和文本特征中的单词-单词关系。通过对模态内相互作用进行建模，我们可以捕捉情绪预测的模态特定模式。由于以不同音调说出句子可能会产生完全不同的情绪，因此有必要对音频和文本之间的帧字关系进行建模。这些是所谓的模态间相互作用。模态间交互是同步的（例如，强调特定的单词）或异步的（例如，说一些有趣的东西后的笑声）。最近，一些工作已经探索融合SER的声学和文本信息。一般来说，这些作品可以分为三种类型。第一类为每种模态建立独立模型，并将其输出结合起来进行最终情绪分类[11,12,13,14]。每种模式都可以采用不同的架构来最好地适应不同的输入。

例如，Yoon等[11]采用两个长短期记忆（LSTM）网络对音频和文本进行编码。

Tripathi等[13]将1D-CNN用于词嵌入，将2D-CNN用于光谱特征。尽管可以捕获模态内交互，但是没有探索模态间交互。

第二种类型利用对齐的音频和文本作为输入[15]。对齐的特征首先被融合，然后被馈送到用于顺序学习的时间模型中。因此，可以在整个过程中捕获模态间交互。但是，成本是提供对齐信息。为了克服这个问题，

第三种类型利用注意机制来推断音频和文本之间的潜在跨模态关系。

Yoon等[16]提出了一种新颖的多跳机制，通过调节另一种模态迭代地选择和聚合来自一种模态的信息。

Xu等人[17]利用注意机制学习每个单词的潜在对齐语音帧。然而，它们都没有明确地模拟音频和文本的模态内和模态间交互。

我们在本文中提出了一种新颖的多模式交叉和自我关注网络（MCSAN）。MCSAN主要由交叉注意模块和两个自我注意模块组成。交叉注意模块利用交叉注意机制在音频和文本之间传播信息，而自我注意模块采用自我注意机制在每种模态内传播信息。由于这些模块，MCSAN可以显式地模拟音频和文本的模型间和模型内交互。为了验证MCSAN的有效性，我们对两个数据集进行了实验。结果表明，它优于最先进的方法。我们还进行消融研究来证明我们模型的设计选择是正确的。

如图1所示，MCSAN首先使用音频编码器和文本编码器分别对声学和文本特征进行编码。然后，编码后的特征序列被送入交叉注意和自我注意模块，以学习音频和文本的模式间和模式内交互。最后，这些模块的输出被连接起来，并发送到一个完全连接的分类器中进行情感预测。具体内容介绍如下。

![]({31}_Multimodal%20Cross-%20and%20Self-Attention%20Network%20for%20Speech%20Emotion%20Recognition@sunMultimodalCrossSelfAttention2021.assets/image-20220603194955.png)

2.1. 音频编码器

假设语音的输入声学特征序列表示为 $\mathbf{X}_{a}=\left\{\mathbf{x}_{a}^{1}, \mathbf{x}_{a}^{2}, \ldots, \mathbf{x}_{a}^{T_{a}^{\prime}}\right\}^{T} \in \mathbb{R}^{T_{a}^{\prime} \times d_{a}}\left(T_{a}^{\prime}\right.$ 是声学帧的数量, $d_{a}$ 是特征数量). 我们采用CNN的体系结构，并采用LSTM作为音频编码器。具体地说，使用两个一维时间卷积层来捕获局部特征。由于 $T_{a}^{\prime}$ 通常较大，因此每个卷积层之后是一个最大合并层，以降低时间分辨率并便于后续学习。然后使用双向LSTM(BiLSTM)层来捕获序列内的时间相关性。对BiLSTM层的前向和后向隐藏特征向量进行平均，得到编码后的声学特征。整个过程可以概括为：
$$
\begin{aligned}
\mathbf{X}^{a} &=\operatorname{ConvBlock}\left(\operatorname{ConvBlock}\left(\mathbf{X}^{a}\right)\right) \\
\overleftarrow{\mathbf{h}}{ }_{a}^{t}, \overrightarrow{\mathbf{h}}_{a}^{t} &=\operatorname{BiLSTM}\left(\mathbf{x}_{a}^{t}, \overleftarrow{\mathbf{h}}_{a}^{t+1}, \overrightarrow{\mathbf{h}}_{a}^{t-1}\right), t=1,2, \ldots, T_{a} \\
\mathbf{h}_{a}^{t} &=\frac{1}{2}\left(\overleftarrow{\mathbf{h}}_{a}^{t}+\overrightarrow{\mathbf{h}}_{a}^{t}\right), t=1,2, \ldots, T_{a}
\end{aligned}
$$
其中， ConvBlock $(\cdot)=\operatorname{Max} \operatorname{Pool}(\operatorname{Conv} 1 D(\cdot)), T_{a}$ 是第二个池层之后的声学帧的数量。 我们将 $\mathbf{H}_{a}=\left\{\mathbf{h}_{a}^{1}, \mathbf{h}_{a}^{2}, \ldots, \mathbf{h}_{a}^{T_{a}}\right\}^{T} \in \mathbb{R}^{T_{a} \times d}$ ( $d$ 是统一的特征维度) 表示为编码的声学特征序列。

2.2. Text Encoder

假设语音对应的的输入文本特征序列表示为 $\mathbf{X}_{l}=\left\{\mathbf{x}_{l}^{1}, \mathbf{x}_{l}^{2}, \ldots, \mathbf{x}_{l}^{T_{l}}\right\}^{T} \in \mathbb{R}^{T_{l} \times d_{l}}$ (T $T_{l}$ 是字数, $d_{l}$ 是特征尺寸). 考虑到 $T_{l}$ 通常很小，我们只使用双向LSTM层来编码词级文本特征。编码后的文本特征序列 $\mathbf{H}_{l}=$ $\left\{\mathbf{h}_{l}^{1}, \mathbf{h}_{l}^{2}, \ldots, \mathbf{h}_{l}^{T_{l}}\right\}^{T} \in \mathbb{R}^{T_{l} \times d}$ 可以通过如下计算得到:
$$
\begin{aligned}
\overleftarrow{\mathbf{h}}_{a}^{t}, \overrightarrow{\mathbf{h}}_{a}^{t} &=\operatorname{BiLSTM}\left(\mathbf{x}_{a}^{t}, \overleftarrow{\mathbf{h}}_{a}^{t+1}, \overrightarrow{\mathbf{h}}_{a}^{t-1}\right), t=1,2, \ldots, T_{a} \\
\mathbf{h}_{l}^{t} &=\frac{1}{2}\left(\overleftarrow{\mathbf{h}}_{l}^{t}+\overrightarrow{\mathbf{h}}_{l}^{t}\right), t=1,2, \ldots, T_{l}
\end{aligned}
$$

2.3. Cross-Attention Module

交叉注意模块旨在捕捉每对声学框架和文本单词之间的跨模式交互。该模块由一个位置嵌入层(为了简单起见，我们没有在图1中描述它)和N个堆叠的交叉注意力层和前馈层组成。位置嵌入层用于将时间信息注入特征序列[18]。该模块的主要观点是利用交叉注意机制来学习两个模态之间的关联，然后根据学习到的关联将信息从一个模态传播到另一个模态。在接下来的部分中，我们详细介绍了交叉注意机制。要了解音频和文本之间的关联，我们首先需要使用线性投影将每个特征序列转换为三个术语，即query, key, 和value：
$$
\begin{gathered}
\mathbf{Q}_{a}, \mathbf{Q}_{l}=\mathbf{W}_{a}^{Q} \mathbf{H}_{a}, \mathbf{W}_{l}^{Q} \mathbf{H}_{l} \\
\mathbf{K}_{a}, \mathbf{K}_{l}=\mathbf{W}_{a}^{K} \mathbf{H}_{a}, \mathbf{W}_{l}^{K} \mathbf{H}_{l} \\
\mathbf{V}_{a}, \mathbf{V}_{l}=\mathbf{W}_{a}^{V} \mathbf{H}_{a}, \mathbf{W}_{l}^{V} \mathbf{H}_{l}
\end{gathered}
$$
其中 $\mathbf{Q}_{m}, \mathbf{K}_{m}, \mathbf{V}_{m} \in \mathbb{R}^{T_{m} \times d}$ 是第m模态特征序列的query, key, 和value $m \in\{a, l\}$. $\mathbf{W}_{m}^{Q}, \mathbf{W}_{m}^{K}, \mathbf{W}_{m}^{V} \in$ $\mathbb{R}^{d \times d}$ 是相应的投影矩阵。

在[18]的基础上，我们以交叉的方式计算音频和文本的query和key的点积，以估计两个模态之间的关联。然后通过Softmax函数对结果进行缩放和逐行归一化，以获得关注度权重。在那之后,。我们使用相应的权重聚合每个特征序列的值项，以获得两个模态之间的传播信息：

$$
\begin{aligned}
&\Delta \mathbf{H}_{a \rightarrow l}=\operatorname{softmax}\left(\mathbf{Q}_{l} \mathbf{K}_{a}^{T} / \sqrt{d}\right) \mathbf{V}_{a} \\
&\Delta \mathbf{H}_{l \rightarrow a}=\operatorname{softmax}\left(\mathbf{Q}_{a} \mathbf{K}_{l}^{T} / \sqrt{d}\right) \mathbf{V}_{l}
\end{aligned}
$$
其中 $\Delta \mathbf{H}_{a \rightarrow l} \in \mathbf{H}^{T_{l} \times d}, \Delta \mathbf{H}_{l \rightarrow a} \in \mathbb{R}^{T_{a} \times d}$ 分别表示从音频到文本和文本到音频的传播信息。

上面描述的过程是单头注意力。在实践中，我们使用多头注意，这可以通过多次做单头注意，然后结合每个头的结果来完成。详情见[18]。

最后，我们用来自另一个模态的传播信息来更新另一个模态的特征。

$$
\begin{aligned}
\mathbf{H}_{a} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{a}+\Delta \mathbf{H}_{l \rightarrow a}\right) \\
\mathbf{H}_{l} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{l}+\Delta \mathbf{H}_{a \rightarrow l}\right)
\end{aligned}
$$
为了进一步增加表示能力，在交叉注意力层之后增加了完全连接的前馈层[18]：
$$
\begin{aligned}
\mathbf{H}_{a} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{a}+F e e d F \operatorname{orward}\left(\mathbf{H}_{a}\right)\right) \\
\mathbf{H}_{l} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{l}+F \text { eedForward }\left(\mathbf{H}_{l}\right)\right)
\end{aligned}
$$
我们将模块中最后一个堆叠层的输出分别表示为$\mathbf{H}_{a}^{c}$ and $\mathbf{H}_{l}^{c}$。

2.4. Self-Attention Module

与交叉注意模块平行，自我注意模块旨在捕获音频和文本中的模态内交互。这个模块与交叉注意模块相似，只是使用了自我注意机制。自我注意机制与交叉注意机制有着相同的精神。唯一的区别是查询、键和值来自相同的模态。因此，在自我注意模块中，一个堆叠层的整个过程可以总结如下：

$$
\begin{aligned}
\Delta \mathbf{H}_{m} &=\operatorname{softmax}\left(\mathbf{Q}_{m} \mathbf{K}_{m}^{T} / \sqrt{d}\right) \mathbf{V}_{m} \\
\mathbf{H}_{m} &=\text { Layer } \operatorname{Norm}\left(\mathbf{H}_{m}+\Delta \mathbf{H}_{m}\right) \\
\mathbf{H}_{m} &=\text { Layer } \operatorname{Norm}\left(\mathbf{H}_{m}+\text { FeedForward }\left(\mathbf{H}_{m}\right)\right)
\end{aligned}
$$
其中 $\Delta \mathbf{H}_{m} \in \mathbb{R}^{T_{m} \times d}$ 模态m内的传播信息， $m \in\{a, l\}$. 我们将图1中两个自我注意模块中最后一层的输出分别表示 $\mathbf{H}_{a}^{s}$ and $\mathbf{H}_{l}^{s}$.

为了执行最终分类，我们首先使用全局最大池层来总结交叉和自我注意模块的每个输出。假设$\mathbf{H}_{a}^{c}, \mathbf{H}_{l}^{c}, \mathbf{H}_{a}^{s}, \mathbf{H}_{l}^{s}$ 的汇总特征分别为 $\mathbf{h}_{a}^{c}, \mathbf{h}_{l}^{c}, \mathbf{h}_{a}^{s}, \mathbf{h}_{l}^{s} \in \mathbb{R}^{d}$. 然后，我们将它们连接起来，以获得语音级的表示。最后，遵循完全连接的网络和Softmax层来预测潜在的情感。利用交叉熵损失对模型进行优化。上述过程概括如下：

$$
\begin{aligned}
&\mathbf{h}=\operatorname{Concat}\left(\mathbf{h}_{a}^{c}, \mathbf{h}_{l}^{c}, \mathbf{h}_{a}^{s}, \mathbf{h}_{l}^{s}\right) \\
&\hat{\mathbf{y}}=\operatorname{Softmax}\left(f_{\theta}(\mathbf{h})\right) \\
&\mathcal{L}=-\sum_{i} y_{i} \log \left(\hat{y}_{i}\right)
\end{aligned}
$$
其中 $\mathbf{y}=\left\{y_{1}, y_{2}, \ldots, y_{n}\right\}^{T}$ 是情感标签的 one-hot 向量, $\hat{\mathbf{y}}=\left\{\hat{y}_{1}, \hat{y}_{2}, \ldots, \hat{y}_{n}\right\}^{T}$ 是预测的概率分布, $n$ 是情感类别的个数, $f_{\theta}$ 是参数为 $\theta$ 的全连接网络。


3.3. 基线

在IEMOCAP数据集上，以下基线用于比较：
1. MDRE[11]采用双递归神经网络对音频和文本进行编码，然后使用完全连接的神经网络将两种模态的结果组合起来用于最终情绪分类。
2. MHA[16]基于MDRE，它另外利用新颖的多跳注意机制自动推断音频和文本之间的相关性。
3. Xu等人[17]提出使用注意机制来学习音频和文本之间的潜在对齐。
4. CAN[14]通过以正常和交叉的方式使用每种模态的注意权重来聚合来自对齐音频和文本的顺序信息。

在MELD数据集上，我们使用两个基线进行比较：
1. cMKL[25]采用CNN进行特征提取，并使用多核学习来融合多模态特征。
2. Liang 等人[22]采用两种深度自动编码器来学习音频和文本的潜在表示，并将它们连接起来进行分类。

### 引文

## 摘录

---
title: "Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition"
description: ""
citekey: sunMultimodalCrossSelfAttention2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:33:05
lastmod: 2023-04-11 11:57:43
---

> [!info] 论文信息
>1. Title：Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition
>2. Author：Licai Sun, Bin Liu, Jianhua Tao, Zheng Lian
>3. Entry：[Zotero link](zotero://select/items/@sunMultimodalCrossSelfAttention2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Sun et al_2021_Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现： https://github.com/david-yoon/attentive-modality-hopping-for-SER, https://github.com/SysCV/pcan
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 语音文本双模态任务引入跨模态交互注意力机制, 并于自注意力机制相结合, 在模态间传播信息。

## 摘要

> [!abstract] Speech Emotion Recognition (SER) requires a thorough understanding of both the linguistic content of an utterance (i.e., textual information) and how the speaker utters it (i.e., acoustic information). The one vital challenge in SER is how to effectively fuse these two kinds of information. In this paper, we propose a novel Multimodal Cross- and Self-Attention Network (MCSAN) to tackle this problem. The core of MCSAN is to employ the parallel cross- and self-attention modules to explicitly model both inter- and intra-modal interactions of audio and text. Specifically, the cross-attention module utilizes the cross-attention mechanism to guide one modality to attend to the other modality and update the features accordingly. Similarly, the self-attention module employs the self-attention mechanism to propagate information within each modality. We evaluate MCSAN on two benchmark datasets, IEMOCAP and MELD. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on both datasets.

> 语音情感识别(SER)需要对话语的语言内容(即文本信息)和说话人如何说出(即声学信息)进行深入的理解。SER面临的一个重要挑战是如何有效地融合这两种信息。在本文中，我们提出了一种新的多模式交叉和自我注意网络(MCSAN)来解决这一问题。MCSAN的核心是使用并行的交叉和自我注意模块来显式地建模音频和文本的模式间和模式内的交互。具体地说，交叉注意模块利用交叉注意机制来引导一个模态关注另一个模态并相应地更新特征。同样，自我注意模块使用自我注意机制在每个模态内传播信息。我们在IEMOCAP和MELD两个基准数据集上对MCSAN进行了评估。实验结果表明，我们提出的模型在两个数据集上都达到了最好的性能。

## 预处理

## 概述

## 结果

IEMOCAP 按照[20,14]进行10倍交叉验证，其中8:1:1分别用于训练，验证和测试。采用加权准确度（WA，即总体准确度）和未加权准确度（UA，即所有情绪类别的平均准确度）作为评估指标。MELD[21]整个数据集分为三部分：训练（9989），验证（1109）和测试（2610）。在[21,22]之后，我们报告了该数据集上的加权平均 F1分数。

我们从语音信号中提取40维Mel频率Cepstral系数（MFCC）。窗口大小和帧移大小分别设置为25毫秒和10毫秒。MFCC特征序列的最大长度设置为1000。我们在将它们输入音频编码器之前执行z-normalization。

对于文本特征，我们首先将word-tokenizer应用于数据集提供的transcripts。然后使用预先训练的GloVe模型将话语中的每个单词嵌入到300维矢量中[23]。我们在PyTorch框架内实现了我们的模型。模型中隐藏的神经元数量是128个。交叉和自我注意模块中的堆叠层数为1。头数设置为8。音频编码器中卷积层和最大池层的内核大小为3。为了训练模型，我们使用Adam优化器[24]，IEMOCAP的学习率为0.001，MELD的学习率为0.0005。批量大小为256。我们在IEMOCAP上训练最多30个epoch，在MELD上训练20个epoch。

## 精读

情绪在人类交流中起着重要作用。言语情绪识别（SER）旨在赋予机器感知情绪的能力。SER在人机交互领域具有广泛的应用[1]。以无处不在的语音助理（如亚马逊的Alexa和Apple的Siri）为例。他们有必要推断用户的情绪并做出适当的回应以增强用户体验。最近关于SER的研究仅关注声学信息。已经开发了各种深度学习模型来从手工声学特征或原始语音信号中提取与情绪相关的信息。如卷积神经网络（CNN）[2,3]，递归神经网络（RNN）[4,5]，自我注意机制[6]及其组合[7,8,9]。嵌入语音中的文本信息较少被利用。这些信息对于SER也是至关重要的，因为在某些情况下，话语的情绪可以由语言语义决定。例如，“这真的是一个不幸的日子？”表示说话者情绪悲伤。然而，融合声学和文本信息并不是微不足道的。在融合多模态信息时，通常需要考虑两种交互，即模态内交互和模态间交互[10]。模态内交互是指单一模态内的细粒度特征交互。例如声学特征中的帧-帧关系和文本特征中的单词-单词关系。通过对模态内相互作用进行建模，我们可以捕捉情绪预测的模态特定模式。由于以不同音调说出句子可能会产生完全不同的情绪，因此有必要对音频和文本之间的帧字关系进行建模。这些是所谓的模态间相互作用。模态间交互是同步的（例如，强调特定的单词）或异步的（例如，说一些有趣的东西后的笑声）。最近，一些工作已经探索融合SER的声学和文本信息。一般来说，这些作品可以分为三种类型。第一类为每种模态建立独立模型，并将其输出结合起来进行最终情绪分类[11,12,13,14]。每种模式都可以采用不同的架构来最好地适应不同的输入。

例如，Yoon等[11]采用两个长短期记忆（LSTM）网络对音频和文本进行编码。

Tripathi等[13]将1D-CNN用于词嵌入，将2D-CNN用于光谱特征。尽管可以捕获模态内交互，但是没有探索模态间交互。

第二种类型利用对齐的音频和文本作为输入[15]。对齐的特征首先被融合，然后被馈送到用于顺序学习的时间模型中。因此，可以在整个过程中捕获模态间交互。但是，成本是提供对齐信息。为了克服这个问题，

第三种类型利用注意机制来推断音频和文本之间的潜在跨模态关系。

Yoon等[16]提出了一种新颖的多跳机制，通过调节另一种模态迭代地选择和聚合来自一种模态的信息。

Xu等人[17]利用注意机制学习每个单词的潜在对齐语音帧。然而，它们都没有明确地模拟音频和文本的模态内和模态间交互。

我们在本文中提出了一种新颖的多模式交叉和自我关注网络（MCSAN）。MCSAN主要由交叉注意模块和两个自我注意模块组成。交叉注意模块利用交叉注意机制在音频和文本之间传播信息，而自我注意模块采用自我注意机制在每种模态内传播信息。由于这些模块，MCSAN可以显式地模拟音频和文本的模型间和模型内交互。为了验证MCSAN的有效性，我们对两个数据集进行了实验。结果表明，它优于最先进的方法。我们还进行消融研究来证明我们模型的设计选择是正确的。

如图1所示，MCSAN首先使用音频编码器和文本编码器分别对声学和文本特征进行编码。然后，编码后的特征序列被送入交叉注意和自我注意模块，以学习音频和文本的模式间和模式内交互。最后，这些模块的输出被连接起来，并发送到一个完全连接的分类器中进行情感预测。具体内容介绍如下。

![]({31}_Multimodal%20Cross-%20and%20Self-Attention%20Network%20for%20Speech%20Emotion%20Recognition@sunMultimodalCrossSelfAttention2021.assets/image-20220603194955.png)

2.1. 音频编码器

假设语音的输入声学特征序列表示为 $\mathbf{X}_{a}=\left\{\mathbf{x}_{a}^{1}, \mathbf{x}_{a}^{2}, \ldots, \mathbf{x}_{a}^{T_{a}^{\prime}}\right\}^{T} \in \mathbb{R}^{T_{a}^{\prime} \times d_{a}}\left(T_{a}^{\prime}\right.$ 是声学帧的数量, $d_{a}$ 是特征数量). 我们采用CNN的体系结构，并采用LSTM作为音频编码器。具体地说，使用两个一维时间卷积层来捕获局部特征。由于 $T_{a}^{\prime}$ 通常较大，因此每个卷积层之后是一个最大合并层，以降低时间分辨率并便于后续学习。然后使用双向LSTM(BiLSTM)层来捕获序列内的时间相关性。对BiLSTM层的前向和后向隐藏特征向量进行平均，得到编码后的声学特征。整个过程可以概括为：
$$
\begin{aligned}
\mathbf{X}^{a} &=\operatorname{ConvBlock}\left(\operatorname{ConvBlock}\left(\mathbf{X}^{a}\right)\right) \\
\overleftarrow{\mathbf{h}}{ }_{a}^{t}, \overrightarrow{\mathbf{h}}_{a}^{t} &=\operatorname{BiLSTM}\left(\mathbf{x}_{a}^{t}, \overleftarrow{\mathbf{h}}_{a}^{t+1}, \overrightarrow{\mathbf{h}}_{a}^{t-1}\right), t=1,2, \ldots, T_{a} \\
\mathbf{h}_{a}^{t} &=\frac{1}{2}\left(\overleftarrow{\mathbf{h}}_{a}^{t}+\overrightarrow{\mathbf{h}}_{a}^{t}\right), t=1,2, \ldots, T_{a}
\end{aligned}
$$
其中， ConvBlock $(\cdot)=\operatorname{Max} \operatorname{Pool}(\operatorname{Conv} 1 D(\cdot)), T_{a}$ 是第二个池层之后的声学帧的数量。 我们将 $\mathbf{H}_{a}=\left\{\mathbf{h}_{a}^{1}, \mathbf{h}_{a}^{2}, \ldots, \mathbf{h}_{a}^{T_{a}}\right\}^{T} \in \mathbb{R}^{T_{a} \times d}$ ( $d$ 是统一的特征维度) 表示为编码的声学特征序列。

2.2. Text Encoder

假设语音对应的的输入文本特征序列表示为 $\mathbf{X}_{l}=\left\{\mathbf{x}_{l}^{1}, \mathbf{x}_{l}^{2}, \ldots, \mathbf{x}_{l}^{T_{l}}\right\}^{T} \in \mathbb{R}^{T_{l} \times d_{l}}$ (T $T_{l}$ 是字数, $d_{l}$ 是特征尺寸). 考虑到 $T_{l}$ 通常很小，我们只使用双向LSTM层来编码词级文本特征。编码后的文本特征序列 $\mathbf{H}_{l}=$ $\left\{\mathbf{h}_{l}^{1}, \mathbf{h}_{l}^{2}, \ldots, \mathbf{h}_{l}^{T_{l}}\right\}^{T} \in \mathbb{R}^{T_{l} \times d}$ 可以通过如下计算得到:
$$
\begin{aligned}
\overleftarrow{\mathbf{h}}_{a}^{t}, \overrightarrow{\mathbf{h}}_{a}^{t} &=\operatorname{BiLSTM}\left(\mathbf{x}_{a}^{t}, \overleftarrow{\mathbf{h}}_{a}^{t+1}, \overrightarrow{\mathbf{h}}_{a}^{t-1}\right), t=1,2, \ldots, T_{a} \\
\mathbf{h}_{l}^{t} &=\frac{1}{2}\left(\overleftarrow{\mathbf{h}}_{l}^{t}+\overrightarrow{\mathbf{h}}_{l}^{t}\right), t=1,2, \ldots, T_{l}
\end{aligned}
$$

2.3. Cross-Attention Module

交叉注意模块旨在捕捉每对声学框架和文本单词之间的跨模式交互。该模块由一个位置嵌入层(为了简单起见，我们没有在图1中描述它)和N个堆叠的交叉注意力层和前馈层组成。位置嵌入层用于将时间信息注入特征序列[18]。该模块的主要观点是利用交叉注意机制来学习两个模态之间的关联，然后根据学习到的关联将信息从一个模态传播到另一个模态。在接下来的部分中，我们详细介绍了交叉注意机制。要了解音频和文本之间的关联，我们首先需要使用线性投影将每个特征序列转换为三个术语，即query, key, 和value：
$$
\begin{gathered}
\mathbf{Q}_{a}, \mathbf{Q}_{l}=\mathbf{W}_{a}^{Q} \mathbf{H}_{a}, \mathbf{W}_{l}^{Q} \mathbf{H}_{l} \\
\mathbf{K}_{a}, \mathbf{K}_{l}=\mathbf{W}_{a}^{K} \mathbf{H}_{a}, \mathbf{W}_{l}^{K} \mathbf{H}_{l} \\
\mathbf{V}_{a}, \mathbf{V}_{l}=\mathbf{W}_{a}^{V} \mathbf{H}_{a}, \mathbf{W}_{l}^{V} \mathbf{H}_{l}
\end{gathered}
$$
其中 $\mathbf{Q}_{m}, \mathbf{K}_{m}, \mathbf{V}_{m} \in \mathbb{R}^{T_{m} \times d}$ 是第m模态特征序列的query, key, 和value $m \in\{a, l\}$. $\mathbf{W}_{m}^{Q}, \mathbf{W}_{m}^{K}, \mathbf{W}_{m}^{V} \in$ $\mathbb{R}^{d \times d}$ 是相应的投影矩阵。

在[18]的基础上，我们以交叉的方式计算音频和文本的query和key的点积，以估计两个模态之间的关联。然后通过Softmax函数对结果进行缩放和逐行归一化，以获得关注度权重。在那之后,。我们使用相应的权重聚合每个特征序列的值项，以获得两个模态之间的传播信息：

$$
\begin{aligned}
&\Delta \mathbf{H}_{a \rightarrow l}=\operatorname{softmax}\left(\mathbf{Q}_{l} \mathbf{K}_{a}^{T} / \sqrt{d}\right) \mathbf{V}_{a} \\
&\Delta \mathbf{H}_{l \rightarrow a}=\operatorname{softmax}\left(\mathbf{Q}_{a} \mathbf{K}_{l}^{T} / \sqrt{d}\right) \mathbf{V}_{l}
\end{aligned}
$$
其中 $\Delta \mathbf{H}_{a \rightarrow l} \in \mathbf{H}^{T_{l} \times d}, \Delta \mathbf{H}_{l \rightarrow a} \in \mathbb{R}^{T_{a} \times d}$ 分别表示从音频到文本和文本到音频的传播信息。

上面描述的过程是单头注意力。在实践中，我们使用多头注意，这可以通过多次做单头注意，然后结合每个头的结果来完成。详情见[18]。

最后，我们用来自另一个模态的传播信息来更新另一个模态的特征。

$$
\begin{aligned}
\mathbf{H}_{a} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{a}+\Delta \mathbf{H}_{l \rightarrow a}\right) \\
\mathbf{H}_{l} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{l}+\Delta \mathbf{H}_{a \rightarrow l}\right)
\end{aligned}
$$
为了进一步增加表示能力，在交叉注意力层之后增加了完全连接的前馈层[18]：
$$
\begin{aligned}
\mathbf{H}_{a} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{a}+F e e d F \operatorname{orward}\left(\mathbf{H}_{a}\right)\right) \\
\mathbf{H}_{l} &=\operatorname{Layer} \operatorname{Norm}\left(\mathbf{H}_{l}+F \text { eedForward }\left(\mathbf{H}_{l}\right)\right)
\end{aligned}
$$
我们将模块中最后一个堆叠层的输出分别表示为$\mathbf{H}_{a}^{c}$ and $\mathbf{H}_{l}^{c}$。

2.4. Self-Attention Module

与交叉注意模块平行，自我注意模块旨在捕获音频和文本中的模态内交互。这个模块与交叉注意模块相似，只是使用了自我注意机制。自我注意机制与交叉注意机制有着相同的精神。唯一的区别是查询、键和值来自相同的模态。因此，在自我注意模块中，一个堆叠层的整个过程可以总结如下：

$$
\begin{aligned}
\Delta \mathbf{H}_{m} &=\operatorname{softmax}\left(\mathbf{Q}_{m} \mathbf{K}_{m}^{T} / \sqrt{d}\right) \mathbf{V}_{m} \\
\mathbf{H}_{m} &=\text { Layer } \operatorname{Norm}\left(\mathbf{H}_{m}+\Delta \mathbf{H}_{m}\right) \\
\mathbf{H}_{m} &=\text { Layer } \operatorname{Norm}\left(\mathbf{H}_{m}+\text { FeedForward }\left(\mathbf{H}_{m}\right)\right)
\end{aligned}
$$
其中 $\Delta \mathbf{H}_{m} \in \mathbb{R}^{T_{m} \times d}$ 模态m内的传播信息， $m \in\{a, l\}$. 我们将图1中两个自我注意模块中最后一层的输出分别表示 $\mathbf{H}_{a}^{s}$ and $\mathbf{H}_{l}^{s}$.

为了执行最终分类，我们首先使用全局最大池层来总结交叉和自我注意模块的每个输出。假设$\mathbf{H}_{a}^{c}, \mathbf{H}_{l}^{c}, \mathbf{H}_{a}^{s}, \mathbf{H}_{l}^{s}$ 的汇总特征分别为 $\mathbf{h}_{a}^{c}, \mathbf{h}_{l}^{c}, \mathbf{h}_{a}^{s}, \mathbf{h}_{l}^{s} \in \mathbb{R}^{d}$. 然后，我们将它们连接起来，以获得语音级的表示。最后，遵循完全连接的网络和Softmax层来预测潜在的情感。利用交叉熵损失对模型进行优化。上述过程概括如下：

$$
\begin{aligned}
&\mathbf{h}=\operatorname{Concat}\left(\mathbf{h}_{a}^{c}, \mathbf{h}_{l}^{c}, \mathbf{h}_{a}^{s}, \mathbf{h}_{l}^{s}\right) \\
&\hat{\mathbf{y}}=\operatorname{Softmax}\left(f_{\theta}(\mathbf{h})\right) \\
&\mathcal{L}=-\sum_{i} y_{i} \log \left(\hat{y}_{i}\right)
\end{aligned}
$$
其中 $\mathbf{y}=\left\{y_{1}, y_{2}, \ldots, y_{n}\right\}^{T}$ 是情感标签的 one-hot 向量, $\hat{\mathbf{y}}=\left\{\hat{y}_{1}, \hat{y}_{2}, \ldots, \hat{y}_{n}\right\}^{T}$ 是预测的概率分布, $n$ 是情感类别的个数, $f_{\theta}$ 是参数为 $\theta$ 的全连接网络。


3.3. 基线

在IEMOCAP数据集上，以下基线用于比较：
1. MDRE[11]采用双递归神经网络对音频和文本进行编码，然后使用完全连接的神经网络将两种模态的结果组合起来用于最终情绪分类。
2. MHA[16]基于MDRE，它另外利用新颖的多跳注意机制自动推断音频和文本之间的相关性。
3. Xu等人[17]提出使用注意机制来学习音频和文本之间的潜在对齐。
4. CAN[14]通过以正常和交叉的方式使用每种模态的注意权重来聚合来自对齐音频和文本的顺序信息。

在MELD数据集上，我们使用两个基线进行比较：
1. cMKL[25]采用CNN进行特征提取，并使用多核学习来融合多模态特征。
2. Liang 等人[22]采用两种深度自动编码器来学习音频和文本的潜在表示，并将它们连接起来进行分类。

---
title: "Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for Speech Emotion Recognition"
description: ""
citekey: gaoDomainAdversarialAutoencoderAttention2021a
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:33:25
lastmod: 2023-04-11 11:59:27
---

> [!info] 论文信息
>1. Title：Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for Speech Emotion Recognition
>2. Author：Yuan Gao, JiaXing Liu, Longbiao Wang, Jianwu Dang
>3. Entry：[Zotero link](zotero://select/items/@gaoDomainAdversarialAutoencoderAttention2021a) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gao et al_2021_Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for2.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 使用迁移学习，由预训练网络从其他大型数据库学习先验知识，以此补偿情感数据库小的问题
- 使用领域对抗神经网络，包括情感预测、说话人分类、语料库分类等任务，通过消除先验知识中的非情感信息。
- 研究[13]已经确定，特征压缩后情绪信息可能会丢失。因此，引入多头注意（MHA）以减少特征融合阶段的信息丢失
- 引入了 valence 和 arousal 作为情感映射的两个标准，将不同数据库的不同标签，映射统一。

## 摘要

> [!abstract] Over the past two decades, although speech emotion recognition (SER) has garnered considerable attention, the problem of insufficient training data has been unresolved. A potential solution for this problem is to pre-train a model and transfer knowledge from large amounts of audio data. However, the data used for pre-training and testing originate from different domains, resulting in the latent representations to contain non-affective information. In this paper, we propose a domain-adversarial autoencoder to extract discriminative representations for SER. Through domain-adversarial learning, we can reduce the mismatch between domains while retaining discriminative information for emotion recognition. We also introduce multi-head attention to capture emotion information from different subspaces of input utterances. Experiments on IEMOCAP show that the proposed model outperforms the state-of-the-art systems by improving the unweighted accuracy by 4.15%, thereby demonstrating the effectiveness of the proposed model.

> 在过去的二十年里，尽管语音情感识别(SER)引起了相当大的关注，但训练数据不足的问题一直没有得到解决。这个问题的一个潜在解决方案是预先训练模型并从大量音频数据中迁移知识。然而，用于预训练和测试的数据来自不同的领域，导致潜在表征包含非情感信息。在本文中，我们提出了一种领域对抗的自动编码器来提取SER的区分表征。通过领域对抗性学习，我们可以减少领域之间的不匹配，同时保留用于情感识别的区分信息。我们还引入了多头注意，从输入话语的不同子空间中捕捉情感信息。在IEMOCAP上的实验表明，该模型比现有的系统提高了4.15%的未加权准确率，从而证明了该模型的有效性。

## 预处理

## 概述

## 结果

使用IEMOCAP中的脚本数据和临时数据对所提出的模型进行了评估。我们实施了将“happy”和“excited”合并为一个情感类别“happy”的普遍做法，产生了5531个话语[15]。

在DAE的预训练中，我们使用了三个社区可用的流行情感数据集：

萨里视听表达情感(SAVEE)[17]，包含四个男性受试者的录音，其中120个是中性的，60个是其他六种情绪的。
柏林情感语音数据库(EMO-DB)[18]，由七种情绪组成，由10名专业演员在录音环境中表演。整个数据集由535个话语组成。
情感声音数据库(EmoV-DB)[19]，包含来自四名男性说话者的7000个样本。这项研究使用了其中的四种情绪，共5256次。

输入信号的采样频率为16 kHz。我们将所有语音分割成265毫秒的片段。为每个片段计算输入频谱图，帧大小为25ms。输入语谱图的维度为32×129（Time × frequency）。批次大小设置为128。我们使用adadelta作为优化器，使用交叉熵作为损失函数。在本研究中，我们随机抽取了IEMOCAP中80%的数据用于训练，20%用于测试。为了解决不同语料库中情感标注不一致的问题，我们引入了valence和arousal作为情感映射的统一标准。Schuller等人。[20]将情绪映射到二元价和唤醒。在这项研究中，我们将这些情绪进一步归类为四类。如表1所示，P(X)中的情绪可以与快乐、中性、愤怒和悲伤共享相同的特征分布。

进行了可视化分析，以验证所提议的DAE的有效性。引入t分布随机邻近嵌入(t-SNE)[21]来可视化预先训练的自动编码器和DAE的特征表示。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104136.png)

如图2所示，自动编码器和DAE ProForm对于愤怒(绿点)和悲伤(蓝点)和愤怒的分离都很好。在DAE的特征分布中，蓝点之间的距离较近。此外，DAE对于幸福(粉点)的表现相对较好。中性用白点表示，它散布在两个地块上。同样的情况在[7]中也有报道。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104207.png)

使用IEMOCAP中的脚本数据和临时数据对所提出的模型进行了评估。我们实施了将“happy”和“excited”合并为一个情感类别“happy”的普遍做法，产生了5531个话语[15]。

在DAE的预训练中，我们使用了三个社区可用的流行情感数据集：

萨里视听表达情感(SAVEE)[17]，包含四个男性受试者的录音，其中120个是中性的，60个是其他六种情绪的。
柏林情感语音数据库(EMO-DB)[18]，由七种情绪组成，由10名专业演员在录音环境中表演。整个数据集由535个话语组成。
情感声音数据库(EmoV-DB)[19]，包含来自四名男性说话者的7000个样本。这项研究使用了其中的四种情绪，共5256次。

输入信号的采样频率为16 kHz。我们将所有语音分割成265毫秒的片段。为每个片段计算输入频谱图，帧大小为25ms。输入语谱图的维度为32×129（Time × frequency）。批次大小设置为128。我们使用adadelta作为优化器，使用交叉熵作为损失函数。在本研究中，我们随机抽取了IEMOCAP中80%的数据用于训练，20%用于测试。为了解决不同语料库中情感标注不一致的问题，我们引入了valence和arousal作为情感映射的统一标准。Schuller等人。[20]将情绪映射到二元价和唤醒。在这项研究中，我们将这些情绪进一步归类为四类。如表1所示，P(X)中的情绪可以与快乐、中性、愤怒和悲伤共享相同的特征分布。

进行了可视化分析，以验证所提议的DAE的有效性。引入t分布随机邻近嵌入(t-SNE)[21]来可视化预先训练的自动编码器和DAE的特征表示。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104136.png)

如图2所示，自动编码器和DAE ProForm对于愤怒(绿点)和悲伤(蓝点)和愤怒的分离都很好。在DAE的特征分布中，蓝点之间的距离较近。此外，DAE对于幸福(粉点)的表现相对较好。中性用白点表示，它散布在两个地块上。同样的情况在[7]中也有报道。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104207.png)

## 精读

由于语音是人类交流中最常见的形式，因此人工智能从语音中学习情绪信息至关重要。言语情绪识别（SER）的发展显着促进了自然和社交的人机通信。因此，这个研究课题引起了工业界和学术界越来越多的兴趣。然而，两个主要瓶颈限制了SER系统的识别能力。首先是缺乏大型自然标记数据集[1]，因为记录和注释情绪相关数据集非常耗时。其次是如何从语音信号中提取最佳特征。随着语音处理和图像分类深度学习的快速发展，深度学习方法在SER领域也取得了显着的进步[2,3]。与传统的手工制作特征相比，深度表征特征在没有任何专业知识的情况下显示出有希望的识别结果[4]。然而，由于人类情绪模棱两可，如何为SER提取最佳特征仍需要持续关注[5]。

训练数据的不足可以通过迁移学习产生额外的特征来补偿[6]，迁移学习可以通过来自不同领域的数据学习先验知识。对于SER任务，一些研究人员通过预先训练模型以无监督的方式学习其他特征来完成这种知识迁移[7,8]。自动编码器[9]在图像重建方面提供了良好的性能，并在许多领域得到广泛应用[10]。我们结合了卷积自动编码器，它是特征建模的最优化结构，作为所提出的SER系统的预训练组件。

但是，只需对自动编码器进行预训练就会产生某些问题。由于无监督学习的自发性[11]，在早期的研究中，预先训练的自动编码器还学习了各种非情感信息，如说话者和录音环境。为了解决这个问题，我们专注于领域对抗神经网络（DANN）[12]。梯度反转层可以防止模型学习域信息。我们使用DANN作为潜在表征的约束，目的是减少源域和目标域之间的不匹配。在所提出的方法中，DANN的域分类器被修改为具有说话者和语料库识别的多任务监督学习。在领域对抗训练的帮助下，我们可以通过消除非情感信息来提取更好的最佳表征。

此外，之前的研究已经确定，特征压缩后情绪信息可能会丢失[13]。因此，引入多头注意（MHA）以减少特征融合阶段的信息丢失[14]。

在本文中，我们提出了一种新的SER表征提取模型。这项研究的主要贡献可以总结如下。
1） 为了克服数据稀缺的问题，对自动编码器进行预训练以从其他情感语料库中提取潜在表征。
2） 域对抗训练被纳入以实现域独立表征。
3） MHA确保我们的模型将有效利用来自不同特征分布的情绪信息。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604100107.png)

2.1. Domain-adversarial autoencoder

为了解决上述瓶颈，我们提出了一种新的表征提取模型，该模型结合了深度CNN体系结构来提供一个有效的SER系统。如图所示，所提出的表征提取模型是通过领域对抗自动编码器(DAAE)实现的。DAAE主要由两部分组成：

(1)预先训练的卷积自动编码器

(2)域对抗神经网络(DANN)。

为了解决情感识别任务中数据不足的问题，我们对卷积自动编码器进行了预训练以提取额外的特征。这种预训练模型可以从额外的数据中迁移知识，以帮助SER任务。然而，自动编码器学习的这种先验知识也包含非情感信息，如说话人和录音环境等。

为了获得更优的潜在特征，我们将DANN直接加入到潜在表征$R_l$中。使用DANN的主要目的是消除$R_l$中的非情感信息。DANN有两个分类器和一个共享特征提取器。考虑到上述目的，Ganin等人。[12]在域分类器和特征提取器之间引入了梯度反转层。这一层通过在反向传播期间乘以某个负常数λ来反转梯度的符号。在我们提出的模型中，Dann可以对RL施加约束，以使该潜在表征的分布与源域保持一致。我们进一步修改了DANN，加入了三个有监督的分类任务，包括作为标签预测者的情感，作为领域分类器的说话人和语料库。典型的预训练模型会自发地从P(X)中学习领域信息，通过DANN可以确保情感识别的区分表征，同时使来自源数据和目标数据的样本无法区分。通过这种方式，所提出的表征提取模型可以实现情感识别的领域无关的表征。DAE的目标函数定义如下：
$$
\begin{gathered}

L_{D A A E}=L_{A E}+\alpha L_{D} \\

L_{D}=L_{e}-\lambda\left(\beta L_{s}+(1-\beta) L_{c}\right)

\end{gathered}
$$

其中$L_{AE}$是预先训练的自动编码器的重构损失，$L_{e}、L_{s}$和$L_{c}$是DANN中情感、说话人和语料库分类任务的损失函数。我们使用$α$和$\beta$作为权衡参数来控制每个损失项的权重。

对于输入数据$x$，该模型分两个阶段进行训练
(1)预训练
(2)特征提取。

在预训练阶段，该模型通过将$P(X)$重构为$P^{\prime}(X)$来更新自动编码器权重。DANN进行了更新，加入了净化特征。自动编码器的目标函数定义为$$L_{A E}=\operatorname{argmin}\left\|P(X)-P^{\prime}(X)\right\|^{2}$$
在特征提取阶段，我们首先使用来自目标数据X的样本对DAE进行微调。这是通过保持DAN的固定权重和偏差并将重建误差反向传播到自动编码器来完成的。以这种方式，潜在表征$R_l$的生成利用了先验知识并从监督DANN中获益，同时保留了用于情感识别的区别性信息。此外，深层表征$R_d$由深层CNN结构提取，这与SATT等人使用的结构类似。[15]。这两种类型的表征连接在一起，然后是一个完全连接的层。输出是公共表征RC，它包含从表征提取模型和深层CNN体系结构中学习的信息。


注意机制允许神经网络捕获输入序列的情绪显著部分。因此，上述两种类型的表示及其串联RC通过MHA融合在一起[14]。因此，我们可以增强输入序列的显著情感部分，并联合处理来自不同表示子空间的信息。实现了对输入特征的降维。每个头部的注意力分数计算如下

$$
\operatorname{Head}_{i}=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{i}}}\right) V
$$

注意的输入是分别对应于查询、关键字和值的潜在表示RD、RL和Rf。我们没有直接给分类器喂食，而是执行了一个线性项目，然后是一个按比例排列的点积注意力。MHA的最终输出是通过连接每个Headi生成的，如下式所示。(8)。

$$
\begin{gathered}

Q_{i}=R_{d} * W_{i}^{Q} \\

K_{i}=R_{l} * W_{i}^{K} \\

V_{i}=R_{f} * W_{i}^{V} \\

M_{h}(Q, K, V)=\text { Concat }\left(H e a d_{1}, \text { Head }_{2}, \ldots, \text { Head }_{n}\right)

\end{gathered}
$$

基于注意力的特征级融合方法的结构细节如图1的右侧所示。由于MHA允许系统对输入序列元素之间的相对依赖关系进行建模，因此该融合方法被用于为双向长期短期记忆(BLSTM)分类器获得稳定而有效的输入。

### 引文

## 摘录

---
title: "Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for Speech Emotion Recognition"
description: ""
citekey: gaoDomainAdversarialAutoencoderAttention2021a
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:33:25
lastmod: 2023-04-11 11:59:27
---

> [!info] 论文信息
>1. Title：Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for Speech Emotion Recognition
>2. Author：Yuan Gao, JiaXing Liu, Longbiao Wang, Jianwu Dang
>3. Entry：[Zotero link](zotero://select/items/@gaoDomainAdversarialAutoencoderAttention2021a) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Gao et al_2021_Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for2.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 使用迁移学习，由预训练网络从其他大型数据库学习先验知识，以此补偿情感数据库小的问题
- 使用领域对抗神经网络，包括情感预测、说话人分类、语料库分类等任务，通过消除先验知识中的非情感信息。
- 研究[13]已经确定，特征压缩后情绪信息可能会丢失。因此，引入多头注意（MHA）以减少特征融合阶段的信息丢失
- 引入了 valence 和 arousal 作为情感映射的两个标准，将不同数据库的不同标签，映射统一。

## 摘要

> [!abstract] Over the past two decades, although speech emotion recognition (SER) has garnered considerable attention, the problem of insufficient training data has been unresolved. A potential solution for this problem is to pre-train a model and transfer knowledge from large amounts of audio data. However, the data used for pre-training and testing originate from different domains, resulting in the latent representations to contain non-affective information. In this paper, we propose a domain-adversarial autoencoder to extract discriminative representations for SER. Through domain-adversarial learning, we can reduce the mismatch between domains while retaining discriminative information for emotion recognition. We also introduce multi-head attention to capture emotion information from different subspaces of input utterances. Experiments on IEMOCAP show that the proposed model outperforms the state-of-the-art systems by improving the unweighted accuracy by 4.15%, thereby demonstrating the effectiveness of the proposed model.

> 在过去的二十年里，尽管语音情感识别(SER)引起了相当大的关注，但训练数据不足的问题一直没有得到解决。这个问题的一个潜在解决方案是预先训练模型并从大量音频数据中迁移知识。然而，用于预训练和测试的数据来自不同的领域，导致潜在表征包含非情感信息。在本文中，我们提出了一种领域对抗的自动编码器来提取SER的区分表征。通过领域对抗性学习，我们可以减少领域之间的不匹配，同时保留用于情感识别的区分信息。我们还引入了多头注意，从输入话语的不同子空间中捕捉情感信息。在IEMOCAP上的实验表明，该模型比现有的系统提高了4.15%的未加权准确率，从而证明了该模型的有效性。

## 预处理

## 概述

## 结果

使用IEMOCAP中的脚本数据和临时数据对所提出的模型进行了评估。我们实施了将“happy”和“excited”合并为一个情感类别“happy”的普遍做法，产生了5531个话语[15]。

在DAE的预训练中，我们使用了三个社区可用的流行情感数据集：

萨里视听表达情感(SAVEE)[17]，包含四个男性受试者的录音，其中120个是中性的，60个是其他六种情绪的。
柏林情感语音数据库(EMO-DB)[18]，由七种情绪组成，由10名专业演员在录音环境中表演。整个数据集由535个话语组成。
情感声音数据库(EmoV-DB)[19]，包含来自四名男性说话者的7000个样本。这项研究使用了其中的四种情绪，共5256次。

输入信号的采样频率为16 kHz。我们将所有语音分割成265毫秒的片段。为每个片段计算输入频谱图，帧大小为25ms。输入语谱图的维度为32×129（Time × frequency）。批次大小设置为128。我们使用adadelta作为优化器，使用交叉熵作为损失函数。在本研究中，我们随机抽取了IEMOCAP中80%的数据用于训练，20%用于测试。为了解决不同语料库中情感标注不一致的问题，我们引入了valence和arousal作为情感映射的统一标准。Schuller等人。[20]将情绪映射到二元价和唤醒。在这项研究中，我们将这些情绪进一步归类为四类。如表1所示，P(X)中的情绪可以与快乐、中性、愤怒和悲伤共享相同的特征分布。

进行了可视化分析，以验证所提议的DAE的有效性。引入t分布随机邻近嵌入(t-SNE)[21]来可视化预先训练的自动编码器和DAE的特征表示。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104136.png)

如图2所示，自动编码器和DAE ProForm对于愤怒(绿点)和悲伤(蓝点)和愤怒的分离都很好。在DAE的特征分布中，蓝点之间的距离较近。此外，DAE对于幸福(粉点)的表现相对较好。中性用白点表示，它散布在两个地块上。同样的情况在[7]中也有报道。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104207.png)

使用IEMOCAP中的脚本数据和临时数据对所提出的模型进行了评估。我们实施了将“happy”和“excited”合并为一个情感类别“happy”的普遍做法，产生了5531个话语[15]。

在DAE的预训练中，我们使用了三个社区可用的流行情感数据集：

萨里视听表达情感(SAVEE)[17]，包含四个男性受试者的录音，其中120个是中性的，60个是其他六种情绪的。
柏林情感语音数据库(EMO-DB)[18]，由七种情绪组成，由10名专业演员在录音环境中表演。整个数据集由535个话语组成。
情感声音数据库(EmoV-DB)[19]，包含来自四名男性说话者的7000个样本。这项研究使用了其中的四种情绪，共5256次。

输入信号的采样频率为16 kHz。我们将所有语音分割成265毫秒的片段。为每个片段计算输入频谱图，帧大小为25ms。输入语谱图的维度为32×129（Time × frequency）。批次大小设置为128。我们使用adadelta作为优化器，使用交叉熵作为损失函数。在本研究中，我们随机抽取了IEMOCAP中80%的数据用于训练，20%用于测试。为了解决不同语料库中情感标注不一致的问题，我们引入了valence和arousal作为情感映射的统一标准。Schuller等人。[20]将情绪映射到二元价和唤醒。在这项研究中，我们将这些情绪进一步归类为四类。如表1所示，P(X)中的情绪可以与快乐、中性、愤怒和悲伤共享相同的特征分布。

进行了可视化分析，以验证所提议的DAE的有效性。引入t分布随机邻近嵌入(t-SNE)[21]来可视化预先训练的自动编码器和DAE的特征表示。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104136.png)

如图2所示，自动编码器和DAE ProForm对于愤怒(绿点)和悲伤(蓝点)和愤怒的分离都很好。在DAE的特征分布中，蓝点之间的距离较近。此外，DAE对于幸福(粉点)的表现相对较好。中性用白点表示，它散布在两个地块上。同样的情况在[7]中也有报道。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604104207.png)

## 精读

由于语音是人类交流中最常见的形式，因此人工智能从语音中学习情绪信息至关重要。言语情绪识别（SER）的发展显着促进了自然和社交的人机通信。因此，这个研究课题引起了工业界和学术界越来越多的兴趣。然而，两个主要瓶颈限制了SER系统的识别能力。首先是缺乏大型自然标记数据集[1]，因为记录和注释情绪相关数据集非常耗时。其次是如何从语音信号中提取最佳特征。随着语音处理和图像分类深度学习的快速发展，深度学习方法在SER领域也取得了显着的进步[2,3]。与传统的手工制作特征相比，深度表征特征在没有任何专业知识的情况下显示出有希望的识别结果[4]。然而，由于人类情绪模棱两可，如何为SER提取最佳特征仍需要持续关注[5]。

训练数据的不足可以通过迁移学习产生额外的特征来补偿[6]，迁移学习可以通过来自不同领域的数据学习先验知识。对于SER任务，一些研究人员通过预先训练模型以无监督的方式学习其他特征来完成这种知识迁移[7,8]。自动编码器[9]在图像重建方面提供了良好的性能，并在许多领域得到广泛应用[10]。我们结合了卷积自动编码器，它是特征建模的最优化结构，作为所提出的SER系统的预训练组件。

但是，只需对自动编码器进行预训练就会产生某些问题。由于无监督学习的自发性[11]，在早期的研究中，预先训练的自动编码器还学习了各种非情感信息，如说话者和录音环境。为了解决这个问题，我们专注于领域对抗神经网络（DANN）[12]。梯度反转层可以防止模型学习域信息。我们使用DANN作为潜在表征的约束，目的是减少源域和目标域之间的不匹配。在所提出的方法中，DANN的域分类器被修改为具有说话者和语料库识别的多任务监督学习。在领域对抗训练的帮助下，我们可以通过消除非情感信息来提取更好的最佳表征。

此外，之前的研究已经确定，特征压缩后情绪信息可能会丢失[13]。因此，引入多头注意（MHA）以减少特征融合阶段的信息丢失[14]。

在本文中，我们提出了一种新的SER表征提取模型。这项研究的主要贡献可以总结如下。
1） 为了克服数据稀缺的问题，对自动编码器进行预训练以从其他情感语料库中提取潜在表征。
2） 域对抗训练被纳入以实现域独立表征。
3） MHA确保我们的模型将有效利用来自不同特征分布的情绪信息。

![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604100107.png)

2.1. Domain-adversarial autoencoder

为了解决上述瓶颈，我们提出了一种新的表征提取模型，该模型结合了深度CNN体系结构来提供一个有效的SER系统。如图所示，所提出的表征提取模型是通过领域对抗自动编码器(DAAE)实现的。DAAE主要由两部分组成：

(1)预先训练的卷积自动编码器

(2)域对抗神经网络(DANN)。

为了解决情感识别任务中数据不足的问题，我们对卷积自动编码器进行了预训练以提取额外的特征。这种预训练模型可以从额外的数据中迁移知识，以帮助SER任务。然而，自动编码器学习的这种先验知识也包含非情感信息，如说话人和录音环境等。

为了获得更优的潜在特征，我们将DANN直接加入到潜在表征$R_l$中。使用DANN的主要目的是消除$R_l$中的非情感信息。DANN有两个分类器和一个共享特征提取器。考虑到上述目的，Ganin等人。[12]在域分类器和特征提取器之间引入了梯度反转层。这一层通过在反向传播期间乘以某个负常数λ来反转梯度的符号。在我们提出的模型中，Dann可以对RL施加约束，以使该潜在表征的分布与源域保持一致。我们进一步修改了DANN，加入了三个有监督的分类任务，包括作为标签预测者的情感，作为领域分类器的说话人和语料库。典型的预训练模型会自发地从P(X)中学习领域信息，通过DANN可以确保情感识别的区分表征，同时使来自源数据和目标数据的样本无法区分。通过这种方式，所提出的表征提取模型可以实现情感识别的领域无关的表征。DAE的目标函数定义如下：
$$
\begin{gathered}

L_{D A A E}=L_{A E}+\alpha L_{D} \\

L_{D}=L_{e}-\lambda\left(\beta L_{s}+(1-\beta) L_{c}\right)

\end{gathered}
$$

其中$L_{AE}$是预先训练的自动编码器的重构损失，$L_{e}、L_{s}$和$L_{c}$是DANN中情感、说话人和语料库分类任务的损失函数。我们使用$α$和$\beta$作为权衡参数来控制每个损失项的权重。

对于输入数据$x$，该模型分两个阶段进行训练
(1)预训练
(2)特征提取。

在预训练阶段，该模型通过将$P(X)$重构为$P^{\prime}(X)$来更新自动编码器权重。DANN进行了更新，加入了净化特征。自动编码器的目标函数定义为$$L_{A E}=\operatorname{argmin}\left\|P(X)-P^{\prime}(X)\right\|^{2}$$
在特征提取阶段，我们首先使用来自目标数据X的样本对DAE进行微调。这是通过保持DAN的固定权重和偏差并将重建误差反向传播到自动编码器来完成的。以这种方式，潜在表征$R_l$的生成利用了先验知识并从监督DANN中获益，同时保留了用于情感识别的区别性信息。此外，深层表征$R_d$由深层CNN结构提取，这与SATT等人使用的结构类似。[15]。这两种类型的表征连接在一起，然后是一个完全连接的层。输出是公共表征RC，它包含从表征提取模型和深层CNN体系结构中学习的信息。


注意机制允许神经网络捕获输入序列的情绪显著部分。因此，上述两种类型的表示及其串联RC通过MHA融合在一起[14]。因此，我们可以增强输入序列的显著情感部分，并联合处理来自不同表示子空间的信息。实现了对输入特征的降维。每个头部的注意力分数计算如下

$$
\operatorname{Head}_{i}=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{i}}}\right) V
$$

注意的输入是分别对应于查询、关键字和值的潜在表示RD、RL和Rf。我们没有直接给分类器喂食，而是执行了一个线性项目，然后是一个按比例排列的点积注意力。MHA的最终输出是通过连接每个Headi生成的，如下式所示。(8)。

$$
\begin{gathered}

Q_{i}=R_{d} * W_{i}^{Q} \\

K_{i}=R_{l} * W_{i}^{K} \\

V_{i}=R_{f} * W_{i}^{V} \\

M_{h}(Q, K, V)=\text { Concat }\left(H e a d_{1}, \text { Head }_{2}, \ldots, \text { Head }_{n}\right)

\end{gathered}
$$

基于注意力的特征级融合方法的结构细节如图1的右侧所示。由于MHA允许系统对输入序列元素之间的相对依赖关系进行建模，因此该融合方法被用于为双向长期短期记忆(BLSTM)分类器获得稳定而有效的输入。
---
title: "Meta-Learning for Low-Resource Speech Emotion Recognition"
description: ""
citekey: chopraMetaLearningLowResourceSpeech2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：Meta-Learning for Low-Resource Speech Emotion Recognition
>2. Author：Suransh Chopra, Puneet Mathur, Ramit Sawhney, Rajiv Ratn Shah
>3. Entry：[Zotero link](zotero://select/items/@chopraMetaLearningLowResourceSpeech2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chopra et al_2021_Meta-Learning for Low-Resource Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/pmathur5k10/Meta-learning-for-Low-Resource-Speech-Emotion-Recognition
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果

## 摘要

> [!abstract] While emotion recognition is a well-studied task, it remains unexplored to a large extent in cross-lingual settings. Speech Emotion Recognition (SER) in low-resource languages poses difficulties as existing approaches for knowledge transfer do not generalize seamlessly. Probing the learning process of generalized representations across languages, we propose a meta-learning approach for low-resource speech emotion recognition. The proposed approach achieves fast adaptation on a number of unseen target languages simultaneously. We evaluate the Model Agnostic Meta-Learning (MAML) algorithm on three low-resource target languages -Persian, Italian, and Urdu. We empirically demonstrate that our proposed method - MetaSER1, considerably outperforms multitask and transfer learning-based methods for speech emotion recognition task, and discuss the benefits, efficiency, and challenges of MetaSER on limited data settings.

> 虽然情绪识别是一项已经研究得很好的任务，但在很大程度上，它在跨语言环境中仍未被探索。低资源语言中的语音情感识别(SER)带来了困难，因为现有的知识转移方法不能无缝地推广。探讨了跨语言广义表征的学习过程，提出了一种用于低资源语音情感识别的元学习方法。该方法同时实现了对多种不可见目标语言的快速适应。我们在三种低资源目标语言波斯语、意大利语和乌尔都语上对模型不可知元学习(MAML)算法进行了评估。实验证明，我们提出的方法MetaSER1在语音情感识别任务中的性能明显优于基于多任务和迁移学习的方法，并讨论了MetaSER在有限数据环境下的优势、效率和挑战。

## 预处理

## 概述

## 结果

在这项工作中，我们使用了四个流行的英语数据集(TESS[13]，RAVEDESS[14]，SAVEE[15]，IEMOCAP[16])和一个德语数据集(EMODB[17])作为预训练的来源。我们选择了三个低资源语言数据集进行适应-意大利语(EMOVO[18])、波斯语(SHEMO[19])和乌尔都语(乌尔都语[20])。表1列出了源数据集和目标数据集的语料库统计数据。每个数据集具有不同数量的情感类别，最小类别为4个，最大类别为8个。为了能够在所有数据集之间进行对称比较，我们限制了我们在四个主要情绪类别上的实验-快乐、悲伤、愤怒和中性，这四个情绪类别通常存在于所有源和目标数据集中。剩余类的剩余数据样本不在范围内。每个语料库中的数据按70：30的比例分为训练和测试。此外，以分层方式选择的训练数据的20%用于模型验证。MFCC特征的选择：Mel频率倒谱系数(MFCC)是通过对由在MEL频率尺度上线性间隔的三角滤波器组成的滤波器组的输出对数能量去相关来获得的。这有助于提取编码说话风格信息的声学特征，这些特征表现出比韵律特征更好和更稳健的性能。在过去SER[21]研究的基础上，我们的所有实验都基于话语的MFCC特征。对于每个话语，我们从最多120帧的输入中提取特征。较短的发音被零填充，较长发音的额外信号被裁剪，以每个发音的(120，20)特征矩阵结束。

![]({35}_Meta-Learning%20for%20Low-Resource%20Speech%20Emotion%20Recognition@chopraMetaLearningLowResourceSpeech2021.assets/image-20220605122440.png)

## 精读

语音情绪识别（SER）是指从音频中识别主要的说话者情绪。任务很复杂，因为情绪无法明确定义，更不用说准确检测了。大多数关于SER的工作都集中在高资源语言上，例如英语，普通话，法语和德语，这些语言具有广泛的数据资源。相比之下，低资源语言很少受到研究界的关注。大多数SER数据集专门设计用于训练人类在不同情绪状态下记录语音。这些数据集根据年龄，性别和说话者的类型有明显的区别，因此为学习概括技术提供了各种各样的训练实例。构建这样的端到端SER系统需要大量带注释的语音数据，这既繁琐又昂贵。有限的数据和各种缺陷在为低资源语言构建强大的SER分类器方面造成严重缺陷。[1] 假设情绪识别是一项与语言无关的任务。它表明，相同情绪的声音线索应该在语言中很常见。在多语言转移学习和多任务学习方法[2]中可以看出，在其他语言源上进行预训练，因为初始化后对目标语言进行微调成为这种低资源设置中的主要方法。已知该技术通过利用特定于每个情绪类别的特征来提高语音情绪分类器的性能[3]。但是，这是一种依赖语料库的方法，专注于语言拟合，而不是跨各种语言家族进行概括。表征学习语音中情绪的目的不应该专门针对特定语言，而应该在有限的监督下学习广义表征。

在本文中，我们研究了SER多语言元学习的一个新的研究方向，它能够同时快速适应几个不同任务的看不见的数据。我们认为每个语料库都是一个要优化的独立任务。元学习遵循“学习到学习”范式，其中从高资源语音语料库学习的参数更新在多个低资源语言目标数据集上以一次性学习策略进行微调，如图1所示。我们使用模型不可知元学习（MAML）算法[5]。我们的主要贡献包括使用简单的LSTM backbone架构评估所提出的元学习算法-MetaSER，使用三种低资源语言-意大利语，波斯语和乌尔都语。当在类似的训练环境中对多个英语和德语数据集进行预训练时，MetaSER的表现与比较转移学习（TransSER）和多任务学习（MultiSER）等价物形成对比。我们将元学习应用于低资源语音情绪识别任务的工作是第一次。

SER中基于多任务和迁移学习的方法

[6]中的基于多任务和迁移学习的方法是第一批分析跨语言情感识别的方法之一，并报告说它无意中导致了性能的下降。
[1]认为在不同语言中，每种情绪都有一些共同的声音线索。
[7]探索了与标准归一化技术相结合的多任务学习策略，用于英语和日语多语言情感识别。
[8]对四种不同语言的迁移学习和多任务学习方法进行了全面的评估，但注意到在辅助任务上的多任务联合学习的成绩不如迁移学习。

元学习在语音中的应用：一些文献报道了元学习在自动语音识别(ASR)中的有效性。
[9]演示了使用元学习调整声学模型所有权重的原则性方法。
基于MAML的元学习方法是ASR中多任务迁移学习方法的竞争性选择[10]。
最近，[11]分别探索了文本到语音和情感歧义的元学习。

元学习的目标是从一组任务中学习模型的初始化参数，以便模型能够快速适应几个数据点中的新任务。在本文中，我们的方法通过MAML[5]来优化基础神经结构。对高级资源语言$\mathcal{T}=\left\{T_{1}, T_{2}, T_{3} \ldots T_{k}\right\}$的分布进行了参数优化，该模型适用于一种称为目标任务的低资源语言。MAML是一个两步过程：

(I)通过共享编码器从$\mathcal{T}$学习良好的初始化参数$\theta^{*}$(称为MetaLearn步骤)；
(II)在$T_{l}$(称为微调步骤)上进行特定于任务的学习，获得公式1所给出的微调参数$\theta_{t}^{*}$。将模型暴露在各种任务中使其能够在几个步骤和较少的数据量内学习新任务。
$$\theta_{t}^{*}=\operatorname{Learn}\left(T_{l}，\theta^{*}\right)=\operatorname{Learn}\left(T_{l}；\operatorname{MetaLearn}(\mathcal{T})\right) $$训练范例元学习和微调分别在第$3.2$节和第3.3节中介绍。

MetaLearn: Learning to Learn

假设有一个模型$f(\theta)$，它具有随机初始化的参数$\theta$，并从任务分配$p(\mathcal{T})$中学习。元学习更新包括从$p(\mathcal{T})$采样一批任务$\left\{T_{2}，\ldots T_{i}\right\}$。对于每个任务$T_{i}$，我们将其拆分为训练集和验证集，分别表示为$T_{i}^{\text{Train}}$和$T_{i}^{\text{val}}$。元学习过程从随机初始化的参数$\theta$开始，在$T_{i}^{\text{Train}}$上模拟特定于语言的训练过程，以获得中间参数$\theta_{i}^{\prime}$(公式2)，然后在$T_{i}^{\text{val}}$上对其求值。
$$\begin{gathered}

\theta_{i}^{\prime}=\text { MetaLearn }\left(T_{i}^{\text {train }}, \theta\right) \\

\mathcal{L}_{\mathcal{T}}^{m}(f)=\sum_{\mathcal{T}_{i}} y^{(k)} \log \left(f\left(x^{(k)}\right)+\left(1-y^{(k)}\right) \log \left(1-f\left(x^{(k)}\right)\right)\right. \\

\theta^{*}=M e t a \operatorname{Learn}(\mathcal{T})=\arg \min _{\theta} \sum_{\mathcal{T}_{i} \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_{i}}^{m}\left(f\left(\theta_{i}^{\prime}\right)\right) \\

\theta \leftarrow \theta-\eta \sum_{\mathcal{T}_{i} \sim p(\mathcal{T})} \nabla_{\theta} \mathcal{L}_{T_{i}^{\text {val }}}^{m}\left(f\left(\theta_{i}^{\prime}\right)\right)

\end{gathered}
$$公式3突出显示了任何数据集$\mathcal{T}_{i}$的损失函数，其中梯度是在所有源任务上计算的。它由$\mathcal{L}_{\mathcal{T}_{i}}^{m}$表示，它是数据集中所有样本$\left(x^{(K)}，y^{(K)}\right)$的交叉熵损失准则。公式4示出了公式3中表示的损失函数的最小化，其通过给出更新用于语音情感识别任务的模型的所有源数据集元梯度的值来结束元学习过程。公式5在数学上描述了元梯度更新步骤，其中$\eta$是元学习率。在公式5的总和中展开内部损耗项得到$\theta$的二阶导数。计算二阶导数项的计算代价很高，并且占用大量内存。受一阶MAML(通常称为FOMAML[12])的启发，我们忽略了vanilla MAML的二阶导数部分，而不会对性能造成任何显著的损失。

Language Specific Fine-tuning

$$

\begin{gathered}

\theta_{l}^{*}=\operatorname{Learn}\left(\mathcal{T}_{l}, f\left(\theta^{*}\right)\right)=\arg \min _{\theta} \mathcal{L}_{\mathcal{T}}\left(f\left(\theta^{*}\right)\right) \\

\theta_{l}^{*}=\min _{\theta} \sum_{(X, C) \in \mathcal{T}}-\log \left(P\left(C \mid X, f\left(\theta^{*}\right)\right)\right)

\end{gathered}

$$共享编码器是通过元学习过程由参数$\theta^{*}$初始化的。为了对目标语言任务之一进行分类，我们需要微调次优模型参数，以在目标数据集$\mathcal{T}_{l}$上实现最佳性能。设低资源语言任务$T_{l}$的数据和标签对表示为$(X，C)$，其中$X=\left\{x_{1}，x_{2}，\ldots x_{n}\right\}$是$n$数据样本的集合，$C=\left\{c_{1}，c_{2}，\ldots c_{l}\right\}$是$l$类标签的集合。特定语言的学习通过对特定任务$T_{l}$的梯度下降来进行，以最小化公式6和7中总结的交叉熵损失。


我们打算通过尝试在所有学习范例中保持训练设置的一致性，对MetSER与MultiSER和TransSER进行比较分析。我们为元学习和转移学习实验采用共同的基础模型来比较和对比类似设置中的性能。基本模型是双层LSTM模型，最后是两个密集层。每个LSTM单元都有128个维度的隐藏状态。完全连接的层分别具有128和4个神经元。

TransSER的训练设置：在转移学习的情况下，模型在源中的每个数据集上按顺序进行预训练。然后冻结LSTM参数并微调目标数据集之一的训练部分上的致密层。

MultiSER的训练设置：多任务学习的训练设置涉及对所有源数据集进行交替的课堂预训练。然后在目标数据集的完整训练部分上微调模型的密集层。通过这种方式，该模型避免了灾难性的遗忘，因为它以循环方式从所有类中学习特征。在我们的实验中执行的多任务学习的形式导致跨所有层的软参数共享。

多任务和转移学习基线的超参数：我们对所有实验使用循环学习速率计划，基本学习速率为2e− 5，最高学习率为1 e− 4.发现最佳批量为64。转移学习的预训练和微调阶段的最大时期数分别保持为1000和2500，直到验证损失收敛。

元学习的训练设置：由于过度拟合的显着缺点，从头开始训练MAML体系结构是一项具有挑战性的任务。其主要原因是在微调[22]步骤中仅在几个数据点上更新了整个网络。我们调查了元学习对源数据集的五种不同组合的影响。元学习的超参数：元学习过程有两个优化过程元更新和特定于任务的微调。两种方法都有自己的学习率：meta-LR和task-LR。MetaLR在MetaLearn阶段进行了优化，然后在调整任务LR时在特定于语言的微调期间保持恒定。我们在两个预训练集上的实验中，meta-LR和微调LR的最优值均为1e− 6和2e− 4，分别。我们用Adam优化了元情节，任务特定学习使用随机梯度下降（SGD）。更新元参数的每次迭代都称为情节。每个模型运行中的最大剧集数保持为1e+4。每个元情节都有内部步骤更新，这被称为内部任务。根据经验，内部任务在设置为8时实现收敛。我们通过限制任何给定批次中可用的样本数量来调查MetaSER对未见数据的普遍性。因此，元学习器的批量大小b由k-shot学习参数k和不同类的数量c（b=k∗ c） 。我们保持c=4不变，同时改变k以根据宏F1分数评估模型性能。

MultiSER和Ranser与MetaSER的性能比较：表2中F1分数的比较清楚地表明，MetaSER在所有目标语言的预训练集的所有组合上均显着优于多任务学习（MultiSER）和转移学习（TransSER）对应物。，其中一个源集中的乌尔都语除外。结果强化了我们的假设，即基于多任务和转移学习的方法更多地关注源语言，而不是概括跨语言培训。鉴于源语言主要属于日耳曼语家族，因此从经验上证明，MAML培训可用于从高资源语言家族推广到低资源语言家族。数据大小对学习能力的影响：为了研究MetaSER在数据稀缺环境中的普遍性，我们在一小部分目标训练数据上微调模型，并测试TransSER，MultiSER和MetaSER的模型性能。对于TransSER和MultiSER，模型在完整的源数据集上进行预训练，并在目标数据集的分数训练部分以0.1的间隔进行微调。对于MetaSER，我们将少数镜头参数k的值作为可用训练数据的一部分进行改变，使得k∈ {0.1, 0.2, 0.3, . . . , 1}. 在所有情况下，目标数据集的测试部分在推理期间保持相同。由此产生的模型F1分数被绘制出来，在TESS，EMODB和RAVEDESS的源集中给出图2，这表明TransSER和MultiSER在早期阶段都饱和，而MetaSER随着更多的训练数据可用而稳步提高，直到它开始胜过它们的拐点。然而，MAML培训的陡坡也揭示了培训MAML架构的复杂性，而不是转移学习。尽管MAML训练在所有目标数据集中提供了更好的F1得分结果，但它需要足够的数据才能超过其性能阈值。

### 引文

## 摘录

---
title: "Meta-Learning for Low-Resource Speech Emotion Recognition"
description: ""
citekey: chopraMetaLearningLowResourceSpeech2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：Meta-Learning for Low-Resource Speech Emotion Recognition
>2. Author：Suransh Chopra, Puneet Mathur, Ramit Sawhney, Rajiv Ratn Shah
>3. Entry：[Zotero link](zotero://select/items/@chopraMetaLearningLowResourceSpeech2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chopra et al_2021_Meta-Learning for Low-Resource Speech Emotion Recognition.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/pmathur5k10/Meta-learning-for-Low-Resource-Speech-Emotion-Recognition
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果

## 摘要

> [!abstract] While emotion recognition is a well-studied task, it remains unexplored to a large extent in cross-lingual settings. Speech Emotion Recognition (SER) in low-resource languages poses difficulties as existing approaches for knowledge transfer do not generalize seamlessly. Probing the learning process of generalized representations across languages, we propose a meta-learning approach for low-resource speech emotion recognition. The proposed approach achieves fast adaptation on a number of unseen target languages simultaneously. We evaluate the Model Agnostic Meta-Learning (MAML) algorithm on three low-resource target languages -Persian, Italian, and Urdu. We empirically demonstrate that our proposed method - MetaSER1, considerably outperforms multitask and transfer learning-based methods for speech emotion recognition task, and discuss the benefits, efficiency, and challenges of MetaSER on limited data settings.

> 虽然情绪识别是一项已经研究得很好的任务，但在很大程度上，它在跨语言环境中仍未被探索。低资源语言中的语音情感识别(SER)带来了困难，因为现有的知识转移方法不能无缝地推广。探讨了跨语言广义表征的学习过程，提出了一种用于低资源语音情感识别的元学习方法。该方法同时实现了对多种不可见目标语言的快速适应。我们在三种低资源目标语言波斯语、意大利语和乌尔都语上对模型不可知元学习(MAML)算法进行了评估。实验证明，我们提出的方法MetaSER1在语音情感识别任务中的性能明显优于基于多任务和迁移学习的方法，并讨论了MetaSER在有限数据环境下的优势、效率和挑战。

## 预处理

## 概述

## 结果

在这项工作中，我们使用了四个流行的英语数据集(TESS[13]，RAVEDESS[14]，SAVEE[15]，IEMOCAP[16])和一个德语数据集(EMODB[17])作为预训练的来源。我们选择了三个低资源语言数据集进行适应-意大利语(EMOVO[18])、波斯语(SHEMO[19])和乌尔都语(乌尔都语[20])。表1列出了源数据集和目标数据集的语料库统计数据。每个数据集具有不同数量的情感类别，最小类别为4个，最大类别为8个。为了能够在所有数据集之间进行对称比较，我们限制了我们在四个主要情绪类别上的实验-快乐、悲伤、愤怒和中性，这四个情绪类别通常存在于所有源和目标数据集中。剩余类的剩余数据样本不在范围内。每个语料库中的数据按70：30的比例分为训练和测试。此外，以分层方式选择的训练数据的20%用于模型验证。MFCC特征的选择：Mel频率倒谱系数(MFCC)是通过对由在MEL频率尺度上线性间隔的三角滤波器组成的滤波器组的输出对数能量去相关来获得的。这有助于提取编码说话风格信息的声学特征，这些特征表现出比韵律特征更好和更稳健的性能。在过去SER[21]研究的基础上，我们的所有实验都基于话语的MFCC特征。对于每个话语，我们从最多120帧的输入中提取特征。较短的发音被零填充，较长发音的额外信号被裁剪，以每个发音的(120，20)特征矩阵结束。

![]({35}_Meta-Learning%20for%20Low-Resource%20Speech%20Emotion%20Recognition@chopraMetaLearningLowResourceSpeech2021.assets/image-20220605122440.png)

## 精读

语音情绪识别（SER）是指从音频中识别主要的说话者情绪。任务很复杂，因为情绪无法明确定义，更不用说准确检测了。大多数关于SER的工作都集中在高资源语言上，例如英语，普通话，法语和德语，这些语言具有广泛的数据资源。相比之下，低资源语言很少受到研究界的关注。大多数SER数据集专门设计用于训练人类在不同情绪状态下记录语音。这些数据集根据年龄，性别和说话者的类型有明显的区别，因此为学习概括技术提供了各种各样的训练实例。构建这样的端到端SER系统需要大量带注释的语音数据，这既繁琐又昂贵。有限的数据和各种缺陷在为低资源语言构建强大的SER分类器方面造成严重缺陷。[1] 假设情绪识别是一项与语言无关的任务。它表明，相同情绪的声音线索应该在语言中很常见。在多语言转移学习和多任务学习方法[2]中可以看出，在其他语言源上进行预训练，因为初始化后对目标语言进行微调成为这种低资源设置中的主要方法。已知该技术通过利用特定于每个情绪类别的特征来提高语音情绪分类器的性能[3]。但是，这是一种依赖语料库的方法，专注于语言拟合，而不是跨各种语言家族进行概括。表征学习语音中情绪的目的不应该专门针对特定语言，而应该在有限的监督下学习广义表征。

在本文中，我们研究了SER多语言元学习的一个新的研究方向，它能够同时快速适应几个不同任务的看不见的数据。我们认为每个语料库都是一个要优化的独立任务。元学习遵循“学习到学习”范式，其中从高资源语音语料库学习的参数更新在多个低资源语言目标数据集上以一次性学习策略进行微调，如图1所示。我们使用模型不可知元学习（MAML）算法[5]。我们的主要贡献包括使用简单的LSTM backbone架构评估所提出的元学习算法-MetaSER，使用三种低资源语言-意大利语，波斯语和乌尔都语。当在类似的训练环境中对多个英语和德语数据集进行预训练时，MetaSER的表现与比较转移学习（TransSER）和多任务学习（MultiSER）等价物形成对比。我们将元学习应用于低资源语音情绪识别任务的工作是第一次。

SER中基于多任务和迁移学习的方法

[6]中的基于多任务和迁移学习的方法是第一批分析跨语言情感识别的方法之一，并报告说它无意中导致了性能的下降。
[1]认为在不同语言中，每种情绪都有一些共同的声音线索。
[7]探索了与标准归一化技术相结合的多任务学习策略，用于英语和日语多语言情感识别。
[8]对四种不同语言的迁移学习和多任务学习方法进行了全面的评估，但注意到在辅助任务上的多任务联合学习的成绩不如迁移学习。

元学习在语音中的应用：一些文献报道了元学习在自动语音识别(ASR)中的有效性。
[9]演示了使用元学习调整声学模型所有权重的原则性方法。
基于MAML的元学习方法是ASR中多任务迁移学习方法的竞争性选择[10]。
最近，[11]分别探索了文本到语音和情感歧义的元学习。

元学习的目标是从一组任务中学习模型的初始化参数，以便模型能够快速适应几个数据点中的新任务。在本文中，我们的方法通过MAML[5]来优化基础神经结构。对高级资源语言$\mathcal{T}=\left\{T_{1}, T_{2}, T_{3} \ldots T_{k}\right\}$的分布进行了参数优化，该模型适用于一种称为目标任务的低资源语言。MAML是一个两步过程：

(I)通过共享编码器从$\mathcal{T}$学习良好的初始化参数$\theta^{*}$(称为MetaLearn步骤)；
(II)在$T_{l}$(称为微调步骤)上进行特定于任务的学习，获得公式1所给出的微调参数$\theta_{t}^{*}$。将模型暴露在各种任务中使其能够在几个步骤和较少的数据量内学习新任务。
$$\theta_{t}^{*}=\operatorname{Learn}\left(T_{l}，\theta^{*}\right)=\operatorname{Learn}\left(T_{l}；\operatorname{MetaLearn}(\mathcal{T})\right) $$训练范例元学习和微调分别在第$3.2$节和第3.3节中介绍。

MetaLearn: Learning to Learn

假设有一个模型$f(\theta)$，它具有随机初始化的参数$\theta$，并从任务分配$p(\mathcal{T})$中学习。元学习更新包括从$p(\mathcal{T})$采样一批任务$\left\{T_{2}，\ldots T_{i}\right\}$。对于每个任务$T_{i}$，我们将其拆分为训练集和验证集，分别表示为$T_{i}^{\text{Train}}$和$T_{i}^{\text{val}}$。元学习过程从随机初始化的参数$\theta$开始，在$T_{i}^{\text{Train}}$上模拟特定于语言的训练过程，以获得中间参数$\theta_{i}^{\prime}$(公式2)，然后在$T_{i}^{\text{val}}$上对其求值。
$$\begin{gathered}

\theta_{i}^{\prime}=\text { MetaLearn }\left(T_{i}^{\text {train }}, \theta\right) \\

\mathcal{L}_{\mathcal{T}}^{m}(f)=\sum_{\mathcal{T}_{i}} y^{(k)} \log \left(f\left(x^{(k)}\right)+\left(1-y^{(k)}\right) \log \left(1-f\left(x^{(k)}\right)\right)\right. \\

\theta^{*}=M e t a \operatorname{Learn}(\mathcal{T})=\arg \min _{\theta} \sum_{\mathcal{T}_{i} \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_{i}}^{m}\left(f\left(\theta_{i}^{\prime}\right)\right) \\

\theta \leftarrow \theta-\eta \sum_{\mathcal{T}_{i} \sim p(\mathcal{T})} \nabla_{\theta} \mathcal{L}_{T_{i}^{\text {val }}}^{m}\left(f\left(\theta_{i}^{\prime}\right)\right)

\end{gathered}
$$公式3突出显示了任何数据集$\mathcal{T}_{i}$的损失函数，其中梯度是在所有源任务上计算的。它由$\mathcal{L}_{\mathcal{T}_{i}}^{m}$表示，它是数据集中所有样本$\left(x^{(K)}，y^{(K)}\right)$的交叉熵损失准则。公式4示出了公式3中表示的损失函数的最小化，其通过给出更新用于语音情感识别任务的模型的所有源数据集元梯度的值来结束元学习过程。公式5在数学上描述了元梯度更新步骤，其中$\eta$是元学习率。在公式5的总和中展开内部损耗项得到$\theta$的二阶导数。计算二阶导数项的计算代价很高，并且占用大量内存。受一阶MAML(通常称为FOMAML[12])的启发，我们忽略了vanilla MAML的二阶导数部分，而不会对性能造成任何显著的损失。

Language Specific Fine-tuning

$$

\begin{gathered}

\theta_{l}^{*}=\operatorname{Learn}\left(\mathcal{T}_{l}, f\left(\theta^{*}\right)\right)=\arg \min _{\theta} \mathcal{L}_{\mathcal{T}}\left(f\left(\theta^{*}\right)\right) \\

\theta_{l}^{*}=\min _{\theta} \sum_{(X, C) \in \mathcal{T}}-\log \left(P\left(C \mid X, f\left(\theta^{*}\right)\right)\right)

\end{gathered}

$$共享编码器是通过元学习过程由参数$\theta^{*}$初始化的。为了对目标语言任务之一进行分类，我们需要微调次优模型参数，以在目标数据集$\mathcal{T}_{l}$上实现最佳性能。设低资源语言任务$T_{l}$的数据和标签对表示为$(X，C)$，其中$X=\left\{x_{1}，x_{2}，\ldots x_{n}\right\}$是$n$数据样本的集合，$C=\left\{c_{1}，c_{2}，\ldots c_{l}\right\}$是$l$类标签的集合。特定语言的学习通过对特定任务$T_{l}$的梯度下降来进行，以最小化公式6和7中总结的交叉熵损失。


我们打算通过尝试在所有学习范例中保持训练设置的一致性，对MetSER与MultiSER和TransSER进行比较分析。我们为元学习和转移学习实验采用共同的基础模型来比较和对比类似设置中的性能。基本模型是双层LSTM模型，最后是两个密集层。每个LSTM单元都有128个维度的隐藏状态。完全连接的层分别具有128和4个神经元。

TransSER的训练设置：在转移学习的情况下，模型在源中的每个数据集上按顺序进行预训练。然后冻结LSTM参数并微调目标数据集之一的训练部分上的致密层。

MultiSER的训练设置：多任务学习的训练设置涉及对所有源数据集进行交替的课堂预训练。然后在目标数据集的完整训练部分上微调模型的密集层。通过这种方式，该模型避免了灾难性的遗忘，因为它以循环方式从所有类中学习特征。在我们的实验中执行的多任务学习的形式导致跨所有层的软参数共享。

多任务和转移学习基线的超参数：我们对所有实验使用循环学习速率计划，基本学习速率为2e− 5，最高学习率为1 e− 4.发现最佳批量为64。转移学习的预训练和微调阶段的最大时期数分别保持为1000和2500，直到验证损失收敛。

元学习的训练设置：由于过度拟合的显着缺点，从头开始训练MAML体系结构是一项具有挑战性的任务。其主要原因是在微调[22]步骤中仅在几个数据点上更新了整个网络。我们调查了元学习对源数据集的五种不同组合的影响。元学习的超参数：元学习过程有两个优化过程元更新和特定于任务的微调。两种方法都有自己的学习率：meta-LR和task-LR。MetaLR在MetaLearn阶段进行了优化，然后在调整任务LR时在特定于语言的微调期间保持恒定。我们在两个预训练集上的实验中，meta-LR和微调LR的最优值均为1e− 6和2e− 4，分别。我们用Adam优化了元情节，任务特定学习使用随机梯度下降（SGD）。更新元参数的每次迭代都称为情节。每个模型运行中的最大剧集数保持为1e+4。每个元情节都有内部步骤更新，这被称为内部任务。根据经验，内部任务在设置为8时实现收敛。我们通过限制任何给定批次中可用的样本数量来调查MetaSER对未见数据的普遍性。因此，元学习器的批量大小b由k-shot学习参数k和不同类的数量c（b=k∗ c） 。我们保持c=4不变，同时改变k以根据宏F1分数评估模型性能。

MultiSER和Ranser与MetaSER的性能比较：表2中F1分数的比较清楚地表明，MetaSER在所有目标语言的预训练集的所有组合上均显着优于多任务学习（MultiSER）和转移学习（TransSER）对应物。，其中一个源集中的乌尔都语除外。结果强化了我们的假设，即基于多任务和转移学习的方法更多地关注源语言，而不是概括跨语言培训。鉴于源语言主要属于日耳曼语家族，因此从经验上证明，MAML培训可用于从高资源语言家族推广到低资源语言家族。数据大小对学习能力的影响：为了研究MetaSER在数据稀缺环境中的普遍性，我们在一小部分目标训练数据上微调模型，并测试TransSER，MultiSER和MetaSER的模型性能。对于TransSER和MultiSER，模型在完整的源数据集上进行预训练，并在目标数据集的分数训练部分以0.1的间隔进行微调。对于MetaSER，我们将少数镜头参数k的值作为可用训练数据的一部分进行改变，使得k∈ {0.1, 0.2, 0.3, . . . , 1}. 在所有情况下，目标数据集的测试部分在推理期间保持相同。由此产生的模型F1分数被绘制出来，在TESS，EMODB和RAVEDESS的源集中给出图2，这表明TransSER和MultiSER在早期阶段都饱和，而MetaSER随着更多的训练数据可用而稳步提高，直到它开始胜过它们的拐点。然而，MAML培训的陡坡也揭示了培训MAML架构的复杂性，而不是转移学习。尽管MAML训练在所有目标数据集中提供了更好的F1得分结果，但它需要足够的数据才能超过其性能阈值。

---
title: "Attentive Modality Hopping Mechanism for Speech Emotion Recognition"
description: ""
citekey: yoonAttentiveModalityHopping2020
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:08
lastmod: 2023-04-11 12:02:03
---

> [!info] 论文信息
>1. Title：Attentive Modality Hopping Mechanism for Speech Emotion Recognition
>2. Author：Seunghyun Yoon, Subhadeep Dey, Hwanhee Lee, Kyomin Jung
>3. Entry：[Zotero link](zotero://select/items/@yoonAttentiveModalityHopping2020) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yoon et al_2020_Attentive Modality Hopping Mechanism for Speech Emotion Recognition.pdf>)
>4. Other：2020 - ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/david-yoon/attentive-modality-hopping-for-SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 引入音频、文本、视频三模态改进情感识别任务
- 引入注意力机制来融合多模态特征信息
- 提出了一种attentive modality hopping process（注意模态跳跃过程），以其他模态为条件，利用注意力机制和前面计算后的特征向量（或经注意后的特征向量）在当前模态中聚合重要的特征信息。

## 摘要

> [!abstract] In this work, we explore the impact of visual modality in addition to speech and text for improving the accuracy of the emotion detection system. The traditional approaches tackle this task by independently fusing the knowledge from the various modalities for performing emotion classification. In contrast to these approaches, we tackle the problem by introducing an attention mechanism to combine the information. In this regard, we first apply a neural network to obtain hidden representations of the modalities. Then, the attention mechanism is defined to select and aggregate important parts of the video data by conditioning on the audio and text data. Furthermore, the attention mechanism is again applied to attend the essential parts of speech and textual data by considering other modalities. Experiments are performed on the standard IEMOCAP dataset using all three modalities (audio, text, and video). The achieved results show a significant improvement of 3.65% in terms of weighted accuracy compared to the baseline system.

> 在这项工作中，我们探索除了语音和文本之外，还有视觉模态对情绪检测系统的影响，以提高情绪检测系统的准确性。传统的方法通过独立地融合来自各种模态的知识来处理这一任务，以执行情感分类。与这些方法不同，我们通过引入注意力机制来结合信息来解决这个问题。在这一点上，我们首先应用神经网络来获得模态的隐藏表征。然后，定义了注意机制，通过对音频和文本数据的条件化处理来选择和聚合视频数据的重要部分。此外，注意机制再次被应用于通过考虑其他模态来注意基本的词性和文本数据。在标准 IEMOCAP 数据集上使用所有三种模态(音频、文本和视频)进行了实验。结果表明，与基线系统相比，加权精度有3.65%的显着提高。

## 预处理

## 概述

## 结果

## 精读

情绪识别在所有人类交流中都起着至关重要的作用。我们的反应取决于情绪状态，并且在传达信息时也提供非语言提示。最近，在计算机辅助技术中已经有许多努力来理解情绪[1]。自动客户服务系统通常设计用于对说话人的情绪进行分类以增强用户体验。情绪分类在语旁领域也是有益的。最近，商业数字辅助应用，如 Siri，发现副语言信息，如情感，有利于识别说话者的意图[2]。人类通常使用多模态信息来识别情绪。然而，多模态对这一领域的影响尚未得到充分研究。在本文中，我们有兴趣利用文本，言语和视觉模型中的信息来识别情绪。在过去，已经从语音信号中探索了不同的情绪识别方法[3,4]。大多数言语情绪技术都专注于提取低级或高级特征。特别地，应用信号处理技术来提取特征，例如 cepstral 和韵律 prosodic 特征。超分割 Suprasegmental 特征（如 cepstral 或韵律 contours）已被证明可以为这项任务提供良好的表现[5]。此外，统计建模技术，如隐马尔可夫模型（HMM）和高斯混合模型（GMM），已成功用于此任务[6,7]。最近，除了用于情绪分类的语音信号之外，研究人员还探索了文本信息的应用。词汇信息通常用于搜索表达说话者情绪状态的关键字。在[8]中，通过使用消息的单词表征来使用词汇信息。最近的方法利用深度神经网络（DNN）强大的建模功能来融合来自两种模态的信息[9,10,11]。模态的隐藏表征用于组合来自声学和文本数据的知识以用于情绪分类。在我们以前的工作[12]中，我们探索了利用文本信息的注意机制。训练注意机制自动总结词汇内容和言语话语。实验评估表明标准 IEMOCAP 数据集具有卓越的性能[13]。

在本文中，我们通过将视觉信息纳入框架并提出一种有效利用多模态知识的注意机制来扩展这种方法。这是由于人类通过面部表情，语音信号和词汇内容表达情感这一事实。我们假设除了语音和词汇信息之外，利用视觉知识将导致卓越的表现。

与**独立组合**来自模态的信息相反，我们建议应用一种关注机制，该机制汇总来自以其他两种模态为条件提取的当前模态的知识。所提出的方法首先从三种模态（语音，文本和视觉）获得隐藏表征的序列。
- 通过将注意力权重与隐藏单元线性组合来获得视觉模型的汇总向量。
- 然后应用该矢量以获得声学数据的注意权重。
- 更新的视觉和声学数据连续用于计算文本数据的注意权重以聚合文本模态的显着部分。

随着这个过程持续多次，我们假设该机制将有效地计算每种模态的相关部分。为了评估所提出的方法的性能，我们在标准的IEMOCAP数据集上进行了情绪识别实验。在该语料库上的实验表明，所提出的方法在加权准确度方面优于基线系统3.65%的相对改善。另外，我们通过增加模态的迭代来获得改进的模型性能。实验结果表明，我们提出的模型通过迭代跳跃过程正确地学习了在模态之间聚合必要信息。

近年来，已有几种神经网络方法被成功地应用于情感分类。研究人员提出了基于卷积神经网络(CNN)的模型，该模型对语音话语进行训练以执行识别[14，15]。也有一些使用注意机制的成功方法[16，17]。特别是，[17]中的工作提出了一种将基于注意的建模合并到递归神经网络(RNN)体系结构中的方法。注意力机制被设计为计算每一帧的权重或相关性。通过对这些加权语音特征的时间聚集来获得发声级别表示。注意单元被设计为自动获取用于情感识别的语音片段。使用声学和词汇知识的情感识别也在文献中进行了探索。这些作品的灵感来自于这样一个事实，即情感对话不仅由言语组成，也由文本内容组成。在[18]中，利用情感关键字来有效地识别类别。最近在[9，10，19]中，已经探索了一种基于长短期记忆(LSTM)的网络来编码两种模态的信息。此外，已经有一些尝试使用注意间机制融合各种模态[11，12]。然而，这些方法的设计只考虑了声学和文本信息之间的交互作用。

本节介绍应用于语音情感识别任务的方法。我们首先引入递归编码器来分别对音频、文本和视频模态进行编码。然后，我们提出了一种逐个利用每种模态的方法。在该技术中，提出了一种细心的模态跳跃过程，通过迭代聚合过程获得每个模态的相关部分。

受文献[9，17，20]中所用结构的启发，我们采用递归神经网络对语音信号中的一系列特征进行编码，并将信号分类为情感类别之一。特别是，我们对每个模态(即，声学、文本、视觉)采用门控递归单元(GRU)[21]来编码信息，如图1所示。GRU通过如下更新其隐藏状态来编码每个模态的特征向量序列：

$$ \mathbf{h}_{t}=f_{\theta}\left(\mathbf{h}_{t-1}, \mathbf{x}_{t}\right), $$ 其中$f_{\theta}$是具有权重参数$\theta的GRU网络，\mathbf{h}_{t}$表示在$t$时间步长的隐藏状态，以及$\mathbf{x}_{t}$表示目标模态中的$t$个连续特征。该递归编码器以与独立音频、文本和视频模态相同的方式使用。对于视频数据，我们从预先训练的ResNet[22]获得每一帧的固定维度表示。

我们提出了一种新的迭代注意过程，称为注意力模态跳跃机制(AMH)，它聚集了每个模态上的显著信息来预测语音的情绪。图1显示了建议的AMH模型的体系结构。以前的研究通过融合每个模态上的信息，使用神经网络模型独立使用多模态信息[9，19]。最近，研究人员还研究了模态的注意间机制[11，12]。与这种方法相反，我们提出了一种神经网络体系结构，它通过迭代过程以其他模态为条件，在一种模态中聚合信息。

![]({36}_Attentive%20Modality%20Hopping%20Mechanism%20for%20Speech%20Emotion%20Recognition@yoonAttentiveModalityHopping2020.assets/image-20220605170157.png)



首先，通过公式(1)使用递归编码器对每个模态的序列特征进行编码。然后，音频递归编码器的最后一步隐藏状态$\mathbf{h}_{\text{Last}}^{A}$和文本递归编码器$\mathbf{h}_{\text{Last}}^{T}$融合在一起以形成上下文知识$\mathbf{C}$。然后，我们将注意力机制应用于视频序列$\mathbf{h}_{\mathbf{t}}^{V}$，以聚集视频形态的显著部分。由于该模型是用单一注意方法开发的，我们将该模型称为AMH-1。AMH-1模型的最终结果$\mathbf{H}_{\text{Hop}1}$计算如下：

$$ \begin{aligned} &\mathbf{H}_{\text {hop } 1}=\left[\mathbf{h}_{\text {last }}^{A} ; \mathbf{h}_{\text {last }}^{T} ; \mathbf{H}_{1}^{V}\right] \\ &\mathbf{H}_{1}^{V}=\sum_{i} a_{i} \mathbf{h}_{i}^{V} \\ &a_{i}=\frac{\exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)},(i=1, \ldots, t), \\ &\mathbf{C}=f\left(\mathbf{h}_{\text {last }}^{A}, \mathbf{h}_{\text {last }}^{T}\right) \end{aligned} $$ 
其中$f$是融合函数(在本研究中我们使用向量连接)，$\mathbf{W}\in\mathbb{R}^{d\times d}$和偏差$\mathbf{b}$是学习的模型参数。整个流程如图1(A)所示。公式(2)中的$\mathbf{H}_{1}^{V}$是一种考虑了音频和文本模态的视觉信息的新模态表示。

利用该信息，我们将称为$\mathbf{A M H}\mathbf{M}$-2的第二注意模态跳跃过程应用于音频序列。AMH-2模型的最终结果$\mathbf{H}_{\text{Hop}2}$计算如下：
$$
\begin{aligned}
&\mathbf{H}_{\mathrm{hop} 2}=\left[\mathbf{H}_{1}^{A} ; \mathbf{h}_{\text {last }}^{T} ; \mathbf{H}_{1}^{V}\right] \\
&\mathbf{H}_{1}^{A}=\sum_{i} a_{i} \mathbf{h}_{i}^{A}, \\
&a_{i}=\frac{\exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{A}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{A}\right)},(i=1, \ldots, t), \\
&\mathbf{C}=f\left(\mathbf{h}_{\text {last }}^{T}, \mathbf{H}_{1}^{V}\right)
\end{aligned}
$$
其中，$\mathbf{H}_{1}^{A}$是考虑AMH-1过程之后的文本和视觉模态的音频信息的新表示。

我们进一步将第三注意模态跳跃过程应用于文本序列，称为$\mathbf{A}\mathbf{M H}-\mathbf{3}$，具有来自等式(2)和(3)的更新的音频和视觉表示。AMH-3模型的最终结果$\mathbf{H}_{\text{Hop}3}$计算如下：

$$
\begin{aligned}
&\mathbf{H}_{\mathrm{hop} 3}=\left[\mathbf{H}_{1}^{A} ; \mathbf{H}_{1}^{T} ; \mathbf{H}_{1}^{V}\right] \\
&\mathbf{H}_{1}^{T}=\sum_{i} a_{i} \mathbf{h}_{i}^{T} \\
&a_{i}=\frac{\exp \left((\mathbf{C})^{\boldsymbol{\top}} \mathbf{W} \mathbf{h}_{i}^{T}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{T}\right)},(i=1, \ldots, t), \\
&\mathbf{C}=f\left(\mathbf{H}_{1}^{A}, \mathbf{H}_{1}^{V}\right)
\end{aligned}
$$

其中，$\mathbf{H}_{1}^{T}$是考虑到音频和视频形态的文本信息的更新表示。

同样，我们可以重复$\mathbf{A}\mathbf{M H}-\mathbf{1}$过程，更新后的模态为$\mathbf{H}_{1}^{A}、\mathbf{H}_{1}^{T}$和$\mathbf{H}_{1}^{V}$，以定义AMH-4过程并计算$\mathbf{H}_{\mathbf{hop}4}$，如下所示：

$$
\begin{aligned}
&\mathbf{H}_{\mathrm{hop} 4}=\left[\mathbf{H}_{1}^{A} ; \mathbf{H}_{1}^{T} ; \mathbf{H}_{2}^{V}\right], \\
&\mathbf{H}_{2}^{V}=\sum_{i} a_{i} \mathbf{h}_{i}^{V}, \\
&a_{i}=\frac{\exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)},(i=1, \ldots, t), \\
&\mathbf{C}=f\left(\mathbf{H}_{1}^{A}, \mathbf{H}_{1}^{T}\right) .
\end{aligned}
$$

由于所提出的AMH机制采用迭代的模态跳跃过程，因此我们可以推广$\mathbf{A}\mathbf{M H}-\mathbf{N}$公式，该公式允许模型通过按顺序重复等式$(2)、(3)$和(4)来在模态上跳$N$次。

因为我们的目标是对语音情感进行分类，所以我们通过Softmax函数传递AMH-N的最终结果$\mathbf{H}_{\mathbf{hop}\mathbf{N}}$来预测七类情感类别。我们采用由下式定义的交叉熵损失函数：

$$
\begin{aligned}
\hat{y}_{c} &=\operatorname{softmax}\left(\left(\mathbf{H}_{\mathrm{hopN}}\right)^{\top} \mathbf{W}+\mathbf{b}\right) \\
\mathcal{L} &=\frac{1}{N} \sum_{j=1}^{N} \sum_{c=1}^{C} y_{j, c} \log \left(\hat{y}_{j, c}\right)
\end{aligned}
$$


其中$y_{j，c}$是真实的标签向量，而$\hat{y}_{j，c}$是来自Softmax层的预测概率。$\mathbf{W}$和偏差$\mathbf{b}$是模型参数。$C$是班级总数，$N$是训练中使用的样本总数。

### 引文

## 摘录

---
title: "Attentive Modality Hopping Mechanism for Speech Emotion Recognition"
description: ""
citekey: yoonAttentiveModalityHopping2020
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:08
lastmod: 2023-04-11 12:02:03
---

> [!info] 论文信息
>1. Title：Attentive Modality Hopping Mechanism for Speech Emotion Recognition
>2. Author：Seunghyun Yoon, Subhadeep Dey, Hwanhee Lee, Kyomin Jung
>3. Entry：[Zotero link](zotero://select/items/@yoonAttentiveModalityHopping2020) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Yoon et al_2020_Attentive Modality Hopping Mechanism for Speech Emotion Recognition.pdf>)
>4. Other：2020 - ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/david-yoon/attentive-modality-hopping-for-SER
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 引入音频、文本、视频三模态改进情感识别任务
- 引入注意力机制来融合多模态特征信息
- 提出了一种attentive modality hopping process（注意模态跳跃过程），以其他模态为条件，利用注意力机制和前面计算后的特征向量（或经注意后的特征向量）在当前模态中聚合重要的特征信息。

## 摘要

> [!abstract] In this work, we explore the impact of visual modality in addition to speech and text for improving the accuracy of the emotion detection system. The traditional approaches tackle this task by independently fusing the knowledge from the various modalities for performing emotion classification. In contrast to these approaches, we tackle the problem by introducing an attention mechanism to combine the information. In this regard, we first apply a neural network to obtain hidden representations of the modalities. Then, the attention mechanism is defined to select and aggregate important parts of the video data by conditioning on the audio and text data. Furthermore, the attention mechanism is again applied to attend the essential parts of speech and textual data by considering other modalities. Experiments are performed on the standard IEMOCAP dataset using all three modalities (audio, text, and video). The achieved results show a significant improvement of 3.65% in terms of weighted accuracy compared to the baseline system.

> 在这项工作中，我们探索除了语音和文本之外，还有视觉模态对情绪检测系统的影响，以提高情绪检测系统的准确性。传统的方法通过独立地融合来自各种模态的知识来处理这一任务，以执行情感分类。与这些方法不同，我们通过引入注意力机制来结合信息来解决这个问题。在这一点上，我们首先应用神经网络来获得模态的隐藏表征。然后，定义了注意机制，通过对音频和文本数据的条件化处理来选择和聚合视频数据的重要部分。此外，注意机制再次被应用于通过考虑其他模态来注意基本的词性和文本数据。在标准 IEMOCAP 数据集上使用所有三种模态(音频、文本和视频)进行了实验。结果表明，与基线系统相比，加权精度有3.65%的显着提高。

## 预处理

## 概述

## 结果

## 精读

情绪识别在所有人类交流中都起着至关重要的作用。我们的反应取决于情绪状态，并且在传达信息时也提供非语言提示。最近，在计算机辅助技术中已经有许多努力来理解情绪[1]。自动客户服务系统通常设计用于对说话人的情绪进行分类以增强用户体验。情绪分类在语旁领域也是有益的。最近，商业数字辅助应用，如 Siri，发现副语言信息，如情感，有利于识别说话者的意图[2]。人类通常使用多模态信息来识别情绪。然而，多模态对这一领域的影响尚未得到充分研究。在本文中，我们有兴趣利用文本，言语和视觉模型中的信息来识别情绪。在过去，已经从语音信号中探索了不同的情绪识别方法[3,4]。大多数言语情绪技术都专注于提取低级或高级特征。特别地，应用信号处理技术来提取特征，例如 cepstral 和韵律 prosodic 特征。超分割 Suprasegmental 特征（如 cepstral 或韵律 contours）已被证明可以为这项任务提供良好的表现[5]。此外，统计建模技术，如隐马尔可夫模型（HMM）和高斯混合模型（GMM），已成功用于此任务[6,7]。最近，除了用于情绪分类的语音信号之外，研究人员还探索了文本信息的应用。词汇信息通常用于搜索表达说话者情绪状态的关键字。在[8]中，通过使用消息的单词表征来使用词汇信息。最近的方法利用深度神经网络（DNN）强大的建模功能来融合来自两种模态的信息[9,10,11]。模态的隐藏表征用于组合来自声学和文本数据的知识以用于情绪分类。在我们以前的工作[12]中，我们探索了利用文本信息的注意机制。训练注意机制自动总结词汇内容和言语话语。实验评估表明标准 IEMOCAP 数据集具有卓越的性能[13]。

在本文中，我们通过将视觉信息纳入框架并提出一种有效利用多模态知识的注意机制来扩展这种方法。这是由于人类通过面部表情，语音信号和词汇内容表达情感这一事实。我们假设除了语音和词汇信息之外，利用视觉知识将导致卓越的表现。

与**独立组合**来自模态的信息相反，我们建议应用一种关注机制，该机制汇总来自以其他两种模态为条件提取的当前模态的知识。所提出的方法首先从三种模态（语音，文本和视觉）获得隐藏表征的序列。
- 通过将注意力权重与隐藏单元线性组合来获得视觉模型的汇总向量。
- 然后应用该矢量以获得声学数据的注意权重。
- 更新的视觉和声学数据连续用于计算文本数据的注意权重以聚合文本模态的显着部分。

随着这个过程持续多次，我们假设该机制将有效地计算每种模态的相关部分。为了评估所提出的方法的性能，我们在标准的IEMOCAP数据集上进行了情绪识别实验。在该语料库上的实验表明，所提出的方法在加权准确度方面优于基线系统3.65%的相对改善。另外，我们通过增加模态的迭代来获得改进的模型性能。实验结果表明，我们提出的模型通过迭代跳跃过程正确地学习了在模态之间聚合必要信息。

近年来，已有几种神经网络方法被成功地应用于情感分类。研究人员提出了基于卷积神经网络(CNN)的模型，该模型对语音话语进行训练以执行识别[14，15]。也有一些使用注意机制的成功方法[16，17]。特别是，[17]中的工作提出了一种将基于注意的建模合并到递归神经网络(RNN)体系结构中的方法。注意力机制被设计为计算每一帧的权重或相关性。通过对这些加权语音特征的时间聚集来获得发声级别表示。注意单元被设计为自动获取用于情感识别的语音片段。使用声学和词汇知识的情感识别也在文献中进行了探索。这些作品的灵感来自于这样一个事实，即情感对话不仅由言语组成，也由文本内容组成。在[18]中，利用情感关键字来有效地识别类别。最近在[9，10，19]中，已经探索了一种基于长短期记忆(LSTM)的网络来编码两种模态的信息。此外，已经有一些尝试使用注意间机制融合各种模态[11，12]。然而，这些方法的设计只考虑了声学和文本信息之间的交互作用。

本节介绍应用于语音情感识别任务的方法。我们首先引入递归编码器来分别对音频、文本和视频模态进行编码。然后，我们提出了一种逐个利用每种模态的方法。在该技术中，提出了一种细心的模态跳跃过程，通过迭代聚合过程获得每个模态的相关部分。

受文献[9，17，20]中所用结构的启发，我们采用递归神经网络对语音信号中的一系列特征进行编码，并将信号分类为情感类别之一。特别是，我们对每个模态(即，声学、文本、视觉)采用门控递归单元(GRU)[21]来编码信息，如图1所示。GRU通过如下更新其隐藏状态来编码每个模态的特征向量序列：

$$ \mathbf{h}_{t}=f_{\theta}\left(\mathbf{h}_{t-1}, \mathbf{x}_{t}\right), $$ 其中$f_{\theta}$是具有权重参数$\theta的GRU网络，\mathbf{h}_{t}$表示在$t$时间步长的隐藏状态，以及$\mathbf{x}_{t}$表示目标模态中的$t$个连续特征。该递归编码器以与独立音频、文本和视频模态相同的方式使用。对于视频数据，我们从预先训练的ResNet[22]获得每一帧的固定维度表示。

我们提出了一种新的迭代注意过程，称为注意力模态跳跃机制(AMH)，它聚集了每个模态上的显著信息来预测语音的情绪。图1显示了建议的AMH模型的体系结构。以前的研究通过融合每个模态上的信息，使用神经网络模型独立使用多模态信息[9，19]。最近，研究人员还研究了模态的注意间机制[11，12]。与这种方法相反，我们提出了一种神经网络体系结构，它通过迭代过程以其他模态为条件，在一种模态中聚合信息。

![]({36}_Attentive%20Modality%20Hopping%20Mechanism%20for%20Speech%20Emotion%20Recognition@yoonAttentiveModalityHopping2020.assets/image-20220605170157.png)



首先，通过公式(1)使用递归编码器对每个模态的序列特征进行编码。然后，音频递归编码器的最后一步隐藏状态$\mathbf{h}_{\text{Last}}^{A}$和文本递归编码器$\mathbf{h}_{\text{Last}}^{T}$融合在一起以形成上下文知识$\mathbf{C}$。然后，我们将注意力机制应用于视频序列$\mathbf{h}_{\mathbf{t}}^{V}$，以聚集视频形态的显著部分。由于该模型是用单一注意方法开发的，我们将该模型称为AMH-1。AMH-1模型的最终结果$\mathbf{H}_{\text{Hop}1}$计算如下：

$$ \begin{aligned} &\mathbf{H}_{\text {hop } 1}=\left[\mathbf{h}_{\text {last }}^{A} ; \mathbf{h}_{\text {last }}^{T} ; \mathbf{H}_{1}^{V}\right] \\ &\mathbf{H}_{1}^{V}=\sum_{i} a_{i} \mathbf{h}_{i}^{V} \\ &a_{i}=\frac{\exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)},(i=1, \ldots, t), \\ &\mathbf{C}=f\left(\mathbf{h}_{\text {last }}^{A}, \mathbf{h}_{\text {last }}^{T}\right) \end{aligned} $$ 
其中$f$是融合函数(在本研究中我们使用向量连接)，$\mathbf{W}\in\mathbb{R}^{d\times d}$和偏差$\mathbf{b}$是学习的模型参数。整个流程如图1(A)所示。公式(2)中的$\mathbf{H}_{1}^{V}$是一种考虑了音频和文本模态的视觉信息的新模态表示。

利用该信息，我们将称为$\mathbf{A M H}\mathbf{M}$-2的第二注意模态跳跃过程应用于音频序列。AMH-2模型的最终结果$\mathbf{H}_{\text{Hop}2}$计算如下：
$$
\begin{aligned}
&\mathbf{H}_{\mathrm{hop} 2}=\left[\mathbf{H}_{1}^{A} ; \mathbf{h}_{\text {last }}^{T} ; \mathbf{H}_{1}^{V}\right] \\
&\mathbf{H}_{1}^{A}=\sum_{i} a_{i} \mathbf{h}_{i}^{A}, \\
&a_{i}=\frac{\exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{A}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{A}\right)},(i=1, \ldots, t), \\
&\mathbf{C}=f\left(\mathbf{h}_{\text {last }}^{T}, \mathbf{H}_{1}^{V}\right)
\end{aligned}
$$
其中，$\mathbf{H}_{1}^{A}$是考虑AMH-1过程之后的文本和视觉模态的音频信息的新表示。

我们进一步将第三注意模态跳跃过程应用于文本序列，称为$\mathbf{A}\mathbf{M H}-\mathbf{3}$，具有来自等式(2)和(3)的更新的音频和视觉表示。AMH-3模型的最终结果$\mathbf{H}_{\text{Hop}3}$计算如下：

$$
\begin{aligned}
&\mathbf{H}_{\mathrm{hop} 3}=\left[\mathbf{H}_{1}^{A} ; \mathbf{H}_{1}^{T} ; \mathbf{H}_{1}^{V}\right] \\
&\mathbf{H}_{1}^{T}=\sum_{i} a_{i} \mathbf{h}_{i}^{T} \\
&a_{i}=\frac{\exp \left((\mathbf{C})^{\boldsymbol{\top}} \mathbf{W} \mathbf{h}_{i}^{T}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{T}\right)},(i=1, \ldots, t), \\
&\mathbf{C}=f\left(\mathbf{H}_{1}^{A}, \mathbf{H}_{1}^{V}\right)
\end{aligned}
$$

其中，$\mathbf{H}_{1}^{T}$是考虑到音频和视频形态的文本信息的更新表示。

同样，我们可以重复$\mathbf{A}\mathbf{M H}-\mathbf{1}$过程，更新后的模态为$\mathbf{H}_{1}^{A}、\mathbf{H}_{1}^{T}$和$\mathbf{H}_{1}^{V}$，以定义AMH-4过程并计算$\mathbf{H}_{\mathbf{hop}4}$，如下所示：

$$
\begin{aligned}
&\mathbf{H}_{\mathrm{hop} 4}=\left[\mathbf{H}_{1}^{A} ; \mathbf{H}_{1}^{T} ; \mathbf{H}_{2}^{V}\right], \\
&\mathbf{H}_{2}^{V}=\sum_{i} a_{i} \mathbf{h}_{i}^{V}, \\
&a_{i}=\frac{\exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)}{\sum_{i} \exp \left((\mathbf{C})^{\top} \mathbf{W} \mathbf{h}_{i}^{V}\right)},(i=1, \ldots, t), \\
&\mathbf{C}=f\left(\mathbf{H}_{1}^{A}, \mathbf{H}_{1}^{T}\right) .
\end{aligned}
$$

由于所提出的AMH机制采用迭代的模态跳跃过程，因此我们可以推广$\mathbf{A}\mathbf{M H}-\mathbf{N}$公式，该公式允许模型通过按顺序重复等式$(2)、(3)$和(4)来在模态上跳$N$次。

因为我们的目标是对语音情感进行分类，所以我们通过Softmax函数传递AMH-N的最终结果$\mathbf{H}_{\mathbf{hop}\mathbf{N}}$来预测七类情感类别。我们采用由下式定义的交叉熵损失函数：

$$
\begin{aligned}
\hat{y}_{c} &=\operatorname{softmax}\left(\left(\mathbf{H}_{\mathrm{hopN}}\right)^{\top} \mathbf{W}+\mathbf{b}\right) \\
\mathcal{L} &=\frac{1}{N} \sum_{j=1}^{N} \sum_{c=1}^{C} y_{j, c} \log \left(\hat{y}_{j, c}\right)
\end{aligned}
$$


其中$y_{j，c}$是真实的标签向量，而$\hat{y}_{j，c}$是来自Softmax层的预测概率。$\mathbf{W}$和偏差$\mathbf{b}$是模型参数。$C$是班级总数，$N$是训练中使用的样本总数。

### 引文

---
title: "Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion Recognition"
description: ""
citekey: atmajaMultitaskLearningMultistage2020
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:19
lastmod: 2023-04-11 12:02:59
---

> [!info] 论文信息
>1. Title：Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion Recognition
>2. Author：Bagus Tris Atmaja, Masato Akagi
>3. Entry：[Zotero link](zotero://select/items/@atmajaMultitaskLearningMultistage2020) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Atmaja_Akagi_2020_Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion.pdf>)
>4. Other：2020 - ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/bagustris/multistage-ser
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 本文目标是增强CCC，因此使用CCC损耗代替MSE损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。- 本文目标是增强 CCC，因此使用 CCC 损耗代替 MSE 损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。

## 摘要

> [!abstract] Due to its ability to accurately predict emotional state using multimodal features, audiovisual emotion recognition has recently gained more interest from researchers. This paper proposes two methods to predict emotional attributes from audio and visual data using a multitask learning and a fusion strategy. First, multitask learning is employed by adjusting three parameters for each attribute to improve the recognition rate. Second, a multistage fusion is proposed to combine results from various modalities' final prediction. Our approach used multitask learning, employed at unimodal and early fusion methods, shows improvement over single-task learning with an average CCC score of 0.431 compared to 0.297. A multistage method, employed at the late fusion approach, significantly improved the agreement score between true and predicted values on the development set of data (from [0.537, 0.565, 0.083] to [0.68, 0.656, 0.443]) for arousal, valence, and liking.

> 视听情绪识别由于能够利用多模态特征准确预测情绪状态，近年来受到研究者的广泛关注。本文提出了两种基于多任务学习和融合策略的基于视听数据的情感属性预测方法。首先，通过调整每个属性的三个参数来进行多任务学习，以提高识别率。其次，提出了一种多阶段融合的方法，将不同模态的最终预测结果进行融合。我们的方法使用了多任务学习，在单峰unimodal（单模态）和早期融合方法中使用，显示出比单任务学习更好的结果，平均Ccc得分为0.431分，而单任务学习方法为0.297分。在晚期融合方法中采用的多阶段方法显著提高了发展数据集上的真实值和预测值之间的一致性分数(从[0.537，0.565，0.083]提高到[0.68，0.656，0.443])，包括性唤醒、效价和喜好。

## 预处理

## 概述

## 结果

本研究使用了[3]中提供的 SEWA 数据集[12]。该数据集包含来自汉语、英语、德语、希腊语、匈牙利语和塞尔维亚语的视听记录，但在这项工作中仅使用德语(DE)和匈牙利语(HU)，因为它们没有提供其他语言的测试标签。提供三个属性来表示情绪状态，即：唤醒、价位和喜好。这些属性的分数是从几个母语人士的注释中获得的：六个德国人和五个匈牙利人。在96名受试者中，68名受试者(每名34名)用于培训，其余28名受试者(每名14名)用于验证/发展。除了数据集，文[3]的作者还提供了表1所示的基线特征。我们没有生成新的特征集，而是将多任务学习和多模式视听融合应用于这些特征集。对于音频和视觉特征，使用相同的处理块，即4.0s 的窗口长度和100ms 的跳跃大小，其中还为每0.1s 给出标签。然后，通过为低于该数字的其他序列填充零，将最长的1768序列(标签号)用于所有对象。对于双峰特征融合，音频和视觉特征在被送入分类器之前被连接起来。

## 精读

已经使用两种观点来实现自动情绪识别：分类视图和维度视图。虽然大多数研究人员试图将人类情绪分为不同的类别（例如幸福，愤怒等），但维度情绪识别是更具挑战性的任务，因为它试图将情绪标记为程度而不是类别。从维度的角度来看，情感被描述为2或3个属性[1]。

- Valence (pleasantness)和arousal (emotion intensity)是2D情绪模型中两个最常见的维度。
- 在3D模型中，使用dominance (degree of control)或liking。
- 其他属性，如expectancy，可以作为第四维（4D）添加[2]。

在本文中，我们评估了三个情绪维度/属性：arousal, valence, 和 liking，这些都是从[3]中的数据集中获得的。此任务是在特定指标上获得最准确的预测。作为回归任务（分类任务），最常见的指标是真实值和预测情绪程度（值）之间的误差。然而，最近的研究人员[3]引入了相关性(correlation)测量来确定真实值与预测情绪程度之间的一致性。

通常使用两种方法来最小化学习过程特征的损失并获得预测情绪维度的最佳模型，即单任务学习（STL）和多任务学习（MTL）。
- 单任务学习仅在多输出学习中最小化单个损失函数。例如，当学习预测维度情绪中的唤醒，效价和喜好时，只有唤醒被最小化。其他维度，效价和喜好，在学习过程中被忽略。通过最小化唤醒误差，学习过程的结果可用于预测一维（唤醒）或所有三个维度（唤醒，效价和喜好）。
- 单任务学习的问题在于，当它用于预测多个输出时，使用单个损失函数预测三个分数。一个维度的高分通常导致另一个维度的得分较低。为了解决这个问题，我们引入了在使用最小化所有情绪维度的真实情绪和预测情绪程度之间的误差情况下的多任务学习。多任务学习中的常用方法是对学习过程中的每个损失函数使用相同的加权因子。因此，损失总和是来自每个情绪维度的三个每个损失函数的总和。我们在本文中提出的方法旨在通过为每个情绪维度的每个损失函数分配不同的加权因子来获得平衡分数。

由于情绪来自多种形式，因此创建融合策略以适应这些模态也是一项挑战。标准方法是通过在相同或不同网络中组合不同模态之间的特征。这被称为早期融合策略。然后训练这两个或多个融合特征以将这些输入映射到标签上。另一个策略是使用后期融合。在这个策略中，每种模态都使用其标签在其相应网络中进行训练。然后将每种模态的识别结果分组以找到对应于标签的最高概率。早期融合和晚期融合的结果也可以通过将这些结果组合在支持向量回归（SVR）中来融合。最后一步的结果可以多阶段重复，以提高识别率。

我们对本文的贡献可以概括如下：
（1）使用多任务学习来最小化损失函数，使用三个参数来处理来自视听特征的三个情绪属性；
（2） 融合策略通过分析早期融合和晚期融合的单峰unimodal（单模态）和双峰bimodal（双模态）特征，结合早期-晚期融合使用多级SVR来提高视听情绪识别率。

Multitask learning

机器学习中的问题之一是获得适当的成本函数或损失函数来对数据进行建模。回归分析（分类任务）中的大多数问题的损失函数都使用真实值和预测值之间的误差计算。损失函数的选择通常由用于评估的指标决定。在维度情感识别的情况下，
- Ringeval 等人建议使用一致性相关系数 (CCC) 来对预测情绪属性的性能进行评分 [3]。
- Parthasarathy 和 Busso 使用多任务学习来最小化维度情感识别中的均方误差 (MSE) [4]。作者使用两个参数来衡量三个情绪属性的损失函数：唤醒、效价和支配。并确定了唤醒和效价的加权因子，以及优势的加权因子（唤醒和效价的加权因子中减去 1 来获得）。所有权重因子都在 0-1 的范围内，其中一个值为 0 的可能性为 33.3%。还发现唤醒和效价的最佳参数是 0.7 和 0.3。在这种情况下，学习过程中忽略了优势，可以将其视为类似于单任务学习的两任务学习。
- 使用共享层和独立层等两种多任务学习方法，与基线单任务学习相比，作者实现了 CCC 分数的提高 [4]。由于系统在较大的网络上比在较小的网络上学习得更好，因此使用的网络越大，获得的改进就越大。
- Chen 等人还使用以 MSE 作为损失函数的多任务学习 [5]。尽管从给定的基线上实现了 CCC 分数的提高，但没有指定与单任务学习的性能比较。这可能会导致难以确定改进是来自多任务学习还是其他使用的策略。

Multimodal Fusion

由于可以从许多模态中识别情感，例如语音、面部图像、运动和语言信息，因此在此类系统中通常考虑使用多模态技术来适应许多特征。 
- [ ] 中描述的数据集包括来自音频和视觉的多模态情感特征。
- 布索等人 [6]提供了来自语音和手势的情感数据集，包括面部表情和手部动作。
- 该数据集的改进版本[7]提供了一个带有视听信息的情感数据库，从而促进了（表演）录音中的自然性。

为了处理从多模态数据集中提取的各种特征，研究人员开发了几类特征融合方法[5,8,9,10,11]。大多数策略可以分为早期融合和晚期融合。
- 在早期融合方法中，也称为特征级融合，在进行分类之前将来自不同模态的特征组合起来。
- 在后期融合方法中，也称为决策层融合，最终的决策概率由每个单峰unimodal（单模态）模型结果通过SVR等方法给出。 Ringeval 等人。为来自 SEWA 数据集的后期融合策略提供基线融合方法 [3, 12]。可以使用静态回归器（即 SVR）将每个模态或特征集的结果组合起来，以根据几种模态的给定结果做出预测的情感属性分数的最终决定。

Multitask learning based on CCC loss

CCC是维度情绪识别中用来衡量真实情绪维度与预测情绪程度一致性的常用度量标准。《CCC》的公式

$$C C C=\frac{2 \rho_{x y} \sigma_{x} \sigma_{y}}{\sigma_{x}^{2}+\sigma_{y}^{2}+\left(\mu_{x}-\mu_{y}\right)^{2}}$$ 


式中，$\rho_{x y}$是$x$与$y$之间的皮尔逊系数相关系数，$\sigma$为标准差，$\mu$为平均值。这是基于林的计算[20]。CCC的范围从$-1$(完全不一致)到$1$(完全一致)。因此，使真实值和预测情感之间的一致性最大化的CCC损失函数(CCCL)可以定义为$$CCCL=1-CCC$$
在单任务学习中，损失函数是来自唤醒$\left(CL_{\text{aro}}\right)$、价值$\left(CL_{\text{val}}\right)$或喜欢$\left(CL_{l Ik}\right)$的损失函数之一。在多任务学习中，当CCC损失用作所有唤醒、价位和喜好的单一度量时，$C L_{\text{Total}}$是这三个CCC损失函数的组合：
$$。

C L_{t o t}=\alpha C L_{a r o}+\beta C L_{v a l}+\gamma C L_{l i k}，

$$
其中，$\alpha、\beta$和$\gamma$是每个情感维度损失函数的权重因子。在常见的方法中，$\alpha、\beta$和$\Gamma$被设置为1，而在[4]中，$\Gamma$被设置为$1-(\alpha+\beta)$以最小化MSE。在该方法中，所有权重系数都在0-1的范围内。

在本文中，我们使用了所有这三个参数，并且这些加权因子的和并不限于仅0-1。由于目标是加强CCC，因此使用CCC损耗代替MSE。

![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203900.png)

如图1所示，视听情感识别系统由3个LSTM层组成，分别为256、128和64个单元。在每个LSTM层之后添加系数为0.4的丢弃层。在一次实验中使用了RMSprop优化器，学习率为0.0005，批次大小为34，持续50个时期。为了补偿在进行注释时的延迟，标签在训练过程中被移到前面0.1，并且在写预测时被移回。

在图1中，该系统从双峰bimodal（双模态）音频和视觉特征集中产生对唤醒、价位和喜好程度的预测。这一结果可以与使用SVR(来自不同特征集)的单峰unimodal（单模态）或双峰bimodal（双模态）(早期)融合的其他结果相结合，并且从SVR得到的预测也可以输入到相同的SVR系统(使用SCRKIT-LINE工具实现[21])。

![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203950.png)

在图2中，这种早期融合和晚期融合的组合分三个阶段进行了说明。首先，使用支持向量机方法对单峰unimodal（单模态）结果(称为结果#1)和多峰(双峰bimodal（双模态）)结果(称为结果#2)或单峰unimodal（单模态）和单峰unimodal（单模态）进行训练。这个学习过程产生一个新的结果(即该图中的结果#3)。将后期融合的结果#3再次馈送到SVR方法结果#4。结果#4再次馈送到SVR方法结果#5。这种多阶段融合可以进行n次以获得CCC评分的改善。为了评估所提出的MTL方法与STL和以前的MTL方法的有效性，我们比较了这些方法之间的CCC分数。表2显示了不同属性的CCC分数及其平均值。我们提出的MTL2的性能优于STL和先前提出的MTL1。

为了找到α，β和γ的最佳参数，我们在0-1的范围内对这些参数进行了随机搜索。表2中使用的参数是最优的，即α，β和γ分别为0.7、0.2和1.0。我们提出的具有三个参数的MTL学习优于STL和以前的MTL[4]。对于STL方法，唤醒和配价在其属性优化时都获得了最高的CCC分数。虽然喜好在STL3中得到了优化，但它仍然是最难估计的。这一问题应在今后的研究中加以解决。


为了获得多级融合结果，执行以下步骤，1。单峰unimodal（单模态）情绪识别：执行此步骤以研究双峰bimodal（双模态）或多峰融合的重要性特征集。2.双峰bimodal（双模态）融合：通过连接来自不同或相同模态的两个特征集来执行该步骤。3.多模态融合：前两步使用DNN进行，第三步使用SVR进行，结合单峰unimodal（单模态）或双峰bimodal（双模态）情绪识别的结果。4.多阶段融合：多模式SVR的输出可以使用相同的SVR进行组合，以提高情绪识别的识别率。我们通过将一个特征集输入到系统中来运行单峰unimodal（单模态）特征集的实验，以找到哪些特征集提供更好的性能。为此，我们使用具有先前解释的LSTM层的小型网络（在Keras[22]中实现）。从12个功能集中，我们通过最高平均CCC分数选择7个功能集。七个特征集的组合产生了21对双峰bimodal（双模态）特征集。请注意，这里双峰bimodal（双模态）的定义不是音频和视觉模态，而是一对两个特征集。从单峰unimodal（单模态）和双峰bimodal（双模态）结果中，我们选择11个最高CCC分数，并将这11个结果输入SVR，以通过后期融合执行多模式视听情绪识别。使用SVR的最后多模态融合可以被认为是1阶段特征融合。通过将SVR的结果输入到相同的SVR系统，可以执行两阶段多模态融合。我们将这种多级多模式融合限制为5次重复。图3显示了CCC评分从1到5个阶段的唤醒，效价和喜好的结果。该图显示，随着阶段数量的增加，CCC分数有所提高。与单峰unimodal（单模态），双峰bimodal（双模态）和多峰融合（1阶段）相比，多级融合获得了显着改善。所提出的多级融合可以将喜欢属性的CCC得分从0.083（基线单峰unimodal（单模态））提高到0.443，这是该任务中最具挑战性的属性。其他两个属性获得相对改善的基线结果分别为26.63%和16.11%的唤醒和效价。

### 引文

## 摘录

---
title: "Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion Recognition"
description: ""
citekey: atmajaMultitaskLearningMultistage2020
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:19
lastmod: 2023-04-11 12:02:59
---

> [!info] 论文信息
>1. Title：Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion Recognition
>2. Author：Bagus Tris Atmaja, Masato Akagi
>3. Entry：[Zotero link](zotero://select/items/@atmajaMultitaskLearningMultistage2020) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Atmaja_Akagi_2020_Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion.pdf>)
>4. Other：2020 - ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/bagustris/multistage-ser
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 本文目标是增强CCC，因此使用CCC损耗代替MSE损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。- 本文目标是增强 CCC，因此使用 CCC 损耗代替 MSE 损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。

## 摘要

> [!abstract] Due to its ability to accurately predict emotional state using multimodal features, audiovisual emotion recognition has recently gained more interest from researchers. This paper proposes two methods to predict emotional attributes from audio and visual data using a multitask learning and a fusion strategy. First, multitask learning is employed by adjusting three parameters for each attribute to improve the recognition rate. Second, a multistage fusion is proposed to combine results from various modalities' final prediction. Our approach used multitask learning, employed at unimodal and early fusion methods, shows improvement over single-task learning with an average CCC score of 0.431 compared to 0.297. A multistage method, employed at the late fusion approach, significantly improved the agreement score between true and predicted values on the development set of data (from [0.537, 0.565, 0.083] to [0.68, 0.656, 0.443]) for arousal, valence, and liking.

> 视听情绪识别由于能够利用多模态特征准确预测情绪状态，近年来受到研究者的广泛关注。本文提出了两种基于多任务学习和融合策略的基于视听数据的情感属性预测方法。首先，通过调整每个属性的三个参数来进行多任务学习，以提高识别率。其次，提出了一种多阶段融合的方法，将不同模态的最终预测结果进行融合。我们的方法使用了多任务学习，在单峰unimodal（单模态）和早期融合方法中使用，显示出比单任务学习更好的结果，平均Ccc得分为0.431分，而单任务学习方法为0.297分。在晚期融合方法中采用的多阶段方法显著提高了发展数据集上的真实值和预测值之间的一致性分数(从[0.537，0.565，0.083]提高到[0.68，0.656，0.443])，包括性唤醒、效价和喜好。

## 预处理

## 概述

## 结果

本研究使用了[3]中提供的 SEWA 数据集[12]。该数据集包含来自汉语、英语、德语、希腊语、匈牙利语和塞尔维亚语的视听记录，但在这项工作中仅使用德语(DE)和匈牙利语(HU)，因为它们没有提供其他语言的测试标签。提供三个属性来表示情绪状态，即：唤醒、价位和喜好。这些属性的分数是从几个母语人士的注释中获得的：六个德国人和五个匈牙利人。在96名受试者中，68名受试者(每名34名)用于培训，其余28名受试者(每名14名)用于验证/发展。除了数据集，文[3]的作者还提供了表1所示的基线特征。我们没有生成新的特征集，而是将多任务学习和多模式视听融合应用于这些特征集。对于音频和视觉特征，使用相同的处理块，即4.0s 的窗口长度和100ms 的跳跃大小，其中还为每0.1s 给出标签。然后，通过为低于该数字的其他序列填充零，将最长的1768序列(标签号)用于所有对象。对于双峰特征融合，音频和视觉特征在被送入分类器之前被连接起来。

## 精读

已经使用两种观点来实现自动情绪识别：分类视图和维度视图。虽然大多数研究人员试图将人类情绪分为不同的类别（例如幸福，愤怒等），但维度情绪识别是更具挑战性的任务，因为它试图将情绪标记为程度而不是类别。从维度的角度来看，情感被描述为2或3个属性[1]。

- Valence (pleasantness)和arousal (emotion intensity)是2D情绪模型中两个最常见的维度。
- 在3D模型中，使用dominance (degree of control)或liking。
- 其他属性，如expectancy，可以作为第四维（4D）添加[2]。

在本文中，我们评估了三个情绪维度/属性：arousal, valence, 和 liking，这些都是从[3]中的数据集中获得的。此任务是在特定指标上获得最准确的预测。作为回归任务（分类任务），最常见的指标是真实值和预测情绪程度（值）之间的误差。然而，最近的研究人员[3]引入了相关性(correlation)测量来确定真实值与预测情绪程度之间的一致性。

通常使用两种方法来最小化学习过程特征的损失并获得预测情绪维度的最佳模型，即单任务学习（STL）和多任务学习（MTL）。
- 单任务学习仅在多输出学习中最小化单个损失函数。例如，当学习预测维度情绪中的唤醒，效价和喜好时，只有唤醒被最小化。其他维度，效价和喜好，在学习过程中被忽略。通过最小化唤醒误差，学习过程的结果可用于预测一维（唤醒）或所有三个维度（唤醒，效价和喜好）。
- 单任务学习的问题在于，当它用于预测多个输出时，使用单个损失函数预测三个分数。一个维度的高分通常导致另一个维度的得分较低。为了解决这个问题，我们引入了在使用最小化所有情绪维度的真实情绪和预测情绪程度之间的误差情况下的多任务学习。多任务学习中的常用方法是对学习过程中的每个损失函数使用相同的加权因子。因此，损失总和是来自每个情绪维度的三个每个损失函数的总和。我们在本文中提出的方法旨在通过为每个情绪维度的每个损失函数分配不同的加权因子来获得平衡分数。

由于情绪来自多种形式，因此创建融合策略以适应这些模态也是一项挑战。标准方法是通过在相同或不同网络中组合不同模态之间的特征。这被称为早期融合策略。然后训练这两个或多个融合特征以将这些输入映射到标签上。另一个策略是使用后期融合。在这个策略中，每种模态都使用其标签在其相应网络中进行训练。然后将每种模态的识别结果分组以找到对应于标签的最高概率。早期融合和晚期融合的结果也可以通过将这些结果组合在支持向量回归（SVR）中来融合。最后一步的结果可以多阶段重复，以提高识别率。

我们对本文的贡献可以概括如下：
（1）使用多任务学习来最小化损失函数，使用三个参数来处理来自视听特征的三个情绪属性；
（2） 融合策略通过分析早期融合和晚期融合的单峰unimodal（单模态）和双峰bimodal（双模态）特征，结合早期-晚期融合使用多级SVR来提高视听情绪识别率。

Multitask learning

机器学习中的问题之一是获得适当的成本函数或损失函数来对数据进行建模。回归分析（分类任务）中的大多数问题的损失函数都使用真实值和预测值之间的误差计算。损失函数的选择通常由用于评估的指标决定。在维度情感识别的情况下，
- Ringeval 等人建议使用一致性相关系数 (CCC) 来对预测情绪属性的性能进行评分 [3]。
- Parthasarathy 和 Busso 使用多任务学习来最小化维度情感识别中的均方误差 (MSE) [4]。作者使用两个参数来衡量三个情绪属性的损失函数：唤醒、效价和支配。并确定了唤醒和效价的加权因子，以及优势的加权因子（唤醒和效价的加权因子中减去 1 来获得）。所有权重因子都在 0-1 的范围内，其中一个值为 0 的可能性为 33.3%。还发现唤醒和效价的最佳参数是 0.7 和 0.3。在这种情况下，学习过程中忽略了优势，可以将其视为类似于单任务学习的两任务学习。
- 使用共享层和独立层等两种多任务学习方法，与基线单任务学习相比，作者实现了 CCC 分数的提高 [4]。由于系统在较大的网络上比在较小的网络上学习得更好，因此使用的网络越大，获得的改进就越大。
- Chen 等人还使用以 MSE 作为损失函数的多任务学习 [5]。尽管从给定的基线上实现了 CCC 分数的提高，但没有指定与单任务学习的性能比较。这可能会导致难以确定改进是来自多任务学习还是其他使用的策略。

Multimodal Fusion

由于可以从许多模态中识别情感，例如语音、面部图像、运动和语言信息，因此在此类系统中通常考虑使用多模态技术来适应许多特征。 
- [ ] 中描述的数据集包括来自音频和视觉的多模态情感特征。
- 布索等人 [6]提供了来自语音和手势的情感数据集，包括面部表情和手部动作。
- 该数据集的改进版本[7]提供了一个带有视听信息的情感数据库，从而促进了（表演）录音中的自然性。

为了处理从多模态数据集中提取的各种特征，研究人员开发了几类特征融合方法[5,8,9,10,11]。大多数策略可以分为早期融合和晚期融合。
- 在早期融合方法中，也称为特征级融合，在进行分类之前将来自不同模态的特征组合起来。
- 在后期融合方法中，也称为决策层融合，最终的决策概率由每个单峰unimodal（单模态）模型结果通过SVR等方法给出。 Ringeval 等人。为来自 SEWA 数据集的后期融合策略提供基线融合方法 [3, 12]。可以使用静态回归器（即 SVR）将每个模态或特征集的结果组合起来，以根据几种模态的给定结果做出预测的情感属性分数的最终决定。

Multitask learning based on CCC loss

CCC是维度情绪识别中用来衡量真实情绪维度与预测情绪程度一致性的常用度量标准。《CCC》的公式

$$C C C=\frac{2 \rho_{x y} \sigma_{x} \sigma_{y}}{\sigma_{x}^{2}+\sigma_{y}^{2}+\left(\mu_{x}-\mu_{y}\right)^{2}}$$ 


式中，$\rho_{x y}$是$x$与$y$之间的皮尔逊系数相关系数，$\sigma$为标准差，$\mu$为平均值。这是基于林的计算[20]。CCC的范围从$-1$(完全不一致)到$1$(完全一致)。因此，使真实值和预测情感之间的一致性最大化的CCC损失函数(CCCL)可以定义为$$CCCL=1-CCC$$
在单任务学习中，损失函数是来自唤醒$\left(CL_{\text{aro}}\right)$、价值$\left(CL_{\text{val}}\right)$或喜欢$\left(CL_{l Ik}\right)$的损失函数之一。在多任务学习中，当CCC损失用作所有唤醒、价位和喜好的单一度量时，$C L_{\text{Total}}$是这三个CCC损失函数的组合：
$$。

C L_{t o t}=\alpha C L_{a r o}+\beta C L_{v a l}+\gamma C L_{l i k}，

$$
其中，$\alpha、\beta$和$\gamma$是每个情感维度损失函数的权重因子。在常见的方法中，$\alpha、\beta$和$\Gamma$被设置为1，而在[4]中，$\Gamma$被设置为$1-(\alpha+\beta)$以最小化MSE。在该方法中，所有权重系数都在0-1的范围内。

在本文中，我们使用了所有这三个参数，并且这些加权因子的和并不限于仅0-1。由于目标是加强CCC，因此使用CCC损耗代替MSE。

![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203900.png)

如图1所示，视听情感识别系统由3个LSTM层组成，分别为256、128和64个单元。在每个LSTM层之后添加系数为0.4的丢弃层。在一次实验中使用了RMSprop优化器，学习率为0.0005，批次大小为34，持续50个时期。为了补偿在进行注释时的延迟，标签在训练过程中被移到前面0.1，并且在写预测时被移回。

在图1中，该系统从双峰bimodal（双模态）音频和视觉特征集中产生对唤醒、价位和喜好程度的预测。这一结果可以与使用SVR(来自不同特征集)的单峰unimodal（单模态）或双峰bimodal（双模态）(早期)融合的其他结果相结合，并且从SVR得到的预测也可以输入到相同的SVR系统(使用SCRKIT-LINE工具实现[21])。

![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203950.png)

在图2中，这种早期融合和晚期融合的组合分三个阶段进行了说明。首先，使用支持向量机方法对单峰unimodal（单模态）结果(称为结果#1)和多峰(双峰bimodal（双模态）)结果(称为结果#2)或单峰unimodal（单模态）和单峰unimodal（单模态）进行训练。这个学习过程产生一个新的结果(即该图中的结果#3)。将后期融合的结果#3再次馈送到SVR方法结果#4。结果#4再次馈送到SVR方法结果#5。这种多阶段融合可以进行n次以获得CCC评分的改善。为了评估所提出的MTL方法与STL和以前的MTL方法的有效性，我们比较了这些方法之间的CCC分数。表2显示了不同属性的CCC分数及其平均值。我们提出的MTL2的性能优于STL和先前提出的MTL1。

为了找到α，β和γ的最佳参数，我们在0-1的范围内对这些参数进行了随机搜索。表2中使用的参数是最优的，即α，β和γ分别为0.7、0.2和1.0。我们提出的具有三个参数的MTL学习优于STL和以前的MTL[4]。对于STL方法，唤醒和配价在其属性优化时都获得了最高的CCC分数。虽然喜好在STL3中得到了优化，但它仍然是最难估计的。这一问题应在今后的研究中加以解决。


为了获得多级融合结果，执行以下步骤，1。单峰unimodal（单模态）情绪识别：执行此步骤以研究双峰bimodal（双模态）或多峰融合的重要性特征集。2.双峰bimodal（双模态）融合：通过连接来自不同或相同模态的两个特征集来执行该步骤。3.多模态融合：前两步使用DNN进行，第三步使用SVR进行，结合单峰unimodal（单模态）或双峰bimodal（双模态）情绪识别的结果。4.多阶段融合：多模式SVR的输出可以使用相同的SVR进行组合，以提高情绪识别的识别率。我们通过将一个特征集输入到系统中来运行单峰unimodal（单模态）特征集的实验，以找到哪些特征集提供更好的性能。为此，我们使用具有先前解释的LSTM层的小型网络（在Keras[22]中实现）。从12个功能集中，我们通过最高平均CCC分数选择7个功能集。七个特征集的组合产生了21对双峰bimodal（双模态）特征集。请注意，这里双峰bimodal（双模态）的定义不是音频和视觉模态，而是一对两个特征集。从单峰unimodal（单模态）和双峰bimodal（双模态）结果中，我们选择11个最高CCC分数，并将这11个结果输入SVR，以通过后期融合执行多模式视听情绪识别。使用SVR的最后多模态融合可以被认为是1阶段特征融合。通过将SVR的结果输入到相同的SVR系统，可以执行两阶段多模态融合。我们将这种多级多模式融合限制为5次重复。图3显示了CCC评分从1到5个阶段的唤醒，效价和喜好的结果。该图显示，随着阶段数量的增加，CCC分数有所提高。与单峰unimodal（单模态），双峰bimodal（双模态）和多峰融合（1阶段）相比，多级融合获得了显着改善。所提出的多级融合可以将喜欢属性的CCC得分从0.083（基线单峰unimodal（单模态））提高到0.443，这是该任务中最具挑战性的属性。其他两个属性获得相对改善的基线结果分别为26.63%和16.11%的唤醒和效价。

### 引文

---
title: "Key-Sparse Transformer for Multimodal Speech Emotion Recognition"
description: ""
citekey: chenKeySparseTransformerMultimodal2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:30
lastmod: 2023-04-11 13:00:05
---

> [!info] 论文信息
>1. Title：Key-Sparse Transformer for Multimodal Speech Emotion Recognition
>2. Author：Weidong Chen, Xiaofeng Xing, Xiangmin Xu, Jichen Yang, Jianxin Pang
>3. Entry：[Zotero link](zotero://select/items/@chenKeySparseTransformerMultimodal2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen et al_2022_Key-Sparse Transformer for Multimodal Speech Emotion Recognition.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] Speech emotion recognition is a challenging research topic that plays a critical role in human-computer interaction. Multimodal inputs further improve the performance as more emotional information is used. However, existing studies learn all the information in the sample while only a small portion of it is about emotion. The redundant information will become noises and limit the system performance. In this paper, a key-sparse Transformer is proposed for efficient emotion recognition by focusing more on emotion related information. The proposed method is evaluated on the IEMOCAP and LSSED. Experimental results show that the proposed method achieves better performance than the state-of-the-art approaches.

> 语音情感识别是一个极具挑战性的研究课题，在人机交互中起着至关重要的作用。随着更多的情感信息被使用，多通道输入进一步改善了性能。然而，现有的研究学习了样本中的所有信息，而只有一小部分是关于情绪的。冗余信息会成为噪声，限制系统的性能。本文通过更多地关注情感相关信息，提出了一种基于密钥稀疏变换的情感识别算法。在IEMOCAP和LSSED上对该方法进行了评估。实验结果表明，该方法取得了比现有方法更好的性能。

## 预处理

## 概述

## 结果

数据集：IEMOCAP 和 LSSED

Experimental setup：

预训练的 wav2vec 和 RoBERTa 取自 [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)

音频和文本特征序列的最大长度分别被设置为 460 和 20。

SGD 优化器学习速率设置为 5×10−4 和 1×10−4 对模型进行优化。

每 30 个 epoch，学习率下降到原来的 50%。利用 p=0.5 的 Dropout 来减小过拟合度。
批次大小为 32。

特征提取模块中的 Vanilla Transformers 个数为 5 个，深度融合模块中的 KS-Transformers 个数为 2 个。多头注意采用 8 个注意力头部。通道交互模块中使用的 CCAB 数量使用 0-4 个，其中 3 为最好，如图所示。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612184405.png)

## 精读

语音情感识别(SER)正迅速成为人机交互(HCI)的重要工具[1]。Ser还揭示了自闭症和老年人护理等统称为医疗保健的问题[2]。例如，患有严重言语和语言障碍的人难以表达自己的情绪。情感识别系统可以帮助患者进行治疗，提高他们的情感交流技能。语音是多模式的，因为它本质上包含文本信息。最新的研究[3，4]也证明了多模方法优于单模方法。因此，多式联运SER成为近年来的研究热点

在本文中，我们使用Transformer作为基本结构来实现情感识别。然而，很少有作品注意到，音频或文本中并不是所有的信息都与情感有关。例如，考虑一篇文章“好吧，看，这是一个美丽的一天。我们为什么要争论呢？“。在IEMOCAP[16]中，Vanilla Transformer中的注意力权重如图1所示。我们可以看到Transformer中的注意力权重被分配给所有单词。然而，“美丽”和“争论”这两个词在这句话中包含了大部分的情感信息。而与情感无关的单词，如it、a、Look等，对于SER任务来说是不必要的，成为噪声，导致系统性能的限制。为了解决这个问题，我们提出了一种新的方法，称为键稀疏转换器(KS-Transformer)，来判断样本中每个单词或语音帧的重要性，帮助模型更多地关注与情感相关的信息。在KS-Transformer的基础上，进一步设计了级联交叉注意块，实现了不同模式的高效融合。

·我们提出了KS-Transformer来判断每个帧或词的重要性，从而帮助模型更加关注情感信息。在KS-Transformer的基础上，我们进一步设计了级联交叉注意块来实现不同通道之间的交互。
·我们在IEMOCAP和LSSED上对该方法进行了评估，结果表明该方法比现有的最新方法取得了更好的结果。


建议的模型，如图所示，主要由三个模块组成。其中，特征提取模块用于学习输入特征，通道交互模块用于学习交互信息，深度融合模块旨在将音频和文本中的信息进一步结合。具体地说，第一个模块(灰色部分)基于Vanilla Transformer，后两个模块(黄色部分)基于KS-Transformer。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612184832.png)

Vanilla Transformer

vanilla transformer最初由编码器和解码器组成。在本文中，我们使用Transformer来表示编码器部分，因为它是实现我们所提出的体系结构所需的部分。Transformer的输入分为Q、K和V，分别由Query, Key 和 Value组成。具体表现形式如下：

$$
\boldsymbol{W}=\operatorname{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^{T}}{\sqrt{d_{Q}}}\right)
$$
$$
\boldsymbol{a t t n}=W \times V
$$

Key-Sparse attention mechanism

key-sparse Transformer的目标是自动发现情感信息。假设Q中的查询向量个数为i，而K中的键向量个数为j，则键稀疏注意机制如图所示。需要注意的是，Transformer中的K和V总是相同的。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185158.png)

KSTransformer中使用的key-sparse注意机制能够自动判断每个语音帧或单词的重要性。如图所示，权重矩阵W是通过将Q和K相乘得到的，W中的每一行都是V中值向量的权重。由于值向量表示音频中的帧或文本中的单词，所以我们将相同值向量的所有权重相加，并将求和用作语音帧或单词在样本中的重要性的判别器。我们选择前k个求和最大的k个值向量，并保持它们在权重矩阵中的关注度不变，而将其余的重置为零。这种操作使得权重矩阵从稠密到稀疏，减少了冗余度，这就是为什么我们将这里使用的Transformer称为KS-Transformer。通过如下公式计算top-k掩码。

$$
\boldsymbol{M}_{\mathbf{z}}= \begin{cases}0 & \text { if } s_{z}<\text { threshold } \\ 1 & \text { if } s_{z} \geq \text { threshold }\end{cases}
$$

Modality interaction module

由于通道交互模块是基于级联交叉注意块(CCAB)的，因此我们首先介绍了CCAB的结构。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185801.png)

如图左侧所示，CCAB是两个KS转换器的级联，其中，第一个KS转换器从通道A创建Q，从通道B创建K、V。使用这种特殊的输入方法，键稀疏注意机制将找出B中与A最相关的部分，并产生组合了A和B信息的输出。由于不同通道之间的情绪信息往往是互补的[3，17，18]，A和B都不能代表准确的情绪。因此，CCAB中的第二个KSTransformer将融合的特征作为输入，并在应用键稀疏注意时考虑来自通道A和通道B的信息。得益于CCAB，A和B被更全面和准确地融合。

如图右侧所示，通道交互模块由一堆CCAB组成，其中后者将前一个CCAB的输出作为Q输入，而K和V输入始终来自通道B。来自B的信息经过一个CCAB被视为一次交互，因为来自B的信息通过key-sparse注意流入A。多个CCAB应用于多次交互。为了保证特征的稳定性，采用了跳跃连接。

Deep fusion module

大多数研究采用融合特征来预测互动后的情绪[12，19]。然而，我们认为融合的特征可能并不是最好的，可以深度融合以进一步提高系统的性能。具体而言，深度融合模块由多个KS-Transformers组成，它们以融合后的特征为输入，利用键稀疏注意力来增强音频和文本之间的交互，实现深度融合。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612190651.png)

为了验证关键稀疏注意的有效性，我们以IEMOCAP中的一个样本为例，通过可视化的方式比较了Vanilla Transformer和KS-Transformer中的注意力权重。如图所示，Vanilla Transformer记录了所有单词，包括与情感无关的嘈杂单词，并有过度适应的趋势。然而，KS-Transformer使连接从密集变为稀疏，它能够忽略大多数噪声，更多地关注情感信息。同时，KS-Transformer的稀疏性可以降低模型的复杂性，减少过拟合度。

为了探索KS-Transformer中的最佳稀疏性，我们将k从0.1变化到0.9。我们设置的k越大，重置为零的关注度权重就越少，稀疏性也就越小。由于LSSED存在样本不平衡的问题，我们使用未加权准确度(UA)作为衡量标准。结果如图6所示。由于IEMOCAP是一个相对较小的语料库，当k大于0.5时，模型容易过度拟合，导致UA分数保持不变。然而，在大规模数据集LSSED上，由于冗余信息的存在，当k>0.5时，会出现显著的下降。相反，当k小于0.5时，模型使用的信息量太少，并且可能收敛到不满意的局部最小值。考虑到IEMOCAP和LSSED语料库上的UA性能曲线，k被设置为0.5，这意味着每个KS-Transformer中默认为50%的关注度权重被重置为零。

通道交互对于多通道系统是至关重要的。为了研究CCAB堆栈的有效性，我们将使用的CCAB数量从0更改为4，其中0表示移除了通道交互模块，结果如表1所示。加权准确度(WA)和UA作为标准。应当指出的是，所使用的CCAB数量代表进行互动的次数。从表1中我们可以看出，当只应用一种CCAB时，不同模式之间的相互作用是肤浅和不足的。性能随着CCAB数量的增加而提高。当数目为3时，性能最好，这证实了CCAB的有效性和多次交互的必要性。

### 引文

Yoon等人。[5]利用双递归神经网络对音频和文本信息进行融合。

以同样的方式，Krishna等人。[6]使用原始音频波形作为音频特征，将手套词嵌入作为文本特征进行多模式学习。
此外，

Peri et al.。[7]结合音视频信息，利用多任务设置进行情感识别。

预训练的自监督学习在自然语言处理[8，9]和语音识别[10]等领域都取得了巨大的成功。

同时，最近的工作[11，12]使用了SSL模型，在SER中取得了令人振奋的结果。

目前，Wav2vec[10]和Roberta[9]是文献中最常用的预训练的SSL模型。

受注意力机制的启发，Transformer[13]在长序列建模方面表现突出，并在自然语言处理方面取得了巨大的成功[11]。

Tarantino等人。[14]在Transformer中使用全局窗口系统来捕捉话语中的深层关系。

此外，Huang et al.。[15]使用Transformer融合不同的情感分析模式。

## 摘录

---
title: "Key-Sparse Transformer for Multimodal Speech Emotion Recognition"
description: ""
citekey: chenKeySparseTransformerMultimodal2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:30
lastmod: 2023-04-11 13:00:05
---

> [!info] 论文信息
>1. Title：Key-Sparse Transformer for Multimodal Speech Emotion Recognition
>2. Author：Weidong Chen, Xiaofeng Xing, Xiangmin Xu, Jichen Yang, Jianxin Pang
>3. Entry：[Zotero link](zotero://select/items/@chenKeySparseTransformerMultimodal2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Chen et al_2022_Key-Sparse Transformer for Multimodal Speech Emotion Recognition.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] Speech emotion recognition is a challenging research topic that plays a critical role in human-computer interaction. Multimodal inputs further improve the performance as more emotional information is used. However, existing studies learn all the information in the sample while only a small portion of it is about emotion. The redundant information will become noises and limit the system performance. In this paper, a key-sparse Transformer is proposed for efficient emotion recognition by focusing more on emotion related information. The proposed method is evaluated on the IEMOCAP and LSSED. Experimental results show that the proposed method achieves better performance than the state-of-the-art approaches.

> 语音情感识别是一个极具挑战性的研究课题，在人机交互中起着至关重要的作用。随着更多的情感信息被使用，多通道输入进一步改善了性能。然而，现有的研究学习了样本中的所有信息，而只有一小部分是关于情绪的。冗余信息会成为噪声，限制系统的性能。本文通过更多地关注情感相关信息，提出了一种基于密钥稀疏变换的情感识别算法。在IEMOCAP和LSSED上对该方法进行了评估。实验结果表明，该方法取得了比现有方法更好的性能。

## 预处理

## 概述

## 结果

数据集：IEMOCAP 和 LSSED

Experimental setup：

预训练的 wav2vec 和 RoBERTa 取自 [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)

音频和文本特征序列的最大长度分别被设置为 460 和 20。

SGD 优化器学习速率设置为 5×10−4 和 1×10−4 对模型进行优化。

每 30 个 epoch，学习率下降到原来的 50%。利用 p=0.5 的 Dropout 来减小过拟合度。
批次大小为 32。

特征提取模块中的 Vanilla Transformers 个数为 5 个，深度融合模块中的 KS-Transformers 个数为 2 个。多头注意采用 8 个注意力头部。通道交互模块中使用的 CCAB 数量使用 0-4 个，其中 3 为最好，如图所示。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612184405.png)

## 精读

语音情感识别(SER)正迅速成为人机交互(HCI)的重要工具[1]。Ser还揭示了自闭症和老年人护理等统称为医疗保健的问题[2]。例如，患有严重言语和语言障碍的人难以表达自己的情绪。情感识别系统可以帮助患者进行治疗，提高他们的情感交流技能。语音是多模式的，因为它本质上包含文本信息。最新的研究[3，4]也证明了多模方法优于单模方法。因此，多式联运SER成为近年来的研究热点

在本文中，我们使用Transformer作为基本结构来实现情感识别。然而，很少有作品注意到，音频或文本中并不是所有的信息都与情感有关。例如，考虑一篇文章“好吧，看，这是一个美丽的一天。我们为什么要争论呢？“。在IEMOCAP[16]中，Vanilla Transformer中的注意力权重如图1所示。我们可以看到Transformer中的注意力权重被分配给所有单词。然而，“美丽”和“争论”这两个词在这句话中包含了大部分的情感信息。而与情感无关的单词，如it、a、Look等，对于SER任务来说是不必要的，成为噪声，导致系统性能的限制。为了解决这个问题，我们提出了一种新的方法，称为键稀疏转换器(KS-Transformer)，来判断样本中每个单词或语音帧的重要性，帮助模型更多地关注与情感相关的信息。在KS-Transformer的基础上，进一步设计了级联交叉注意块，实现了不同模式的高效融合。

·我们提出了KS-Transformer来判断每个帧或词的重要性，从而帮助模型更加关注情感信息。在KS-Transformer的基础上，我们进一步设计了级联交叉注意块来实现不同通道之间的交互。
·我们在IEMOCAP和LSSED上对该方法进行了评估，结果表明该方法比现有的最新方法取得了更好的结果。


建议的模型，如图所示，主要由三个模块组成。其中，特征提取模块用于学习输入特征，通道交互模块用于学习交互信息，深度融合模块旨在将音频和文本中的信息进一步结合。具体地说，第一个模块(灰色部分)基于Vanilla Transformer，后两个模块(黄色部分)基于KS-Transformer。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612184832.png)

Vanilla Transformer

vanilla transformer最初由编码器和解码器组成。在本文中，我们使用Transformer来表示编码器部分，因为它是实现我们所提出的体系结构所需的部分。Transformer的输入分为Q、K和V，分别由Query, Key 和 Value组成。具体表现形式如下：

$$
\boldsymbol{W}=\operatorname{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^{T}}{\sqrt{d_{Q}}}\right)
$$
$$
\boldsymbol{a t t n}=W \times V
$$

Key-Sparse attention mechanism

key-sparse Transformer的目标是自动发现情感信息。假设Q中的查询向量个数为i，而K中的键向量个数为j，则键稀疏注意机制如图所示。需要注意的是，Transformer中的K和V总是相同的。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185158.png)

KSTransformer中使用的key-sparse注意机制能够自动判断每个语音帧或单词的重要性。如图所示，权重矩阵W是通过将Q和K相乘得到的，W中的每一行都是V中值向量的权重。由于值向量表示音频中的帧或文本中的单词，所以我们将相同值向量的所有权重相加，并将求和用作语音帧或单词在样本中的重要性的判别器。我们选择前k个求和最大的k个值向量，并保持它们在权重矩阵中的关注度不变，而将其余的重置为零。这种操作使得权重矩阵从稠密到稀疏，减少了冗余度，这就是为什么我们将这里使用的Transformer称为KS-Transformer。通过如下公式计算top-k掩码。

$$
\boldsymbol{M}_{\mathbf{z}}= \begin{cases}0 & \text { if } s_{z}<\text { threshold } \\ 1 & \text { if } s_{z} \geq \text { threshold }\end{cases}
$$

Modality interaction module

由于通道交互模块是基于级联交叉注意块(CCAB)的，因此我们首先介绍了CCAB的结构。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185801.png)

如图左侧所示，CCAB是两个KS转换器的级联，其中，第一个KS转换器从通道A创建Q，从通道B创建K、V。使用这种特殊的输入方法，键稀疏注意机制将找出B中与A最相关的部分，并产生组合了A和B信息的输出。由于不同通道之间的情绪信息往往是互补的[3，17，18]，A和B都不能代表准确的情绪。因此，CCAB中的第二个KSTransformer将融合的特征作为输入，并在应用键稀疏注意时考虑来自通道A和通道B的信息。得益于CCAB，A和B被更全面和准确地融合。

如图右侧所示，通道交互模块由一堆CCAB组成，其中后者将前一个CCAB的输出作为Q输入，而K和V输入始终来自通道B。来自B的信息经过一个CCAB被视为一次交互，因为来自B的信息通过key-sparse注意流入A。多个CCAB应用于多次交互。为了保证特征的稳定性，采用了跳跃连接。

Deep fusion module

大多数研究采用融合特征来预测互动后的情绪[12，19]。然而，我们认为融合的特征可能并不是最好的，可以深度融合以进一步提高系统的性能。具体而言，深度融合模块由多个KS-Transformers组成，它们以融合后的特征为输入，利用键稀疏注意力来增强音频和文本之间的交互，实现深度融合。

![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612190651.png)

为了验证关键稀疏注意的有效性，我们以IEMOCAP中的一个样本为例，通过可视化的方式比较了Vanilla Transformer和KS-Transformer中的注意力权重。如图所示，Vanilla Transformer记录了所有单词，包括与情感无关的嘈杂单词，并有过度适应的趋势。然而，KS-Transformer使连接从密集变为稀疏，它能够忽略大多数噪声，更多地关注情感信息。同时，KS-Transformer的稀疏性可以降低模型的复杂性，减少过拟合度。

为了探索KS-Transformer中的最佳稀疏性，我们将k从0.1变化到0.9。我们设置的k越大，重置为零的关注度权重就越少，稀疏性也就越小。由于LSSED存在样本不平衡的问题，我们使用未加权准确度(UA)作为衡量标准。结果如图6所示。由于IEMOCAP是一个相对较小的语料库，当k大于0.5时，模型容易过度拟合，导致UA分数保持不变。然而，在大规模数据集LSSED上，由于冗余信息的存在，当k>0.5时，会出现显著的下降。相反，当k小于0.5时，模型使用的信息量太少，并且可能收敛到不满意的局部最小值。考虑到IEMOCAP和LSSED语料库上的UA性能曲线，k被设置为0.5，这意味着每个KS-Transformer中默认为50%的关注度权重被重置为零。

通道交互对于多通道系统是至关重要的。为了研究CCAB堆栈的有效性，我们将使用的CCAB数量从0更改为4，其中0表示移除了通道交互模块，结果如表1所示。加权准确度(WA)和UA作为标准。应当指出的是，所使用的CCAB数量代表进行互动的次数。从表1中我们可以看出，当只应用一种CCAB时，不同模式之间的相互作用是肤浅和不足的。性能随着CCAB数量的增加而提高。当数目为3时，性能最好，这证实了CCAB的有效性和多次交互的必要性。

### 引文

Yoon等人。[5]利用双递归神经网络对音频和文本信息进行融合。

以同样的方式，Krishna等人。[6]使用原始音频波形作为音频特征，将手套词嵌入作为文本特征进行多模式学习。
此外，

Peri et al.。[7]结合音视频信息，利用多任务设置进行情感识别。

预训练的自监督学习在自然语言处理[8，9]和语音识别[10]等领域都取得了巨大的成功。

同时，最近的工作[11，12]使用了SSL模型，在SER中取得了令人振奋的结果。

目前，Wav2vec[10]和Roberta[9]是文献中最常用的预训练的SSL模型。

受注意力机制的启发，Transformer[13]在长序列建模方面表现突出，并在自然语言处理方面取得了巨大的成功[11]。

Tarantino等人。[14]在Transformer中使用全局窗口系统来捕捉话语中的深层关系。

此外，Huang et al.。[15]使用Transformer融合不同的情感分析模式。

---
title: "Multimodal Transformer with Learnable Frontend and Self Attention for Emotion Recognition"
description: ""
citekey: duttaMultimodalTransformerLearnable2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:41
lastmod: 2023-04-11 13:01:26
---

> [!info] 论文信息
>1. Title：Multimodal Transformer with Learnable Frontend and Self Attention for Emotion Recognition
>2. Author：Soumya Dutta, Sriram Ganapathy
>3. Entry：[Zotero link](zotero://select/items/@duttaMultimodalTransformerLearnable2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dutta_Ganapathy_2022_Multimodal Transformer with Learnable Frontend and Self Attention for Emotion.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/iiscleap/multimodal_emotion_recognition
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 引入了一个可学习的前端语音特征提取器

## 摘要

> [!abstract] In this work, we propose a novel approach for multi-modal emotion recognition from conversations using speech and text. The audio representations are learned jointly with a learnable audio front-end (LEAF) model feeding to a CNN based classifier. The text representations are derived from pre-trained bidirectional encoder representations from transformer (BERT) along with a gated recurrent network (GRU). Both the textual and audio representations are separately processed using a bidirectional GRU network with self-attention. Further, the multi-modal information extraction is achieved using a transformer that is input with the textual and audio embeddings at the utterance level. The experiments are performed on the IEMOCAP database, where we show that the proposed framework improves over the current state-of-the-art results under all the common test settings. This is primarily due to the improved emotion recognition performance achieved in the audio domain. Further, we also show that the model is more robust to textual errors caused by an automatic speech recognition (ASR) system.

> 在这项工作中，我们提出了一种新的基于语音和文本的多模态情感识别方法。音频表示与馈送到基于 CNN 的分类器的可学习音频前端(叶)模型联合学习。文本表示是从来自变压器的预先训练的双向编码器表示(BERT)和门控递归网络(GRU)导出的。使用具有自我注意的双向 GRU 网络分别处理文本和音频表示。此外，多模态信息提取是使用在发声级别与文本和音频嵌入一起输入的转换器来实现的。实验在 IEMOCAP 数据库上进行，实验表明，在所有常见测试设置下，所提出的框架比目前最先进的结果有所改善。这主要是由于在音频域中实现了改进的情感识别性能。此外，我们还表明，该模型对自动语音识别(ASR)系统引起的文本错误具有更强的鲁棒性。

## 预处理

## 概述

## 结果

音频分类器的 Leaf-CNN 模型在其基本实现中是内存密集型的。为了避免这个问题，叶子特征的步长增加到30ms，而不是缺省值10ms。Leaf-cnn 特征抽取器的训练批大小为16，学习率为1×10−5。检查来自 cnn 分类器的音频表示的维度是否有3个不同的值50、100和200值(表1)。基于对验证数据的性能，来自 Leaf-CNN 分类器的音频表示维度被固定为100。同样，文本特征抽取器的 BERT-GRU 模型被训练成32批大小和相同的学习率。对于这个发音级别的嵌入抽取器，发现维度100在验证集上给出了最好的准确率。对于文本和音频，具有自我注意的 BiGRU 的训练批次为32，学习率为1×10−3。BiGRU 层中有自我注意和无自我注意的通道的准确率如表1所示。这对于文本和音频形态来说都是一致的。模型的最后一个部分，即多模式转换器，训练的批次为32，学习率为1×10−4。在不同的隐含层数和隐含层维度的组合下进行了多次实验，如表2所示。结果发现，结果随注意头数的变化可以忽略不计，因此将其固定为12。最终的多模式转换器配置被选择为隐含层维度为120，有12个注意头和3个隐含层。

## 精读

随着对会话代理和个人助理的需求不断增长，人类情感的自动识别已经成为增强用户体验的关键任务。利用文本、语音和视频的多模态数据进行人类情感识别，对智能手机、可穿戴设备、智能扬声器、自动驾驶监控、情绪分析和心理健康等各种应用都有重要影响。开发情商的这一领域将使机器在交互中更像人类[1]。情感识别的问题具有挑战性，主要是因为情感的复杂表达是高度个人化的。人类互动中的情绪可以通过面部表情[2]、言语[3]、手势[4]和呼吸等生理信号来检测。此外，不同的模态包含与情感相关的不同程度的信息，因此，设计一种联合的多模态情感识别方法被认为是为了提高这些系统的性能[6]。虽然需要以一种多通道的方式感知情绪的能力，但也需要以一种健壮的方式通过每一种通道感知情绪。在本文中，我们探索了一种语音和文本的情感识别任务。

对于多模态情感识别，Sikka等人探索了Logistic回归和支持向量机分类器。[7]和Castellano et.。Al[8].。

深度学习方面的进展也有利于从语音[9]、视听情感识别[10]以及从联合文本、语音和视觉形式[11]进行情感识别。

Yoon等人的作品。[12]和Lee et.。艾尔。[13]专注于基于语音和文本的情感识别，

Majumder等人。[14]探索使用文本、语音和视频功能的三模态融合进行情感识别。Log-Mel滤波器组特征与其他声学指标一起在与语音中的情感识别相关的工作中被广泛使用[15，16，17]。这些特征是知识驱动的，这意味着数据集中的任何可变性都不会对从音频文件中提取的特征起到明确的作用。这促使研究人员开发可学习的音频特征提取程序[18，19，20]。

在这项工作中，我们提出了一种对话中的情感识别方法，其中我们首先分别使用Leaf[19]和Bert[21]从音频和文本中提取可学习的特征。我们提出了一种跨话语的信息融合方法，该方法针对两种模态中的每一种都使用自我注意网络。这两个模态被组合在多模态转换器中，以便使用长期多模态上下文信息进行更好的分类。我们的模型在广泛使用的IEMOCAP数据集[22]上进行了评估。

对于音频情感识别，已经研究了韵律特征和其他声学特征[23，24]。这里，MEL频率倒谱系数(MFCC)与基于基音的特征一起使用。OpenSMALL[25]是一个被广泛使用的音频特征提取工具包，已经被用于像[15，16]这样的几个作品中。

Wu et.。Al[17]除了音调特征外，还使用了帧长度为250ms的长期对数-梅尔滤波器组特征。

在过去的几年里，随着双向编码表示从转换器(BERT)[21]的迁移学习能力的增强，从文本中提取特征已经有了长足的进步。这在[17]中已经被用于文本特征提取。在另一项工作中，对单词2vec表示[26]训练的卷积神经网络已用于基于文本的情感识别[15，16]。单词表示的全局向量(手套)[27]也被Tripari et应用于情感识别。

艾尔。[28]和Pepino et.。Al[29]。文献[30]利用双向LSTM(bi-LSTM)网络实现了多模态信息的融合。这一点得到了Poria et的进一步改进。

艾尔。[15]其中以分层方式执行融合。

在[12，13]中已经探索了使用注意机制来保持长期背景。自我注意网络的体系结构也被应用于多模态情感识别。模型体系结构的主要组件的示意图如图1所示。

![]({39}_Multimodal%20Transformer%20with%20Learnable%20Frontend%20and%20Self%20Attention%20for%20Emotion%20Recognition@duttaMultimodalTransformerLearnable2022.assets/image-20220613162838.png)

音频特征提取

使用可学习的音频分类前端(LEAF)[19]模型来提取音频特征，该模型使用filtering,、pooling、compression和normalization等组件来学习特征。此外，考虑到模型的小参数优点，可学习的前端可以集成到更大的情感识别网络中，并且可以联合学习。LEAF模型对原始音频文件采用一维卷积网络，生成区分语谱图的音频表示。LEAF输出在CNN网络中使用，平均池之后是完全连接的输出层。Leaf-CNN网络与发声级别标签（utterance level labels）联合训练，并且来自后续全连接层的100维特征被用作每个音频发声的嵌入。为了后续模型的学习，冻结了这个叶子-CNN网络，并将来自这一层的嵌入特征用作发声级别的音频特征。

我们使用基于BERT的特征来表示文本[21]。在每个发声文本通过BERT模型之后，来自编码器的最后四个隐藏层被拼接在一起。为了向表征添加更多上下文，BERT输出通过具有隐藏维度100的两层双向门控循环单元(GRU)模型[31]。BERT-GRU模型与发音级别标签联合训练，并将GRU模型输出端的表征作为后续带有自我注意块的BiGRU的发音级别文本嵌入。与音频的Leaf-CNN模型一样，BERT-GRU网络在随后的学习阶段被冻结，嵌入内容被用作发声级别的文本特征。

3.3. 多话语自我关注

对话中的每个话语都有一个情感标签。目的是利用前一时间实例u(t−1)、u(t−2)、...的嵌入表征有条件地预测话语u（t）的嵌入表征输出... 以及未来的时间实例，u（t+1），u（t+2）...的嵌入表征输出。Poria等[15]启发了这项工作中提出的背景信息的增加。有注意力的Bi-GRU网络输入用于音频模态的LEAF-CNN话语嵌入和用于文本的BERT-GRU话语嵌入。注意网络的目标是从先前和未来的话语中选择性地结合上下文，以便能够预测当前话语中的情绪标签。

设从BiGRU到自我关注层的输入的维度为$(B\times S\times H)$。这里，$B$表示批大小，$S$表示对话中的发声数量，$H=$$\left[H_{f}；H_{b}\right]$是发声级别的前向$\left(H_{f}\right)$和后向$\left(H_{b}\right)$输出维度的串联。为简单起见，在这些计算中假定批大小为1。当$B>1$时，矩阵乘积将被张量积代替。

设$O_{f}$和$O_{b}$表示输入到自我注意网络的BiGRU的输出。设$W_{f}^{a}$和$W_{b}^{a}$这两个权重矩阵表示关注层参数。向前方向的注意力计算为，

$$
\begin{aligned}

A_{f} &=\left(O_{f} W_{f}^{a}\right)\left(O_{f} W_{f}^{a}\right)^{T} \\

A_{f}^{i j} &=\frac{\exp \left(A_{f}^{i j}\right)}{\sum_{j=1}^{S} \exp \left(A_{f}^{i j}\right)} \forall i, j \in\{1,2, \ldots, S\} \\

O_{f}^{a} &=A_{f} O_{f}

\end{aligned}
$$

向后方向的注意力是相同的。具有自我注意块的Bi GRU被训练来共同预测视频中所有话语的标签。该输出$\left[O_{f}^{a} ; O_{b}^{a}\right]$用作后续多模变压器的每个话语的嵌入。与特征提取器一样，具有自我注意网络的Bi-GRU在训练后续多模态变压器时被冻结。

具有注意力的BiGRU为语音和文本生成发声级别的表征。这些嵌入是拼接的。采用transformer encoder[32]来融合音频和文本信息以改进情感识别。假设具有自我关注层的转换器能够处理长期依赖关系，并且能够比LSTM或GRU模型更有效地组合模态。

使用语音和文本进行情感识别的深度学习模型使用数据集中提供的音频文件及其文本转录进行训练。在实践中，我们很少遇到带有转录的音频。当用提供的抄本训练模型并在ASR输出上测试时，该测试设置评估模型对文本通道中的噪声的稳健性。为了获得ASR文本，我们使用了Google Speech to Text2 on IEMOCAP音频文件。表4显示了此测试设置下的结果。结果表明，即使有ASR输出，在所有三个测试设置下，模型的性能也不会急剧下降。在CV-5测试策略下，我们的模型的性能比[17]中报告的结果有了相当大的改善(相对改善了31%)。这项工作的主要改进主要是在音频域。如表4所示，该模型对文本通道中的噪声的稳健性进一步证实了仅使用语音输入时性能的改善。精度的提高可以归因于所提出的系统中的许多因素。Leaf的可学习前端功能使模型能够捕获音频的本地时频模式。将GRU与BiGRU层的注意力相结合，有助于模型描述音频信号中的长期信息。此外，多模式转换器有助于文本和音频模式的融合，并允许在音频域中观察到的改进以增强多模式情感识别性能。

### 引文

## 摘录

---
title: "Multimodal Transformer with Learnable Frontend and Self Attention for Emotion Recognition"
description: ""
citekey: duttaMultimodalTransformerLearnable2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:41
lastmod: 2023-04-11 13:01:26
---

> [!info] 论文信息
>1. Title：Multimodal Transformer with Learnable Frontend and Self Attention for Emotion Recognition
>2. Author：Soumya Dutta, Sriram Ganapathy
>3. Entry：[Zotero link](zotero://select/items/@duttaMultimodalTransformerLearnable2022) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Dutta_Ganapathy_2022_Multimodal Transformer with Learnable Frontend and Self Attention for Emotion.pdf>)
>4. Other：2022 - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：https://github.com/iiscleap/multimodal_emotion_recognition
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 引入了一个可学习的前端语音特征提取器

## 摘要

> [!abstract] In this work, we propose a novel approach for multi-modal emotion recognition from conversations using speech and text. The audio representations are learned jointly with a learnable audio front-end (LEAF) model feeding to a CNN based classifier. The text representations are derived from pre-trained bidirectional encoder representations from transformer (BERT) along with a gated recurrent network (GRU). Both the textual and audio representations are separately processed using a bidirectional GRU network with self-attention. Further, the multi-modal information extraction is achieved using a transformer that is input with the textual and audio embeddings at the utterance level. The experiments are performed on the IEMOCAP database, where we show that the proposed framework improves over the current state-of-the-art results under all the common test settings. This is primarily due to the improved emotion recognition performance achieved in the audio domain. Further, we also show that the model is more robust to textual errors caused by an automatic speech recognition (ASR) system.

> 在这项工作中，我们提出了一种新的基于语音和文本的多模态情感识别方法。音频表示与馈送到基于 CNN 的分类器的可学习音频前端(叶)模型联合学习。文本表示是从来自变压器的预先训练的双向编码器表示(BERT)和门控递归网络(GRU)导出的。使用具有自我注意的双向 GRU 网络分别处理文本和音频表示。此外，多模态信息提取是使用在发声级别与文本和音频嵌入一起输入的转换器来实现的。实验在 IEMOCAP 数据库上进行，实验表明，在所有常见测试设置下，所提出的框架比目前最先进的结果有所改善。这主要是由于在音频域中实现了改进的情感识别性能。此外，我们还表明，该模型对自动语音识别(ASR)系统引起的文本错误具有更强的鲁棒性。

## 预处理

## 概述

## 结果

音频分类器的 Leaf-CNN 模型在其基本实现中是内存密集型的。为了避免这个问题，叶子特征的步长增加到30ms，而不是缺省值10ms。Leaf-cnn 特征抽取器的训练批大小为16，学习率为1×10−5。检查来自 cnn 分类器的音频表示的维度是否有3个不同的值50、100和200值(表1)。基于对验证数据的性能，来自 Leaf-CNN 分类器的音频表示维度被固定为100。同样，文本特征抽取器的 BERT-GRU 模型被训练成32批大小和相同的学习率。对于这个发音级别的嵌入抽取器，发现维度100在验证集上给出了最好的准确率。对于文本和音频，具有自我注意的 BiGRU 的训练批次为32，学习率为1×10−3。BiGRU 层中有自我注意和无自我注意的通道的准确率如表1所示。这对于文本和音频形态来说都是一致的。模型的最后一个部分，即多模式转换器，训练的批次为32，学习率为1×10−4。在不同的隐含层数和隐含层维度的组合下进行了多次实验，如表2所示。结果发现，结果随注意头数的变化可以忽略不计，因此将其固定为12。最终的多模式转换器配置被选择为隐含层维度为120，有12个注意头和3个隐含层。

## 精读

随着对会话代理和个人助理的需求不断增长，人类情感的自动识别已经成为增强用户体验的关键任务。利用文本、语音和视频的多模态数据进行人类情感识别，对智能手机、可穿戴设备、智能扬声器、自动驾驶监控、情绪分析和心理健康等各种应用都有重要影响。开发情商的这一领域将使机器在交互中更像人类[1]。情感识别的问题具有挑战性，主要是因为情感的复杂表达是高度个人化的。人类互动中的情绪可以通过面部表情[2]、言语[3]、手势[4]和呼吸等生理信号来检测。此外，不同的模态包含与情感相关的不同程度的信息，因此，设计一种联合的多模态情感识别方法被认为是为了提高这些系统的性能[6]。虽然需要以一种多通道的方式感知情绪的能力，但也需要以一种健壮的方式通过每一种通道感知情绪。在本文中，我们探索了一种语音和文本的情感识别任务。

对于多模态情感识别，Sikka等人探索了Logistic回归和支持向量机分类器。[7]和Castellano et.。Al[8].。

深度学习方面的进展也有利于从语音[9]、视听情感识别[10]以及从联合文本、语音和视觉形式[11]进行情感识别。

Yoon等人的作品。[12]和Lee et.。艾尔。[13]专注于基于语音和文本的情感识别，

Majumder等人。[14]探索使用文本、语音和视频功能的三模态融合进行情感识别。Log-Mel滤波器组特征与其他声学指标一起在与语音中的情感识别相关的工作中被广泛使用[15，16，17]。这些特征是知识驱动的，这意味着数据集中的任何可变性都不会对从音频文件中提取的特征起到明确的作用。这促使研究人员开发可学习的音频特征提取程序[18，19，20]。

在这项工作中，我们提出了一种对话中的情感识别方法，其中我们首先分别使用Leaf[19]和Bert[21]从音频和文本中提取可学习的特征。我们提出了一种跨话语的信息融合方法，该方法针对两种模态中的每一种都使用自我注意网络。这两个模态被组合在多模态转换器中，以便使用长期多模态上下文信息进行更好的分类。我们的模型在广泛使用的IEMOCAP数据集[22]上进行了评估。

对于音频情感识别，已经研究了韵律特征和其他声学特征[23，24]。这里，MEL频率倒谱系数(MFCC)与基于基音的特征一起使用。OpenSMALL[25]是一个被广泛使用的音频特征提取工具包，已经被用于像[15，16]这样的几个作品中。

Wu et.。Al[17]除了音调特征外，还使用了帧长度为250ms的长期对数-梅尔滤波器组特征。

在过去的几年里，随着双向编码表示从转换器(BERT)[21]的迁移学习能力的增强，从文本中提取特征已经有了长足的进步。这在[17]中已经被用于文本特征提取。在另一项工作中，对单词2vec表示[26]训练的卷积神经网络已用于基于文本的情感识别[15，16]。单词表示的全局向量(手套)[27]也被Tripari et应用于情感识别。

艾尔。[28]和Pepino et.。Al[29]。文献[30]利用双向LSTM(bi-LSTM)网络实现了多模态信息的融合。这一点得到了Poria et的进一步改进。

艾尔。[15]其中以分层方式执行融合。

在[12，13]中已经探索了使用注意机制来保持长期背景。自我注意网络的体系结构也被应用于多模态情感识别。模型体系结构的主要组件的示意图如图1所示。

![]({39}_Multimodal%20Transformer%20with%20Learnable%20Frontend%20and%20Self%20Attention%20for%20Emotion%20Recognition@duttaMultimodalTransformerLearnable2022.assets/image-20220613162838.png)

音频特征提取

使用可学习的音频分类前端(LEAF)[19]模型来提取音频特征，该模型使用filtering,、pooling、compression和normalization等组件来学习特征。此外，考虑到模型的小参数优点，可学习的前端可以集成到更大的情感识别网络中，并且可以联合学习。LEAF模型对原始音频文件采用一维卷积网络，生成区分语谱图的音频表示。LEAF输出在CNN网络中使用，平均池之后是完全连接的输出层。Leaf-CNN网络与发声级别标签（utterance level labels）联合训练，并且来自后续全连接层的100维特征被用作每个音频发声的嵌入。为了后续模型的学习，冻结了这个叶子-CNN网络，并将来自这一层的嵌入特征用作发声级别的音频特征。

我们使用基于BERT的特征来表示文本[21]。在每个发声文本通过BERT模型之后，来自编码器的最后四个隐藏层被拼接在一起。为了向表征添加更多上下文，BERT输出通过具有隐藏维度100的两层双向门控循环单元(GRU)模型[31]。BERT-GRU模型与发音级别标签联合训练，并将GRU模型输出端的表征作为后续带有自我注意块的BiGRU的发音级别文本嵌入。与音频的Leaf-CNN模型一样，BERT-GRU网络在随后的学习阶段被冻结，嵌入内容被用作发声级别的文本特征。

3.3. 多话语自我关注

对话中的每个话语都有一个情感标签。目的是利用前一时间实例u(t−1)、u(t−2)、...的嵌入表征有条件地预测话语u（t）的嵌入表征输出... 以及未来的时间实例，u（t+1），u（t+2）...的嵌入表征输出。Poria等[15]启发了这项工作中提出的背景信息的增加。有注意力的Bi-GRU网络输入用于音频模态的LEAF-CNN话语嵌入和用于文本的BERT-GRU话语嵌入。注意网络的目标是从先前和未来的话语中选择性地结合上下文，以便能够预测当前话语中的情绪标签。

设从BiGRU到自我关注层的输入的维度为$(B\times S\times H)$。这里，$B$表示批大小，$S$表示对话中的发声数量，$H=$$\left[H_{f}；H_{b}\right]$是发声级别的前向$\left(H_{f}\right)$和后向$\left(H_{b}\right)$输出维度的串联。为简单起见，在这些计算中假定批大小为1。当$B>1$时，矩阵乘积将被张量积代替。

设$O_{f}$和$O_{b}$表示输入到自我注意网络的BiGRU的输出。设$W_{f}^{a}$和$W_{b}^{a}$这两个权重矩阵表示关注层参数。向前方向的注意力计算为，

$$
\begin{aligned}

A_{f} &=\left(O_{f} W_{f}^{a}\right)\left(O_{f} W_{f}^{a}\right)^{T} \\

A_{f}^{i j} &=\frac{\exp \left(A_{f}^{i j}\right)}{\sum_{j=1}^{S} \exp \left(A_{f}^{i j}\right)} \forall i, j \in\{1,2, \ldots, S\} \\

O_{f}^{a} &=A_{f} O_{f}

\end{aligned}
$$

向后方向的注意力是相同的。具有自我注意块的Bi GRU被训练来共同预测视频中所有话语的标签。该输出$\left[O_{f}^{a} ; O_{b}^{a}\right]$用作后续多模变压器的每个话语的嵌入。与特征提取器一样，具有自我注意网络的Bi-GRU在训练后续多模态变压器时被冻结。

具有注意力的BiGRU为语音和文本生成发声级别的表征。这些嵌入是拼接的。采用transformer encoder[32]来融合音频和文本信息以改进情感识别。假设具有自我关注层的转换器能够处理长期依赖关系，并且能够比LSTM或GRU模型更有效地组合模态。

使用语音和文本进行情感识别的深度学习模型使用数据集中提供的音频文件及其文本转录进行训练。在实践中，我们很少遇到带有转录的音频。当用提供的抄本训练模型并在ASR输出上测试时，该测试设置评估模型对文本通道中的噪声的稳健性。为了获得ASR文本，我们使用了Google Speech to Text2 on IEMOCAP音频文件。表4显示了此测试设置下的结果。结果表明，即使有ASR输出，在所有三个测试设置下，模型的性能也不会急剧下降。在CV-5测试策略下，我们的模型的性能比[17]中报告的结果有了相当大的改善(相对改善了31%)。这项工作的主要改进主要是在音频域。如表4所示，该模型对文本通道中的噪声的稳健性进一步证实了仅使用语音输入时性能的改善。精度的提高可以归因于所提出的系统中的许多因素。Leaf的可学习前端功能使模型能够捕获音频的本地时频模式。将GRU与BiGRU层的注意力相结合，有助于模型描述音频信号中的长期信息。此外，多模式转换器有助于文本和音频模式的融合，并允许在音频域中观察到的改进以增强多模式情感识别性能。

---
title: "Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"
description: ""
citekey: dingScalingYourKernels2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:52
lastmod: 2023-04-11 17:41:33
---

> [!info] 论文信息
>1. Title：Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs
>2. Author：Xiaohan Ding, Xiangyu Zhang, Jungong Han, Guiguang Ding
>3. Entry：[Zotero link](zotero://select/items/@dingScalingYourKernels2022) [URL link](https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ding et al_2022_Scaling Up Your Kernels to 31x31.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\3FNAJGAC\\Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html>)
>4. Other：2022 -      -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] 

> 我们重新审视现代卷积神经网络 (CNN) 中的大卷积核设计。受 vision transformers (ViT) 最新进展的启发，在本文中，我们证明了使用一些大型卷积核而不是一堆小卷积核可能是一种更强大的方法。我们提出了五个指导方针，例如，应用重参数化( re-parameterized)的大深度卷积来设计高效的高性能大卷积核 CNN。根据指南，我们提出了 RepLKNet 网络，这是一种纯 CNN 架构，其内核大小高达 31×31，而不是常用的 3×3。 RepLKNet 极大地缩小了 CNN 和 ViT 之间的性能差距，例如，在 ImageNet 和一些典型的下游任务上实现了与 Swin Transformer 相当或更好的结果，并且延迟更低。 RepLKNet 还表现出对大数据和大型模型的良好可扩展性，在多个测试任务中都得到了效果提升.

## 预处理

## 概述

## 结果

## 精读

卷积神经网络 (CNN) [41, 54] 曾经是现代计算机视觉系统中视觉编码器的常见选择。然而，最近，CNNs [41, 54] 受到了视觉转换器 (ViTs) [34, 60, 85, 94] 的极大挑战，它们在许多视觉任务上都表现出了领先的表现——不仅是图像分类 [34, 105] 和表示学习 [4, 10, 16, 101]，还有许多下游任务，例如对象检测 [24, 60]，语义分割 [94, 99] 和图像恢复 [11, 55]。

为什么 ViT 超级强大？

1. 一些工作认为，ViTs 中的多头自我注意 (MHSA) 机制起着关键作用。他们提供了实证结果来证明，MHSA 更灵活 [51]，能够（更少的归纳偏差）[20]，对失真更稳健 [67, 99]，或者能够对长期依赖关系进行建模 [70, 89]。
2. 但是一些工作挑战了 MHSA [116] 的必要性，将 ViT 的高性能归因于适当的构建块 [33] 和/或动态稀疏权重 [39,111]。更多作品 [20,39,43,96,116] 从不同的角度解释了 ViT 的优越性。
3. 在这项工作中，我们专注于一个观点：建立大感受野的方式。在 ViT 中，MHSA 通常设计为全局 [34、76、94] 或局部但具有大内核 [60、71、88]，因此单个 MHSA 层的每个输出都能够从大区域收集信息。然而，大内核在 CNN 中并不普遍使用（第一层 [41] 除外）。相反，一种典型的方式是使用一堆小的空间卷积 1 [41, 45, 48, 69, 75, 80, 109]（例如，3×3）来扩大最先进的感受野 CNN。只有一些老式的网络，如 AlexNet [54]、Inceptions [77-79] 以及从神经架构搜索 [38,44,57,117] 衍生的少数架构采用大空间卷积（其大小大于 5）作为主要部分。

上面的观点自然会引出一个问题：如果我们对传统的 CNN 使用一些大的而不是许多小的内核会怎样？大内核还是构建大感受野的方式是缩小 CNN 和 ViT 之间性能差距的关键？为了回答这个问题，我们系统地探索了 CNN 的大内核设计。我们遵循一个非常简单的“哲学”：只需将大的深度卷积引入常规网络，其大小范围从 3×3 到 31×31，尽管还有其他替代方案可以通过单层或几层引入大的感受野，例如特征金字塔 [92]、扩张卷积 [14、102、103] 和可变形卷积 [23]。通过一系列实验，我们总结了有效使用大卷积的五个经验指南：

1. 虽然大核卷积计算成本更高,但是通过改进可以使得大核深度卷积计算更高效；

通常，大核卷积的计算量很大，因为网络结构的参数量和 FLOPs(每秒浮点运算次数)与卷积核尺寸的平方成正比。但是这一缺点可以通过应用深度卷积(DW: Depth-wise)方法来克服[17，44]。


例如在本文提出的 RepLKNet 网络架构中，虽然在不同阶段将卷积核尺寸从原始的[3，3，3，3]增加到了[31，29，27，13]，但是 Flops 和参数量仅增加了 18.6%的 10.4%，这在可接受范围内。实际上, 剩下的 1×1 卷积运算控制了大部分的计算复杂性。


![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709112959.png)

有人可能会担心，DW 卷积在现代并行计算设备(如 GPU)上的计算效率可能非常低。对于传统的卷积核大小为 3×3 的 DW [44，75,109]卷积来说确实如此，这是因为 DW 卷积操作的存算比太低导致的, 即计算过程与存储器访问的比率[64]，这对现代计算体系结构不友好。然而，我们发现，当卷积核大小变大时，计算密度会增加, 因此存算比更高：例如，在核大小为 11×11 的 DW 卷积运算中，每次对特征映射进行计算时，它最多可参与 121 次乘法运算，而在 3×3 内核中，这一数字仅为 9 次。根据 Roofline 模型，当卷积核大小变大时，实际延迟应该不会随着 Flops 的增加而增加。

[大核卷积的实现优化背后原理](https://zhuanlan.zhihu.com/p/479182218)

此外，我们发现现有的深度学习工具(如 Pytorch)对大型 DW 卷积的支持很差，如表所示。



![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709114851.png)

因此，本文尝试了几种方法来优化 CUDA 内核。理论上, 基于 FFT 的方法[65]对于加速大核卷积似乎是有效果的。然而，在实践中，我们发现 block-wise(inverse) implicit gemm 算法是一个更好的选择。该实现方案已经集成到开源框架 MegEngine[1]中。同时也发布了一个高效的 PyTorch 实现方案。上表显示，与 Pytorch 基线相比，本文的实现方案要高效得多。通过优化，RepLKNet 中 DW 卷积的推理延迟从 49.5%降低到 12.3%，这与 Flops 占用大致成正比。

2. 短路连接(恒等跳过连接)对大核卷积网络非常重要；

为了演示这一点，我们使用 MobileNet V 2[75]进行基准测试，因为它大量使用 DW 层，并且有两个已发布的变体(带或不带 shortcuts)。对于大核对应的模型，我们只需将所有的 DW 3×3 层替换为 13×13。所有的模型都在 ImageNet 上进行了 100 个历元的相同训练配置的训练(详见附录 A)。表 2 显示，大型内核通过 shortcuts 将 MobileNet V 2 的精确度提高了 0.77%。然而，在没有 shortcuts 的情况下，大核函数的准确率仅为53.98%。

该指南也适用于 VITS。最近的一项工作[33]发现，如果没有 identity shortcut，注意力的排名会随着深度的增加而成倍增加，从而导致过度平滑问题。虽然大核 CNN 可能会以不同于 VIT 的机制退化，但我们也观察到，如果没有 shortcut，网络很难捕获局部细节。从与[91]类似的角度来看，shortcut 使模型成为由具有不同感受场(RF)的众多模型组成的隐式集合，因此它可以从更大的最大 RF 中受益，同时不会失去捕捉小规模模式的能力。

3. 用小内核重参数化[30]有助于弥补优化问题(性能下降问题,小数据集优化问题)；

我们将 MobileNet V 2 的 3×3 层分别替换为 9×9 和 13×13，并可选地采用结构重新参数化[26，27，30]方法。具体地说，我们构造了一个与大的层平行的 3×3 层，然后在批归一化(BN)[49]层(图 2)后将它们的输出相加。训练后，我们将小核以及 BN 参数合并到大核中，这样得到的模型与训练模型等价，但不再有小核。表 3 显示了直接将内核大小从 9 增加到 13 会降低精度，而重新参数化解决了这个问题。然后，我们将 ImageNet 训练的模型转移到使用 DeepLabv 3+[15]对城市景观进行语义分割[21]。我们只更换主干，并保留彩信[19]提供的所有默认训练设置。观察结果与 ImageNet 上的类似：3×3 重新参数使 9×9 模型的 MIU 值提高了 0.19，13×13 模型的 MIU 值提高了 0.93。有了这样简单的重新参数化，将内核大小从 9 增加到 13 不再降低 ImageNet 和 Cityscape 上的性能。

众所周知，VITS 存在优化问题，特别是在小数据集上[34，57]。一种常见的解决方法是引入卷积优先，例如，向每个自我注意块[18，96]添加一个 DW 3×3 卷积，这与我们的类似。这些策略在网络之前引入了额外的翻译等价性和局部性，使得在不损失通用性的情况下更容易在小数据集上进行优化。类似于 VIT 的行为[34]，我们还发现，当预训练数据集增加到 7300 万个图像时(参见下一节中的 RepLKNet-XL)，可以在不降级的情况下省略重新参数化。

4. 大核卷积对下游任务的提升更加明显；

表 3(重新参数后)显示了将 MobileNet V 2 的核大小从 3×3 增加到 9×9，使 ImageNet 的准确率提高了 1.33%，而城市景观提高了 3.99%。表 5 显示了类似的趋势：当内核大小从[3，3，3，3]增加到[31，29，27，13]时，ImageNet 的精度只提高了 0.96%，而 ADE 20 K[114]上的 MIEU 提高了 3.12%。这种现象表明，ImageNet 得分相近的模型在下游任务中可能具有非常不同的能力(就像表 5 中排名最低的3个模型一样)。

是什么导致了这一现象？首先，大核设计显著增加了有效感受野(ERF)[63]。大量的研究已经证明，暗示大量 ERF 的“上下文”信息在许多下游任务中是至关重要的，例如对象检测和语义分割[61，67，93,101,102]。我们将在 SEC 讨论这个话题。5.第二，我们认为另一个原因可能是较大的内核设计会给网络带来更多的形状偏差。简而言之，ImageNet 图片可以根据纹理或形状进行正确分类，如[7，35]中所建议的。然而，人类识别物体主要是基于形状线索而不是纹理，因此形状偏好较强的模型可能会更好地转移到下游任务。最近的一项研究[88]指出，VITS 的形状偏见很强，这在一定程度上解释了 VITS 在转移任务中超级强大的原因。相比之下，在 ImageNet 上训练的传统 CNN 倾向于偏向纹理[7，35]。幸运的是，我们发现简单地增大 CNN 中的核大小可以有效地改善形状偏差。详情请参阅附录 C。

5. 即使在小特征图上，大核卷积也很有用。

为了验证这一点，我们将 MobileNet V 2 最后阶段的 DW 卷积扩大到 7×7 或 13×13，因此内核大小与特征映射大小（默认为 7×7）相当甚至更大。我们按照指南 3 的建议对大内核应用重新参数化。表 4 显示尽管最后阶段的卷积已经涉及非常大的感受野，但是进一步增加内核大小仍然会导致性能改进，特别是在诸如 Cityscapes 的下游任务上。

当核大小变大时，注意 CNN 的平移等价并不严格成立。如图 3 所示，相邻空间位置的两个输出仅共享一小部分核权重，即，通过不同的映射进行变换。该物业也符合 VITS 的“哲学”--在获得更多容量之前放松对称性。有趣的是，我们发现在变压器领域广泛使用的二维相对位置嵌入[5，76]也可以被视为大的深度方向的核，其大小为(2 H−1)×(2 W−1)，其中 H 和 W 分别为特征图的高度和宽度。大核函数不仅有助于学习概念之间的相对位置，而且由于填充效应还可以对绝对位置信息进行编码[51]。

基于上述指导方针，我们提出了一种名为 RepLKNet 的新架构，这是一种纯 CNN 架构网络，其中使用重参数化方法的大卷积来构建大的感受野。我们的网络总体上遵循 Swin Transformer [60] 的宏观架构，并进行了一些修改，同时用大的深度卷积替换了多头自注意力。

据我们所知，到目前为止，CNN 仍然主导着小模型[108,110]，而视觉转换器被认为在更复杂的预算下比 CNN 更好。因此，在本文中，我们主要关注相对较大的模型(其复杂度与 ResNet-152[40]或 Swin-B[59]相当或更大)，以验证较大的内核设计是否能够消除 CNN 和 VITS 之间的性能差距。

我们主要对中型和大型模型进行基准测试，因为过去人们认为 ViT 在大数据和模型上优于 CNN。在 ImageNet 分类上，我们的基线（与 Swin-B 相似的模型大小），其内核大小高达 31×31，仅在 ImageNet 1 K 数据集上训练的 top-1 准确率达到 84.8%，比 Swin-B 好 0.3%，但延迟效率更高。

更重要的是，我们发现大内核设计在下游任务上特别强大。例如，在相似的复杂度和参数预算下，我们的网络在 COCO 检测 [56] 上比 ResNeXt-101 [100] 或 ResNet 101 [41] 骨干网高出 4.4%，在 ADE 20 K 分割 [115] 上高出 6.1%，这也与甚至比对应的 Swin Transformers 更好，但推理速度更快。鉴于更多的预训练数据（例如 73 M 图像）和更多的计算预算，我们的最佳模型在具有相似模型大小的最先进技术中获得了非常有竞争力的结果，例如 ImageNet 上 87.8% 的 top-1 准确率和 ADE 20 K 上 56.0% 的准确率，这表明对大规模深度学习应用程序具有出色的可扩展性。我们认为 RepLKNet 的高性能主要是因为我们通过大内核构建的大有效感受野 (ERF) [64]，如图 1 所示。此外，我们的实验表明 RepLKNet 比传统的 CNN 利用更多的形状信息，部分符合人类的认知。我们希望我们的发现可以帮助理解 CNN 和 ViT 的内在机制。

Models with Large Kernels

除了少数像 inceptions[79-81]这样的老式模型外，大型内核模型在 VGG-Net 之后变得不再流行[77]。一个有代表性的工作是全局卷积网络(GCNS)[67]，它使用非常大的 1×K 和 K×1 的卷积来改进语义分割任务。然而，据报道，较大的内核会损害 ImageNet 的性能。

局部关系网络(Local Relationship Networks，LRNet)[45]提出了一种空间聚集算子(LRLayer)来代替标准卷积，后者可以被视为动态卷积。7×7 的核对 LR-Net 有好处，9×9 的 LR-Net 性能下降。当核的大小和特征图一样大时，TOP-1 的准确率从 75.7%降到 68.4%。

最近，Swin Transformers[59]提出了通过转移窗口注意力来捕捉空间模式，其窗口大小从 7 到 12，这也可以被视为大核的变体。后续的[32，58]使用了更大的窗口尺寸。

受这些局部变压器的成功启发，最近的一项工作[38]用静态或动态的 7×7 沿深度方向的卷积取代了 MHSA 层[59]，同时仍然保持了可比较的结果。虽然[38]提出的网络与我们的设计模式相似，但动机不同：[38]没有研究 ERF、大核与性能之间的关系；相反，它将视觉转换器的优越性能归因于稀疏连接、共享参数和动态机制。

另外三部代表作是 Global Filter Networks(GFNets)[72]、CKConv[74]和 FlexConv[73]。GFNet 优化了傅立叶域中的空间连接权值，相当于空间域中的圆形全局卷积。CKConv 将核函数表示为连续函数来处理序列数据，可以构造任意大小的核函数。FlexConv 为不同的层学习不同的内核大小，可以与功能图一样大。尽管它们使用非常大的内核，但它们并不打算回答我们想要的关键问题：为什么传统的 CNN 性能不如 VITS，以及如何在普通 CNN 中应用大的内核。此外，[38]和[72]都不在强基线上评估他们的模型，例如，大于 SwinL 的模型。因此，目前还不清楚大核 CNN 是否能像变压器一样扩大规模。

ConvMixer[87]使用高达 9×9 的卷积来替换 VITS[34]或 MLP[84，85]的“混合器”组件。MetaFormer[103]指出，集中注意力是自我关注的替代方式。ConvNeXt[60]采用 7×7 深度卷积来设计强结构，突破了 CNN 性能的极限。虽然这些作品表现出出色的性能，但它们并没有从更大的卷积(例如，31×31)中获得好处。

Model Scaling Techniques

给定一个小模型，为了获得更好的性能，通常的做法是将其放大，因此，缩放策略在最终的精度和效率之间的权衡中起着至关重要的作用。对于 CNN，现有的伸缩方法通常关注模型深度、宽度、输入分辨率[31，68，82]、瓶颈比和组宽度[31，68]。然而，内核大小经常被忽略。在证券交易委员会。3，我们将证明核大小也是 CNN 中一个重要的伸缩维，特别是对于下游任务。

Structural Re-parameterization

结构再参数化[26-30]是一种通过转换参数来等价转换模型结构的方法。例如，RepVGG 针对深度推理时类似 VGG(例如，无分支)的模型，并在训练过程中构建了平行于 3×3 层的额外 ResNet 样式的快捷方式。与难以训练的真实 VGG 模型相比，这种捷径帮助该模型达到了令人满意的性能。经过训练后，通过一系列的线性变换将捷径吸收到并行的 3×3 核中，从而使得到的模型成为类 VGG 模型。在本文中，我们使用这种方法将一个相对较小的(例如，3×3 或 5×5)核添加到一个非常大的核中。通过这种方式，我们使非常大的核能够捕获小规模的模式，从而提高了模型的性能。

![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709111517.png)

Stem 指的是起始层。由于我们的目标是下游密度预测任务的高性能，我们希望在开始时通过几个卷积层捕获更多细节。在第一次 3×3 和 2×下采样之后，我们安排了一个 DW 3×3 层来捕获低电平图案，一个 1×1 卷积，以及另一个 DW 3×3 层用于下采样。

Stages 1-4 每个都包含几个 RepLK 块，它们使用快捷方式(准则 2)和 DW 大内核(准则 1)。我们通常在 DW 转换之前和之后使用 1×1 转换。请注意，每个 DW 大卷积使用一个 5×5 的核进行重新参数化(准则 3)，图 4 中没有显示。除了大卷积层提供了足够的接受场和聚集空间信息的能力外，模型的表示能力也与深度密切相关。为了提供更多的非线性和跨通道的信息通信，我们希望使用 1×1 层来增加深度。受广泛应用于变压器[34，59]和 MLP[26，84，85]的前馈网络(FFN)的启发，我们使用了类似于 CNN 式的块，它由快捷方式、BN、两个 1×1 层和 Gelu[41]组成，因此被称为 ConvFFN 块。与传统的 FFN 在完全连通层之前使用层归一化[3]相比，BN 的优点是可以融合到 Conv 中进行有效的推理。通常，ConvFFN 模块的内部通道数为 4×作为输入。只需在交织注意块和 FFN 块的 Vit 和 Swin 之后，我们在每个 RepLK 块之后放置一个 ConvFFN。

Transition Blocks 放置在 stages 之间，首先通过 1×1 卷积增加通道尺寸，然后以 DW 3×3 卷积进行 2×下采样。

总而言之，每个阶段具有三个体系结构超参数：RepLK 块的数量 B、通道维度 C 和内核大小


我们通过固定 B=[2，2，18，2]，C=[128,256,512,1024]，变化 K，并观察分类和语义分割的性能，来继续评估 RepLKNet 上的大核函数。在没有仔细调整超参数的情况下，我们随意地将核大小分别设置为[13，13，13，13]，[25，25，25，13]，[31，29，27，13]，并将这些模型称为 RepLKNet-13/25/31。我们还构造了两条小内核基线，其中内核大小都是 3 或 7(RepLKNet-3/7)。在 ImageNet 上，我们使用 AdamW[62]优化器、随机增强[22]、Mixup[106]、CutMix[105]、随机擦除[113]和随机深度[48]，在最近的工作[4，59，60，86]的基础上，训练了 120 个纪元。对于语义分割，我们使用的是 ADE 20 K[114]，这是一个广泛使用的大规模语义分割数据集，包含 150 个类别的 20 K 图像用于训练，2 K 用于验证。我们使用经过 ImageNet 训练的模型作为主干，采用 MMS eggation[19]在 80 K 迭代训练设置下实现的 UperNet[97]，并对单尺度 MEU 进行了测试。表 5 显示了不同内核大小下的结果。在 ImageNet 上，虽然将内核大小从 3 增加到 13 会提高精度，但将内核大小增加到更大并不会带来进一步的改进。然而，在 ADE 20 K 上，将内核从[13，13，13，13]扩展到[31，29，27，13]会带来 0.82 个更高的 MIU 值，仅增加 5.3%的参数和 3.5%的 FLOPS，这突显了大内核对于下游任务的重要性。在接下来的小节中，我们使用具有更强训练配置的 RepLKNet 31 来与 ImageNet 分类、城市景观/ADE 20 K 语义分割和 COCO[55]对象检测的最新技术进行比较。我们将上述模型称为 RepLKNet-31 B(B 表示基本)，将 C=[192,384,768,1536]的更广泛的模型称为 RepLKNet 31 L(大)。我们构造了另一个 RepLKNet-XL，C=[256,512,1024,2048]，在 RepLK 块中采用1.5×反向瓶颈设计(即 DW 大卷积层的沟道为1.5×作为输入)。


由于 RepLKNet 的整体架构类似于 Swin，我们希望首先进行比较。对于 ImageNet-1 K 上的 RepLKNet-31 B，我们将上述训练时间表扩展到 300 个纪元，以便进行公平的比较。然后对输入分辨率为 384×384 的 Swin-B 模型进行 30 个历元的精调，使总的训练成本大大低于 Swin-B 模型从零开始训练的 384×384。然后在 ImageNet-22 K 上对 RepLKNet-B/L 模型进行预训练，在 ImageNet-1 K 上对 Finetune 模型进行预训练。RepLKNetXL 在我们的私有半监督数据集 MegData 73 M 上进行了预训练，该数据集在附录中介绍。我们还给出了在相同的 2080 Ti GPU 上测试的批处理大小为 64 的吞吐量。附录中提供了培训配置。表 6 显示，尽管非常大的核不是用于 ImageNet 分类的，但我们的 RepLKNet 模型显示了精度和效率之间的良好折衷。值得注意的是，仅通过 ImageNet-1 K 训练，RepLKNet-31 B 的准确率达到 84.8%，比 Swin-B 高 0.3%，运行速度快 43%。尽管 RepLKNet-XL 比 Swin-L 有更高的 FLOPS，但它运行得更快，这突出了非常大的内核的效率。

4.4.。语义分割

我们然后使用预先训练的模型作为城市景观(表 7)和 ADE 20 K(表 8)的主干。具体地说，我们使用由 MMS egationation[19]实现的 UperNet[97]，对于城市景观具有 80 K 迭代训练时间表，对于 ADE 20 K 具有 160 K 迭代训练时间表。由于我们只希望评估主干，所以我们不使用任何高级技术、技巧或定制算法。在城市景观上，ImageNet-1 K 预先训练的 RepLKNet 31 B 的表现远远超过 Swin-B(单刻度 Miou 为 2.7)，甚至超过 ImageNet 22 K 预先训练的 Swin-L。即使配备了为视觉转换器定制的 DiversePatch[36]技术，22 K 预训练的 Swin-L 的单比例尺 Mou 仍然低于我们的 1 K 预训练的 RepLKNet-31 B，尽管前者有 2×参数。在 ADE 20 K 上，RepLKNet-31 B 在 1 K 和 22 K 的预训练中都优于 Swin-B，单尺度 Miou 的差距尤为显著。在我们的半监督数据集 MegData 73 M 的预训练下，RepLKNet-XL 达到了 56.0 的 MIU 值，这表明了 RepLKNet-XL 对于大规模视觉应用的可行可扩展性。

对于目标检测，我们使用 RepLKNets 作为 FCOS[83]和 Cascade MASK R-CNN[8，39]的主干，这两种方法分别是一阶段和两阶段检测方法的代表，以及 MMDetect 中的默认配置[12]。FCOS 模型使用 2 x(24 个纪元)的训练时间表进行训练，以便与来自相同代码库的 X 101(ResNeXt-101[99]的缩写)基线进行公平比较[19]，而使用级联掩码 R-CNN 的其他结果都使用 3 x(36 纪元)。同样，我们只是更换主干，不使用任何先进技术。表 9 显示，RepLKNet 的性能比 ResNeXt-101-64 x 4 d 高出 4.4 MAP，同时具有更少的参数和更低的 FLOPS。请注意，使用 HTC[11]、HTC++[59]、软 NMS[6]或 6 x(72 个纪元)时间表等高级技术可以进一步改进结果。与 Swin 相比，RepLKNet 以更少的参数和更低的 FLOPS 实现了更高或更接近的 MAP。值得注意的是，RepLKNet-XL 达到了55.5的 MAP，这再次证明了可伸缩性。

### 引文

## 摘录

---
title: "Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"
description: ""
citekey: dingScalingYourKernels2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:34:52
lastmod: 2023-04-11 17:41:33
---

> [!info] 论文信息
>1. Title：Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs
>2. Author：Xiaohan Ding, Xiangyu Zhang, Jungong Han, Guiguang Ding
>3. Entry：[Zotero link](zotero://select/items/@dingScalingYourKernels2022) [URL link](https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Ding et al_2022_Scaling Up Your Kernels to 31x31.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\3FNAJGAC\\Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html>)
>4. Other：2022 -      -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] 

> 我们重新审视现代卷积神经网络 (CNN) 中的大卷积核设计。受 vision transformers (ViT) 最新进展的启发，在本文中，我们证明了使用一些大型卷积核而不是一堆小卷积核可能是一种更强大的方法。我们提出了五个指导方针，例如，应用重参数化( re-parameterized)的大深度卷积来设计高效的高性能大卷积核 CNN。根据指南，我们提出了 RepLKNet 网络，这是一种纯 CNN 架构，其内核大小高达 31×31，而不是常用的 3×3。 RepLKNet 极大地缩小了 CNN 和 ViT 之间的性能差距，例如，在 ImageNet 和一些典型的下游任务上实现了与 Swin Transformer 相当或更好的结果，并且延迟更低。 RepLKNet 还表现出对大数据和大型模型的良好可扩展性，在多个测试任务中都得到了效果提升.

## 预处理

## 概述

## 结果

## 精读

卷积神经网络 (CNN) [41, 54] 曾经是现代计算机视觉系统中视觉编码器的常见选择。然而，最近，CNNs [41, 54] 受到了视觉转换器 (ViTs) [34, 60, 85, 94] 的极大挑战，它们在许多视觉任务上都表现出了领先的表现——不仅是图像分类 [34, 105] 和表示学习 [4, 10, 16, 101]，还有许多下游任务，例如对象检测 [24, 60]，语义分割 [94, 99] 和图像恢复 [11, 55]。

为什么 ViT 超级强大？

1. 一些工作认为，ViTs 中的多头自我注意 (MHSA) 机制起着关键作用。他们提供了实证结果来证明，MHSA 更灵活 [51]，能够（更少的归纳偏差）[20]，对失真更稳健 [67, 99]，或者能够对长期依赖关系进行建模 [70, 89]。
2. 但是一些工作挑战了 MHSA [116] 的必要性，将 ViT 的高性能归因于适当的构建块 [33] 和/或动态稀疏权重 [39,111]。更多作品 [20,39,43,96,116] 从不同的角度解释了 ViT 的优越性。
3. 在这项工作中，我们专注于一个观点：建立大感受野的方式。在 ViT 中，MHSA 通常设计为全局 [34、76、94] 或局部但具有大内核 [60、71、88]，因此单个 MHSA 层的每个输出都能够从大区域收集信息。然而，大内核在 CNN 中并不普遍使用（第一层 [41] 除外）。相反，一种典型的方式是使用一堆小的空间卷积 1 [41, 45, 48, 69, 75, 80, 109]（例如，3×3）来扩大最先进的感受野 CNN。只有一些老式的网络，如 AlexNet [54]、Inceptions [77-79] 以及从神经架构搜索 [38,44,57,117] 衍生的少数架构采用大空间卷积（其大小大于 5）作为主要部分。

上面的观点自然会引出一个问题：如果我们对传统的 CNN 使用一些大的而不是许多小的内核会怎样？大内核还是构建大感受野的方式是缩小 CNN 和 ViT 之间性能差距的关键？为了回答这个问题，我们系统地探索了 CNN 的大内核设计。我们遵循一个非常简单的“哲学”：只需将大的深度卷积引入常规网络，其大小范围从 3×3 到 31×31，尽管还有其他替代方案可以通过单层或几层引入大的感受野，例如特征金字塔 [92]、扩张卷积 [14、102、103] 和可变形卷积 [23]。通过一系列实验，我们总结了有效使用大卷积的五个经验指南：

1. 虽然大核卷积计算成本更高,但是通过改进可以使得大核深度卷积计算更高效；

通常，大核卷积的计算量很大，因为网络结构的参数量和 FLOPs(每秒浮点运算次数)与卷积核尺寸的平方成正比。但是这一缺点可以通过应用深度卷积(DW: Depth-wise)方法来克服[17，44]。


例如在本文提出的 RepLKNet 网络架构中，虽然在不同阶段将卷积核尺寸从原始的[3，3，3，3]增加到了[31，29，27，13]，但是 Flops 和参数量仅增加了 18.6%的 10.4%，这在可接受范围内。实际上, 剩下的 1×1 卷积运算控制了大部分的计算复杂性。


![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709112959.png)

有人可能会担心，DW 卷积在现代并行计算设备(如 GPU)上的计算效率可能非常低。对于传统的卷积核大小为 3×3 的 DW [44，75,109]卷积来说确实如此，这是因为 DW 卷积操作的存算比太低导致的, 即计算过程与存储器访问的比率[64]，这对现代计算体系结构不友好。然而，我们发现，当卷积核大小变大时，计算密度会增加, 因此存算比更高：例如，在核大小为 11×11 的 DW 卷积运算中，每次对特征映射进行计算时，它最多可参与 121 次乘法运算，而在 3×3 内核中，这一数字仅为 9 次。根据 Roofline 模型，当卷积核大小变大时，实际延迟应该不会随着 Flops 的增加而增加。

[大核卷积的实现优化背后原理](https://zhuanlan.zhihu.com/p/479182218)

此外，我们发现现有的深度学习工具(如 Pytorch)对大型 DW 卷积的支持很差，如表所示。



![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709114851.png)

因此，本文尝试了几种方法来优化 CUDA 内核。理论上, 基于 FFT 的方法[65]对于加速大核卷积似乎是有效果的。然而，在实践中，我们发现 block-wise(inverse) implicit gemm 算法是一个更好的选择。该实现方案已经集成到开源框架 MegEngine[1]中。同时也发布了一个高效的 PyTorch 实现方案。上表显示，与 Pytorch 基线相比，本文的实现方案要高效得多。通过优化，RepLKNet 中 DW 卷积的推理延迟从 49.5%降低到 12.3%，这与 Flops 占用大致成正比。

2. 短路连接(恒等跳过连接)对大核卷积网络非常重要；

为了演示这一点，我们使用 MobileNet V 2[75]进行基准测试，因为它大量使用 DW 层，并且有两个已发布的变体(带或不带 shortcuts)。对于大核对应的模型，我们只需将所有的 DW 3×3 层替换为 13×13。所有的模型都在 ImageNet 上进行了 100 个历元的相同训练配置的训练(详见附录 A)。表 2 显示，大型内核通过 shortcuts 将 MobileNet V 2 的精确度提高了 0.77%。然而，在没有 shortcuts 的情况下，大核函数的准确率仅为53.98%。

该指南也适用于 VITS。最近的一项工作[33]发现，如果没有 identity shortcut，注意力的排名会随着深度的增加而成倍增加，从而导致过度平滑问题。虽然大核 CNN 可能会以不同于 VIT 的机制退化，但我们也观察到，如果没有 shortcut，网络很难捕获局部细节。从与[91]类似的角度来看，shortcut 使模型成为由具有不同感受场(RF)的众多模型组成的隐式集合，因此它可以从更大的最大 RF 中受益，同时不会失去捕捉小规模模式的能力。

3. 用小内核重参数化[30]有助于弥补优化问题(性能下降问题,小数据集优化问题)；

我们将 MobileNet V 2 的 3×3 层分别替换为 9×9 和 13×13，并可选地采用结构重新参数化[26，27，30]方法。具体地说，我们构造了一个与大的层平行的 3×3 层，然后在批归一化(BN)[49]层(图 2)后将它们的输出相加。训练后，我们将小核以及 BN 参数合并到大核中，这样得到的模型与训练模型等价，但不再有小核。表 3 显示了直接将内核大小从 9 增加到 13 会降低精度，而重新参数化解决了这个问题。然后，我们将 ImageNet 训练的模型转移到使用 DeepLabv 3+[15]对城市景观进行语义分割[21]。我们只更换主干，并保留彩信[19]提供的所有默认训练设置。观察结果与 ImageNet 上的类似：3×3 重新参数使 9×9 模型的 MIU 值提高了 0.19，13×13 模型的 MIU 值提高了 0.93。有了这样简单的重新参数化，将内核大小从 9 增加到 13 不再降低 ImageNet 和 Cityscape 上的性能。

众所周知，VITS 存在优化问题，特别是在小数据集上[34，57]。一种常见的解决方法是引入卷积优先，例如，向每个自我注意块[18，96]添加一个 DW 3×3 卷积，这与我们的类似。这些策略在网络之前引入了额外的翻译等价性和局部性，使得在不损失通用性的情况下更容易在小数据集上进行优化。类似于 VIT 的行为[34]，我们还发现，当预训练数据集增加到 7300 万个图像时(参见下一节中的 RepLKNet-XL)，可以在不降级的情况下省略重新参数化。

4. 大核卷积对下游任务的提升更加明显；

表 3(重新参数后)显示了将 MobileNet V 2 的核大小从 3×3 增加到 9×9，使 ImageNet 的准确率提高了 1.33%，而城市景观提高了 3.99%。表 5 显示了类似的趋势：当内核大小从[3，3，3，3]增加到[31，29，27，13]时，ImageNet 的精度只提高了 0.96%，而 ADE 20 K[114]上的 MIEU 提高了 3.12%。这种现象表明，ImageNet 得分相近的模型在下游任务中可能具有非常不同的能力(就像表 5 中排名最低的3个模型一样)。

是什么导致了这一现象？首先，大核设计显著增加了有效感受野(ERF)[63]。大量的研究已经证明，暗示大量 ERF 的“上下文”信息在许多下游任务中是至关重要的，例如对象检测和语义分割[61，67，93,101,102]。我们将在 SEC 讨论这个话题。5.第二，我们认为另一个原因可能是较大的内核设计会给网络带来更多的形状偏差。简而言之，ImageNet 图片可以根据纹理或形状进行正确分类，如[7，35]中所建议的。然而，人类识别物体主要是基于形状线索而不是纹理，因此形状偏好较强的模型可能会更好地转移到下游任务。最近的一项研究[88]指出，VITS 的形状偏见很强，这在一定程度上解释了 VITS 在转移任务中超级强大的原因。相比之下，在 ImageNet 上训练的传统 CNN 倾向于偏向纹理[7，35]。幸运的是，我们发现简单地增大 CNN 中的核大小可以有效地改善形状偏差。详情请参阅附录 C。

5. 即使在小特征图上，大核卷积也很有用。

为了验证这一点，我们将 MobileNet V 2 最后阶段的 DW 卷积扩大到 7×7 或 13×13，因此内核大小与特征映射大小（默认为 7×7）相当甚至更大。我们按照指南 3 的建议对大内核应用重新参数化。表 4 显示尽管最后阶段的卷积已经涉及非常大的感受野，但是进一步增加内核大小仍然会导致性能改进，特别是在诸如 Cityscapes 的下游任务上。

当核大小变大时，注意 CNN 的平移等价并不严格成立。如图 3 所示，相邻空间位置的两个输出仅共享一小部分核权重，即，通过不同的映射进行变换。该物业也符合 VITS 的“哲学”--在获得更多容量之前放松对称性。有趣的是，我们发现在变压器领域广泛使用的二维相对位置嵌入[5，76]也可以被视为大的深度方向的核，其大小为(2 H−1)×(2 W−1)，其中 H 和 W 分别为特征图的高度和宽度。大核函数不仅有助于学习概念之间的相对位置，而且由于填充效应还可以对绝对位置信息进行编码[51]。

基于上述指导方针，我们提出了一种名为 RepLKNet 的新架构，这是一种纯 CNN 架构网络，其中使用重参数化方法的大卷积来构建大的感受野。我们的网络总体上遵循 Swin Transformer [60] 的宏观架构，并进行了一些修改，同时用大的深度卷积替换了多头自注意力。

据我们所知，到目前为止，CNN 仍然主导着小模型[108,110]，而视觉转换器被认为在更复杂的预算下比 CNN 更好。因此，在本文中，我们主要关注相对较大的模型(其复杂度与 ResNet-152[40]或 Swin-B[59]相当或更大)，以验证较大的内核设计是否能够消除 CNN 和 VITS 之间的性能差距。

我们主要对中型和大型模型进行基准测试，因为过去人们认为 ViT 在大数据和模型上优于 CNN。在 ImageNet 分类上，我们的基线（与 Swin-B 相似的模型大小），其内核大小高达 31×31，仅在 ImageNet 1 K 数据集上训练的 top-1 准确率达到 84.8%，比 Swin-B 好 0.3%，但延迟效率更高。

更重要的是，我们发现大内核设计在下游任务上特别强大。例如，在相似的复杂度和参数预算下，我们的网络在 COCO 检测 [56] 上比 ResNeXt-101 [100] 或 ResNet 101 [41] 骨干网高出 4.4%，在 ADE 20 K 分割 [115] 上高出 6.1%，这也与甚至比对应的 Swin Transformers 更好，但推理速度更快。鉴于更多的预训练数据（例如 73 M 图像）和更多的计算预算，我们的最佳模型在具有相似模型大小的最先进技术中获得了非常有竞争力的结果，例如 ImageNet 上 87.8% 的 top-1 准确率和 ADE 20 K 上 56.0% 的准确率，这表明对大规模深度学习应用程序具有出色的可扩展性。我们认为 RepLKNet 的高性能主要是因为我们通过大内核构建的大有效感受野 (ERF) [64]，如图 1 所示。此外，我们的实验表明 RepLKNet 比传统的 CNN 利用更多的形状信息，部分符合人类的认知。我们希望我们的发现可以帮助理解 CNN 和 ViT 的内在机制。

Models with Large Kernels

除了少数像 inceptions[79-81]这样的老式模型外，大型内核模型在 VGG-Net 之后变得不再流行[77]。一个有代表性的工作是全局卷积网络(GCNS)[67]，它使用非常大的 1×K 和 K×1 的卷积来改进语义分割任务。然而，据报道，较大的内核会损害 ImageNet 的性能。

局部关系网络(Local Relationship Networks，LRNet)[45]提出了一种空间聚集算子(LRLayer)来代替标准卷积，后者可以被视为动态卷积。7×7 的核对 LR-Net 有好处，9×9 的 LR-Net 性能下降。当核的大小和特征图一样大时，TOP-1 的准确率从 75.7%降到 68.4%。

最近，Swin Transformers[59]提出了通过转移窗口注意力来捕捉空间模式，其窗口大小从 7 到 12，这也可以被视为大核的变体。后续的[32，58]使用了更大的窗口尺寸。

受这些局部变压器的成功启发，最近的一项工作[38]用静态或动态的 7×7 沿深度方向的卷积取代了 MHSA 层[59]，同时仍然保持了可比较的结果。虽然[38]提出的网络与我们的设计模式相似，但动机不同：[38]没有研究 ERF、大核与性能之间的关系；相反，它将视觉转换器的优越性能归因于稀疏连接、共享参数和动态机制。

另外三部代表作是 Global Filter Networks(GFNets)[72]、CKConv[74]和 FlexConv[73]。GFNet 优化了傅立叶域中的空间连接权值，相当于空间域中的圆形全局卷积。CKConv 将核函数表示为连续函数来处理序列数据，可以构造任意大小的核函数。FlexConv 为不同的层学习不同的内核大小，可以与功能图一样大。尽管它们使用非常大的内核，但它们并不打算回答我们想要的关键问题：为什么传统的 CNN 性能不如 VITS，以及如何在普通 CNN 中应用大的内核。此外，[38]和[72]都不在强基线上评估他们的模型，例如，大于 SwinL 的模型。因此，目前还不清楚大核 CNN 是否能像变压器一样扩大规模。

ConvMixer[87]使用高达 9×9 的卷积来替换 VITS[34]或 MLP[84，85]的“混合器”组件。MetaFormer[103]指出，集中注意力是自我关注的替代方式。ConvNeXt[60]采用 7×7 深度卷积来设计强结构，突破了 CNN 性能的极限。虽然这些作品表现出出色的性能，但它们并没有从更大的卷积(例如，31×31)中获得好处。

Model Scaling Techniques

给定一个小模型，为了获得更好的性能，通常的做法是将其放大，因此，缩放策略在最终的精度和效率之间的权衡中起着至关重要的作用。对于 CNN，现有的伸缩方法通常关注模型深度、宽度、输入分辨率[31，68，82]、瓶颈比和组宽度[31，68]。然而，内核大小经常被忽略。在证券交易委员会。3，我们将证明核大小也是 CNN 中一个重要的伸缩维，特别是对于下游任务。

Structural Re-parameterization

结构再参数化[26-30]是一种通过转换参数来等价转换模型结构的方法。例如，RepVGG 针对深度推理时类似 VGG(例如，无分支)的模型，并在训练过程中构建了平行于 3×3 层的额外 ResNet 样式的快捷方式。与难以训练的真实 VGG 模型相比，这种捷径帮助该模型达到了令人满意的性能。经过训练后，通过一系列的线性变换将捷径吸收到并行的 3×3 核中，从而使得到的模型成为类 VGG 模型。在本文中，我们使用这种方法将一个相对较小的(例如，3×3 或 5×5)核添加到一个非常大的核中。通过这种方式，我们使非常大的核能够捕获小规模的模式，从而提高了模型的性能。

![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709111517.png)

Stem 指的是起始层。由于我们的目标是下游密度预测任务的高性能，我们希望在开始时通过几个卷积层捕获更多细节。在第一次 3×3 和 2×下采样之后，我们安排了一个 DW 3×3 层来捕获低电平图案，一个 1×1 卷积，以及另一个 DW 3×3 层用于下采样。

Stages 1-4 每个都包含几个 RepLK 块，它们使用快捷方式(准则 2)和 DW 大内核(准则 1)。我们通常在 DW 转换之前和之后使用 1×1 转换。请注意，每个 DW 大卷积使用一个 5×5 的核进行重新参数化(准则 3)，图 4 中没有显示。除了大卷积层提供了足够的接受场和聚集空间信息的能力外，模型的表示能力也与深度密切相关。为了提供更多的非线性和跨通道的信息通信，我们希望使用 1×1 层来增加深度。受广泛应用于变压器[34，59]和 MLP[26，84，85]的前馈网络(FFN)的启发，我们使用了类似于 CNN 式的块，它由快捷方式、BN、两个 1×1 层和 Gelu[41]组成，因此被称为 ConvFFN 块。与传统的 FFN 在完全连通层之前使用层归一化[3]相比，BN 的优点是可以融合到 Conv 中进行有效的推理。通常，ConvFFN 模块的内部通道数为 4×作为输入。只需在交织注意块和 FFN 块的 Vit 和 Swin 之后，我们在每个 RepLK 块之后放置一个 ConvFFN。

Transition Blocks 放置在 stages 之间，首先通过 1×1 卷积增加通道尺寸，然后以 DW 3×3 卷积进行 2×下采样。

总而言之，每个阶段具有三个体系结构超参数：RepLK 块的数量 B、通道维度 C 和内核大小


我们通过固定 B=[2，2，18，2]，C=[128,256,512,1024]，变化 K，并观察分类和语义分割的性能，来继续评估 RepLKNet 上的大核函数。在没有仔细调整超参数的情况下，我们随意地将核大小分别设置为[13，13，13，13]，[25，25，25，13]，[31，29，27，13]，并将这些模型称为 RepLKNet-13/25/31。我们还构造了两条小内核基线，其中内核大小都是 3 或 7(RepLKNet-3/7)。在 ImageNet 上，我们使用 AdamW[62]优化器、随机增强[22]、Mixup[106]、CutMix[105]、随机擦除[113]和随机深度[48]，在最近的工作[4，59，60，86]的基础上，训练了 120 个纪元。对于语义分割，我们使用的是 ADE 20 K[114]，这是一个广泛使用的大规模语义分割数据集，包含 150 个类别的 20 K 图像用于训练，2 K 用于验证。我们使用经过 ImageNet 训练的模型作为主干，采用 MMS eggation[19]在 80 K 迭代训练设置下实现的 UperNet[97]，并对单尺度 MEU 进行了测试。表 5 显示了不同内核大小下的结果。在 ImageNet 上，虽然将内核大小从 3 增加到 13 会提高精度，但将内核大小增加到更大并不会带来进一步的改进。然而，在 ADE 20 K 上，将内核从[13，13，13，13]扩展到[31，29，27，13]会带来 0.82 个更高的 MIU 值，仅增加 5.3%的参数和 3.5%的 FLOPS，这突显了大内核对于下游任务的重要性。在接下来的小节中，我们使用具有更强训练配置的 RepLKNet 31 来与 ImageNet 分类、城市景观/ADE 20 K 语义分割和 COCO[55]对象检测的最新技术进行比较。我们将上述模型称为 RepLKNet-31 B(B 表示基本)，将 C=[192,384,768,1536]的更广泛的模型称为 RepLKNet 31 L(大)。我们构造了另一个 RepLKNet-XL，C=[256,512,1024,2048]，在 RepLK 块中采用1.5×反向瓶颈设计(即 DW 大卷积层的沟道为1.5×作为输入)。


由于 RepLKNet 的整体架构类似于 Swin，我们希望首先进行比较。对于 ImageNet-1 K 上的 RepLKNet-31 B，我们将上述训练时间表扩展到 300 个纪元，以便进行公平的比较。然后对输入分辨率为 384×384 的 Swin-B 模型进行 30 个历元的精调，使总的训练成本大大低于 Swin-B 模型从零开始训练的 384×384。然后在 ImageNet-22 K 上对 RepLKNet-B/L 模型进行预训练，在 ImageNet-1 K 上对 Finetune 模型进行预训练。RepLKNetXL 在我们的私有半监督数据集 MegData 73 M 上进行了预训练，该数据集在附录中介绍。我们还给出了在相同的 2080 Ti GPU 上测试的批处理大小为 64 的吞吐量。附录中提供了培训配置。表 6 显示，尽管非常大的核不是用于 ImageNet 分类的，但我们的 RepLKNet 模型显示了精度和效率之间的良好折衷。值得注意的是，仅通过 ImageNet-1 K 训练，RepLKNet-31 B 的准确率达到 84.8%，比 Swin-B 高 0.3%，运行速度快 43%。尽管 RepLKNet-XL 比 Swin-L 有更高的 FLOPS，但它运行得更快，这突出了非常大的内核的效率。

4.4.。语义分割

我们然后使用预先训练的模型作为城市景观(表 7)和 ADE 20 K(表 8)的主干。具体地说，我们使用由 MMS egationation[19]实现的 UperNet[97]，对于城市景观具有 80 K 迭代训练时间表，对于 ADE 20 K 具有 160 K 迭代训练时间表。由于我们只希望评估主干，所以我们不使用任何高级技术、技巧或定制算法。在城市景观上，ImageNet-1 K 预先训练的 RepLKNet 31 B 的表现远远超过 Swin-B(单刻度 Miou 为 2.7)，甚至超过 ImageNet 22 K 预先训练的 Swin-L。即使配备了为视觉转换器定制的 DiversePatch[36]技术，22 K 预训练的 Swin-L 的单比例尺 Mou 仍然低于我们的 1 K 预训练的 RepLKNet-31 B，尽管前者有 2×参数。在 ADE 20 K 上，RepLKNet-31 B 在 1 K 和 22 K 的预训练中都优于 Swin-B，单尺度 Miou 的差距尤为显著。在我们的半监督数据集 MegData 73 M 的预训练下，RepLKNet-XL 达到了 56.0 的 MIU 值，这表明了 RepLKNet-XL 对于大规模视觉应用的可行可扩展性。

对于目标检测，我们使用 RepLKNets 作为 FCOS[83]和 Cascade MASK R-CNN[8，39]的主干，这两种方法分别是一阶段和两阶段检测方法的代表，以及 MMDetect 中的默认配置[12]。FCOS 模型使用 2 x(24 个纪元)的训练时间表进行训练，以便与来自相同代码库的 X 101(ResNeXt-101[99]的缩写)基线进行公平比较[19]，而使用级联掩码 R-CNN 的其他结果都使用 3 x(36 纪元)。同样，我们只是更换主干，不使用任何先进技术。表 9 显示，RepLKNet 的性能比 ResNeXt-101-64 x 4 d 高出 4.4 MAP，同时具有更少的参数和更低的 FLOPS。请注意，使用 HTC[11]、HTC++[59]、软 NMS[6]或 6 x(72 个纪元)时间表等高级技术可以进一步改进结果。与 Swin 相比，RepLKNet 以更少的参数和更低的 FLOPS 实现了更高或更接近的 MAP。值得注意的是，RepLKNet-XL 达到了55.5的 MAP，这再次证明了可伸缩性。

---
title: "MAEC: Multi-Instance Learning with an Adversarial Auto-Encoder-Based Classifier for Speech Emotion Recognition"
description: ""
citekey: fuMAECMultiInstanceLearning2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:36:01
lastmod: 2023-04-11 17:51:40
---

> [!info] 论文信息
>1. Title：MAEC: Multi-Instance Learning with an Adversarial Auto-Encoder-Based Classifier for Speech Emotion Recognition
>2. Author：Changzeng Fu, Chaoran Liu, Carlos Toshinori Ishi, Hiroshi Ishiguro
>3. Entry：[Zotero link](zotero://select/items/@fuMAECMultiInstanceLearning2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fu et al_2021_MAEC.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] In this paper, we propose an adversarial auto-encoder-based classifier, which can regularize the distribution of latent representation to smooth the boundaries among categories. Moreover, we adopt multi-instance learning by dividing speech into a bag of segments to capture the most salient moments for presenting an emotion. The proposed model was trained on the IEMOCAP dataset and evaluated on the in-corpus validation set (IEMOCAP) and the cross-corpus validation set (MELD). The experiment results show that our model outperforms the baseline on in-corpus validation and increases the scores on cross-corpus validation with regularization.

> 在本文中，我们提出了一种基于对抗性自动编码器的分类器，该分类器可以正则化潜在表示的分布，以平滑类别之间的边界。此外，我们采用多实例学习，将语音分成一组片段，以捕捉表达情感的最显著时刻。所提出的模型在 IEMOCAP 数据集上进行训练，并在语料库内验证集（IEMOCAP）和跨语料库验证集（MELD）上进行评估。实验结果表明，我们的模型在语料库内验证方面优于基线，并通过正则化提高了跨语料库验证的分数。

## 预处理

## 概述

## 结果

在本研究中，提出的模型在 IEMOCAP 上进行了训练和评估[18]。我们还对 MELD 进行了跨语料库验证[19]。以下描述了数据集。IEMOCAP 包含与十位发言者进行双向对话的视频。每个视频都包含一个二元对话，该对话被分割成话语并用情感标签进行注释。在这项研究中，我们对四个类别进行了情绪分类：“快乐”、“悲伤”、“中性”和“愤怒”。请注意，我们将“快乐”和“兴奋”两个类别合并为“快乐”，将“悲伤”和“沮丧”合并为“悲伤”，就像之前的许多研究一样。

MELD 是一个多方对话数据集，收集自一部美国电视剧。在这项研究中，我们仅从“快乐”、“悲伤”、“中性”和“愤怒”类别中提取样本，以与 IEMOCAP 中的训练集保持一致，“快乐”被视为“快乐”。考虑到使跨语料库验证数据具有与训练集相同的数据结构，我们从 MELD 数据集中选择了 2000 个样本，其中说话者的前一轮和对话者的轮是可用的。

我们随机将 80%的样本分成训练集，20%分成测试集，正如 Liu 等人[20]所做的那样，并进行了 5 次交叉验证。

批次大小为 32。我们选择 Adam 作为优化器，学习率为 1e-4。使用未加权精度（UA）和加权精度（WA）评估性能，其中 UA 表示总体精度的平均值，WA 表示正确分类样本的百分比。我们对 IEMOCAP 数据集进行了 5 次交叉验证，与训练和测试集的分裂比为 0.8-0.2。然后，使用最佳模型进行跨语料库验证。这项研究将 Liu 等人的[20]工作作为基线，他们在其中提出了一种用于语音情感识别的局部全局感知深度表示学习方法，并报告了他们的模型以超过 4%的绝对增量优于最先进的模型。此外，为了验证正则化阶段的效果，我们配置了一个没有鉴别器的模型，该模型的注释为表 2 中的 MEC。

## 精读

情绪是一种重要的内在状态，它影响人类的目的表达、意图理解和决策[1]。为了提供有效的服务，一些公司采用语音情感识别技术来跟踪客户的感受[2]。人机交互领域的其他研究人员开发了情感检测系统，以改善用户体验[3]。近几十年来，特征工程[4，5]和深度学习模型[6，7]的发展极大地改善了语音情感识别。由于语音样本具有时间序列特性，许多研究都采用了门控递归单元（GRU）[8]、长短记忆网络（LSTMs）[9]或卷积神经网络（CNN）和 LSTMs 的组合[10]来感知时间序列信息。除了考虑语音中包含的特征序列之外，还提出了可以分析对话中的上下文信息的新模型。Yeh 等人[8]提出了一种交互感知网络，以考虑对话中的上下文。Jiao 等人[11]通过考虑上下文，构建了用于话语级情感识别的分层门控重复单元。上述研究在上下文信息的帮助下显著提高了语音情感识别的性能。为了捕捉最有助于标签分配的情感框架，研究人员采用了注意机制[12]，并借用了多实例学习的思想来识别和关注语音话语中最显著的部分[13]。此外，由于语音中的情感因素因性别和人而异，Latif 等人[14]提出了一种多任务学习框架，以提高语音情感识别的性能。尽管在语音情感识别领域取得了进展，但很少有研究探索数据正则化的好处。众所周知，言语情感表达因说话人而异，这种偏见可能会给同一类别下的样本形成聚类或具有不规则边界（非高斯分布）的聚类带来困难。这种情况可能会使模型容易在域内数据集上过度拟合，对模式分布的局部不规则性敏感，并且跨域实现的得分也会降低[15，16，17]。为了通过正则化潜在表示的分布来平滑边界，并赋予模型专注于最显著的语音片段的能力，在本研究中，我们提出了一种基于多实例学习的对抗性自动编码器分类器。我们提出的模型中的鉴别器可以将潜在表示映射到高维高斯分布。我们还采用了性别和情感分类的多任务学习策略。所提出的模型在 IEMOCAP[18]数据集上进行训练，并在语料库内（IEMOCAP）和跨语料库验证集（MELD[19]）上进行评估。我们的论文结构如下。我们在第 2 节描述了我们提出的方法的细节。第 3 节描述了实验设置和结果。第 4 节介绍了讨论，第 5 节简要总结了我们的工作。

我们使用 openSMILE 工具包[5]基于 Emobase 2010 Config 提取了低级声学特征，包括 Mel 频率倒谱系数（MFCC）、F 0、过零率及其在每个帧中的统计数据。对于每个音频样本，预处理方法产生一个 45 维的低级声学特征。然后，我们使用具有固定宽度（50 ms）和步幅（10 ms）的窗口将特征分成若干段，以便将每个样本转换为一组实例（如等式 1 所示）x[i] = [s 1, s 2, .., sn]。

![]({46}_MAEC%20Multi-Instance%20Learning%20with%20an%20Adversarial%20Auto-Encoder-Based%20Classifier%20for%20Speech%20Emotion%20Recognition@fuMAECMultiInstanceLearning2021.assets/image-20221110092854.png)

图 1 显示了模型的架构。总体结构类似于对抗式自动编码器（AAE），但我们提出的模型只有一个 encoder、一个 discriminator，并用分类器代替 decoder。所提出的模型的输入是用户的前一回合 (the interlocutor’s turn)、对话者的回合 (the user’s current utterance)和用户的当前话语 ( the user’s previous turn )。

编码器的功能是将子样本的前一回合、对话者的回合和当前话语的信息压缩为潜在表征 Z。

首先，我们构建了一个具有[卷积捷径](https://www.cnblogs.com/linzzz98/articles/13454369.html))的空洞卷积块，以处理片段级特征。其中空洞卷积块有两个普通卷积层，和一个空洞卷积层，而卷积捷径仅由普通卷积层组成。然后我们使用全连接层作为过滤信息的 Bottleneck（降低维度，减少参数量）。请注意，空洞卷积块和 Bottleneck 都是逐片段处理特征，并根据原始顺序将其连接为时间序列。

空洞卷积块中每一层的滤波器数量为 512、512 和 128，卷积核大小为 1、3 和 1。空洞卷积层的扩展速率被设置为 2。

bottleneck 的神经元数量被设置为 512，双向 GRU 的神经元数量被设置为 256。

$$
\begin{aligned}
&h_i=\text { DilatedConv }\left(x_i\right) \\
&H=\left[h_1, h_2, \ldots, h_n\right] \\
&b t_i=\text { Bottleneck }\left(h_i\right) \\
&B T=\left[b t_1, b t_2, \ldots, b t_n\right]
\end{aligned}
$$

然后，通过注意力机制得到 HF。其中 Key 部分来自于双向 GRU 处理 H 的输出。Query 部分来自于 Bottleneck 的输出 BT。Value 部分通过对 Key 进行全局池化得到。

$$
\begin{aligned}
&\text { Key }=\operatorname{BiGRU}(H) \\
&A=\text { Attention }(\text { Key, BT }) \\
&K Q=\text { GlobalPooling }(A) \\
&V=G \text { lobalPooling }(\text { Key) } \\
&H F=\text { Multiply }(K Q, V)
\end{aligned}
$$

通过将用户的前一回合、对话者的回合和用户的当前话语放入前述模型中运算。可以得到（分别为 HFpre、HFInt 和 HFcur）三个隐藏层特征，最后融合它们以获得最终的潜在表征 Z。

$$
\begin{aligned}
&P I=\operatorname{Softmax}\left(\operatorname{Multiply}\left(H F_{\text {pre }}, H F_{\text {Int }}\right)\right) \\
&Z=\operatorname{Concatenate}\left(\operatorname{Multiply}\left(HF_{\text {cur }}, PI\right), HF_{cur}\right)
\end{aligned}
$$

在获得潜在表征 Z 之后，我们提出的模型有两个最终阶段：带鉴别器（discriminator）的正则化约束阶段（D）和分类阶段。

在正则化约束阶段，我们构造了一个有两个全连接层的 discriminator，其功能是区分服从高斯分布 N（μ，σ）的 $\mathcal{E}$  （True）和潜在表征 Z（False）。 $\mathcal{E}$ 的维数与潜在表征 Z 相同，这样做的目的是通过学习对更接近真实样本的样本进行编码来愚弄鉴别器。

$$
L_D=\max \left(\mathbb{E}_{\mathcal{E}}[\log D(\mathcal{E})]+\mathbb{E}_Z[\log (1-D(Z))]\right)
$$

在分类阶段，我们借鉴了多任务学习的思想，构建了一个具有两个分支的分类器：性别分类和情感分类。每个分支由一个全连接层组成。Cg 表示性别分类，Ce 表示情感分类。

$$
L_C=\min \left(\mathbb{E}_Z\left[\log C_g(Z)\right]+\mathbb{E}_Z\left[\log \left(C_e(Z)\right)\right]\right)
$$

在训练过程中，编码器和鉴别器首先运行，然后是分类阶段。请注意，编码器在正则化和分类阶段都进行了训练。在这个训练序列中，潜在表示阶段不仅受到正则化阶段的影响，还受到监督分类阶段的影响。

$$
\begin{aligned}
L_{E n c}=& \min \left(\mathbb{E}_X[\log (1-D(\operatorname{Enc}(X)))]\right.\\
&+\mathbb{E}_X\left[\log \left(C_g(\operatorname{Enc}(X))\right)\right] \\
&\left.+\mathbb{E}_X\left[\log \left(C_e(\operatorname{Enc}(X))\right)\right]\right)
\end{aligned}
$$

### 引文

## 摘录

---
title: "MAEC: Multi-Instance Learning with an Adversarial Auto-Encoder-Based Classifier for Speech Emotion Recognition"
description: ""
citekey: fuMAECMultiInstanceLearning2021
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-02-15 14:36:01
lastmod: 2023-04-11 17:51:40
---

> [!info] 论文信息
>1. Title：MAEC: Multi-Instance Learning with an Adversarial Auto-Encoder-Based Classifier for Speech Emotion Recognition
>2. Author：Changzeng Fu, Chaoran Liu, Carlos Toshinori Ishi, Hiroshi Ishiguro
>3. Entry：[Zotero link](zotero://select/items/@fuMAECMultiInstanceLearning2021) [URL link]() [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Fu et al_2021_MAEC.pdf>)
>4. Other：2021 - ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- 

## 摘要

> [!abstract] In this paper, we propose an adversarial auto-encoder-based classifier, which can regularize the distribution of latent representation to smooth the boundaries among categories. Moreover, we adopt multi-instance learning by dividing speech into a bag of segments to capture the most salient moments for presenting an emotion. The proposed model was trained on the IEMOCAP dataset and evaluated on the in-corpus validation set (IEMOCAP) and the cross-corpus validation set (MELD). The experiment results show that our model outperforms the baseline on in-corpus validation and increases the scores on cross-corpus validation with regularization.

> 在本文中，我们提出了一种基于对抗性自动编码器的分类器，该分类器可以正则化潜在表示的分布，以平滑类别之间的边界。此外，我们采用多实例学习，将语音分成一组片段，以捕捉表达情感的最显著时刻。所提出的模型在 IEMOCAP 数据集上进行训练，并在语料库内验证集（IEMOCAP）和跨语料库验证集（MELD）上进行评估。实验结果表明，我们的模型在语料库内验证方面优于基线，并通过正则化提高了跨语料库验证的分数。

## 预处理

## 概述

## 结果

在本研究中，提出的模型在 IEMOCAP 上进行了训练和评估[18]。我们还对 MELD 进行了跨语料库验证[19]。以下描述了数据集。IEMOCAP 包含与十位发言者进行双向对话的视频。每个视频都包含一个二元对话，该对话被分割成话语并用情感标签进行注释。在这项研究中，我们对四个类别进行了情绪分类：“快乐”、“悲伤”、“中性”和“愤怒”。请注意，我们将“快乐”和“兴奋”两个类别合并为“快乐”，将“悲伤”和“沮丧”合并为“悲伤”，就像之前的许多研究一样。

MELD 是一个多方对话数据集，收集自一部美国电视剧。在这项研究中，我们仅从“快乐”、“悲伤”、“中性”和“愤怒”类别中提取样本，以与 IEMOCAP 中的训练集保持一致，“快乐”被视为“快乐”。考虑到使跨语料库验证数据具有与训练集相同的数据结构，我们从 MELD 数据集中选择了 2000 个样本，其中说话者的前一轮和对话者的轮是可用的。

我们随机将 80%的样本分成训练集，20%分成测试集，正如 Liu 等人[20]所做的那样，并进行了 5 次交叉验证。

批次大小为 32。我们选择 Adam 作为优化器，学习率为 1e-4。使用未加权精度（UA）和加权精度（WA）评估性能，其中 UA 表示总体精度的平均值，WA 表示正确分类样本的百分比。我们对 IEMOCAP 数据集进行了 5 次交叉验证，与训练和测试集的分裂比为 0.8-0.2。然后，使用最佳模型进行跨语料库验证。这项研究将 Liu 等人的[20]工作作为基线，他们在其中提出了一种用于语音情感识别的局部全局感知深度表示学习方法，并报告了他们的模型以超过 4%的绝对增量优于最先进的模型。此外，为了验证正则化阶段的效果，我们配置了一个没有鉴别器的模型，该模型的注释为表 2 中的 MEC。

## 精读

情绪是一种重要的内在状态，它影响人类的目的表达、意图理解和决策[1]。为了提供有效的服务，一些公司采用语音情感识别技术来跟踪客户的感受[2]。人机交互领域的其他研究人员开发了情感检测系统，以改善用户体验[3]。近几十年来，特征工程[4，5]和深度学习模型[6，7]的发展极大地改善了语音情感识别。由于语音样本具有时间序列特性，许多研究都采用了门控递归单元（GRU）[8]、长短记忆网络（LSTMs）[9]或卷积神经网络（CNN）和 LSTMs 的组合[10]来感知时间序列信息。除了考虑语音中包含的特征序列之外，还提出了可以分析对话中的上下文信息的新模型。Yeh 等人[8]提出了一种交互感知网络，以考虑对话中的上下文。Jiao 等人[11]通过考虑上下文，构建了用于话语级情感识别的分层门控重复单元。上述研究在上下文信息的帮助下显著提高了语音情感识别的性能。为了捕捉最有助于标签分配的情感框架，研究人员采用了注意机制[12]，并借用了多实例学习的思想来识别和关注语音话语中最显著的部分[13]。此外，由于语音中的情感因素因性别和人而异，Latif 等人[14]提出了一种多任务学习框架，以提高语音情感识别的性能。尽管在语音情感识别领域取得了进展，但很少有研究探索数据正则化的好处。众所周知，言语情感表达因说话人而异，这种偏见可能会给同一类别下的样本形成聚类或具有不规则边界（非高斯分布）的聚类带来困难。这种情况可能会使模型容易在域内数据集上过度拟合，对模式分布的局部不规则性敏感，并且跨域实现的得分也会降低[15，16，17]。为了通过正则化潜在表示的分布来平滑边界，并赋予模型专注于最显著的语音片段的能力，在本研究中，我们提出了一种基于多实例学习的对抗性自动编码器分类器。我们提出的模型中的鉴别器可以将潜在表示映射到高维高斯分布。我们还采用了性别和情感分类的多任务学习策略。所提出的模型在 IEMOCAP[18]数据集上进行训练，并在语料库内（IEMOCAP）和跨语料库验证集（MELD[19]）上进行评估。我们的论文结构如下。我们在第 2 节描述了我们提出的方法的细节。第 3 节描述了实验设置和结果。第 4 节介绍了讨论，第 5 节简要总结了我们的工作。

我们使用 openSMILE 工具包[5]基于 Emobase 2010 Config 提取了低级声学特征，包括 Mel 频率倒谱系数（MFCC）、F 0、过零率及其在每个帧中的统计数据。对于每个音频样本，预处理方法产生一个 45 维的低级声学特征。然后，我们使用具有固定宽度（50 ms）和步幅（10 ms）的窗口将特征分成若干段，以便将每个样本转换为一组实例（如等式 1 所示）x[i] = [s 1, s 2, .., sn]。

![]({46}_MAEC%20Multi-Instance%20Learning%20with%20an%20Adversarial%20Auto-Encoder-Based%20Classifier%20for%20Speech%20Emotion%20Recognition@fuMAECMultiInstanceLearning2021.assets/image-20221110092854.png)

图 1 显示了模型的架构。总体结构类似于对抗式自动编码器（AAE），但我们提出的模型只有一个 encoder、一个 discriminator，并用分类器代替 decoder。所提出的模型的输入是用户的前一回合 (the interlocutor’s turn)、对话者的回合 (the user’s current utterance)和用户的当前话语 ( the user’s previous turn )。

编码器的功能是将子样本的前一回合、对话者的回合和当前话语的信息压缩为潜在表征 Z。

首先，我们构建了一个具有[卷积捷径](https://www.cnblogs.com/linzzz98/articles/13454369.html))的空洞卷积块，以处理片段级特征。其中空洞卷积块有两个普通卷积层，和一个空洞卷积层，而卷积捷径仅由普通卷积层组成。然后我们使用全连接层作为过滤信息的 Bottleneck（降低维度，减少参数量）。请注意，空洞卷积块和 Bottleneck 都是逐片段处理特征，并根据原始顺序将其连接为时间序列。

空洞卷积块中每一层的滤波器数量为 512、512 和 128，卷积核大小为 1、3 和 1。空洞卷积层的扩展速率被设置为 2。

bottleneck 的神经元数量被设置为 512，双向 GRU 的神经元数量被设置为 256。

$$
\begin{aligned}
&h_i=\text { DilatedConv }\left(x_i\right) \\
&H=\left[h_1, h_2, \ldots, h_n\right] \\
&b t_i=\text { Bottleneck }\left(h_i\right) \\
&B T=\left[b t_1, b t_2, \ldots, b t_n\right]
\end{aligned}
$$

然后，通过注意力机制得到 HF。其中 Key 部分来自于双向 GRU 处理 H 的输出。Query 部分来自于 Bottleneck 的输出 BT。Value 部分通过对 Key 进行全局池化得到。

$$
\begin{aligned}
&\text { Key }=\operatorname{BiGRU}(H) \\
&A=\text { Attention }(\text { Key, BT }) \\
&K Q=\text { GlobalPooling }(A) \\
&V=G \text { lobalPooling }(\text { Key) } \\
&H F=\text { Multiply }(K Q, V)
\end{aligned}
$$

通过将用户的前一回合、对话者的回合和用户的当前话语放入前述模型中运算。可以得到（分别为 HFpre、HFInt 和 HFcur）三个隐藏层特征，最后融合它们以获得最终的潜在表征 Z。

$$
\begin{aligned}
&P I=\operatorname{Softmax}\left(\operatorname{Multiply}\left(H F_{\text {pre }}, H F_{\text {Int }}\right)\right) \\
&Z=\operatorname{Concatenate}\left(\operatorname{Multiply}\left(HF_{\text {cur }}, PI\right), HF_{cur}\right)
\end{aligned}
$$

在获得潜在表征 Z 之后，我们提出的模型有两个最终阶段：带鉴别器（discriminator）的正则化约束阶段（D）和分类阶段。

在正则化约束阶段，我们构造了一个有两个全连接层的 discriminator，其功能是区分服从高斯分布 N（μ，σ）的 $\mathcal{E}$  （True）和潜在表征 Z（False）。 $\mathcal{E}$ 的维数与潜在表征 Z 相同，这样做的目的是通过学习对更接近真实样本的样本进行编码来愚弄鉴别器。

$$
L_D=\max \left(\mathbb{E}_{\mathcal{E}}[\log D(\mathcal{E})]+\mathbb{E}_Z[\log (1-D(Z))]\right)
$$

在分类阶段，我们借鉴了多任务学习的思想，构建了一个具有两个分支的分类器：性别分类和情感分类。每个分支由一个全连接层组成。Cg 表示性别分类，Ce 表示情感分类。

$$
L_C=\min \left(\mathbb{E}_Z\left[\log C_g(Z)\right]+\mathbb{E}_Z\left[\log \left(C_e(Z)\right)\right]\right)
$$

在训练过程中，编码器和鉴别器首先运行，然后是分类阶段。请注意，编码器在正则化和分类阶段都进行了训练。在这个训练序列中，潜在表示阶段不仅受到正则化阶段的影响，还受到监督分类阶段的影响。

$$
\begin{aligned}
L_{E n c}=& \min \left(\mathbb{E}_X[\log (1-D(\operatorname{Enc}(X)))]\right.\\
&+\mathbb{E}_X\left[\log \left(C_g(\operatorname{Enc}(X))\right)\right] \\
&\left.+\mathbb{E}_X\left[\log \left(C_e(\operatorname{Enc}(X))\right)\right]\right)
\end{aligned}
$$

### 引文

## 摘录
