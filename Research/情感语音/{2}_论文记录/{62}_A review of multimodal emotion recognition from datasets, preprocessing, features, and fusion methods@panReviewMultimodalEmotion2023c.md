---
title: "A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods"
description: ""
citekey: panReviewMultimodalEmotion2023c
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-12-29 19:29:06
lastmod: 2024-03-15 13:37:24
---

> [!info] 论文信息
>1. Title：A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods
>2. Author：Bei Pan, Kaoru Hirota, Zhiyang Jia, Yaping Dai
>3. Entry：[Zotero link](zotero://select/items/@panReviewMultimodalEmotion2023c) [URL link](https://www.sciencedirect.com/science/article/pii/S092523122300989X) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Pan et al_2023_A review of multimodal emotion recognition from datasets, preprocessing,.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\BGJUS5UN\\S092523122300989X.html>)
>4. Other：2023 - Neurocomputing     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- :fas_question:   
- :obs_pdf_file:   
- :obs_graph_glyph:   
- :obs_wand_glyph:   

## 摘要

> [!abstract] Affective computing is one of the most important research fields in modern human–computer interaction (HCI). The goal of affective computing is to study and develop the theories, methods, and systems that can recognize, explain, process, and simulate human emotions. As a branch of affective computing, emotion recognition aims to enlighten the machine/computer automatically analyzing human emotions, which has received increasing attention from researchers in various fields. Human beings generally observe and understand the emotional states of one person by integrating the perceived information from his/her facial expressions, voice tone, speech content, behavior, or physiological features. To imitate the emotion observation manner of humans, researchers have been devoted to constructing multimodal emotion recognition models by fusing information from two or more modalities. In this paper, we provide a comprehensive review of multimodal emotion recognition from the perspectives of multimodal datasets, data preprocessing, unimodal feature extraction, and multimodal information fusion methods in recent decades. Furthermore, challenges and future research directions of the topic are specified and discussed. The main motivations of this review are to conclude the recent emergence of abundant works on multimodal emotion recognition and to provide potential guidance to researchers in the related field for understanding the pipeline and mainstream approaches to multimodal emotion recognition.

> 情感计算是现代人机交互（HCI）中最重要的研究领域之一。情感计算的目标是研究和开发能够识别、解释、处理和模拟人类情感的理论、方法和系统。情感识别作为情感计算的一个分支，旨在启发机器/计算机自动分析人类情感，越来越受到各领域研究人员的关注。人类通常通过整合来自一个人的面部表情、语气、言语内容、行为或生理特征的感知信息来观察和理解一个人的情绪状态。为了模仿人类的情绪观察方式，研究人员一直致力于通过融合来自两种或多种模态的信息来构建多模态情绪识别模型。在本文中，我们从多模态数据集、数据预处理、单模态特征提取和多模态信息融合方法的角度对多模态情感识别进行了全面的回顾。此外，还指出并讨论了该主题的挑战和未来的研究方向。本综述的主要动机是总结最近出现的关于多模态情感识别的大量工作，并为相关领域的研究人员理解多模态情感识别的流程和主流方法提供潜在的指导。

## 预处理

## 概述

## 结果

## 精读

情感是人类智力的重要组成部分。人类的行为不仅取决于理性思维和逻辑推理，而且很大程度上受到情感的影响。自从人类社会进入信息时代以来，人机交互（HCI）有了相当大的增长。人类的首要需求除了充足的物质需求外，还包括精神层面的满足。为了实现情感人机交互，计算机需要具备观察、理解和产生各种情感的能力。相应地，这些需求催生了情感计算的概念[1]。过去二十年来，心理学、神经生理学、认知科学、计算机科学等学科的研究人员致力于情感计算的研究，并已应用于各个领域。应用领域大致可分为五类：（1）服务业。银行、医院、餐饮、政府服务等行业的机器人为客户提供情感服务，可以改善客户在服务过程中甚至整个服务过程中的体验。 (2)教育。通过监测学生在课堂上的情绪状态或注意力，机器人可以帮助教师和学生分别提高教学质量和学习效率。 (3) 医疗保健。将认知情感计算融入医疗机器人中，辅助医生治疗心理疾病，为患者提供情感安慰。 (4)智能驾驶。将情绪分析和疲劳检测技术融入智能驾驶，有助于预防交通事故。 （五）娱乐业。在电脑游戏中加入情感识别和交互技术，可以构建更加真实的虚拟场景，减轻玩家疲劳，增加游戏娱乐性。

情感识别作为情感计算的重要组成部分受到了相当多的关注。其目的是发现外部情绪表征与内部情绪状态之间的映射关系，以识别当前人类情绪的类型。情感识别论文分布如图 1 所示，检索自 Web of Science、Scopus、Engineering Village。很明显，十年来越来越多的情绪识别相关方法被报道。人类情绪主要可以通过面部表情、言语、文字和生理信号来识别。面部表情是人类传达情感最直接的方式。机器通过处理和分析摄像头采集的面部图像或视频来感知面部表情。语音信号传递的情感不仅包含明确、具体的内容，还包含说话者的声音信息。机器可以通过分析韵律、语音质量特征和文本信息来从语音信号中捕获情感信息。生理信号，例如脑电图（EEG）、肌电图（EMG）和心电图（ECG），是隐含的情绪表达。随着先进无创设备的发展，基于生理信号的情绪识别受到了广泛的研究关注。因此，机器探索生理数据与特定情绪之间的关系是合理的。

由于情绪是通过多种方式表达的，人类很容易通过结合面部表情、言语或其他信息来感知他人的情绪或意图。为了提高机器情感识别的性能，在过去的二十年里，该领域的许多研究致力于融合多模态信息以实现全面、准确的情感识别。从 Web of Science、Scopus 和 Engineering Village 检索到的多模态情感识别出版物的分布如图 2 所示。显然，人们对多模态情感识别的研究越来越感兴趣。视听、语音-文本、视听-文本、视觉-生理或多种生理模式的融合是一个流行的研究方向。在本研究中，我们从多模态情感数据集、数据预处理、单模态特征提取和多模态融合情感识别的角度回顾了最近的多模态情感识别工作。

我们将我们的论文与表 1 中五个属性的多模态情感识别的已发表调查进行比较：(1) 数据集、(2) 数据预处理、(3) 从不同模态提取特征、(4) 融合方法和 (5) 组合的多种方式。从比较中可以清楚地看出，我们的论文是有区别的。具体来说，与大多数关于多模态情感识别的调查都绕回或忽略数据预处理的介绍相比，本文分别介绍了不同单一模态的数据预处理方法。此外，对于特征提取方法，代表了一些最近流行的基于深度学习的情感信息提取技术。此外，现有的大多数评论主要集中在视听多模态融合，并根据融合策略引入多模态情感识别。为了清楚地呈现各种模态的组合，我们从融合模态的新角度全面介绍多模态融合。包含并讨论了九种不同模式的组合，这有助于了解各个模式的具体特征以及各种模式的互补。

本文的其余部分安排如下。第 2 节介绍了情感模型和过程；第 3 节介绍并列出了几个可用的多模态情感数据集；数据预处理方法在第 4 节中描述；第 5 节讨论单峰特征提取方法；第 6 节提供了不同情感识别模式的融合方法。最后，第 7 节讨论了有关多模态情感识别的结论、挑战和未来工作。

### 2. Emotion model and procedure 

#### 2.1. Categorization of emotion models

一般来说，情感模型的类型可以分为离散表示和维度表示。对于离散情绪模型，情绪被描述为六个基本类别，即愤怒、厌恶、恐惧、快乐、悲伤和惊讶，由 Ekman 于 1971 年定义[9]。这六种基本情绪在人类种族和文化中是普遍存在的，并且可以用来复合其他情绪。基本情绪有以下特点：（一）情绪源于本能； (ii) 不同的人在相同的情况下产生相同的情绪； (iii) 不同的人表达的基本情感相似。离散情感表示的显着优点之一是分类情感方案可以描述人们在日常生活中的情感体验。另一个是根据六种情感标签来描述情感是直观的。因此，许多努力致力于离散情感识别。

另一种情感描述是维度情感。一些心理学家和人工智能专家认为情绪可以通过连续的维度来表示。与离散情感相反，维度情感理论将不同的情感定义为维度空间中的点。效价-唤醒（VA）[10]和愉悦-唤醒-支配（PAD）[11]是两种典型且被广泛接受的维度情感模型。对于 VA 模型，效价维度衡量积极和消极的情绪状态，而唤醒维度则表示情绪的强度。 PAD 模型在 VA 模型的基础上增加了支配维度，定义为对周围环境和他人的控制和影响的感觉。图 3 给出了两种典型的 VA 和 PAD 维度情感模型。值得注意的是，离散和维度情感表示可以在某种程度上相互转化。

#### 2.2. Procedures of emotion recognition

单模态和多模态信号都可以参与情感识别相关任务，实现情感分类。由于本文主要关注多模态情感识别，因此将介绍基于多模态的情感识别的一般流程。典型的多模态情感识别系统的流程如图 4 所示。 [12]
一．数据收集：借助图像、音乐、视频等情感刺激，诱导受试者产生情绪。收集并处理所有相关的情感数据以获得情感数据集。
二.数据预处理：消除每种模态数据的噪声或其他干扰。例如，检测并标准化视觉模态中的面部。
三.情感特征提取：设计适当的特征描述符来学习不同模态的代表性特征。
四.情感识别：设计有效的融合策略，整合每种模态的特征或分类分数，以做出最终的情感决策。

### 3. Multimodal emotion datasets

情感识别模型的性能很大程度上取决于情感数据。丰富的标记数据是构建高性能和泛化的情感识别模型的先决条件。情感数据集通常是在实验室控制的环境或野外收集的。实验室收集的情绪可以分为两类：行为情绪和自发情绪。通过要求参与者表达不同的情绪来记录行为情绪数据集，这些情绪通常是夸张的行为，有利于情绪分类。然而，所表现的情绪可能隐藏了外显表达和内隐情绪状态之间的实际关系。因此，研究人员设计了各种环境并创造了情绪诱发材料来唤起和记录外部和内在的反应。情感诱发的途径主要有三种：（1）提供不同的视频、音乐、图像等材料来诱发被试的不同情绪，这是收集情感数据的主要途径。 (2)设计一些模拟场景来回忆被试过去生活中难忘的情感经历。 （3）为主体构建互动场景，他们可以谈论一些产品、电影、歌曲或他们的生活经历，以唤起彼此的情感。与行为情感相比，诱发的情感是自发的，更接近实际交互中表达的情感，有利于实际应用。

eINTERFACE 05 数据集。这是 Martin 等人构建的视听数据集。 [13] 2006 年。受试者被要求听六个连续的短篇故事，每个故事都会引发一种特定的情感。然后，受试者被要求对愤怒、厌恶、恐惧、快乐、悲伤和惊讶等情绪做出反应。每种情绪模拟五种反应。两名专家检查了每个记录的样本，以确定每个样本是否明确表达了所要求的情绪，并保留了来自 14 个不同国家的 42 名受试者。每个视频序列的帧速率等于每秒 25 帧，单声道每个音频样本的采样率为 48 kHz。

RML 数据集。这是 Wang 等人构建的视听数据集。 2008 年[23]。要求参与者根据提供的情绪句子尽可能自然地表达自己的情绪，旨在回忆他们经历过的情绪事件。为了使数据能够用于更广泛的应用，讲六种语言的受试者被邀请参与数据收集，即英语、普通话、乌尔都语、旁遮普语、波斯语和意大利语。为了确保每个受试者都表达出预期的情绪，至少选择了两名不懂相应语言的受试者来测试情绪。此外，当所有受试者检测到预期情绪时，视频样本被添加到数据集中。收集了五百个视频样本，包含六种基本情感。每个剪辑均以 22 050 Hz 的采样率和单通道 16 位数字化进行记录。

BAUM-1 数据集。这是由 Zhalehpour 等人开发的视听数据集。 [29] 2016 年。31 名用土耳其语说话的受试者被要求观看一系列静止图像和短片，这些图像和短片旨在唤起各种情绪和精神状态。然后，他们必须在没有任何指导或脚本的情况下表达他们对屏幕上看到的刺激的感受和想法。该数据集包含六种基本情绪，即愤怒、厌恶、恐惧、快乐、悲伤和惊讶。此外，还包含无聊、轻蔑、迷惑、思考、专注、烦恼、中立等几种心理状态。五位注释者被邀请对数据集中的每个剪辑进行注释并给出分数。最后，五位注释者的多数投票给每个片段一个情感标签。

MELD 数据集。这是 Poria 等人开发的会话数据集。 [33] 2018 年。它包含电视剧《老友记》中 1433 个对话的 13000 个话语。每个话语都涉及音频、视觉和文本模式的数据。邀请三名注释者观看可用的话语视频片段，并采用多数投票的方法给出每个话语的最终情感和情感标签。

IEMOCAP 数据集。这是 Busso 等人构建的视听对话数据集。 [39]2007 年。他们设计了两种不同的方法来诱导和表达情绪。其中一个是基于一组剧本，十名演员被要求记住和排练。另一种方法要求受试者根据旨在引发特定情绪的假设场景即兴发挥。该语料库包含大约 12 小时的数据，记录演员在脚本和自发的口头交流场景中的面部、头部和手部动作。在后处理中，对话在对话轮次级别被手动分段。邀请了六位注释者来评估数据集的情感内容。选择愤怒、厌恶、兴奋、恐惧、沮丧、快乐、悲伤、惊讶和中性状态的情绪作为注释。

SEMAINE 数据集。这是由 McKeown 等人开发的视听数据集。 [44] 2011 年。敏感人工监听器 (SAL) 代理的建立是为了让一个人参与持续的、带有情感色彩的对话。交互包含两方，“用户”和“操作员”（机器或模拟机器的人）。实验参与者与两种版本的 SAL 进行交互，一种具有最好的非语言技能，另一种则具有较差的非语言技能。数据集中记录了 150 名参与者和 959 条对话。每个剪辑有 6-8 个注释器，带有五个维度和 27 个相关类别。

RECOLA 数据集。这是由 Ringeval 等人构建的法语自发交互的多模态语料库。 [46] 2013 年。由于该语料库基于一项专注于远程协作期间情绪感知的研究，因此 46 名参与者被平均分成两个团队。每个团队参与者被要求在单独的房间里单独解决生存任务，并收到一份问卷来评估他们的初始情绪状态。使用情绪诱导技术来平衡协作任务的交互环境。音频、视频、心电图（ECG）和皮肤电活动（EDA）模式的数据被连续、同步记录。此外，六位注释者测量了唤醒和效价维度上的情绪以及五个维度上的社会行为标签。

MAHNOB-HCI。这是由 Soleymani 等人开发的多模态数据集。 [50]2012 年。记录了 27 名参与者的面部视频、音频信号、眼睛注视数据和周围生理信号。为了收集数据，进行了两个实验。在第一个实验中，参与者被要求观看 20 个励志视频，并使用情绪标签以及唤醒度、效价、主导性和可预测性写下他们的情绪状态。在第二个实验中，他们首先观看没有标签的短视频和图像，然后观看带有正确或不正确标签的短视频和图像。最后，参与者用不同的标签来评估他们的同意或不同意。

DEAP 数据集。这是由 Koelstra 等人开发的基于生理信号的情感数据集。 [54] 2012 年。为了引发情感，选取了 40 段一分钟长的音乐视频片段进行播放。记录了 32 名受试者在观看励志视频时的脑电图（EEG）和周围生理信号。此外，还录制了 22 名受试者的正面视频。受试者对每个视频的唤起程度、效价、喜欢/不喜欢、主导地位和熟悉程度进行评价。

### 4. Data preprocessing

数据预处理是多模态情感识别的基本步骤。数据预处理的首要目的是消除无关信息，最大限度地简化数据，增强与情感相关的特征的可检测性，提高特征提取和识别的可靠性。对于多模态数据集，根据不同模态的数据特征分别进行数据预处理。因此，下面分别介绍人脸图像、语音信号和生理信号的数据预处理方法。

4.1. Face image preprocessing

人脸检测、对齐和归一化是人脸图像预处理的三个主要任务。人脸检测是定位图像中的人脸，并根据边界框从图像中分割人脸[59]。经典且最常用的人脸检测方法之一是由 Viola 和 Jones 提出的弱分类器增强级联 [60]。基于深度学习的方法也被开发用于人脸检测，例如级联卷积神经网络（CNN）[61]和基于判别性的完整特征的 CNN[62]。人脸对齐是将检测到的人脸进行旋转和正面处理，以保证不同人脸的面内一致性。面部标志点的坐标对于面部对齐来说是简单且有效的。因此，各种标志性检测器，例如主动外观模型（AAM）[63]和多任务级联 CNN（MTCNN）[64]，被设计用于精确检测。面部归一化的重点是消除光照变化、头部姿势和其他对面部表情识别的影响[65]。直方图均衡、伽玛强度校正（G​​IC）[66]和同态滤波器[67]是图像归一化的典型算法。对于姿态归一化，可以通过生成对抗网络（GAN）很好地解决[68]。


4.2. Speech signal preprocessing

原始语音信号是非平稳的，但可以看作在短时间内不变。因此，在预处理的第一阶段，将语音信号分成长度为 20 至 30 ms 的若干段。然后，对每一帧应用窗函数以减少能量泄漏并获得更接近自然频谱的信号。汉明窗、矩形窗和汉宁窗用于语音加窗。话语包含浊音，无声的言语和沉默。有声语音反映了说话过程中的言语活动，包含与情感相关的信息。有必要检测有声信号并去除无声和无声帧[25]。在语音活动检测中，过零率、短时能量和自相关是三种常用方法。语音信号预处理的最后一步是消除或减少噪声。为了去除背景噪声，谱减法、最小均方误差 (MMSE) 和对数谱幅度 MMSE 是常用的方法 [69]。

4.3. Text preprocessing

文本预处理的主要任务之一是将字符流分离成一组类似单词的元素[70]。在预处理过程中应特别注意标点符号、空格和表情符号等字符。具体来说，在特征学习之前去除标点符号是很常见的，这对于分析速度和模型性能的提高至关重要。在情感分析中，空间被认为是单词之间的边界，没有任何意义。因此，大多数情况下通常会删除不必要的空格。表情符号是描绘一些面部表情的键盘字符，例如微笑和皱眉。这些字符有助于区分情感极性，并且通常被翻译成话语中的相应单词。其他文本预处理组件（例如大写字母转换、首字母缩略词扩展、拼写纠正和短词删除）对于高性能情感识别也很重要。

4.4. Physiological signal preprocessing

收集的生理数据通常包含噪声，干扰信号并阻碍有效的情绪相关特征提取。因此，在特征学习之前需要对生理信号进行降噪处理。高频滤波器、低频滤波器、陷波滤波器和巴特沃斯带通滤波器通常用于降噪[71]。大脑中包含情绪活动的 Delta (0.5–4 Hz)、theta (4–8 Hz)、alpha(8–13 Hz)、beta(13–30 Hz) 和 gamma(30–43 Hz) 频段会被过滤采用巴特沃斯滤波器分别从不同的生理信号中提取。此外，还采用主成分分析（PCA）、独立成分分析（ICA）和公共空间模式（CSP）等常用方法来消除生理信号的伪影和噪声。

### 5. Emotion feature extraction

根据模态的使用，情感识别可以大致分为两种类型，即单模态情感识别和多模态情感识别。单模态情感识别方法通常采用单通道，例如人脸图像、语音信号、文本和生理信号来对不同的情绪状态进行分类。多模态情感识别利用两个或多个情感通道来综合分析情感。情感特征提取是单模态和多模态情感识别的重要组成部分，因为不同的特征可以促进精确的结果。单模态情感识别的特征学习方法可以用于多模态情感识别。多模态情感识别中根据单一模态的特点分别提取特征。因此，在下面的小节中，将详细介绍和讨论不同模态的特征提取方法。

5.1. Facial expression features

面部表情传达了有关他人的情绪、感受、意图和身体状态的重要信息。自动面部情绪识别（FER）旨在检测、分析和理解人类的面部表情，是基础研究的重要焦点。对面部表情的详尽调查发表在[72-77]中。在大多数多模态情感识别研究中，面部表情是一个重要组成部分，在提供外观信息方面发挥着至关重要的作用。 FER 和面部表情相关的多模态情感识别最关键的任务之一是提取面部表情特征以进行情感分类。特征提取旨在从图像中学习和提取有区别的情感信息。面部表情特征提取的方法分为基于浅层学习的方法和基于深度学习的方法。因此，将按类别简要介绍各种面部表情特征提取的文献。

基于浅层学习或手工制作的面部表情特征已广泛用于 FER。一般来说，手工制作的特征可以分为三类：几何、外观和运动[78]。几何特征是指面部标志的轨迹，可以表现为基准点、面部网格、眼睛和嘴巴周围点之间位移的活动形状模型变化[79-82]。外观特征表示皮肤纹理的变化，代表性的外观特征描述符是局部二值模式（LBP）[83]、梯度直方图（HOG）[84]、Gabor 小波[85,86]等。运动特征包括光流[87]和运动历史图像（MHI）[88]。

上述特征描述符旨在从单个图像中提取静态特征，而沿时域的动态信息会丢失。开发了几种动态特征提取器，以更好地理解面部表情动态变化的感知，以从具有有序帧的图像序列中捕获时空特征。例如，结合运动和外观的体积局部二值模式（VLBP）、三个正交平面上的局部二值模式（LBP-TOP）[89]以及三个正交平面的定向梯度直方图（HOG-TOP） [90]。作为静态纹理描述符的扩展，通过这些时空纹理描述符可以很好地跟踪和计算面部的动态事件。在[91]中，通过地标跟踪框架和参数空间利用来自几何和纹理特征的动态信息。他们的实验证明，与静态特征相比，动态特征可以改善面部表情识别，并且对视频序列中一些不受控制的变化更加鲁棒。

考虑到在真实场景中，收集的视频序列会附加各种干扰，例如姿势变化、主体差异和动态背景。人们努力学习稳健且通用的面部表情表示。萨里亚尼迪等人。 [92]设计了一个动态特征提取框架，将面部表情变化表示为局部基函数的线性组合。设计的面部表情表示不仅可以识别不同强度的表情，还可以处理不同数据集中存在的时间不一致问题。在[93]中，作者提出了一种基于动态内核的表示，该表示在大型通用高斯混合模型（uGMM）中同化使用局部时空表示捕获的面部运动。通过动态核，可以获得面部各个部分的局部表示，并且不同表情的动态变化是不同的，以便进行分类。

随着深度学习（DL）的进步[94,95]，由于其强大的特征学习能力而被广泛用于深度面部特征提取[96-100]。通过使用分层架构，基于深度学习的面部表情特征描述符可以从图像中提取高级情感信息[101]。在面部表情特征学习中，CNN 经常被用来提取局部面部特征，通过使用一组用于识别重要模式或特征的过滤器。一般来说，基于深度学习的方法的性能在很大程度上依赖于数据集的规模。为了获得高水平的判别特征，需要丰富多样的面部表情训练数据。然而，大多数用于 FER 的数据集规模较小，这很难进行高效的深度学习模型训练。为了解决这个问题，通过微调预训练的深度 CNN 模型（例如 AlexNet、ResNet 和 VGG）中的参数，将迁移学习应用于 FER，这些模型是在大规模数据集上训练的 [102,103]。对预训练模型的某些层进行微调不仅可以降低计算成本，而且可以获得独特的面部表情特征。

近年来，GAN 被广泛应用于面部表情识别任务中，用于数据增强并减少与情绪无关的变量的负面影响。在[104]中，基于 GAN 的框架旨在扩展训练数据集，并从图像中分离出表情、身份和姿势，以促进判别性特征提取。类似地，在[105]中，提出了一种由几何信息引导的基于 GAN 的结构，以生成具有不同姿势和表情的身份保留人脸图像。通过生成标记的面部表情图像来扩大训练数据集，使模型能够有效地学习不同情绪的特征。

凭借发现图像中显着区域的能力，注意力机制已嵌入到深度学习模型中，根据特征在面部表情识别任务中的重要性对特征进行加权[106]。为了减少头部姿势变化、遮挡和低图像分辨率的影响，Liu 等人。 [107]使用“视觉注意”机制从不受约束的环境中的图像中学习基于显着性引导的面部斑块的深层独特表达特征。基于注意力的方法侧重于从野外收集的图像中学习显着特征，这可以抑制不相关变量的影响并增加面部表情特征的辨别力。

为了模拟面部表情的动态演化，循环神经网络 (RNN)、长短期记忆 (LSTM)、门控循环单元 (GRU) 和 3 维 CNN (3 D-CNN) 已被应用于学习面部表情的时间关系。有序图像序列[108,109]。此外，已经构建了许多时空特征提取框架来从单个图像中学习空间特征并捕获帧的动态变化[110,111]。考虑到并非所有帧对于时空特征提取都同样重要，在[112]中，提出了帧注意网络来自适应地组装帧特征以形成单个不同的表示。对于单帧特征，首先通过全连接层和 softmax 函数学习自注意力权重。然后，通过对两个框架之间的关系进行建模来对其进行细化。

大多数面部表情特征是在同一分布数据库上提取和评估的，这是一个具有挑战性的问题，这可能使得对分布在不同域上的特征进行分类变得困难。为了解决跨域问题，Zong 等人。 [113]提出了一种转导转移回归模型（TTRM）来弥合源域和目标域之间的特征分布差距。 TTRM 可以获得具有区分性的表情特征，并且能够量化不同面部局部区域的贡献。通过分析学习复杂性的影响，Xia 等人。 [114]提出了一种循环卷积网络（RCN）来探索较浅的架构和较低分辨率的输入数据。陈等人。 [115]构建了一个对抗性图表示适应（AGRA）框架来实现有效的跨域局部-全局特征协同适应。结果，获得了领域不变的特征和更详细的内容来区分不同的表达。

5.2. Speech emotion features

言语是表达情绪状态的自然而有效的媒介。人们已经进行了各种研究来提取情感信息并从语音信号中预测和分析人类情感[116-118]。值得注意的是，语音信号通常与面部表情或文本结合在一起，是多模态情感识别的重要组成部分。声学特征提取是语音情感识别（SER）和语音相关的多模态情感识别中的一项重要且具有挑战性的任务。韵律、语音质量和频谱特征（称为手工声学特征）已被深入探索并用于 SER。此外，凭借学习能力强的优势，深度学习已被应用于探索有区别的深层语音情感特征[119,120]。例如，CNN 通常用于通过分析信号的频谱图从语音信号中提取局部声学特征，例如频率内容随时间的变化。

一般来说，不同说话者表达的情感语音数据在声学特性上表现出很大的差异，即使他们想要表达相同的情感。因此，提取不依赖于说话人的区分性和与说话人无关的特征具有重要意义[121-124]。在[125]中，提出了两层模糊多重随机森林来提取语音情感特征。非个性化特征是通过基本声学特征的推导并与个性化特征融合而获得的。在[126]中，使用 OpenSmile 工具箱中的语音特征和高阶频谱特征开发了一种独立于主题的方法。设计了一种粒子群优化辅助的基于生物地理学的优化（PSOBBO）算法来去除不相关和冗余的特征。这种特征选择策略可以有利于获取显着的语音情感特征。


与视频剪辑中的图像帧类似，话语中语音片段之间的动态信息对于区分情绪至关重要。因此，为了更好地对语音情感进行分类，对片段之间的时间依赖性进行建模并获得动态语音表示具有重要意义。凭借捕获上下文信息的能力，LSTM 经常与 CNN 结合，在各种研究中得到探索。通常，CNN 用于从片段中提取特征，LSTM 网络用于捕获片段特征的时间动态依赖性[127,128]。根据实验结果，这些研究表明，与单独使用 CNN 相比，CNN 和 LSTM 的配合可以增加语音特征的辨别力。最近，基于注意力机制的网络已被开发用于语音特征学习[129,130​​]。在[131]中，双向 LSTM（BLSTM）中添加了自注意力机制来计算两个帧之间的相似度并自动为帧分配权重。注意力机制的引入有助于网络发现显着的情感成分并学习代表性特征。

在大多数情况下，当测试集的分布接近训练集时，SER 会取得良好的性能。然而，如果训练集和测试集具有不同的分布，则可能会得到不满意的识别结果。为了解决这个问题，一些研究人员致力于研究能够同时获取多个数据集并构建更强大的 SER 模型的跨语料库和跨语言学习[132-135]。领域自适应表示学习[136]和领域对抗性神经网络[137,138]在跨语料库和跨语言 SER 中很流行。前者试图最小化源域和目标域之间的差异。后者侧重于学习共同特征表示。 Song 和 Zheng[139]提出了一种称为基于特征选择的转移子空间学习（FSTSL）的框架来学习鲁棒的低维语料库不变特征表示并提高跨语料库 SER 模型的泛化能力。此外，Song [140]开发了一种迁移线性子空间学习（TLSL）框架来学习源和目标语料库的共同特征子空间。在这些研究中，获得了鲁棒的语料库不变特征表示，提高了跨语料库语音情感的辨别力。

5.3. Text features

文本信息与语音中的声学特征相结合在多模态情感识别中起着至关重要的作用。语法分析和语义分析是文本特征提取的两个重要内容。与面部表情和语音模态类似，文本特征表示方法可以大致分为传统方法和基于深度学习的方法。词袋（BOW）、基于规则的技术和统计方法是典型的传统文本特征提取方法[141]。基于情感词典，Jin 等人。 [142]设计了一种称为 eVector 的文本情感表示，它与 BOW 结合以获得词汇特征表示。

由于强大的特征学习能力，深度学习已被广​​泛应用于学习文本的情感表示。苏等人。 [143]提出从 word 2 vec 模型中提取情感词向量，并采用自动编码器来获取瓶颈特征。最终的文本特征是通过将语义词向量中的特征和瓶颈特征连接在一起获得的。刘等人。 [144]提出了一种结合 BOW 和 CNN 的 CBOW 方法来学习文本情感特征。具体来说，首先设计基于前馈神经网络的 CBOW 模型用于文本的向量表示，然后训练 CNN 用于语义特征学习。 LSTM 广泛用于从文本信息中提取不同的情感特征，以学习话语中的上下文依赖性。王等人。 [145]提出了一种树结构的 CNN-LSTM 模型，从句子中提取局部和全局信息。他们训练 CNN 模型从分割的文本区域而不是整个文本中学习有益的情感信息。然后，通过 LSTM 顺序整合区域信息，以学习整个句子之间的长距离依赖关系。黄等人。 [146]设计了情感增强 LSTM（ELSTM），通过引入情绪智力和注意力机制来提高 LSTM 的特征学习能力。

5.4. Physiological signal emotion features

...


5.5. Discussion

在本节中，讨论不同模态的流行特征提取方法。尽管不同模态的数据类型各不相同，但可以得出结论，这些模态存在相似的特征提取机制。首先，手工制作（或浅层）和基于深度学习（或深度）的特征是这些模式中的常见类别。研究人员根据数据和情感表达的特点，为每种模态设计了各种手工制作的特征描述符。手工特征提取方法具有易于理解、计算简单、内存消耗低等优点。尽管越来越多强大的基于 DL 的方法被提出并应用于深度情感特征提取。对于某些模式，例如语音和脑电图信号，手工特征仍然是基础，并且在基于深度学习的高级语义特征学习中发挥着至关重要的作用。

最近，CNN 和 RNN 等深度神经网络已成功用于所有模态的判别性情感特征提取。对于高级代表性特征学习，语音和脑电图信号显示为图像并输入深度模型，以利用 CNN 的分层学习。除了使用 CNN 学习空间特征之外，还开发了不同模态的 3 D-CNN、CNN-LSTM 和图卷积网络，用于从序列数据学习时空特征。由于其能够专注于关键部分，因此在特征提取过程中使用注意力机制来识别情感区域和通道。也可以将某些注意力模块从一种模式转移到另一种模式。此外，在从不同模态提取特征时，还考虑了跨域的特征变化。已经开发了许多域适应策略来处理源域和目标域之间的变化。

### 6. Multimodal emotion recognition

多模态情感识别在情感计算中引起了越来越多的关注[161]。为了让计算机更好地理解人类的情感，有必要模仿人类观察情感的方式。人类通过综合分析交互过程中面部表情、声音、话语内容、手势等所呈现的信息来判断他人的情绪。因此，收集与情绪表达相关的信息并融合不同的模态有助于计算机全面识别情绪。值得注意的是，尽管不同模态的数据形式是异构的，但语义的内在一致性使得多种模态的协作能够获得更令人信服的情感识别模型[162]。特别是，在[163]中，作者通过理论处理讨论了多模态联合何时以及为何优于单模态。在前面的章节中，我们讨论了个体模态的情感特征提取方法。在本节中，将详细讨论多模态情感识别的最先进方法。


视频、音频、文本和生理信号是多模态情感识别中常用的模态。此外，在过去的十年中，人们提出了各种单独模态的组合，例如音频和视频、语音和文本，以及用于情感识别的多种生理信号融合。融合方法一般分为特征级融合、决策级融合和混合融合。特征级融合首先连接不同模态的特征，然后训练分类器进行情感分类。在决策级融合中，不同模态的分类器分别进行训练，并将分类结果融合为最终决策。混合融合集成了特征级融合和决策级融合。图 5 显示了用于情感识别的多模态融合方法，其中选择音频、视频和文本模态作为示例。


6.1. Classifier

决定潜在情感的分类器在多模态情感识别中起着至关重要的作用。已经实现了各种用于情感识别的分类器，例如支持向量机（SVM）、随机森林（RF）、人工神经网络（ANN）等等。由于每个分类器都有自己的优点和局限性，因此很难决定哪个分类器最适合情感分类。因此，研究人员通常根据具体任务或情绪特征的特点来选择或设计情绪分类器。一般来说，分类器的特征分离能力极大地影响情感识别性能。作为情感识别中最常用的分类器，SVM 的目标是定位能够准确分离各种特征组的超平面。由于大多数情感特征无法线性分离，SVM 涉及到线性、多项式、高斯等核函数，将特征转换到高维空间进行线性分离。在基于深度学习的情感识别中，高级特征通过网络末端的损失层进行分离。损失函数极大地影响分类精度。最小化预测概率和真实地面分布的交叉熵的 softmax 损失是 CNN 模型中最常用的函数 [75,164]。


6.2. Audio-visual emotion recognition

音频和视觉模态是多模态情感识别研究界中最常用的两种模态。音频模态拥有与情绪强度相关的重要信息。相比之下，视觉模态描述了图像序列的表达，其中包含丰富的外观信息。这两种方式包含了近 90% 的显性情感信息，并且相辅相成。因此，通过这两种方式的整合有利于情感识别性能的提高。许多研究都集中在融合音频和视觉方式的情感信息。本节按照三种融合策略介绍近年来代表性的视听情感识别方法。表 4 列出了融合音频和视觉信息进行情感识别的不同方法的比较。

6.2.1. Feature-level fusion

由于深度信念网络（DBN）能够有效地学习输入数据的高级关系，因此它已被用来融合音频和视觉特征并学习特征表示。在[25]中，执行语音活动检测（VAD）来区分音频帧是否显着，并为相应面部表情帧的显着帧和语音帧分配 0 或 1 的权重。通过 CNN 模型从预处理的数据中提取面部和声音的情感特征，然后通过 DBN 进行融合。虽然 eNTERFACE 数据集上单一模态的平均准确度为 69.8%，但所提出的融合方法产生的准确度明显更高，达到 85.69%。张等人。 [24]提出了一种深度混合模型来弥合情感和视听特征之间的情感差距。首先将音频和视频信号分割成固定帧，然后使用 CNN 和 3 D-CNN 分别学习音频和视觉片段特征。最后，将输出特征连接起来并放入 DBN 中，以学习有区别的视听片段特征表示。皮尼等人。 [15]提出了一种由三个子网络组成的多模态深度学习架构。 2 维 (2 D) 和 3 D 网络是针对静态面部特征和动态模式。 LSTM 应用于音频分支来捕获音频特征的时间演变。从三个网络训练的特征被连接并发送到全连接层以进行最终分类。

同样，在[17]中，音频和视觉特征是通过 2 D 和 3 D CNN 模型分别学习的。采用极限学习机 (ELM) 的非线性融合方法来融合从两个 CNN 模型获得的特征。他们设计了包含两个 ELM 模型的两级 ELM。第一个 ELM 进行性别分离训练，训练后移除最后一层。第一个 ELM 的隐藏层被输入到第二个 ELM 中进行情感分类。这样就可以根据性别来识别情绪，从本质上消除了性别的影响，提高了准确性。为了利用不同模式的依赖性和关系，Ghaleb 等人。 [14]提出了一种度量学习方法来共同获得潜在空间中的判别分数和鲁棒表示。开发的框架具有可扩展性，因为它可以在不施加任何限制的情况下学习特定于模态的指标。此外，所提出的距离的基本原理是直观的，这有利于根据单一模态的重要性进行模型解释。坎西佐格鲁等人。 [26]提出了一种在线自主情感识别范式来利用面部和语音情感信息。为了区分时间变化下的不同情绪状态，融合的特征被传送到 LSTM 层，并由强化学习代理进行监控。音频和视觉特征由深度神经网络（DNN）融合。单峰特征的平均准确率约为 44%，而融合特征在 BAUM 数据集上的平均准确率约为 58%。

阮等人。 [20]开发了一个通用框架，级联 3 维 CNN (C 3 D) 和 DBN，以从音频和视觉模态中提取时空特征。他们采用多模态紧凑双线性池（MCB）来捕获两种模态之间复杂且内在的关联。单峰特征集的平均准确率达到 83.09%，而音频和视觉的融合实现了约 7% 的提升。此外，当应用 MCB 时，准确率从 89.39% 提高到 90.85%，这进一步支持了融合策略的有效性。黄等人。 [165]使用变压器模型来融合音频和视觉模式。在该框架中，采用多头注意力从公共语义特征空间产生多模态的中间表示。他们发现音频和视觉特征的顺序会影响识别准确性。结果表明，左侧的音频和右侧的视频可以提高性能，因为视觉特征被捕获为主要部分。这一观察结果进一步表明，视觉特征在情感识别中优于音频特征。

这些方法侧重于将视听特征与基于神经网络的方法融合。生成了高级判别性融合表示，但单一模态的具体特征丢失了。在[47]中，引入了交叉注意融合方法来编码模态间信息并保留模态内特征。这种融合策略通过计算视听模态的交叉注意权重来探索模态间关系。来自两种模态的显着特征表示被组合并馈送到完全连接的层以进行效价和唤醒预测。 RECOLA 数据集上单一模式的效价和唤醒的平均准确度分别为 55.25% 和 70.2%，而所提出的模型的准确度明显更高，分别为 69% 和 83.8%。

6.2.2. Decision-level fusion

在[167]中，遗传算法（GA）被用来学习最适合语音系统的 HMM 结构。对于视觉系统，采用 PCA 进行降维和情感识别。遗传算法还旨在优化人工神经网络的结构，提高视觉系统的性能。语音和视觉系统的识别概率的加权和产生了最终的识别结果。诺鲁齐等人。 [18]提出通过 k-means 聚类定义每个视频的一组关键帧。使用几何变形和基于 CNN 的模型从选定的关键帧描述视觉特征。使用音调、强度和梅尔频率倒谱系数 (MFCC) 等音频特征集计算 88 维特征向量。针对每种模态训练 SVM 分类器，并使用获得的置信度输出来定义最终预测的新特征空间。范等人。 [168]开发了一种结合 CNN-RNN 和 C 3 D 的混合网络，以同时对外观和运动特征进行建模。 SVM 模型针对音频模态进行训练，并通过加权求和将三个模型的预测结果结合起来。

在[169]中，分别设计了三个网络来从视觉和音频模态中学习情感特征。时空纹理特征和动态几何特征分别从 C 3 D 和 CNN-LSTM 网络中提取。从基于音频的网络中提取了可以补充基于图像的网络的局限性的声学特征。此外，他们通过给定的验证数据集测量每种情绪的识别准确性，提出了一种情绪自适应融合策略。作为提高分类性能的有力工具，集成学习已应用于决策级融合情感识别[170]。传统的集成规则包括最大值、最小值、总和、平均值和乘积，它们用于组合多个分类器。在[171]中，作者利用简单的加权平均值，提出了一种基于随机超参数搜索聚合模型的新方法。在[16]中，手动和深度特征都是从音频和视觉模式中分开学习的。他们设计了混合集成算法来融合 SVM 和多任务 CNN 分类器的分类结果，以进行最终的情感决策。这种融合方法的准确率明显高于音频 (56.33%) 和视觉 (66.93%) 方式，达到 81.36%。从这些方法可以得出结论：与特征级融合相比，决策级融合策略保留了单一模态的特征，但忽略了模态间的关系。

6.2.3. Hybrid fusion

混合融合结合了特征级和决策级的优点。保留不同模态的独特属性，并探索模态之间的相关性。然而，其局限性在于模型的复杂性可能会增加。贝贾尼等人。 [21]通过结合面部表情和言语情感信息来模拟人类的情感状态。提取韵律特征、MFCC 和共振峰频率作为声音特征。集成时间运动图像（ITMI）和量化图像矩阵（QIM）图像用于面部表情特征提取。然后，分别训练基于音频和视觉的情感分类器以进行单峰情感识别。为了实现特征融合，将从两种模态提取的所有特征进行级联，然后通过变化分析进行选择。最终的识别结果是通过结合特征和决策层融合的优点获得的。这种混合融合方法在 eNTERFACE 数据集上的准确率达到 77.78%，与单独使用音频 (54.99%) 或视觉 (39.27%) 方式相比，这是一个显着的改进。在[166]中，探索并融合了三种互补的线索，即面部标志性动作、面部纹理和音频信号，用于从野外收集的视频剪辑中进行情感识别。混合融合应用于多线索融合框架，其中动态面部特征被级联以进行特征级融合，并且视觉和音频的分类结果被集成以进行决策级融合。他们的结果表明，通过整合来自视听模式的多种线索，可以提高野外情绪识别性能。

6.3. Audio, visual and text modalities fusion

....

6.4. Multiple physiological signals fusion

....

6.5. Other multimodal fusion

....

6.6. Discussion

从所有有关多模态情感识别的研究文献中，可以得出结论，通过适当的融合策略，多模态情感识别的准确性优于单模态情感识别。由于列出的研究之间存在多种差异，因此直接比较研究之间的准确性是不合适的。为了保证合理性，研究水平因素应保持不变[5]。因此，多模态情感识别与单模态情感识别的准确性比较是在同一研究中进行的，而不是跨研究进行的。经计算，从单模态到多模态融合，准确率从 2.35%提高到 19.73%，其中单模态准确率是所有使用模态的平均值。这一结果表明，通过适当地融合不同的模态，可以提高情感识别性能，因为不同的模态是相互补充的。

所有提出的多模态情感识别方法都努力从不同角度提高性能，例如，考虑到个体模态特征的独特特征表示、利用多种模态互补性的精心设计的融合策略以及具有强大识别能力的分类器。区分类别。这些方法的优点已经凸显，但它们在可靠性、鲁棒性和效率方面各自的局限性也不容忽视。例如，在特征级融合中，忽略了不同信号的异构性，特征级联的可靠性需要进一步讨论。此外，跨领域的情感识别已经引起了一些关注，但如何深入挖掘多模态融合的潜力，同时解决跨学科、跨会话、跨文化的问题仍然需要付出很大的努力。

7. Conclusions and future work

多模态情感识别引起了情感识别界越来越多的关注。本文从多模态情感数据集、数据预处理、单模态特征提取和多模态信息融合等方面回顾了近年来多模态情感识别的有前景的研究。可以得出结论，对于情感特征提取，仍然使用和开发经典和先进的手工描述符来提取判别性情感特征。这些特征提取方法对于情感识别的发展至关重要，因为它们结合了先验知识并使模型具有解释能力。随着深度学习技术的发展，它已被广泛应用于从不同模态（尤其是面部图像和文本）中提取高级情感特征。

此外，融合策略在高性能多模态情感识别中起着至关重要的作用。基于深度学习的算法已被开发用于融合特征并学习判别性表示。最近出现了一些新的概念和策略，旨在探索不同模式之间的关系和互补性，并保持各个模式的独特特征。对于情感识别，传统的基于机器学习的分类器仍然是不同融合框架的主流，例如 SVM、RF、ELM 等。此外，基于深度学习的端到端情感识别框架不断出现，它可以直接将分类结果输出到给定的输入。

多模态情感识别方法在各个领域具有广泛的潜在应用，包括人机交互（HCI）、教育、心理学和神经科学。在人机交互系统中，它有潜力创建更自然、更有效的人机界面，能够理解和响应用户的情绪状态，增强虚拟代理的情绪表达能力。在教育领域，多模态情感识别可用于开发智能辅导系统，能够适应教师和学生的情绪状态，提供个性化反馈。在心理学和神经科学中，多模态情绪识别可以深入了解情绪处理背后的神经机制，并有助于心理障碍的诊断和治疗，例如识别情绪障碍患者情绪调节的微妙变化。通过跨多种方式识别情绪，它有可能改变我们对人类沟通和互动方式的理解。

尽管已经为多模态情感识别的发展做出了许多贡献，但仍然存在一些需要解决的关键问题，总结如下：

- 基础理论研究。专注于探索代表性特征以进行准确的情绪分类或预测的情绪识别相关技术已经得到了很好的发展。相比之下，情感计算中显式表达与隐式信息之间的关系需要进一步研究。探索这种关系对于理解不同信号所代表的各种情绪状态具有重要意义。情感识别技术应与认知技术相结合，以弥合人类客观情感识别与主观情感认知之间的距离。
- 关于野外自发情绪的多模态情绪数据集。目前的多模态数据集主要包括在受控实验室中收集的行为或自发情绪。然而，约束环境的噪音较小，真实的隐性情绪可能会受到抑制。因此，当应用在所需数据集上训练的模型来识别真实场景中的情绪时，性能受到限制。因此，为了便于多模态情感识别系统在真实情况下使用，应该收集更多接近现实生活的自发情感数据。
- 特征提取方法。各种外部或内部因素可能会阻碍情感相关特征的提取。外部因素包括环境噪声、传感器噪声、拍摄角度和文化差异。个人性格差异是主要的内部因素。然而，当前的多模态情感识别研究在设计特征提取器时几乎没有考虑外部和内部因素。如何消除这些不相关的元素并提取本质的情感相关特征仍然没有解决。去除这些不相关的变量并找到内在的情感特征对于提高情感识别性能具有重要意义。
- 多模态融合策略。最近的研究显示了使用基于深度学习的技术进行特征级融合的明确趋势。然而，这些方法忽略了单一模式的具体特征。模态之间的相互关系和单个模态的具体特征对于识别情感至关重要。因此，如何探索不同模态之间的关系并保留单一模态的独特属性有待解决。此外，利用和分析各种模态的相关性和特征有助于设计多模态情感识别的有效融合策略。
- 对话中的多模态情感识别。谈话占据了我们生活和工作的大部分时间。然而，很少有研究人员关注识别对话中的情绪。一个人所表达的情绪往往是由谈话内容或其他人引起或影响的，很难定量分析。识别对话中的情绪是一项挑战，因为需要考虑各种因素，例如个人情绪、背景和他人的影响。如果对话中的情绪识别得到很好的解决，将有助于多模态情绪识别的进步和应用。

### 引文

## 摘录
