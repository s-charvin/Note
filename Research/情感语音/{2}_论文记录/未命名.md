---
title: "{{title}}"
description: ""
author: ""
tags: [""]
categories: ""
keywords:  [""]
draft: true
layout: ""
date: 2024-03-16 08:44:19
lastmod: 2024-03-16 09:08:40
---

![]({16}_Efficient%20Speech%20Emotion%20Recognition%20Using%20Multi-Scale%20CNN%20and%20Attention@pengEfficientSpeechEmotion2021.assets/image-20220418092230.png)


![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426110552.png)


![]({30}_The%20Role%20of%20Task%20and%20Acoustic%20Similarity%20in%20Audio%20Transfer%20Learning_%20Insights%20from%20the%20Speech%20Emotion%20Recognition%20Case@triantafyllopoulosRoleTaskAcoustic2021.assets/image-20220603181014.png)


![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604100107.png)

## ⭐ 重点

- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果

## 摘要

> [!abstract] While emotion recognition is a well-studied task, it remains unexplored to a large extent in cross-lingual settings. Speech Emotion Recognition (SER) in low-resource languages poses difficulties as existing approaches for knowledge transfer do not generalize seamlessly. Probing the learning process of generalized representations across languages, we propose a meta-learning approach for low-resource speech emotion recognition. The proposed approach achieves fast adaptation on a number of unseen target languages simultaneously. We evaluate the Model Agnostic Meta-Learning (MAML) algorithm on three low-resource target languages -Persian, Italian, and Urdu. We empirically demonstrate that our proposed method - MetaSER1, considerably outperforms multitask and transfer learning-based methods for speech emotion recognition task, and discuss the benefits, efficiency, and challenges of MetaSER on limited data settings.

> 虽然情绪识别是一项已经研究得很好的任务，但在很大程度上，它在跨语言环境中仍未被探索。低资源语言中的语音情感识别(SER)带来了困难，因为现有的知识转移方法不能无缝地推广。探讨了跨语言广义表征的学习过程，提出了一种用于低资源语音情感识别的元学习方法。该方法同时实现了对多种不可见目标语言的快速适应。我们在三种低资源目标语言波斯语、意大利语和乌尔都语上对模型不可知元学习(MAML)算法进行了评估。实验证明，我们提出的方法MetaSER1在语音情感识别任务中的性能明显优于基于多任务和迁移学习的方法，并讨论了MetaSER在有限数据环境下的优势、效率和挑战。

## ⭐ 重点

- 引入音频、文本、视频三模态改进情感识别任务
- 引入注意力机制来融合多模态特征信息
- 提出了一种attentive modality hopping process（注意模态跳跃过程），以其他模态为条件，利用注意力机制和前面计算后的特征向量（或经注意后的特征向量）在当前模态中聚合重要的特征信息。

## 摘要

> [!abstract] In this work, we explore the impact of visual modality in addition to speech and text for improving the accuracy of the emotion detection system. The traditional approaches tackle this task by independently fusing the knowledge from the various modalities for performing emotion classification. In contrast to these approaches, we tackle the problem by introducing an attention mechanism to combine the information. In this regard, we first apply a neural network to obtain hidden representations of the modalities. Then, the attention mechanism is defined to select and aggregate important parts of the video data by conditioning on the audio and text data. Furthermore, the attention mechanism is again applied to attend the essential parts of speech and textual data by considering other modalities. Experiments are performed on the standard IEMOCAP dataset using all three modalities (audio, text, and video). The achieved results show a significant improvement of 3.65% in terms of weighted accuracy compared to the baseline system.

> 在这项工作中，我们探索除了语音和文本之外，还有视觉模态对情绪检测系统的影响，以提高情绪检测系统的准确性。传统的方法通过独立地融合来自各种模态的知识来处理这一任务，以执行情感分类。与这些方法不同，我们通过引入注意力机制来结合信息来解决这个问题。在这一点上，我们首先应用神经网络来获得模态的隐藏表征。然后，定义了注意机制，通过对音频和文本数据的条件化处理来选择和聚合视频数据的重要部分。此外，注意机制再次被应用于通过考虑其他模态来注意基本的词性和文本数据。在标准 IEMOCAP 数据集上使用所有三种模态(音频、文本和视频)进行了实验。结果表明，与基线系统相比，加权精度有3.65%的显着提高。



![]({36}_Attentive%20Modality%20Hopping%20Mechanism%20for%20Speech%20Emotion%20Recognition@yoonAttentiveModalityHopping2020.assets/image-20220605170157.png)

## ⭐ 重点

- 本文目标是增强CCC，因此使用CCC损耗代替MSE损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。- 本文目标是增强 CCC，因此使用 CCC 损耗代替 MSE 损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。

## 摘要

> [!abstract] Due to its ability to accurately predict emotional state using multimodal features, audiovisual emotion recognition has recently gained more interest from researchers. This paper proposes two methods to predict emotional attributes from audio and visual data using a multitask learning and a fusion strategy. First, multitask learning is employed by adjusting three parameters for each attribute to improve the recognition rate. Second, a multistage fusion is proposed to combine results from various modalities' final prediction. Our approach used multitask learning, employed at unimodal and early fusion methods, shows improvement over single-task learning with an average CCC score of 0.431 compared to 0.297. A multistage method, employed at the late fusion approach, significantly improved the agreement score between true and predicted values on the development set of data (from [0.537, 0.565, 0.083] to [0.68, 0.656, 0.443]) for arousal, valence, and liking.

> 视听情绪识别由于能够利用多模态特征准确预测情绪状态，近年来受到研究者的广泛关注。本文提出了两种基于多任务学习和融合策略的基于视听数据的情感属性预测方法。首先，通过调整每个属性的三个参数来进行多任务学习，以提高识别率。其次，提出了一种多阶段融合的方法，将不同模态的最终预测结果进行融合。我们的方法使用了多任务学习，在单峰unimodal（单模态）和早期融合方法中使用，显示出比单任务学习更好的结果，平均Ccc得分为0.431分，而单任务学习方法为0.297分。在晚期融合方法中采用的多阶段方法显著提高了发展数据集上的真实值和预测值之间的一致性分数(从[0.537，0.565，0.083]提高到[0.68，0.656，0.443])，包括性唤醒、效价和喜好。



![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203900.png)


![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203900.png)


![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203950.png)

## 摘要

> [!abstract] Speech emotion recognition is a challenging research topic that plays a critical role in human-computer interaction. Multimodal inputs further improve the performance as more emotional information is used. However, existing studies learn all the information in the sample while only a small portion of it is about emotion. The redundant information will become noises and limit the system performance. In this paper, a key-sparse Transformer is proposed for efficient emotion recognition by focusing more on emotion related information. The proposed method is evaluated on the IEMOCAP and LSSED. Experimental results show that the proposed method achieves better performance than the state-of-the-art approaches.

> 语音情感识别是一个极具挑战性的研究课题，在人机交互中起着至关重要的作用。随着更多的情感信息被使用，多通道输入进一步改善了性能。然而，现有的研究学习了样本中的所有信息，而只有一小部分是关于情绪的。冗余信息会成为噪声，限制系统的性能。本文通过更多地关注情感相关信息，提出了一种基于密钥稀疏变换的情感识别算法。在IEMOCAP和LSSED上对该方法进行了评估。实验结果表明，该方法取得了比现有方法更好的性能。


![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612184832.png)


![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185158.png)


![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185801.png)

## ⭐ 重点

- 引入了一个可学习的前端语音特征提取器

## 摘要

> [!abstract] In this work, we propose a novel approach for multi-modal emotion recognition from conversations using speech and text. The audio representations are learned jointly with a learnable audio front-end (LEAF) model feeding to a CNN based classifier. The text representations are derived from pre-trained bidirectional encoder representations from transformer (BERT) along with a gated recurrent network (GRU). Both the textual and audio representations are separately processed using a bidirectional GRU network with self-attention. Further, the multi-modal information extraction is achieved using a transformer that is input with the textual and audio embeddings at the utterance level. The experiments are performed on the IEMOCAP database, where we show that the proposed framework improves over the current state-of-the-art results under all the common test settings. This is primarily due to the improved emotion recognition performance achieved in the audio domain. Further, we also show that the model is more robust to textual errors caused by an automatic speech recognition (ASR) system.

> 在这项工作中，我们提出了一种新的基于语音和文本的多模态情感识别方法。音频表示与馈送到基于 CNN 的分类器的可学习音频前端(叶)模型联合学习。文本表示是从来自变压器的预先训练的双向编码器表示(BERT)和门控递归网络(GRU)导出的。使用具有自我注意的双向 GRU 网络分别处理文本和音频表示。此外，多模态信息提取是使用在发声级别与文本和音频嵌入一起输入的转换器来实现的。实验在 IEMOCAP 数据库上进行，实验表明，在所有常见测试设置下，所提出的框架比目前最先进的结果有所改善。这主要是由于在音频域中实现了改进的情感识别性能。此外，我们还表明，该模型对自动语音识别(ASR)系统引起的文本错误具有更强的鲁棒性。


![]({39}_Multimodal%20Transformer%20with%20Learnable%20Frontend%20and%20Self%20Attention%20for%20Emotion%20Recognition@duttaMultimodalTransformerLearnable2022.assets/image-20220613162838.png)


![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709111517.png)

## ⭐ 重点

- 

## 摘要

> [!abstract] We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.

> 本文提出了一个通过对抗过程评估（ estimating）生成式模型的新框架。在此框架中，通过同时训练两个模型：一个用来捕获数据分布的生成模型 G，以及一个估计样本是否来自训练数据概率的判别模型 D（注意： G 的训练过程是最大化 D 出错的概率）。该框架类似于（corresponds to）一个最小最大化的博弈过程（two-player game）。对于任意的生成函数 G 和判别函数 D 的解空间，存在唯一解，即生成模型 G 能够恢复（模拟）训练数据分布，而判别模型 D 的输出恒等于$1/2$。如果 G 和 D 都定义为多层感知机，则整个系统可以通过反向传播算法进行训练。在此框架中，无论是在训练阶段，或是样本生成阶段，不需要任何马尔可夫链或滚动展开的近似推理网络（approximate inference net）。本文的实验部分，通过对 G 生成的样本数据，进行了定性和定量评估，证明了框架的潜力。

## ⭐ 重点

- 

## 摘要

> [!abstract] In this paper, we propose an adversarial auto-encoder-based classifier, which can regularize the distribution of latent representation to smooth the boundaries among categories. Moreover, we adopt multi-instance learning by dividing speech into a bag of segments to capture the most salient moments for presenting an emotion. The proposed model was trained on the IEMOCAP dataset and evaluated on the in-corpus validation set (IEMOCAP) and the cross-corpus validation set (MELD). The experiment results show that our model outperforms the baseline on in-corpus validation and increases the scores on cross-corpus validation with regularization.

> 在本文中，我们提出了一种基于对抗性自动编码器的分类器，该分类器可以正则化潜在表示的分布，以平滑类别之间的边界。此外，我们采用多实例学习，将语音分成一组片段，以捕捉表达情感的最显著时刻。所提出的模型在 IEMOCAP 数据集上进行训练，并在语料库内验证集（IEMOCAP）和跨语料库验证集（MELD）上进行评估。实验结果表明，我们的模型在语料库内验证方面优于基线，并通过正则化提高了跨语料库验证的分数。




![]({46}_MAEC%20Multi-Instance%20Learning%20with%20an%20Adversarial%20Auto-Encoder-Based%20Classifier%20for%20Speech%20Emotion%20Recognition@fuMAECMultiInstanceLearning2021.assets/image-20221110092854.png)

## 摘要

> [!abstract] Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.

> 深度卷积网络在静止图像的视觉识别领域取得了巨大成功。然而，对于视频中的动作识别，这些传统方法的优势并不明显。本文旨在研究(discover the principles)如何为视频动作识别设计高效的卷积网络架构，并在有限的训练样本下学习这些模型。我们的第一个贡献是 temporal segment network（TSN），一种基于长时间结构建模(long-range temporal structure modeling)的思想, 用于视频动作识别的新框架。它结合了稀疏时间采样(sparse temporal sampling)策略和视频层级监督，以实现使用整个动作视频的高效学习。另一个贡献是我们在 TSN 网络的帮助下, 对视频数据上学习卷积网络 s 的一系列优秀的实践研究。我们的方法在 HMDB51（69.4%）和 UCF101（94.2%）的数据集上获得了最先进的性能。同时, 我们还将学习到的卷积网络模型进行了可视化，定性地证明了 TSN 网络的有效性和实践结果。



给定一个视频 $V$ ，我们将其分成 $K$ 个时长相等的视频段 ${S_1，S_2，···，S_K}$ 。然后，TSN 网络对此视频段序列的建模如下：

$$
\operatorname{TSN}\left(T_1, T_2, \cdots, T_K\right)=\mathcal{H}\left(\mathcal{G}\left(\mathcal{F}\left(T_1 ; \mathbf{W}\right), \mathcal{F}\left(T_2 ; \mathbf{W}\right), \cdots, \mathcal{F}\left(T_K ; \mathbf{W}\right)\right)\right)
$$

这里的 $\left(T_1, T_2, \cdots, T_K\right)$ 是一个视频帧序列，其中每个帧片段 $T_k$ 随机取样自对应的视频段 $S_k$ 。 $\mathcal{F}\left(T_k ; \mathbf{W}\right)$ 是表示带有参数 $\mathbf{W}$ 的卷积网络函数，该函数对视频帧 $T_k$ 进行计算，并生成类别评分。分段共识(Segmental consensus)函数 $\mathcal{G}$ 会将来自多个视频段的输出组合在一起，以获得其中类别预测的共识。基于这一共识，预测函数 $\mathcal{H}$ (如 Softmax 函数)可以计算出整个视频中每个类别的概率。结合标准分类交叉熵 (standard categorical cross-entropy) 损失函数,  通过分段共识 $\mathbf{G}=\mathcal{G}\left(\mathcal{F}\left(T_1 ; \mathbf{W}\right), \mathcal{F}\left(T_2 ; \mathbf{W}\right), \cdots, \mathcal{F}\left(T_K ; \mathbf{W}\right)\right)$ ，可以得到最终损失函数公式为：

$$
\mathcal{L}(y, \mathbf{G})=-\sum_{i=1}^C y_i\left(G_i-\log \sum_{j=1}^C \exp G_j\right),
$$

其中 $C$ 是类别的数量， $y_i$ 是关于类 $i$ 的真实标签。在实际实验中，根据之前 $[16,17]$ 的研究，将视频段的数量 $K$ 设置为 3。而共识函数 $\mathcal{G}$ ，在本文工作中我们使用了最简单的形式：聚合函数 $g$ （平均、最大值、加权平均），即 $G_i=$ $g\left(\mathcal{F}_i\left(T_1\right), \ldots, \mathcal{F}_i\left(T_K\right)\right)$ 。

## 摘要

> [!abstract] We present Reversible Vision Transformers, a memory efficient architecture design for visual recognition. By decoupling the GPU memory footprint from the depth of the model, Reversible Vision Transformers enable memory efficient scaling of transformer architectures. We adapt two popular models, namely Vision Transformer and Multiscale Vision Transformers, to reversible variants and benchmark extensively across both model sizes and tasks of image classification, object detection and video classification. Reversible Vision Transformers achieve a reduced memory footprint of up to 15.5× at identical model complexity, parameters and accuracy, demonstrating the promise of reversible vision transformers as an efficient backbone for resource limited training regimes. Finally, we find that the additional computational burden of recomputing activations is more than overcome for deeper models, where throughput can increase up to 3.9 × over their non-reversible counterparts. Code and models are available at https://github.com/facebookresearch/mvit.

> 我们介绍了 Reversible Vision Transformers，这是一种用于视觉识别的内存高效架构设计。通过将 GPU 内存需求与模型的深度分离，Reversible Vision Transformers 可以通过高效的内存使用来扩展架构。我们将两种流行的模型（即 Vision Transformer 和 Multiscale Vision Transformers）应用于 reversible 变体，并在模型大小和图像分类、对象检测和视频分类任务中广泛进行基准测试。 Reversible Vision Transformers 在大致相同的模型复杂度、参数和准确性下实现了高达 15.5 倍的内存占用减少，证明了 Reversible Vision Transformers 作为硬件资源有限训练条件下的的有效解决方案。最后，我们发现重新计算 activations 的额外计算负担对于更深的模型来说是可以克服的，其中吞吐量可以比不可逆模型增加 2.3 倍。


![]({52}_Reversible%20Vision%20Transformers@mangalamReversibleVisionTransformers2022.assets/image-20230223111716.png)


![]({52}_Reversible%20Vision%20Transformers@mangalamReversibleVisionTransformers2022.assets/image-20230223124305.png)

## ⭐ 重点

- :fas_question:   超参数选择问题和**训练数据库小样本问题**
- :obs_pdf_file:   在这篇文章中，作者提出了两种不同的方法来帮助 DL 网络更有效地收敛，即添加智能训练样本和参考样本方法。
- :obs_graph_glyph:   在这篇文章中, 作者使用了一种贪心算法来选择智能训练样本和参考样本. 这种算法可以应用于训练和参考样本的选择. 通过比较随机选择和基于贪心算法的选择策略, 作者发现基于贪心算法的选择策略对于训练样本和参考样本都具有显著优势.
- :obs_wand_glyph:   通过选择最具代表性, 信息量更丰富的训练样本和参考样本来提高模型的效率和准确性。

## 摘要

> [!abstract] Deep learning (DL) has attracted more and more attention in computational electromagnetism. Particularly, the convolutional neural network (CNN) is one of the most popular learning models in DL due to its excellent capacity for feature extraction and convergence. The efficiency of CNN mainly depends on how many training samples are needed to effectively converge the network. The sample preparation process often involves a lot of numerical computations, which can be very expensive and time-consuming. In this article, based on the traditional DL network training procedure, two different approaches, namely adding smart training samples and reference samples, are proposed to help the DL network converge. The smart sample selection is based on a greedy algorithm, which can be applied for both training and reference samples. The influences of these two approaches on the CNN training process are investigated by an example of the coupled magneto-thermal computation applied to a transformer. Numerical results show that the two proposed approaches can significantly help the network to converge and improve the efficiency of the DL model.

> 深度学习 (DL) 在电磁学计算领域引起了越来越多的关注。特别是，卷积神经网络 (CNN) 是深度学习中最流行的学习模型之一，因为它具有出色的特征提取和收敛能力。 CNN 的效率主要取决于网络收敛需要多少有效训练样本。这些样本的制作过程通常涉及大量数值计算，这可能非常昂贵且耗时。在本文中，基于传统的深度学习网络训练过程，提出了两种不同的方法，即在训练过程中添加智能选择的训练样本和参考样本，以帮助深度学习网络更好的收敛。智能样本选择基于贪心算法，可应用于获取训练样本和参考样本。本文通过变压器耦合磁热计算任务研究了这两种方法对 CNN 训练过程的影响。数值结果表明，本文所提出的两种方法可以显著的帮助网络收敛并提高深度学习模型的训练效率。



![]({54}_Training%20Sample%20Selection%20Strategy%20Applied%20to%20CNN%20in%20Magneto-Thermal%20Coupled%20Analysis@gongTrainingSampleSelection2021.assets/image-20230313150837.png)

## 摘要

> [!abstract] Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs. The code will be released at https://github.com/OpenGVLab/InternImage.

> 与近年来大规模视觉转换器 (ViTs) 的巨大进步相比，基于卷积神经网络 (CNN) 的大规模模型仍处于早期状态。这项工作提出了一种新的基于 CNN 的大规模基础模型，称为 InternImage，它可以从增加参数和训练数据（如 ViTs）中获得收益。不同于最近关注大密集核的 CNN，InternImage 以**可变形卷积**为核心算子，使我们的模型不仅具有检测和分割等下游任务所需的大有效感受野，而且具有自适应空间聚合以输入和任务信息为条件。因此，所提出的 InternImage 减少了传统 CNN 的严格归纳偏差，并使得从 ViT 等海量数据中学习具有大规模参数的更强、更鲁棒的模式成为可能。我们模型的有效性在具有挑战性的基准测试中得到了证明，包括 ImageNet、COCO 和 ADE20K。值得一提的是，InternImage-H 在 COCO test-dev 上取得了 65.4 mAP 的新纪录，在 ADE20K 上取得了 62.9 mIoU 的新纪录，优于当前领先的 CNN 和 ViT。



![]({58}_InternImage_%20Exploring%20Large-Scale%20Vision%20Foundation%20Models%20with%20Deformable%20Convolutions@wangInternImageExploringLargeScale2023.assets/image-20230605193829.png)

## 摘要

> [!abstract] Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.

> 由于其构建模块中的固定几何结构，卷积神经网络 (CNN) 本质上仅限于模拟几何变换。在这项工作中，我们引入了两个新模块来增强 CNN 的转换建模能力，即可变形卷积和可变形 RoI 池化。两者都是基于这样的想法，即在没有额外监督的情况下，使用额外的偏移量来增加模块中的空间采样位置，并从目标任务中学习偏移量。新模块可以很容易地替换现有 CNN 中的普通模块，并且可以通过标准反向传播轻松地进行端到端训练，从而产生可变形的卷积网络。广泛的实验验证了我们方法的性能。我们首次表明，在深度 CNN 中学习密集空间变换对于复杂的视觉任务（例如对象检测和语义分割）是有效的。代码发布在 https://github.com/msracver/Deformable-ConvNets。



![]({59}_Deformable%20Convolutional%20Networks@daiDeformableConvolutionalNetworks2017.assets/image-20230606131501.png)


![]({59}_Deformable%20Convolutional%20Networks@daiDeformableConvolutionalNetworks2017.assets/image-20230606140134.png)

## 摘要

> [!abstract] Accurate segmentation of topological tubular structures, such as blood vessels and roads, is crucial in various fields, ensuring accuracy and efficiency in downstream tasks. However, many factors complicate the task, including thin local structures and variable global morphologies. In this work, we note the specificity of tubular structures and use this knowledge to guide our DSCNet to simultaneously enhance perception in three stages: feature extraction, feature fusion, and loss constraint. First, we propose a dynamic snake convolution to accurately capture the features of tubular structures by adaptively focusing on slender and tortuous local structures. Subsequently, we propose a multi-view feature fusion strategy to complement the attention to features from multiple perspectives during feature fusion, ensuring the retention of important information from different global morphologies. Finally, a continuity constraint loss function, based on persistent homology, is proposed to constrain the topological continuity of the segmentation better. Experiments on 2 D and 3 D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods. Our codes will be publicly available.

> 血管、道路等拓扑管状结构的精确分割在各个领域都至关重要，可确保下游任务的准确性和效率。然而，许多因素使任务变得复杂，包括薄的局部结构和可变的全局形态。在这项工作中，我们注意到管状结构的特殊性，并利用这些知识来指导我们的 DSCNet 在三个阶段同时增强感知：特征提取、特征融合和损失约束。首先，我们提出了一种动态蛇卷积，通过自适应地关注细长和曲折的局部结构来准确捕获管状结构的特征。随后，我们提出了一种多视图特征融合策略，以补充特征融合过程中多角度对特征的关注，确保保留来自不同全局形态的重要信息。最后，提出了一种基于持久同源性的连续性约束损失函数，以更好地约束分割的拓扑连续性。 2 D 和 3 D 数据集上的实验表明，与多种方法相比，我们的 DSCNet 在管状结构分割任务上提供了更好的准确性和连续性。我们的代码是公开可用的 1。


![]({60}_Dynamic%20Snake%20Convolution%20based%20on%20Topological%20Geometric%20Constraints%20for%20Tubular%20Structure%20Segmentation@qiDynamicSnakeConvolution2023.assets/image-20231024095610.png)
