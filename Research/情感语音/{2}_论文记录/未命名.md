---
title: "{{title}}"
description: ""
author: ""
tags: [""]
categories: ""
keywords:  [""]
draft: true
layout: ""
date: 2024-03-16 08:44:19
lastmod: 2024-03-16 09:00:03
---

![]({16}_Efficient%20Speech%20Emotion%20Recognition%20Using%20Multi-Scale%20CNN%20and%20Attention@pengEfficientSpeechEmotion2021.assets/image-20220418092230.png)


![]({19}_Speech%20Emotion%20Recognition%20Considering%20Nonverbal%20Vocalization%20in%20Affective%20Conversations@hsuSpeechEmotionRecognition2021.assets/image-20220426110552.png)


![]({30}_The%20Role%20of%20Task%20and%20Acoustic%20Similarity%20in%20Audio%20Transfer%20Learning_%20Insights%20from%20the%20Speech%20Emotion%20Recognition%20Case@triantafyllopoulosRoleTaskAcoustic2021.assets/image-20220603181014.png)


![]({33}_Domain-Adversarial%20Autoencoder%20with%20Attention%20Based%20Feature%20Level%20Fusion%20for%20Speech%20Emotion%20Recognition@gaoDomainAdversarialAutoencoderAttention2021a.assets/image-20220604100107.png)

## ⭐ 重点

- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果- 使用 Meta-Learning 方法从高资源语言数据库学习网络参数设置，然后在低资源语言数据库中微调训练，缓解低资源问题。
- 比较了迁移学习，多任务学习，元学习三者在本文结构中的效果

## 摘要

> [!abstract] While emotion recognition is a well-studied task, it remains unexplored to a large extent in cross-lingual settings. Speech Emotion Recognition (SER) in low-resource languages poses difficulties as existing approaches for knowledge transfer do not generalize seamlessly. Probing the learning process of generalized representations across languages, we propose a meta-learning approach for low-resource speech emotion recognition. The proposed approach achieves fast adaptation on a number of unseen target languages simultaneously. We evaluate the Model Agnostic Meta-Learning (MAML) algorithm on three low-resource target languages -Persian, Italian, and Urdu. We empirically demonstrate that our proposed method - MetaSER1, considerably outperforms multitask and transfer learning-based methods for speech emotion recognition task, and discuss the benefits, efficiency, and challenges of MetaSER on limited data settings.

> 虽然情绪识别是一项已经研究得很好的任务，但在很大程度上，它在跨语言环境中仍未被探索。低资源语言中的语音情感识别(SER)带来了困难，因为现有的知识转移方法不能无缝地推广。探讨了跨语言广义表征的学习过程，提出了一种用于低资源语音情感识别的元学习方法。该方法同时实现了对多种不可见目标语言的快速适应。我们在三种低资源目标语言波斯语、意大利语和乌尔都语上对模型不可知元学习(MAML)算法进行了评估。实验证明，我们提出的方法MetaSER1在语音情感识别任务中的性能明显优于基于多任务和迁移学习的方法，并讨论了MetaSER在有限数据环境下的优势、效率和挑战。

## ⭐ 重点

- 引入音频、文本、视频三模态改进情感识别任务
- 引入注意力机制来融合多模态特征信息
- 提出了一种attentive modality hopping process（注意模态跳跃过程），以其他模态为条件，利用注意力机制和前面计算后的特征向量（或经注意后的特征向量）在当前模态中聚合重要的特征信息。

## 摘要

> [!abstract] In this work, we explore the impact of visual modality in addition to speech and text for improving the accuracy of the emotion detection system. The traditional approaches tackle this task by independently fusing the knowledge from the various modalities for performing emotion classification. In contrast to these approaches, we tackle the problem by introducing an attention mechanism to combine the information. In this regard, we first apply a neural network to obtain hidden representations of the modalities. Then, the attention mechanism is defined to select and aggregate important parts of the video data by conditioning on the audio and text data. Furthermore, the attention mechanism is again applied to attend the essential parts of speech and textual data by considering other modalities. Experiments are performed on the standard IEMOCAP dataset using all three modalities (audio, text, and video). The achieved results show a significant improvement of 3.65% in terms of weighted accuracy compared to the baseline system.

> 在这项工作中，我们探索除了语音和文本之外，还有视觉模态对情绪检测系统的影响，以提高情绪检测系统的准确性。传统的方法通过独立地融合来自各种模态的知识来处理这一任务，以执行情感分类。与这些方法不同，我们通过引入注意力机制来结合信息来解决这个问题。在这一点上，我们首先应用神经网络来获得模态的隐藏表征。然后，定义了注意机制，通过对音频和文本数据的条件化处理来选择和聚合视频数据的重要部分。此外，注意机制再次被应用于通过考虑其他模态来注意基本的词性和文本数据。在标准 IEMOCAP 数据集上使用所有三种模态(音频、文本和视频)进行了实验。结果表明，与基线系统相比，加权精度有3.65%的显着提高。



![]({36}_Attentive%20Modality%20Hopping%20Mechanism%20for%20Speech%20Emotion%20Recognition@yoonAttentiveModalityHopping2020.assets/image-20220605170157.png)

## ⭐ 重点

- 本文目标是增强CCC，因此使用CCC损耗代替MSE损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。- 本文目标是增强 CCC，因此使用 CCC 损耗代替 MSE 损失函数
- 比较了多任务学习（计算多个维度损失值的总损失）、单任务学习（计算单一维度损失值的损失）在单模态、多模态模型中的效果，得出了多任务学习更好些。
- 比较了早期融合方法和晚期融合方法。
- 在多任务学习中，提出了一种多阶段融合的方法，提升了最终效果。

## 摘要

> [!abstract] Due to its ability to accurately predict emotional state using multimodal features, audiovisual emotion recognition has recently gained more interest from researchers. This paper proposes two methods to predict emotional attributes from audio and visual data using a multitask learning and a fusion strategy. First, multitask learning is employed by adjusting three parameters for each attribute to improve the recognition rate. Second, a multistage fusion is proposed to combine results from various modalities' final prediction. Our approach used multitask learning, employed at unimodal and early fusion methods, shows improvement over single-task learning with an average CCC score of 0.431 compared to 0.297. A multistage method, employed at the late fusion approach, significantly improved the agreement score between true and predicted values on the development set of data (from [0.537, 0.565, 0.083] to [0.68, 0.656, 0.443]) for arousal, valence, and liking.

> 视听情绪识别由于能够利用多模态特征准确预测情绪状态，近年来受到研究者的广泛关注。本文提出了两种基于多任务学习和融合策略的基于视听数据的情感属性预测方法。首先，通过调整每个属性的三个参数来进行多任务学习，以提高识别率。其次，提出了一种多阶段融合的方法，将不同模态的最终预测结果进行融合。我们的方法使用了多任务学习，在单峰unimodal（单模态）和早期融合方法中使用，显示出比单任务学习更好的结果，平均Ccc得分为0.431分，而单任务学习方法为0.297分。在晚期融合方法中采用的多阶段方法显著提高了发展数据集上的真实值和预测值之间的一致性分数(从[0.537，0.565，0.083]提高到[0.68，0.656，0.443])，包括性唤醒、效价和喜好。



![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203900.png)


![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203900.png)


![]({37}_Multitask%20Learning%20and%20Multistage%20Fusion%20for%20Dimensional%20Audiovisual%20Emotion%20Recognition@atmajaMultitaskLearningMultistage2020.assets/image-20220605203950.png)

## 摘要

> [!abstract] Speech emotion recognition is a challenging research topic that plays a critical role in human-computer interaction. Multimodal inputs further improve the performance as more emotional information is used. However, existing studies learn all the information in the sample while only a small portion of it is about emotion. The redundant information will become noises and limit the system performance. In this paper, a key-sparse Transformer is proposed for efficient emotion recognition by focusing more on emotion related information. The proposed method is evaluated on the IEMOCAP and LSSED. Experimental results show that the proposed method achieves better performance than the state-of-the-art approaches.

> 语音情感识别是一个极具挑战性的研究课题，在人机交互中起着至关重要的作用。随着更多的情感信息被使用，多通道输入进一步改善了性能。然而，现有的研究学习了样本中的所有信息，而只有一小部分是关于情绪的。冗余信息会成为噪声，限制系统的性能。本文通过更多地关注情感相关信息，提出了一种基于密钥稀疏变换的情感识别算法。在IEMOCAP和LSSED上对该方法进行了评估。实验结果表明，该方法取得了比现有方法更好的性能。


![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612184832.png)


![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185158.png)


![]({38}_Key-Sparse%20Transformer%20for%20Multimodal%20Speech%20Emotion%20Recognition@chenKeySparseTransformerMultimodal2022.assets/image-20220612185801.png)

## ⭐ 重点

- 引入了一个可学习的前端语音特征提取器

## 摘要

> [!abstract] In this work, we propose a novel approach for multi-modal emotion recognition from conversations using speech and text. The audio representations are learned jointly with a learnable audio front-end (LEAF) model feeding to a CNN based classifier. The text representations are derived from pre-trained bidirectional encoder representations from transformer (BERT) along with a gated recurrent network (GRU). Both the textual and audio representations are separately processed using a bidirectional GRU network with self-attention. Further, the multi-modal information extraction is achieved using a transformer that is input with the textual and audio embeddings at the utterance level. The experiments are performed on the IEMOCAP database, where we show that the proposed framework improves over the current state-of-the-art results under all the common test settings. This is primarily due to the improved emotion recognition performance achieved in the audio domain. Further, we also show that the model is more robust to textual errors caused by an automatic speech recognition (ASR) system.

> 在这项工作中，我们提出了一种新的基于语音和文本的多模态情感识别方法。音频表示与馈送到基于 CNN 的分类器的可学习音频前端(叶)模型联合学习。文本表示是从来自变压器的预先训练的双向编码器表示(BERT)和门控递归网络(GRU)导出的。使用具有自我注意的双向 GRU 网络分别处理文本和音频表示。此外，多模态信息提取是使用在发声级别与文本和音频嵌入一起输入的转换器来实现的。实验在 IEMOCAP 数据库上进行，实验表明，在所有常见测试设置下，所提出的框架比目前最先进的结果有所改善。这主要是由于在音频域中实现了改进的情感识别性能。此外，我们还表明，该模型对自动语音识别(ASR)系统引起的文本错误具有更强的鲁棒性。


![]({39}_Multimodal%20Transformer%20with%20Learnable%20Frontend%20and%20Self%20Attention%20for%20Emotion%20Recognition@duttaMultimodalTransformerLearnable2022.assets/image-20220613162838.png)


![]({40}_Scaling%20Up%20Your%20Kernels%20to%2031x31_%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs@dingScalingYourKernels2022.assets/image-20220709111517.png)


## ⭐ 重点

- 

## 摘要

> [!abstract] We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.

> 本文提出了一个通过对抗过程评估（ estimating）生成式模型的新框架。在此框架中，通过同时训练两个模型：一个用来捕获数据分布的生成模型 G，以及一个估计样本是否来自训练数据概率的判别模型 D（注意： G 的训练过程是最大化 D 出错的概率）。该框架类似于（corresponds to）一个最小最大化的博弈过程（two-player game）。对于任意的生成函数 G 和判别函数 D 的解空间，存在唯一解，即生成模型 G 能够恢复（模拟）训练数据分布，而判别模型 D 的输出恒等于$1/2$。如果 G 和 D 都定义为多层感知机，则整个系统可以通过反向传播算法进行训练。在此框架中，无论是在训练阶段，或是样本生成阶段，不需要任何马尔可夫链或滚动展开的近似推理网络（approximate inference net）。本文的实验部分，通过对 G 生成的样本数据，进行了定性和定量评估，证明了框架的潜力。

