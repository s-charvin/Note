---
title: "An Attention Pooling based Representation Learning Method for Speech Emotion Recognition"
description: ""
citekey: liAttentionPoolingBased2018
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
---

> [!info] 论文信息
>1. Title：An Attention Pooling based Representation Learning Method for Speech Emotion Recognition
>2. Author：Pengcheng Li, Yan Song, Ian Vince McLoughlin, Wu Guo, Li-Rong Dai
>3. Entry：[Zotero link](zotero://select/items/@liAttentionPoolingBased2018) [URL link](http://dx.doi.org/10.21437/Interspeech.2018-1242) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Li et al_2018_An Attention Pooling based Representation Learning Method for Speech Emotion.pdf>)
>4. Other：2018 -   International Speech Communication Association Hyderabad, India  -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***


## ⭐ 重点

- 

## 摘要

> [!abstract] This paper proposes an attention pooling based representation learning method for speech emotion recognition (SER). The emotional representation is learned in an end-to-end fashion by applying a deep convolutional neural network (CNN) directly to spectrograms extracted from speech utterances. Motivated by the success of GoogleNet, two groups of filters with different shapes are designed to capture both temporal and frequency domain context information from the input spectrogram. The learned features are concatenated and fed into the subsequent convolutional layers. To learn the final emotional representation, a novel attention pooling method is further proposed. Compared with the existing pooling methods, such as max-pooling and average-pooling, the proposed attention pooling can effectively incorporate class-agnostic bottom-up, and class-specific top-down, attention maps. We conduct extensive evaluations on benchmark IEMOCAP data to assess the effectiveness of the proposed representation. Results demonstrate a recognition performance of 71.8% weighted accuracy (WA) and 68% unweighted accuracy (UA) over four emotions, which outperforms the state-of-the-art method by about 3% absolute for WA and 4% for UA.

> 

## 预处理

## 概述

## 结果

## 精读

### 引文

## 摘录
