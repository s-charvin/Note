---
title: "Visual Attention Methods in Deep Learning: An In-Depth Survey"
description: ""
citekey: hassaninVisualAttentionMethods2022
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-04-21 15:50:40
lastmod: 2023-04-21 15:55:12
---

> [!info] 论文信息
>1. Title：Visual Attention Methods in Deep Learning: An In-Depth Survey
>2. Author：Mohammed Hassanin, Saeed Anwar, Ibrahim Radwan, Fahad S. Khan, Ajmal Mian
>3. Entry：[Zotero link](zotero://select/items/@hassaninVisualAttentionMethods2022) [URL link](http://arxiv.org/abs/2204.07756) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Hassanin et al_2022_Visual Attention Methods in Deep Learning.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\TNLRSNH4\\2204.html>)
>4. Other：2022 - arxiv:2204.07756     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- :fas_question:   
- :obs_pdf_file:   
- :obs_graph_glyph:   
- :obs_wand_glyph:   

## 摘要

> [!abstract] Inspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be incorporated into large networks. Furthermore, multiple complementary attention mechanisms can be incorporated in one network. Hence, attention techniques have become extremely attractive. However, the literature lacks a comprehensive survey specific to attention techniques to guide researchers in employing attention in their deep models. Note that, besides being demanding in terms of training data and computational resources, transformers only cover a single category in self-attention out of the many categories available. We fill this gap and provide an in-depth survey of 50 attention techniques categorizing them by their most prominent features. We initiate our discussion by introducing the fundamental concepts behind the success of attention mechanism. Next, we furnish some essentials such as the strengths and limitations of each attention category, describe their fundamental building blocks, basic formulations with primary usage, and applications specifically for computer vision. We also discuss the challenges and open questions related to attention mechanism in general. Finally, we recommend possible future research directions for deep attention.

> 受人类认知系统的启发，注意力机制是一种模仿人类对特定信息的认知意识的机制，放大关键细节以更加关注数据的本质方面。深度学习利用注意力来提高许多应用程序的性能。有趣的是，相同的注意力设计可以适合处理不同的数据模式，并且可以很容易地融入大型网络。此外，多个互补的注意机制可以合并到一个网络中。因此，注意力技术变得极具吸引力。然而，文献缺乏针对注意力技术的全面调查来指导研究人员在他们的深度模型中使用注意力。请注意，除了在训练数据和计算资源方面的要求外，transformers 仅涵盖许多可用类别中的自我注意类别。我们填补了这一空白，并对 50 种注意力技术进行了深入调查，并按其最突出的特征对其进行了分类。我们通过介绍注意力机制成功背后的基本概念来开始我们的讨论。接下来，我们提供了一些基本要素，例如每个注意力类别的优势和局限性，描述了它们的基本构建块、具有主要用途的基本公式以及专门用于计算机视觉的应用程序。我们还讨论了与一般注意力机制相关的挑战和未解决的问题。最后，我们推荐了深度关注的未来可能的研究方向。

## 预处理

## 概述

## 结果

## 精读

### 引文

## 摘录
