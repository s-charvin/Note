---
title: "Multimodal sentiment analysis: A survey"
description: ""
citekey: laiMultimodalSentimentAnalysis2023
author: "石昌文"
tags: [""]
categories: "PaperNote"
keywords:  [""]
draft: true
layout: "blog"
date: 2023-12-29 19:30:57
lastmod: 2024-01-06 18:06:31
---

> [!info] 论文信息
>1. Title：Multimodal sentiment analysis: A survey
>2. Author：Songning Lai, Xifeng Hu, Haoxuan Xu, Zhaoxia Ren, Zhi Liu
>3. Entry：[Zotero link](zotero://select/items/@laiMultimodalSentimentAnalysis2023) [URL link](https://www.sciencedirect.com/science/article/pii/S0141938223001968) [PDF link](<file:///C\:\\Users\\19115\\OneDrive - stu.suda.edu.cn\\Zotero\\Lai et al_2023_Multimodal sentiment analysis.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\E445TPCN\\1-s2.0-S0141938223001968-main.pdf,E\:\\mypack\\人生规划\\ 3 _进修\\ 2 _升学\\ 4 _硕士学习\\ 4 _研究\\Zotero\\storage\\LFMQFN6Z\\S0141938223001968.html>)
>4. Other：2023 - Displays     -   

>- :luc_github: 论文实现：
>- :luc_external_link: 论文解读：
>- :luc_linkedin: 相关笔记：***

## ⭐ 重点

- :fas_question:   
- :obs_pdf_file:   
- :obs_graph_glyph:   
- :obs_wand_glyph:   

## 摘要

> [!abstract] Multimodal sentiment analysis has emerged as a prominent research field within artificial intelligence, benefiting immensely from recent advancements in deep learning. This technology has unlocked unprecedented possibilities for application and research, rendering it a highly sought-after area of study. In this review, we aim to present a comprehensive overview of multimodal sentiment analysis by delving into its definition, historical context, and evolutionary trajectory. Furthermore, we explore recent datasets and state-of-the-art models, with a particular focus on the challenges encountered and the future prospects that lie ahead. By offering constructive suggestions for promising research directions and the development of more effective multimodal sentiment analysis models, this review intends to provide valuable guidance to researchers in this dynamic field.

> 多模态情感分析已成为人工智能领域的重要研究领域。随着深度学习的最新进展，这项技术已经达到了新的高度。它具有巨大的应用和研究潜力，使其成为一个热门的研究课题。本综述概述了多模态情感分析的定义、背景和发展。它还涵盖了最新的数据集和先​​进模型，强调了这项技术的挑战和未来前景。最后对未来的研究方向进行了展望。值得注意的是，这篇综述为有前景的研究方向和构建性能更好的多模态情感分析模型提供了建设性建议，可以帮助该领域的研究人员。

## 预处理

## 概述

## 结果

## 精读

情绪是有机体对外部刺激的主观反应[?,1]。人类拥有强大的情感分析能力，研究人员目前正在探索如何将这种能力提供给人工智能[2]。
情感分析涉及通过可用信息分析情感极性 [3, 4]。随着人工智能、计算机视觉、自然语言处理等领域的快速发展，人工智能体实现情感分析的可能性越来越大。情感分析是一个跨学科的研究领域，包括计算机科学、心理学、社会科学等领域[5-7]。

几十年来，科学家们一直致力于赋予人工智能代理情感分析能力。这是类人人工智能的关键组成部分，使人工智能更像人类.

情感分析具有重要的研究价值[811]。随着互联网数据的爆发式增长，厂商可以利用评论、评论视频等评价数据来改进自己的产品。情感分析还具有无数的研究价值，例如测谎、审讯、娱乐等。下面将详细阐述情感分析的应用和研究价值。

过去，情感分析主要集中在单一模态（视觉模态、语音模态或文本模态）[12]。基于文本的情感分析 [13-15] 在 NLP 领域取得了长足的进步。基于视觉的情感分析更关注人类的面部表情[16]和运动姿势。基于语音的情感分析主要提取语音中的音高、音色、气质等特征进行情感分析[17]。随着深度学习的发展，这三种模式在情感分析中获得了一定的立足点。

然而，使用单一模式进行情感分析有其局限性[18-21]。单一模态所包含的情感信息是有限且不完整的。结合多种方式的信息可以提供更深层次的情感极性。只分析一个模态导致结果有限，并且难以准确分析动作的情感。

研究人员逐渐意识到多模态情感分析的必要性，并且出现了许多多模态情感分析模型来完成这一任务。文本特征在深层情感分析中占据主导地位并发挥关键作用[22]。表情和姿势特征的视觉模态提取可以有效辅助文本情感分析和判断[23]。一方面，语音模态可以提取文本特征，另一方面，可以识别语音语气以揭示文本在每个时间点的状态[24]。图 1 显示了更经典的多模态情感分析的模型架构。整体架构由三部分组成：一部分用于各个模态的特征提取，一部分用于每种模态的特征融合，一部分用于融合特征的情感分析。这三个部分非常重要，研究人员已经开始对这三个部分进行一一优化[25]。

在这篇综述中，我们对多模态情感分析领域进行了全面的概述。综述包括数据集的总结和简要介绍，可以帮助研究人员选择合适的数据集。我们对多模态情感分析中具有重要研究意义的模型进行了比较和分析，并为模型构建提供建议。我们详细阐述了三种类型的模态融合方法，并解释了不同模态融合方法的优点和缺点。最后，我们展望了多模态情感分析面临的挑战和未来的发展方向，提供了几个有前景的研究方向。与同一领域的其他评论相比，我们的重点是为有前景的研究方向提供建设性建议，并构建性能更好的多模态情感分析模型。我们强调这些技术的挑战和未来前景。

2 多模态情感分析数据集

随着互联网的发展，数据爆炸的时代已经到来[26-28]。许多研究人员从互联网上广泛收集这些数据（视频、评论等），并根据自己的需求构建情感数据集。表 1 总结了常用的多模态数据集。第一列表示数据集的名称。第二列是情绪数据发布的年份。第三列是情感数据集中包含的模式类别。第四列是数据集来源的平台。第五列是数据集使用的语言。第六列是数据集包含的数据量。每个数据集都有自己的特点。本节列出了社区中知名的数据集，旨在帮助研究人员梳理各个数据集的特征，方便他们选择数据集。

2.1 IEMOCAP[29]

IEMOCAP，语音分析与解释实验室 2008 发布的情感分析数据集，是一个多模态数据集，包含 1,039 个对话片段，视频总长度为 12 小时。研究参与者参与了五种不同的场景，按照预设的场景表现出情绪。该数据集不仅包括音频、视频和文本信息，-还包括通过附加传感器获得的面部表情和姿势信息。数据点分为十种情绪：中性、快乐、悲伤、愤怒、惊讶、害怕、厌恶、沮丧、兴奋等。总体而言，IEMOCAP 为研究人员探索多种模式的情感分析提供了丰富的资源。

2.2 DEAP [30]

DEAP 是专门为情感分析而设计的数据集，利用生理信号作为数据源（Koelstra 等人，2011）。该数据集检查了 32 名受试者的脑电图数据，其中男性和女性参与者的比例为 1:1。从受试者大脑的不同区域（包括额叶、顶叶、枕叶和颞叶）收集 512 Hz 的脑电图信号。为了注释脑电图信号，受试者被要求从三个方面对相应的视频进行评分：效价、唤醒度和支配性，评分范围为 1 到 9。该数据集为研究人员探索使用生理信号进行情感分析提供了宝贵的资源。

2.3 CMU-MOSI [31]

CMU-MOSI 数据集由 93 个关键 YouTube 视频组成，涵盖一系列主题（Zadeh 等人，2016）。这些视频经过精心挑选，以确保其中只有一名面向镜头的发言者，从而可以清晰地捕捉面部表情。虽然摄像机型号、距离或演讲者场景没有限制，但所有演示和评论均由 89 名不同的演讲者用英语进行，其中包括 41 名女性和 48 名男性。这 93 个视频被分为 2,199 个主观意见片段，并用从强烈负面到强烈正面（-3 到 3）的情绪强度进行注释。总体而言，CMU-MOSI 数据集为研究情感分析的研究人员提供了宝贵的资源。

2.4 CMU-MOSEI [32]

CMU-MOSEI 是一个流行的情感分析数据集，包含 3,228 个 YouTube 视频（Zadeh 等人，2018）。这些视频分为 23,453 个片段，并具有来自三种不同模式的特征数据：文本、视觉和声音。该数据集来自 1,000 位演讲者的贡献，涵盖 250 个不同主题，提供了多种观点。所有视频均为英文，并提供情感和情感注释。六种情绪类别包括快乐、悲伤、愤怒、害怕、厌恶和惊讶，而情感类别强度标记的范围从强负到强正（-3 到 3）。总体而言，CMU-MOSEI 对于研究人员探索多种模式的情感分析来说是宝贵的资源。

2.5 MELD [33]

MELD 是一个综合数据集，其中包括热门电视剧《老友记》中的视频剪辑。数据集包括与文本数据相对应的文本、音频和视频信息。它包含 1400 个视频，进一步分为 13,000 个单独的片段。该数据集有七类注释：愤怒、厌恶、悲伤、喜悦、中性、惊讶和恐惧。每个片段都有三种情绪注释：积极、消极和中性。


2.6 Multi-ZOL [34]

Multi-ZOL 是一个专为图像和文本的双模态情感分类而设计的数据集。该数据集包含从 ZOL.com 收集的手机评论。它包含 5288 组多模态数据点，涵盖多个品牌的各种型号手机。这些数据点用六个方面的情绪强度评级从 1 到 10 进行注释。

2.7 CH-SIMS [35]

CH-SIMS 是一个独特的数据集，由来自网络的 60 个开源视频组成，这些视频被分成 2281 个视频剪辑。该数据集仅关注中文（普通话）语言，并确保每个片段仅包含一个角色的面部和声音。它涵盖了广泛的场景和说话者年龄，并为每种模式进行了单独标记，使其成为研究人员的宝贵资源。数据集注释包含从负面到正面（-1 到 1）的情绪强度评级，还包括其他属性（例如年龄和性别）的注释。


2.8 CMU-MOSEA [36]

CMU-MOSEA 是一个多功能数据集，包含多种语言，例如西班牙语、葡萄牙语、德语和法语。该数据集包含 40,000 个句子片段，涵盖 250 个不同主题和 1645 个说话者。注释分为两类：情绪强度和二元。每个句子都用[-3,3]区间内的情感强度进行注释，二进制包括说话者是否表达了观点或做出了客观陈述。每个句子的情绪分为六类：快乐、悲伤、恐惧、厌恶、惊讶和注释。

2.9 FACTIFY [37]

FACTIFY 是一个假新闻检测数据集，专注于实施验证。它包括图像和文本模式的数据，并包含 50,000 组数据。大多数数据的主张都涉及政治和政府。数据集被注释为三类：支持、无证据和反驳。对于有兴趣检测和打击假新闻传播的研究人员来说，该数据集是宝贵的资源。

2.10 MEMOTION [38]

MEMOTION 是一个基于模因的数据集，其中包括与政治、宗教和体育相关的流行模因。该数据集包含 10,000 个数据点，分为三个子任务：情绪分析、情绪分类和情绪类别的规模/强度。注释者在不同的子任务下对每个数据点进行不同的注释。子任务一将每个数据点注释为三类（负面、中性和正面）。子任务二将每个数据点注释为四类（幽默、讽刺、冒犯、动机）。子任务三注释区间 [0,4] 中的每个数据点以指示情绪强度。该数据集为研究人员提供了一个独特的机会来分析模因作为现代文化中的交流和表达手段的使用。

现代文化中的压力。上述数据集都有一定的局限性。我们总结了每个数据集的局限性，以帮助研究人员在选择实验数据集时做出明智的决定。对于 IEMOCAP，数据集中有限的参与者数量可能会导致过度拟合问题。此外，数据集中的情绪类别可能不够全面，无法涵盖所有​​情绪类型。对于 DEAP、CMU-MOSI 和 CMU-MOSEI，数据集中的情感类别可能不够全面，无法涵盖所有​​情感类型。此外，数据集中的视频剪辑可能太短，无法充分表达情感。对于 MELD，数据集中的音频样本可能太短而无法充分表达情感。此外，面部表情可能会受到照明等环境因素的影响。对于 Multi-ZOL 来说，由于社交媒体评论的样本来源多种多样，因此可能存在一定程度的噪音。此外，该数据集仅适用于社交媒体评论的情感分析任务。而 CH-SIMS，该数据集仅适用于学生负面情绪识别任务。此外，由于样本来源有限，仅限于学生问卷，可能存在一定程度的主观偏差。对于 CMU-MOSEA，该数据集仅适用于电影场景中的多模态信息和情感识别任务。此外，由于样本来源有限，仅限于电影场景标注，可能存在一定程度的主观偏差。对于 FACTIFY 来说，该数据集仅适用于新闻文章中的主题和情感识别任务。由于样本来源有限仅限于新闻文章注释，可能存在一定程度的主观偏差。

3 Multimodal fusion
多模态数据从不同角度描述对象，比单模态数据提供更多信息。来自不同模式的数据信息可以相互补充。在多模态情感分析任务中，融合不同模态之间的数据特征，保持模态的语义完整性，实现不同模态之间的良好融合是一项非常重要且具有挑战性的任务。根据模态融合的不同模式，可以概括为前期基于特征的多模态融合、中期基于模型的多模态融合、后期基于决策的多模态融合。

3.1 Early feature-based approaches for multimodal fusion
早期基于特征的多模态融合方法在早期特征提取后进行浅层融合。在模型浅层融合不同模态的特征相当于将不同单一模态的特征统一到同一参数空间中。由于不同模态之间的信息存在差异，特征通常包含大量冗余信息，常常需要使用降维方法来去除冗余信息。将降维后的特征输入到模型中，完成特征提取和预测。早期特征融合希望模型在特征建模开始时就考虑来自多种模态的输入信息。然而，由于不同模式的参数空间存在差异，在输入层统一多个不同参数空间的方法可能无法达到预期的效果。该类模型可以有效地处理多模态情感识别任务，具有高精度和鲁棒性。然而，这些模型需要大量的训练数据才能获得良好的性能，并且其结构相对复杂，需要较长的训练时间。早期基于特征的多模态融合方法的总体框架如图 2 所示。一些具有代表性的型号有：

3.1.1 THMM (Tri-modal Hidden Markov Model) [39]
多模态序列建模和分析的一种方法是将多种模态的特征向量表示为高阶张量，并使用张量分解方法来提取隐藏状态和转移概率。这种方法有效地利用了多模态数据之间的相关性和互补性，同时避免了维数灾难和过度拟合。但缺点是张量的顺序和秩以及隐藏状态的数量必须预先确定，这可能会影响模型的性能和效率。

3.1.2 RMFN (Recurrent Multistage Fusion Network) [40]
另一种方法是使用多个循环神经网络层逐渐融合不同模态的特征，从局部到全局，从低级到高级，最终获得全面的情感表示。该模型使用注意力机制来调整不同模态的特征在语义空间中的位置，允许同一个词在不同的非语言行为下表现出不同的情感

3.1.3 RAVEN (Recurrent Attended Variation Embedding Network) [41]
提出了一种用于多模态情感分析的分层融合网络，其中包括局部融合模块和全局融合模块。通过滑动窗口探索局部跨模态融合，有效降低了计算复杂度。

3.1.4 HFFN(Hierarchical feature fusion network with local and global perspectives for multimodal affective computing) [42]
另一种方法是使用循环神经网络和对抗性学习来学习不同模态之间的联合表示，从而提高单模态表示和处理缺失模态或噪声的能力。

3.1.5 MCTN (Multimodal Cyclic Translation Network) [43]
基于中期模型的多模态融合方法涉及将多模态数据输入网络，模型的中间层执行模态之间的特征融合。基于模型的模态融合方法可以选择模态特征融合的位置来实现中间交互。基于模型的融合通常使用多核学习、神经网络、图模型和替代方法。

3.2 Medium-term model-based multimodal fusion method
中期基于模型的多模态融合方法是将多模态数据输入网络，模型的中间层执行模态之间的特征融合。基于模型的模态融合方法可以选择模态特征融合的位置来实现中间交互。基于模型的融合通常使用多核学习、神经网络、图模型和替代方法。基于中期模型的多模态融合方法的总体框架如图 3 所示。

3.2.1 MKL (Multiple Kernel Learning) [44]
该模型是一种多核学习方法。它用不同的核函数来表示不同的模态信息，通过优化目标函数来选择最优的核函数组合，实现多模态信息的融合。该模型具有较高的灵活性，可以自适应地选择不同的核函数，从而提高模型的鲁棒性和准确性。然而，模型结构相对简单，可能无法处理复杂的情感表达。


3.2.2 BERT-like (Self Supervised Models to Improve Multimodal Speech Emotion Recognition) [45]

该模型是一种基于 Transformer 的多模态情感分析方法，可以利用自注意力机制实现文本和图像之间的对齐和融合。该模型采用自监督学习方法，能够有效处理多模态情感识别任务，且具有较高的准确率和鲁棒性。此外，模型可能会受到数据和注释质量的影响。

3.3 Multimodal model based on decision fusion in the later stage
决策级融合方法用于融合来自不同模态的信息。决策级融合是指对来自不同模态的数据分别训练模型，将不同模态的输出合并到最终决策中。基于决策融合的多模态模型通常使用平均、多数投票、加权和可学习模型等方法来融合模态。此类模型通常重量轻且灵活。当缺少任何模式时，可以使用其余模式做出决定。后期方法中基于决策融合的多模态模型总体框架如图 4 所示。

3.3.1 Deep Multimodal Fusion Architecture [46]
在该模型中，每种模态都有一个独立的分类器。对每个分类器的置信度得分进行平均后输出预测结果。该模型结构简单、易于实现，能够有效处理多模态情感识别任务。然而，该模型无法处理模态之间的交互信息，可能会遭受信息丢失。

3.3.2 SAL-CNN (Select-Additive Learning CNN) [47]
该模型是基于 CNN 和注意力机制的多模态情感分析模型。它使用自适应注意力机制来融合文本和图像特征，使用空间注意力机制来提取图像中的文本相关区域，最后使用完全连接层对输出进行分类。该模型采用注意力机制，能够有效处理多模态情感识别任务，具有较高的准确性和鲁棒性。但该模型需要大量的训练数据才能达到良好的性能，且模型结构相对复杂，需要较长的训练时间。


3.3.3 TSAM (Temporally Selective Attention Model) [48]
所提出的模型是时间选择性注意力模型，通过注意力机制分配权重来帮助模型选择时间步，最后将其发送到不同的 SDL 损失函数模型进行情感分析。 SDL 是一种基于自蒸馏学习的多模态情感分析方法，可以利用不同模态之间的互补性来提高模型的泛化能力和鲁棒性。该模型采用时间选择性注意力机制，能够有效处理多模态情感识别任务，且具有较高的准确性和鲁棒性。

4 Latest Multimodal Sentiment Analysis Models
近年来，多模态情感分析领域已经发展成为一个庞大的系统，出现了许多实用高效的模型和架​​构。在这里很难涵盖所有模型。在本章中，我们介绍了一些最新的、前沿的多模态情感分析模型。这些模型大多被后来的研究者作为基准模型进行实验参考。表 2 总结了这些模型。第一列是模型的名称。第二列是模型发布的年份。第三列是模型使用的数据集。第四列是该数据集下的准确率。

4.0.1 MultiSentiNet-Att [49]
该模型使用 LSTM 网络将文本信息合并到词向量中。 VGG 用于提取图像的目标特征信息和场景特征信息。目标和场景特征向量用于与词向量进行跨模态注意机制学习。即，结合图像的目标特征信息和场景特征信息，对文本中与情感相关的词向量赋予特殊的权重。得到的特征被输入多层感知器以完成情感分析任务。

4.0.2 DFF-ATMF [50]
所提出的模型主要考虑文本模态和音频模态。主要贡献是提出了新的多特征融合策略和多模态融合策略。两个并行分支用于学习文本模态的特征和音频模态的特征。针对这两种模态的特征，采用多模态注意力融合模块来完成多模态融合。

4.0.3 AHRM [51]
该模型主要用于捕获文本模态和视觉模态之间的关系。作者提出了一种基于注意力机制的异构关系模型，能够很好地整合文本模态和视觉模态各自的高质量表示信息。这种渐进式双重注意力机制可以很好地突出图像和文本信息的通道级语义信息。为了整合社会属性，引入社会关系以从社会背景中捕获互补信息，并构建异构网络以整合特征。

4.0.4 SFNN [52]
所提出的模型是基于语义特征融合的神经网络。卷积神经网络和注意力机制用于提取视觉特征。视觉特征映射到文本特征，并与文本模态特征结合进行情感分析。

4.0.5 MISA [53]
所提出的模型提出了一种新颖的多模态情感分析框架。特征提取后，每种模态都被映射到两个不同的特征空间。一个特征空间主要学习模态的不变特征，另一个特征空间学习模态的独特特征。

4.0.6 MAG-BERT [54]
作者提出了一种“多模态”适应架构并将其应用于 BERT。该模型可以在微调期间接收来自多种模式的输入。 MAG 可以被认为是一种向量嵌入结构，它允许我们输入多模态信息并将其作为序列嵌入到 BERT 中。

4.0.7 TIMF [55]
该模型的主要思想是每个模态单独学习特征并对每个模态的特征进行张量融合。在数据集融合阶段，每种模态的特征融合是通过张量融合网络实现的。在决策融合阶段，通过软融合的方式对上游结果进行融合，以调整决策结果。

4.0.8 Auto-ML based Fusion [56]
作者建议将文本和图像个体情感分析结合到基于 AutoML 的最终融合分类中。这种方法使用 Auto-ML 生成的最佳模型将各个分类器组合成最终分类。这是决策级融合的典型模型。

4.0.9 Self-MM [57]
在本文中，作者将自监督学习和多任务学习结合起来，构建了一种新颖的情感分析架构。为了学习每种模态的私有信息，作者构建了一个基于自监督学习的单模态标签生成模块 ULGM。该模块对应的损失函数旨在使用权重调整策略将三个自监督学习子任务学到的私有特征合并到原始多模态情感分析模型中。所提出的模型表现良好，并且基于自监督学习的 ULGM 模块还具有单模态标签校准的能力。

4.0.10 DISRFN [58]
该模型是一个动态不变的表示特定融合网络。改进联合域分离网络以获得所有模态的联合域分离表示，从而可以有效地利用冗余信息。其次，利用 HGFN 网络动态融合各模态的特征信息，学习多模态交互的特征。同时，构建提高融合效果的损失函数，帮助模型学习子空间中各模态的表示信息。

4.0.11 TEDT [59]
该模型提出了一种带有变压器的多模态编码解码翻译网络，以解决多模态情感分析的挑战，特别是个体模态数据的影响和非自然语言特征的质量差。该方法使用文本作为主要信息，声音和图像作为次要信息，并使用模态增强交叉注意模块将非自然语言特征转换为自然语言特征以提高其质量。此外，动态过滤机制过滤掉跨模态交互中产生的错误信息。该模型的优点在于能够提高多模态融合的效果，更准确地分析人类情感。然而，它可能需要大量的计算资源，并且可能不适合实时分析。

4.0.12 TETFN [60]
文本增强型变压器融合网络 (TETFN) 是一种针对多模态情感分析 (MSA) 提出的新颖方法，可解决文本、视觉和声学模态不同贡献的挑战。该方法学习面向文本的成对跨模态映射，以获得有效的统一多模态表示。它通过基于文本的多头注意力将文本信息合并到学习情感相关的非语言表示中，并通过单模态标签预测保留模态之间的差异信息。此外，利用视觉预训练模型 Vision-Transformer 从原始视频中提取视觉特征，以保留人脸的全局和局部信息。该模型的优势在于它能够合并文本信息，以提高 MSA 中非语言模态的有效性，同时保留模态间和模态内的关系。

4.0.13 SPIL [61]
该模型提出了一种深度模态共享信息学习模块，用于多模态情感分析任务中的有效表示学习。所提出的模块通过使用协方差矩阵捕获模态之间的共享信息和自监督学习策略来捕获私有信息，以完整的模态表示形式捕获共享信息和私有信息。该模块即插即用，可以调整模式之间的信息交换关系来学习私有或共享信息。此外，采用多任务学习策略来帮助模型将注意力集中在模态区分训练数据上。所提出的模型在三个公共数据集的大多数指标上都优于当前最先进的方法，并且探索了使用该模块的更多组合技术。

5 Model comparison and suggestions

本节评估五种最先进的多模态情感分析模型：DFF-ATMF、MAG-BERT、TIMF、Self-MM 和 DISRFN。虽然 DFF-ATMF 不考虑视觉模态，但其他模型从音频、文本和视觉三种模态分析情感。
对于多模态数据的交互关系，DFF-ATMF 和 TIMF 构建基于变压器的模型来学习数据之间的复杂关系。 MAGBERT 使用简单而有效的多模态自适应门融合策略。 Self-MM 采用自监督多任务学习作为融合策略，自监督生成单模态标签，并组合单模态标签完成多模态情感分析任务。 DISRFN 使用动态不变特定表示融合网络来获得所有模态的联合域分离表示，并通过分层图融合网络动态融合每个表示。

DFF-ATMF 使用两个并行分支来融合音频和文本模式。其核心机制是特征向量融合和多模态注意力融合，可以学习到更全面的情感信息。然而，由于使用多层神经网络和复杂的融合方法，可能会发生过度拟合。优点：结构简单，易于实现，可以通过特征向量融合和多模态注意力融合学习全面的情感信息。缺点：没有考虑视觉模态，由于使用多层神经网络和复杂的融合方法，可能会出现过拟合。

MAG-BERT 使用多模态适应门来适应 BERT 的内部，该门采用简单而有效的融合策略，而不改变 BERT 的结构和参数。然而，多模态注意力只能在同一时间步内进行，而不能跨时间步进行，这可能会忽略一些时间关系。此外，MAG-BERT 需要冻结 BERT 的参数，而无法微调 BERT，这可能会导致 BERT 的表示不适合特定的任务或领域。优点：采用简单有效的多模态自适应门融合策略，无需改变 BERT 的结构和参数。缺点：多模态注意力只能在同一时间步内进行，不能跨时间步进行，这可能会忽略一些时间关系。需要冻结 BERT 的参数，而无法微调 BERT，这可能会导致 BERT 的表示不适合特定任务或领域。

TIMF 利用 Transformer 的自注意力机制来学习多模态数据之间的复杂交互并生成统一的情感表示。虽然它的优点是能够学习模态之间的复杂关系，但它可能会遇到极端的计算复杂性、较长的训练时间以及大量标记数据的问题。优点：利用 Transformer 的自注意力机制来学习多模态数据之间的复杂交互并生成统一的情感表示。缺点：可能会遇到计算复杂度极高、训练时间长以及大量标记数据的问题。

Self-MM 是一种自监督的多模态情感分析模型，它使用多任务学习策略来学习多模态和单模态情感识别任务。它的优点是可以使用自我监督的方法生成单模态标签，节省手动标记的成本和时间。然而，多个任务之间可能会出现干扰和不平衡，需要设计合适的权重调整策略来平衡不同任务的学习进度。优点：以自监督多任务学习为融合策略，自监督生成单模态标签，并组合单模态标签完成多模态情感分析任务。缺点：可能需要大量标记数据才能获得良好的性能，并且自监督学习过程的计算成本可能很高。

DISRFN 是一种基于深度残差网络的多模态情感分析模型，利用动态不变特定表示融合网络策略来提高情感识别能力。其优点在于，它可以有效地利用冗余信息，通过改进的联合域分离网络获得所有模态的联合域分离表示，并通过层次图融合网络动态融合各个表示，以获得多模态数据的交互信息。然而，与 Self-MM 一样，多个任务之间可能会出现干扰和不平衡，需要设计合适的权重调整策略来平衡不同任务的学习进度。优点：使用动态不变特定表示融合网络来获得所有模态的联合域分离表示，并通过分层图融合网络动态融合每个表示。缺点：可能需要大量的标记数据才能获得良好的性能，并且分层图融合网络的计算成本可能很高。

TEDT 提出了一种带有变压器的多模态编码解码翻译网络，以解决多模态情感分析的挑战。该模型的优点在于能够提高多模态融合的效果，更准确地分析人类情感。通过结合模态强化交叉注意力模块和动态过滤机制，该模型能够解决个体模态数据影响和非自然语言特征质量差的挑战。为了构建有效的多模态情感分析模型，建议仔细考虑每种模态的贡献以及如何有效地整合它们。此外，应注意解决个体模态数据影响和非自然语言特征质量差等挑战。最后，重要的是要考虑模型的计算要求并确保它适合预期的用例。

TETFN 是为 MSA 提出的一种新颖方法，可解决文本、视觉和声学模态不同贡献的挑战。与 TEDT 模型相比，TETFN 模型侧重于合并文本信息，以提高 MSA 中非语言模态的有效性，同时保留模态内的交互性和关系。 TETFN 模型通过使用基于文本的多头注意力和单模态标签预测来保留模态之间的差异化信息来实现这一目标。相比之下，TEDT 模型使用模态强化交叉注意力模块将非自然语言特征转换为自然语言特征，并使用动态过滤机制过滤掉跨模态交互中产生的错误信息。 TETFN 模型的优势在于它能够有效地合并文本信息，以提高 MSA 中非语言模态的有效性，同时保留模态间和模态内的关系。此外，使用视觉预训练模型 Vision-Transformer 有助于从原始视频中提取视觉特征，以保留人脸的全局和局部信息。为了构建有效的多模态情感分析模型，建议仔细考虑每种模态的贡献以及如何有效地整合它们。此外，应注意解决个体模态数据影响和非自然语言特征质量差等挑战。

SPIL 提出了一种深度模态共享信息学习模块，用于多模态情感分析任务中的有效表示学习。所提出的模块通过使用协方差矩阵捕获模态之间的共享信息和自监督学习策略来捕获私有信息，以完整的模态表示形式捕获共享信息和私有信息。该模块即插即用，可以调整模式之间的信息交换关系来学习私有或共享信息。此外，采用多任务学习策略来帮助模型将注意力集中在模态区分训练数据上。与 TEDT 和 TETFN 模型相比，SPIL 模型还专注于以完整的模态表示形式捕获共享和私有信息。然而，SPIL 模型使用协方差矩阵来捕获模态之间的共享信息，并使用自监督学习策略来捕获私有信息，而 TETFN 模型使用基于文本的多头注意力和单模态标签预测来保留模态之间的差异信息。 SPIL 模型还采用多任务学习策略来帮助模型将注意力集中在模态区分训练数据上，而 TEDT 和 TETFN 模型没有明确提及这一点。 SPIL 模型的优势在于它能够以完整的模态表示形式捕获共享信息和私有信息，并且可以根据手头的具体任务进行调整。此外，使用多任务学习策略有助于通过将注意力集中在模态区分训练数据上来提高模型的性能。 SPIL 模型以完整的模态表示形式捕获共享信息和私有信息的方法值得在未来的模型中考虑。

表 3 显示了这五个模型在 CMU-MOSI 和 CMU-MOSEI 数据集下的性能指标。在分析这五个模型在这些数据集上的性能指标时，我们建议使用 BERT 提取文本信息的特征，同时使用 LSTM 提取视频和音频模态信息的特征，因为它需要捕获时间序列中的模态信息。

DFF-ATMF 不考虑视觉模态，导致性能指标相对较低。视觉信息可以提供有关人类表情、姿势、场景等的附加信息，可以增强文本和语音模态的信息并对其进行补充。因此，视觉模态信息值得在多模态情感分析中考虑和探索。

6 Challenges and Future Scope

随着深度学习的发展，多模态情感分析技术也得到了迅速发展[62-65]。然而，多模态情感分析仍面临诸多挑战。本小节分析多模态情感分析的研究现状、挑战和未来发展。

6.1 Dataset
在多模态情感分析中，数据集起着至关重要的作用。目前，缺少多种语言的大型数据集。考虑到许多国家语言和种族的多样性，可以使用大型、多样化的数据集来训练泛化性强、用途广泛的多模态情感分析模型。此外，当前的多模态数据集标注精度仍然较低，尚未达到绝对连续值，需要研究人员对多模态数据集进行更精细的标注。目前大多数多模态数据仅包含视觉、语音和文本模态，缺乏与脑电波和脉搏等生理信号相结合的模态信息。

6.2 Detection of Hidden Emotions
多模态情感分析任务一直存在一个公认的困难：隐藏情感的分析。隐藏情绪[66, 67]包括：讽刺情绪（如讽刺话语）、需要结合上下文具体分析的情绪以及复杂情绪[68, 69]（如人的快乐和悲伤）。探索这些隐藏的情绪很重要。这是人类和人工智能之间的差距[70]。

6.3 Multiple forms of video data
在多模态情感分析任务中，视频数据尤其具有挑战性。虽然说话者面向摄像头，视频数据分辨率保持在较高水平，但实际情况更为复杂，要求模型对噪声具有鲁棒性并适用于低分辨率视频数据。捕捉说话者的微表情和微手势进行情感分析也是值得研究人员探索的领域。

6.4 Multiform language data
多模态情感分析任务中的文本数据形式通常是单一的。然而，在线社区中的评价文本往往是跨语言的，审稿人使用多种语言来做出更生动的评论。具有混合情感的文本数据仍然是多模态情感分析任务的挑战。充分利用文本中混合的模因是一个重要的研究课题，因为模因通常包含有关审稿人的极其强烈的情感信息。此外，大多数文本数据是直接通过语音转录的，这使得在多人交谈时分析一个人的情绪特别困难。结合不同地区和国家的文化特征，相同的文本数据可能反映不同的情感。

6.5 Future Prospectsn
多模态情感分析技术的前景非常光明，下面列出了一些未来的应用。用于心理健康实时评估的多模态情绪分析[71-73]；多模态犯罪语言欺骗检测模型[74]；攻击性语言检测；类人情感感知机器人等。多模态情感分析是一种识别和分析情感的技术。结合多模态信息数据进行情感分析的模型可以有效提高情感分析的准确性。未来，多模态情感分析技术将逐步完善。也许有一天会出现一种具有大量参数的多模态情感分析模型，具有与人类相同的情感分析能力。这是一件令人欣喜若狂的事情。

7 Conclusion
多模态情感分析技术已被各个领域的研究人员认识到其重要性，使其成为自然语言处理和计算机视觉领域的中心研究课题。在这篇综述中，我们详细描述了多模态情感分析的各个方面，包括其研究背景、定义和发展过程。我们还在表 1 中总结了常用的基准数据集，并比较和分析了最新的多模态情感分析模型。最后，我们提出了多模态情感分析领域带来的挑战，并探讨了未来可能的发展。
许多前瞻性的工作正在积极开展，甚至已经大规模实施。然而，仍然存在需要解决的挑战，从而导致以下有意义的研究方向：
（1）构建多种语言的大型多模态情感数据集。 
（2）解决视频、文本、语音模态数据的域转移问题。
（3）构建统一、大规模、泛化性能优良的多模态情感分析模型。 
（4）减少模型参数，优化算法，降低算法复杂度。
（5）解决多模态情感分析中的多语言混合问题。
（6）讨论模态融合的权重问题，给出不同情况下不同模态权重分配的最合理方案。 
（7）讨论模态之间的相关性以及它们之间单独的共享和私有信息，以提高模型性能和可解释性。
（8）构建能够很好完成隐藏情感的多模态情感分析模型。

### 引文

## 摘录
