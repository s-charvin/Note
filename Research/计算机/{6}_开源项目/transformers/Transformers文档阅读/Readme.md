---
title: ""
author: "çŸ³æ˜Œæ–‡"
tags: [""]
description: ""
categories: [""]
keywords:  [""]
type: ""
draft: true
layout: 
data: 2021-12-07 10:05:49
lastmod: 2022-03-27 14:12:21
---

# ğŸ¤— Transformers

â€é€‚ç”¨äº Jaxã€Pytorch å’Œ TensorFlowâ€æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ â€â€æ¡†æ¶

ğŸ¤— Transformersï¼ˆä»¥å‰ç§°ä¸º*pytorch-transformers*å’Œ*pytorch-pretrained-bertï¼‰*æä¾›äº†æ•°åƒä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºåœ¨æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘ç­‰ä¸åŒæ¨¡æ€ä¸Šæ‰§è¡Œä»»åŠ¡ã€‚

è¿™äº›æ¨¡å‹å¯ä»¥åº”ç”¨äºï¼š

- ğŸ“ Text, ç”¨äº 100 å¤šç§è¯­è¨€çš„æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æå–ã€é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
- ğŸ–¼ï¸ Images, ç”¨äºå›¾åƒåˆ†ç±»ã€å¯¹è±¡æ£€æµ‹å’Œåˆ†å‰²ç­‰ä»»åŠ¡ã€‚
- ğŸ—£ï¸ Audio, ç”¨äºè¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘åˆ†ç±»ç­‰ä»»åŠ¡ã€‚

Transformeræ¨¡å‹è¿˜å¯ä»¥åœ¨**å¤šç§ç»„åˆçš„æ¨¡æ€**ä¸Šæ‰§è¡Œä»»åŠ¡ï¼Œä¾‹å¦‚ tableQAï¼Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼Œæ‰«ææ–‡æ¡£ä¿¡æ¯æå–ï¼Œè§†é¢‘åˆ†ç±»å’Œ VQAã€‚

ğŸ¤— Transformers æä¾›äº† APIï¼Œç”¨äºåœ¨ç»™å®šæ–‡æœ¬ä¸Šå¿«é€Ÿä¸‹è½½å’Œä½¿ç”¨è¿™äº›é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æ‚¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒå®ƒä»¬ï¼Œç„¶ååœ¨æˆ‘ä»¬çš„[æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)ä¸Šä¸ç¤¾åŒºå…±äº«å®ƒä»¬ã€‚åŒæ—¶ï¼Œæ¯ä¸ªæ¶æ„çš„pythonæ¨¡å—éƒ½æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œå¯ä»¥è¿›è¡Œä¿®æ”¹ä»¥å®ç°å¿«é€Ÿçš„ç ”ç©¶å®éªŒã€‚

ğŸ¤— Transformers å¯ä»¥åœ¨ä¸‰ä¸ªæœ€æµè¡Œçš„æ·±åº¦å­¦ä¹ åº“[Jaxã€PyTorch](https://pytorch.org/)å’Œ[TensorFlow](https://www.tensorflow.org/)ä¹‹é—´æ— ç¼è¡”æ¥ã€‚åœ¨åŠ è½½ä½ çš„æ¨¡å‹åº”ç”¨åˆ°å…¶ä»–æ¨¡å‹å‰ï¼Œä½¿ç”¨åº“ä¸­çš„æ¨¡å‹è®­ç»ƒä½ çš„æ¨¡å‹å˜å¾—éå¸¸ç®€å•ã€‚

è¿™æ˜¯æˆ‘ä»¬ [Transformers ](https://github.com/huggingface/transformers)çš„æ–‡æ¡£ã€‚æ‚¨è¿˜å¯ä»¥å…³æ³¨æˆ‘ä»¬çš„[åœ¨çº¿è¯¾ç¨‹](https://huggingface.co/course)ï¼Œè¯¥è¯¾ç¨‹æ•™æˆäº†å¦‚ä½•ä½¿ç”¨æ­¤åº“ï¼Œä»¥åŠHugging Faceå’ŒHubå¼€å‘çš„å…¶ä»–åº“ã€‚T

## å¦‚æœæ‚¨æ­£åœ¨å¯»æ‰¾ Hugging Face å›¢é˜Ÿçš„è‡ªå®šä¹‰æ”¯æŒ

[![HuggingFace Expert Acceleration Program](Readme.assets/support-16388476576101.png)](https://huggingface.co/support)

## ç‰¹ç‚¹

1. è½»æ¾ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹:
	- åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘ä»»åŠ¡æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚
	- æ•™è‚²å·¥ä½œè€…å’Œä»ä¸šäººå‘˜çš„è¿›å…¥é—¨æ§›ä½ã€‚
	- å¾ˆå°‘æœ‰é¢å‘ç”¨æˆ·çš„æŠ½è±¡ï¼Œåªæœ‰ä¸‰ä¸ªç±»éœ€è¦å­¦ä¹ ã€‚
	- åªæœ‰ä¸€ä¸ªç»Ÿä¸€çš„ APIï¼Œç”¨äºä½¿ç”¨æ‰€æœ‰é¢„è®­ç»ƒçš„æ¨¡å‹ã€‚
2. æ›´ä½çš„è®¡ç®—æˆæœ¬ï¼Œæ›´ä½çš„ç¢³è¶³è¿¹ï¼š
	- ç ”ç©¶äººå‘˜å¯ä»¥å…±äº«ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯æ€»æ˜¯é‡æ–°è®­ç»ƒã€‚
	- ä»ä¸šäººå‘˜å¯ä»¥å‡å°‘è®¡ç®—æ—¶é—´å’Œç”Ÿäº§æˆæœ¬ã€‚
	- æ•°åç§æ¶æ„ï¼ŒåŒ…å« 20ï¼Œ000 å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶ä¸­ä¸€äº›æ¨¡å‹æ”¯æŒ 100 å¤šç§è¯­è¨€ã€‚
3. ä¸ºæ¨¡å‹è¿è¡Œå‘¨æœŸçš„æ¯ä¸ªéƒ¨åˆ†é€‰æ‹©æ­£ç¡®çš„æ¡†æ¶:
	- åœ¨ 3 è¡Œä»£ç ä¸­è®­ç»ƒæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚
	- éšæ„åœ¨ TF2.0/PyTorch/JAX æ¡†æ¶ä¹‹é—´åˆ‡æ¢å•ä¸ªæ¨¡å‹ã€‚
	- ç®€å•è€Œæ­£ç¡®çš„é€‰æ‹©è®­ç»ƒã€è¯„ä¼°å’Œç”Ÿæˆæ¡†æ¶ã€‚
4. æ ¹æ®æ‚¨çš„éœ€æ±‚è½»æ¾è‡ªå®šä¹‰æ¨¡å‹æˆ–ç¤ºä¾‹ï¼š
	- æˆ‘ä»¬ä¸ºæ¯ç§æ¶æ„æä¾›ç¤ºä¾‹ï¼Œä»¥é‡ç°å…¶åŸå§‹ä½œè€…å‘å¸ƒçš„ç»“æœã€‚
	- æ¨¡å‹å†…éƒ¨å°½å¯èƒ½ä¸€è‡´åœ°å…¬å¼€ã€‚
	- æ¨¡å‹æ–‡ä»¶å¯ä»¥ç‹¬ç«‹äºåº“ä½¿ç”¨ï¼Œä»¥ä¾¿è¿›è¡Œå¿«é€Ÿè¯•éªŒã€‚

[æ‰€æœ‰çš„ checkpoints æ¨¡å‹æ–‡ä»¶](https://huggingface.co/models) éƒ½å¯ä»¥ä» huggingface.co [æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/) ä¸­æ— ç¼é›†æˆã€‚æ¨¡å‹ä¸­å¿ƒçš„æ¨¡å‹éƒ½æ˜¯ç”± [ç”¨æˆ·](https://huggingface.co/users) å’Œ [ç»„ç»‡](https://huggingface.co/organizations)ä¸Šä¼ çš„ã€‚

## å†…å®¹

æœ¬æ–‡æ¡£åˆ†ä¸ºäº”ä¸ªéƒ¨åˆ†ï¼š

- **GET STARTED** åŒ…å«å¿«é€Ÿæµè§ˆã€å®‰è£…è¯´æ˜ä»¥åŠæœ‰å…³æˆ‘ä»¬çš„åŸç†ï¼ˆphilosophyï¼‰å’Œæœ¯è¯­è¡¨ï¼ˆglossary.ï¼‰çš„ä¸€äº›æœ‰ç”¨ä¿¡æ¯ã€‚
- **USING ğŸ¤— TRANSFORMERS** åŒ…å«æœ‰å…³å¦‚ä½•ä½¿ç”¨è¯¥åº“çš„ä¸€èˆ¬æ•™ç¨‹ã€‚
- **ADVANCED GUIDES** åŒ…å«æ›´é«˜çº§çš„æŒ‡å—ï¼Œè¿™äº›æŒ‡å—æ›´ä¸“æ³¨äºæŸä¸ªè„šæœ¬æˆ–åº“çš„æŸä¸€éƒ¨åˆ†ã€‚
- **RESEARCH** ä¸æ€ä¹ˆæ•™ç»™ä½ æ€ä¹ˆä½¿ç”¨åº“ï¼Œæ›´å¤šåœ°ä¾§é‡äº transformers æ¨¡å‹çš„ä¸€èˆ¬ç ”ç©¶çš„æ•™ç¨‹ã€‚
- **API** åŒ…å«æ¯ä¸ªå…¬å…±ç±»å’Œå‡½æ•°çš„æ–‡æ¡£ï¼Œè¿™äº›ç±»å’Œå‡½æ•°æŒ‰ä»¥ä¸‹é¡ºåºåˆ†ç»„ï¼š
	- **MAIN CLASSES** ä¸»è¦ç±»ï¼Œå…¬å¼€åº“çš„é‡è¦ APIã€‚
	- **MODELS** ä¸åº“ä¸­æ¯ä¸ªæ¨¡å‹å®ç°ç›¸å…³çš„ç±»å’Œå‡½æ•°
	- **INTERNAL HELPERS** ç”¨äºæˆ‘ä»¬åœ¨å†…éƒ¨ä½¿ç”¨çš„ç±»å’Œå‡½æ•°ã€‚

è¯¥åº“ç›®å‰åŒ…å«ä»¥ä¸‹æ¨¡å‹åœ¨Jaxã€PyTorch å’Œ Tensorflowä¸­çš„å®ç°ã€é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€è„šæœ¬ä½¿ç”¨å’Œè½¬æ¢ç¨‹åºã€‚

### å—æ”¯æŒçš„æ¨¡å‹

1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** 

	[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

2. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)**

	[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf) 

3. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** 

	[BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321)

4. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)**  

	[BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) 

5. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)**

	[BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)

6. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** 

	[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

7. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** 

	[BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/)

8. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bertgeneration)**

	[Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461)

9. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/bigbird)**

	[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)

10. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** 

	[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)

11. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)**

	[Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637)

12. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot_small)**

	[Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637)

13. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)**

	[Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499)

14. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)**

	[ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)

15. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)**

	[CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)

16. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)**

	[CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874)

17. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)**

	[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

18. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)**

	[ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496)

19. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)**

	[CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413)

20. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)**

	[CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)

21. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)**

	[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) 

22. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta_v2)**

	[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) 

23. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)**

	[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)

24. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)**

	[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)

25. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)**

	[DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)

26. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)**

	[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/master/examples/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/master/examples/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/master/examples/distillation) and a German version of DistilBERT.

27. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)**

	[Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)

28. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoderdecoder)**

	[Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461)

29. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)**

	[ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)

30. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)**

	[FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)

31. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)**

	[FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) 

32. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)**

	[Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)

33. **[GPT](https://huggingface.co/docs/transformers/model_doc/gpt)**

	[Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/)

34. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2) **

	[Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/)

35. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** 

	[kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/)

36. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)**

	[EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo)**[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** 

	[HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447)

38. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** 

	[I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321)

39. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** 

	[Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/)

40. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** 

	[LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318)

41. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** 

	[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)

42. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** 

	[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)

43. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** 

	[Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) 

44. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** 

	[Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) 

45. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** 

	[LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) 

46. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** 

	[LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490)

47. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** 

	[Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125)

48. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** 

	Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by JÃ¶rg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.

49. **[MBart](https://huggingface.co/docs/transformers/model_doc/mbart)** 

	[Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210)

50. **[MBart-50](https://huggingface.co/docs/transformers/model_doc/mbart)** 

	[Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401)

51. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron_bert)** 

	[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)

52. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** 

	[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)

53. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** 

	[MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)

54. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** 

	[mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934)

55. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** 

	[PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)

56. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** 

	[PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/)

57. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** 

	[ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063)

58. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** 

	[Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602)

59. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** 

	[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

60. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)**

	[Rethinking embedding coupling in pre-trained language models](https://arxiv.org/pdf/2010.12821.pdf)

61. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** 

	(https://arxiv.org/abs/1907.11692)

62. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** 

	(https://arxiv.org/pdf/2104.09864v1.pdf)

63. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** 

	[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203)

64. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** 

	[Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870)

65. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** 

	[Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870)

66. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** 

	[fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171)

67. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** 

	[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678)

68. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** 

	[Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438)

69. **[SqueezeBert](https://huggingface.co/docs/transformers/model_doc/squeezebert)** 

	[SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316)

70. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** 

	[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)

71. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** 

	[google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)

72. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** 

	[TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349)

73. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transformerxl)** 

	[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)

74. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** 

	[TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)

75. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** 

	[UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597)

76. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech_sat)** 

	[UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752)

77. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** 

	[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

78. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** 

	[VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557)

79. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** 

	[wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) 

80. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** 

	[Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)

81. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlmprophetnet)** 

	[ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063)

82. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlmroberta)** 

	[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)

83. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** 

	[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

84. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** 

	[Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979)

### æ”¯æŒçš„æ¡†æ¶

ä¸‹è¡¨è¡¨ç¤ºåº“ä¸­å¯¹æ¯ä¸ªæ¨¡å‹çš„å½“å‰æ”¯æŒï¼Œæ— è®ºå®ƒä»¬æ˜¯å¦å…·æœ‰Python tokenizerï¼ˆ"slow"ï¼‰ã€‚ç”±ğŸ¤— Tokenizersåº“æ”¯æŒçš„â€œfastâ€ tokenizer(æ— è®ºæ˜¯å¦æ”¯æŒ Jaxï¼ˆé€šè¿‡ Flaxï¼‰ï¼ŒPyTorch å’Œ/æˆ–TensorFlow)

| Model                       | Tokenizer slow | Tokenizer fast | PyTorch support | TensorFlow support | Flax Support |
| --------------------------- | -------------- | -------------- | --------------- | ------------------ | ------------ |
| ALBERT                      | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| BART                        | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| BEiT                        | âŒ              | âŒ              | âœ…               | âŒ                  | âœ…            |
| BERT                        | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| Bert Generation             | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| BigBird                     | âœ…              | âœ…              | âœ…               | âŒ                  | âœ…            |
| BigBirdPegasus              | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| Blenderbot                  | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| BlenderbotSmall             | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| CamemBERT                   | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| Canine                      | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| CLIP                        | âœ…              | âœ…              | âœ…               | âŒ                  | âœ…            |
| ConvBERT                    | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| CTRL                        | âœ…              | âŒ              | âœ…               | âœ…                  | âŒ            |
| DeBERTa                     | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| DeBERTa-v2                  | âœ…              | âŒ              | âœ…               | âœ…                  | âŒ            |
| DeiT                        | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| DETR                        | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| DistilBERT                  | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| DPR                         | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| ELECTRA                     | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| Encoder decoder             | âŒ              | âŒ              | âœ…               | âœ…                  | âœ…            |
| FairSeq Machine-Translation | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| FlauBERT                    | âœ…              | âŒ              | âœ…               | âœ…                  | âŒ            |
| FNet                        | âœ…              | âœ…              | âœ…               | âŒ                  | âŒ            |
| Funnel Transformer          | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| GPT Neo                     | âŒ              | âŒ              | âœ…               | âŒ                  | âœ…            |
| GPT-J                       | âŒ              | âŒ              | âœ…               | âŒ                  | âœ…            |
| Hubert                      | âŒ              | âŒ              | âœ…               | âœ…                  | âŒ            |
| I-BERT                      | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| ImageGPT                    | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| LayoutLM                    | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| LayoutLMv2                  | âœ…              | âœ…              | âœ…               | âŒ                  | âŒ            |
| LED                         | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| Longformer                  | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| LUKE                        | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| LXMERT                      | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| M2M100                      | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| Marian                      | âœ…              | âŒ              | âœ…               | âœ…                  | âœ…            |
| mBART                       | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| MegatronBert                | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| MobileBERT                  | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| MPNet                       | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| mT5                         | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| OpenAI GPT                  | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| OpenAI GPT-2                | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| Pegasus                     | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| ProphetNet                  | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| QDQBert                     | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| RAG                         | âœ…              | âŒ              | âœ…               | âœ…                  | âŒ            |
| Reformer                    | âœ…              | âœ…              | âœ…               | âŒ                  | âŒ            |
| RemBERT                     | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| RetriBERT                   | âœ…              | âœ…              | âœ…               | âŒ                  | âŒ            |
| RoBERTa                     | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| RoFormer                    | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| SegFormer                   | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| SEW                         | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| SEW-D                       | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| Speech Encoder decoder      | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| Speech2Text                 | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| Speech2Text2                | âœ…              | âŒ              | âŒ               | âŒ                  | âŒ            |
| Splinter                    | âœ…              | âœ…              | âœ…               | âŒ                  | âŒ            |
| SqueezeBERT                 | âœ…              | âœ…              | âœ…               | âŒ                  | âŒ            |
| T5                          | âœ…              | âœ…              | âœ…               | âœ…                  | âœ…            |
| TAPAS                       | âœ…              | âŒ              | âœ…               | âœ…                  | âŒ            |
| Transformer-XL              | âœ…              | âŒ              | âœ…               | âœ…                  | âŒ            |
| TrOCR                       | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| UniSpeech                   | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| UniSpeechSat                | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| Vision Encoder decoder      | âŒ              | âŒ              | âœ…               | âŒ                  | âœ…            |
| VisionTextDualEncoder       | âŒ              | âŒ              | âœ…               | âŒ                  | âœ…            |
| VisualBert                  | âŒ              | âŒ              | âœ…               | âŒ                  | âŒ            |
| ViT                         | âŒ              | âŒ              | âœ…               | âœ…                  | âœ…            |
| Wav2Vec2                    | âœ…              | âŒ              | âœ…               | âœ…                  | âœ…            |
| XLM                         | âœ…              | âŒ              | âœ…               | âœ…                  | âŒ            |
| XLM-RoBERTa                 | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
| XLMProphetNet               | âœ…              | âŒ              | âœ…               | âŒ                  | âŒ            |
| XLNet                       | âœ…              | âœ…              | âœ…               | âœ…                  | âŒ            |
