---
title: ""
author: "çŸ³æ˜Œæ–‡"
tags: [""]
description: ""
categories: [""]
keywords:  [""]
type: ""
draft: true
layout: 
data: 2021-12-07 11:06:40
lastmod: 2022-03-30 08:37:02
---

# Quick tour

å¿«é€Ÿæµè§ˆ ğŸ¤— Transformers åº“åŠŸèƒ½ã€‚è¯¥åº“å¯ä»¥ä¸‹è½½é€‚ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ ï¼ˆNLUï¼‰ å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆ ï¼ˆNLGï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œ

## ä½¿ç”¨ pipeline å¼€å§‹æ‰§è¡Œä»»åŠ¡

åœ¨ç»™å®šä»»åŠ¡ä¸Šï¼Œå¯ä»¥è½»æ¾åˆ©ç”¨  [pipelineï¼ˆï¼‰](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) API ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚

ğŸ¤— Transformers æä¾›ä»¥ä¸‹å¯ä»¥ç›´æ¥ä½¿ç”¨çš„åŠŸèƒ½ï¼š

**æ–‡æœ¬**

- Sentiment analysis: å¯¹æ–‡æœ¬æƒ…æ„Ÿæ­£è´Ÿé¢è¿›è¡Œåˆ†ç±»ã€‚
- Text generation (in English): ç»™å®šè¾“å…¥ç”Ÿæˆæ–‡æœ¬ã€‚
- Name entity recognition (NER): åœ¨è¾“å…¥å¥å­ä¸­ï¼Œç”¨å®é™…å¯¹è±¡ï¼ˆäººã€æ—¥æœŸã€åœ°ç‚¹ç­‰ï¼‰æ ‡è®°æ¯ä¸ªå•è¯ã€‚
- Question answering: ä¸ºæ¨¡å‹æä¾›ä¸€äº›ä¸Šä¸‹æ–‡å’Œé—®é¢˜ï¼Œä»ä¸Šä¸‹æ–‡ä¸­æå–ç­”æ¡ˆã€‚
- Filling masked text: ç»™å®šåŒ…å«å±è”½è¯çš„æ–‡æœ¬ï¼ˆæ›¿æ¢ä¸º `[MASK]`ï¼‰ï¼Œå¡«å……ç©ºç™½ã€‚
- Summarization: ç”Ÿæˆé•¿æ–‡æœ¬æˆ–æ–‡æ¡£çš„æ‘˜è¦ã€‚
- Translation: å°†æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€‚
- Feature extraction: è¿”å›æ–‡æœ¬çš„å¼ é‡è¡¨ç¤ºã€‚

**å›¾ç‰‡**ï¼š

- Image classificatioï¼šå¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚
- Image segmentationï¼šå¯¹å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»ã€‚
- Object detectionï¼šæ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡ã€‚

**éŸ³é¢‘**ï¼š

- Audio classificationï¼šä¸ºç»™å®šçš„éŸ³é¢‘ç‰‡æ®µåˆ†é…æ ‡ç­¾ã€‚
- Automatic speech recognition (ASR)ï¼šå°†éŸ³é¢‘æ•°æ®è½¬å½•ä¸ºæ–‡æœ¬ã€‚

> æœ‰å…³ [pipelineï¼ˆï¼‰](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline) å’Œç›¸å…³ä»»åŠ¡çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤å¤„](https://huggingface.co/docs/transformers/main_classes/pipelines)çš„æ–‡æ¡£ã€‚

### Pipeline ä½¿ç”¨æ–¹æ³•

åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œä½ å°†ä½¿ç”¨ [pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline) è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚

å¦‚æœå°šæœªå®‰è£…ä»¥ä¸‹ä¾èµ–é¡¹ï¼Œè¯·å®‰è£…ä»¥ä¸‹ä¾èµ–é¡¹ï¼š

```shell
pip install torch
```

å¯¼å…¥ [pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline) å¹¶æŒ‡å®šè¦å®Œæˆçš„ä»»åŠ¡ï¼š

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
```

pipelineä¼šä¸‹è½½å¹¶ç¼“å­˜é»˜è®¤çš„ [é¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) å’Œ tokenizerï¼Œç°åœ¨æ‚¨å°±å¯ä»¥åœ¨ç›®æ ‡æ–‡æœ¬ä¸Šä½¿ç”¨ï¼š`classifier` å¯¹å…¶è¿›è¡Œæƒ…ç»ªåˆ†æï¼š

```python
classifier("We are very happy to show you the ğŸ¤— Transformers library.")
# [{'label': 'POSITIVE', 'score': 0.9998}]
```

å¯¹äºå¤šä¸ªå¥å­ï¼Œå°†å¥å­åˆ—è¡¨ä¼ é€’ç»™[pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline)ï¼Œè¯¥pipelineä¼šè¿”å›ä¸€ä¸ªå¯¹åº”çš„å­—å…¸åˆ—è¡¨ï¼š

```python
results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
for result in results:
	print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
# label: POSITIVE, with score: 0.9998
# label: NEGATIVE, with score: 0.5309
```

 [pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline) è¿˜å¯ä»¥å¾ªç¯è®¿é—®æ•´ä¸ªæ•°æ®é›†ã€‚é¦–å…ˆéœ€è¦å®‰è£… [ğŸ¤— Datasets](https://huggingface.co/docs/datasets/) åº“ï¼š

```shell
pip install datasets
```

æ ¹æ®è¦è¦å®ç°çš„ä»»åŠ¡å’Œè¦ä½¿ç”¨çš„æ¨¡å‹åˆ›å»º[pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline) ç„¶åå°†å‚æ•°è®¾ç½®ä¸ºå°†tensoræ”¾ç½®åœ¨ CUDA ä¸Šï¼š`device` `0`

```python
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½è¦å¾ªç¯è®¿é—®çš„ğŸ¤— Datasets ï¼ˆæ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— Datasets [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/datasets/quickstart.html)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒåŠ è½½ [SUPERB æ•°æ®é›†](https://huggingface.co/datasets/superb)ï¼š

```python
import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test")
```

å¯ä»¥ä¼ é€’æ•´ä¸ªdataset pipelineï¼š

```python
files = dataset["file"]
speech_recognizer(files[:4])
# [{'text': 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE'},
# {'text': 'STUFFERED INTO YOU HIS BELLY COUNSELLED HIM'},
# {'text': 'AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS'},
# {'text': 'HO BERTIE ANY GOOD IN YOUR MIND'}]
```

å¯¹äºè¾“å…¥è¾ƒå¤§çš„è¾ƒå¤§æ•°æ®é›†ï¼ˆå¦‚è¯­éŸ³æˆ–è§†è§‰ï¼‰ï¼Œå°†éœ€è¦ä¼ é€’generatorï¼Œè€Œä¸æ˜¯åœ¨å†…å­˜ä¸­åŠ è½½æ‰€æœ‰è¾“å…¥ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[pipeline æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/pipelines)ã€‚

### åœ¨ç®¡é“ä¸­ä½¿ç”¨å…¶ä»–æ¨¡å‹å’Œåˆ†è¯å™¨

[pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline) å¯ä»¥è°ƒç”¨ [Model Hub](https://huggingface.co/models)ä¸­çš„ä»»ä½•æ¨¡å‹ï¼Œä»è€Œå¯ä»¥è½»æ¾å°†[pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline) åº”ç”¨åˆ°å…¶ä»–å®ä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæƒ³è¦ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ³•è¯­æ–‡æœ¬çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨Model Hubä¸Šçš„tagsæ¥ç­›é€‰å¾—åˆ°åˆé€‚çš„æ¨¡å‹ã€‚æœ€é¡¶éƒ¨çš„ç­›é€‰ç»“æœè¿”å›çš„æ˜¯é’ˆå¯¹æƒ…ç»ªåˆ†æè¿›è¡Œå¾®è°ƒçš„å¤šè¯­è¨€[BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) ã€‚

```python
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

ä½¿ç”¨ [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) å’Œ[â€˜AutoTokenizerâ€™]åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹åŠå…¶å…³è”çš„tokenizerï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

ç„¶åï¼Œå¯ä»¥åœ¨[pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline)ä¸­æŒ‡å®šæ¨¡å‹å’Œtokenizer, å¹¶å°†`classifier`åº”ç”¨äºç›®æ ‡æ–‡æœ¬ï¼š

```python
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")
# [{'label': '5 stars', 'score': 0.7273}]
```

Iå¦‚æœæ‰¾ä¸åˆ°é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹çš„æ¨¡å‹ï¼Œåˆ™éœ€è¦æ ¹æ®æ•°æ®å¾®è°ƒ(fine-tune)é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…·ä½“æƒ…å†µå¯ä»¥æŸ¥çœ‹ [fine-tuning æ•™ç¨‹](https://huggingface.co/docs/transformers/training) ã€‚æœ€åï¼Œåœ¨å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåï¼Œè¯·è€ƒè™‘åœ¨Model Hubä¸ç¤¾åŒºä¸Šå…±äº«å®ƒ([åˆ†äº«æ•™ç¨‹](https://huggingface.co/docs/transformers/model_sharing)) ! ğŸ¤—

## AutoClass

[AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) å’Œ [AutoTokenizer](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer) ç±»ååŒå·¥ä½œï¼Œä¸º [pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline)æä¾›æ”¯æŒã€‚ [AutoClass](https://huggingface.co/docs/transformers/model_doc/auto)  æ˜¯ä¸€ç§å¿«æ·æ–¹å¼ï¼Œå¯æ ¹æ®é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„ï¼Œè‡ªåŠ¨æ£€ç´¢å…¶ä½“ç³»ç»“æ„ã€‚æ‚¨å› æ­¤éœ€è¦é€‰æ‹©é€‚åˆçš„ä»»åŠ¡ï¼Œå¹¶å°†å®ƒä¸[AutoTokenizer](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer)ç›¸å…³è”ã€‚`AutoClass`

å›åˆ°ç¤ºä¾‹ï¼Œçœ‹çœ‹å¦‚ä½•ä½¿ç”¨`AutoClass`æ¥é‡ç°[pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline)çš„ç»“æœã€‚

### AutoTokenizer

tokenizerè´Ÿè´£å°†æ–‡æœ¬é¢„å¤„ç†ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚é¦–å…ˆï¼Œtokenizerä¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºç§°ä¸º*tokens*çš„å•è¯ã€‚æœ‰å¤šä¸ªè§„åˆ™ç”¨äºç®¡ç†tokenizationè¿‡ç¨‹,ï¼ŒåŒ…æ‹¬å¦‚ä½•æ‹†åˆ†å•è¯ä»¥åŠåœ¨å“ªä¸ªlevel (learn more about tokenization [here](https://huggingface.co/docs/transformers/tokenizer_summary))æ‹†åˆ†ã€‚é‡è¦çš„æ˜¯ï¼Œéœ€è¦ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹åç§°å®ä¾‹åŒ–tokenizer ï¼Œä»¥ç¡®ä¿ä½¿ç”¨çš„æ˜¯æ¨¡å‹é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„ç›¸åŒçš„tokenizationè§„åˆ™ã€‚

ä½¿ç”¨[AutoTokenizer](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer)åŠ è½½tokenizeï¼š

```python
from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

æ¥ä¸‹æ¥ï¼Œtokenizerå°†tokensè½¬æ¢ä¸ºæ•°å­—ï¼Œä»¥ä¾¿æ„é€ tensorä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œè¿™ç§°ä¸ºæ¨¡å‹çš„*vocabulary*ã€‚

å°†æ–‡æœ¬ä¼ é€’ç»™tokenizerï¼š

```python
encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(encoding)
# {'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

tokenizerå°†è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ï¼š

- [input_ids](https://huggingface.co/docs/transformers/glossary#input-ids): tokensæ•°å­—è¡¨ç¤ºã€‚
- [atttention_mask](https://huggingface.co/docs/transformers/.glossary#attention-mask): æŒ‡ç¤ºåº”å…³æ³¨å“ªäº›tokensã€‚

åƒ[pipeline()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline)ä¸€æ ·ï¼Œtokenizerå¯ä»¥æ¥å—è¾“å…¥åˆ—è¡¨ï¼Œæ­¤å¤–ï¼Œå®ƒå¯ä»¥å¡«å……å’Œæˆªæ–­æ–‡æœ¬ä»¥è¿”å›é•¿åº¦ç»Ÿä¸€çš„batchï¼š

```python
pt_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],padding=True,truncation=True,max_length=512,return_tensors="pt",)
```

æŸ¥çœ‹ [preprocessing](https://huggingface.co/docs/transformers/preprocessing) æ•™ç¨‹ï¼Œäº†è§£æœ‰å…³tokenizationçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

### AutoModel

ğŸ¤— Transformers æä¾›äº†ä¸€ç§ç®€å•è€Œç»Ÿä¸€çš„æ–¹å¼æ¥åŠ è½½é¢„è®­ç»ƒçš„å®ä¾‹ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åƒåŠ è½½[AutoTokenizer](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer)ä¸€æ ·åŠ [AutoModel](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModel) ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯éœ€è¦ä¸ºå½“å‰ä»»åŠ¡é€‰æ‹©é€‚åˆçš„[AutoModel](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModel) ã€‚æœ¬æ–‡æ­£åœ¨åšText æˆ–sequenceåˆ†ç±»ï¼Œå› æ­¤è¯·åŠ è½½[AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification)ã€‚

```python
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

> æ›´å¤šä¿¡æ¯å‚é˜… [task summary](https://huggingface.co/docs/transformers/task_summary) ï¼Œäº†è§£å“ªä¸€ä¸ª [AutoModel](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModel) é€‚ç”¨äºå“ªä¸€ä¸ªä»»åŠ¡ã€‚

ç°åœ¨ï¼Œå¯ä»¥å°†é¢„å¤„ç†çš„è¾“å…¥batchç›´æ¥ä¼ é€’ç»™æ¨¡å‹ï¼ˆæ³¨æ„ï¼šä½¿ç”¨PyTorch æ¨¡å‹æ—¶ï¼Œéœ€è¦é€šè¿‡æ·»åŠ `**`æ¥è§£å‹ç¼©å­—å…¸ï¼‰ï¼š

```python
pt_outputs = pt_model(**pt_batch)
```

æ¨¡å‹è¾“å‡ºçš„final activationsï¼Œåœ¨`logits` å±æ€§ä¸­ï¼Œå› æ­¤å¯¹`logits`åº”ç”¨ softmax å‡½æ•°å°±å¯ä»¥è·å¾—åˆ†ç±»æ¦‚ç‡ï¼š

```python
from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)
# tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```

> æ‰€æœ‰çš„ğŸ¤— Transformers æ¨¡å‹ (PyTorch or TensorFlow) åœ¨æœ€ç»ˆæ¿€æ´»å‡½æ•°ï¼ˆå¦‚softmaxï¼‰ä¹‹å‰è¾“å‡ºtensorsï¼Œbecauseæœ€ç»ˆçš„æ¿€æ´»å‡½æ•°é€šå¸¸ä¸lossèåˆã€‚

ğŸ¤— Transformers æ¨¡å‹æ˜¯æ ‡å‡†çš„ [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) æˆ–[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨é€šå¸¸çš„è®­ç»ƒä¸­ä½¿ç”¨å®ƒä»¬ã€‚ä½†æ˜¯ï¼Œä¸ºäº†ä½¿äº‹æƒ…å˜å¾—æ›´å®¹æ˜“ï¼ŒğŸ¤— Transformersä¸ºPyTorchæä¾›äº†ä¸€ä¸ª[Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer)ç±»ï¼Œè¯¥ç±»æ·»åŠ äº†åˆ†å¸ƒå¼è®­ç»ƒ(distributed training)ï¼Œæ··åˆç²¾åº¦(mixed precision)ç­‰åŠŸèƒ½ã€‚å¯¹äºTensorFlowï¼Œå¯ä»¥ä½¿ç”¨[Keras](https://keras.io/)çš„`fit`æ–¹æ³•ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥æŸ¥é˜… [training tutorial](https://huggingface.co/docs/transformers/training) ã€‚

> ğŸ¤— Transformersæ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®ç±»(dataclasse)ï¼Œå› æ­¤å®ƒä»¬çš„å±æ€§ä¼šåœ¨IDEä¸­è‡ªåŠ¨ç”Ÿæˆã€‚å¹¶ä¸”æ¨¡å‹è¾“å‡ºåŒtuple æˆ–dictionaryç±»ä¼¼ï¼Œå¯ä»¥ä½¿ç”¨æ•´æ•°, åˆ‡ç‰‡æˆ–å­—ç¬¦ä¸²è¿›è¡Œç´¢å¼•ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¸º `None`çš„å±æ€§ä¼šè¢«å¿½ç•¥ã€‚

### Save a model

å¾®è°ƒåçš„æ¨¡å‹å’Œtokenizerï¼Œå¯ä»¥ä½¿ç”¨[PreTrainedModel.save_pretrained()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)ä¿å­˜ï¼š

```python
pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)
```

å½“æ‚¨æƒ³è¦å†æ¬¡ä½¿ç”¨è¯¥æ¨¡å‹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨[PreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)é‡æ–°åŠ è½½ï¼š

```python
pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```

ğŸ¤— Transformers è¿˜æœ‰ä¸€ä¸ªå¾ˆé…·çš„åŠŸèƒ½ï¼Œå¯ä»¥é€šè¿‡ç»™å®šçš„å‚æ•° `from_pt`æˆ–`from_tf`å°†ä¿å­˜çš„æ¨¡å‹é‡æ–°åŠ è½½ä¸ºPyTorchæˆ–TensorFlowæ¨¡å‹ï¼Œå¹¶ä¸”æ”¯æŒæ¡†æ¶è½¬æ¢ã€‚

```python
from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
